<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[读书旁注——第2章Java内存区域与内存溢出异常《深入理解Java虚拟机：JVM高级特性与最佳实践（第3版）》]]></title>
    <url>%2F2020%2F01%2F06%2F%E7%AC%AC2%E7%AB%A0-Java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E4%B8%8E%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%E5%BC%82%E5%B8%B8%E3%80%8A%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA%EF%BC%9AJVM%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%E4%B8%8E%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%88%E7%AC%AC3%E7%89%88%EF%BC%89%E3%80%8B%2F</url>
    <content type="text"><![CDATA[很抱歉的是：由于会显示原书全文，因此受制于以下版权，不对外开放，仅博主可见!在豆瓣阅读书店查看：《深入理解Java虚拟机：JVM高级特性与最佳实践（第3版）》 — 周志明，本作品由华章数媒授权豆瓣阅读全球范围内电子版制作与发行。© 版权所有，侵权必究。 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]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Java，JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Paper Google Bigtable 翻译与总结]]></title>
    <url>%2F2019%2F12%2F31%2FPaper-Google-Bigtable_translation-and-summary_online%2F</url>
    <content type="text"><![CDATA[前言 第一部分主要是论文的翻译与旁注：按照论文原文结构一步步翻译 第二部分主要是BigTable思想总结：BigTable论文相比GFS、MapReduce两篇复杂，行文并不流畅（可能本渣渣太弱），文中甚至没有总体结构说明和一些难点解释（例如：BigTable首创的并且在后来众多出名的开源组件中常用的SSTable文件格式：LSM 都没有详细说明），因此在总结处弥补这方面的说明。 论文翻译与旁注AbstractBigtable is a distributed storage system for managing structured data, which is designed to scale to a very large scale: petabytes of data in thousands of commercial servers. Many Google projects store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different requirements on Bigtable in terms of data size (from URL to web page to satellite imagery) and latency requirements (from back-end batch processing to real-time data services). Despite the varied requirements, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this article, we describe the simple data model provided by Bigtable, which provides customers with dynamic control over data layout and format, and describes the design and implementation of Bigtable. Bigtable是用于管理结构化数据的分布式存储系统，该系统旨在扩展到非常大的规模：数千个商用服务器中的PB级数据。 Google的许多项目都将数据存储在Bigtable中，包括网络索引，Google Earth和Google Finance。 这些应用程序在数据大小（从URL到网页到卫星图像）和延迟要求（从后端批量处理到实时数据服务）方面都对Bigtable提出了截然不同的要求。尽管需求千差万别，Bigtable已成功为所有这些Google产品提供了一种灵活的高性能解决方案。 在本文中，我们描述了Bigtable提供的简单数据模型，该模型为客户提供了对数据布局和格式的动态控制，并描述了Bigtable的设计和实现。 1 IntroductionIntroduction Over the last two and a half years we have designed, implemented, and deployed a distributed storage system for managing structured data at Google called Bigtable. Bigtable is designed to reliably scale to petabytes of data and thousands of machines. Bigtable has achieved several goals: wide applicability, scalability, high performance, and high availability. Bigtable is used by more than sixty Google products and projects, including Google Analytics, Google Finance, Orkut, Personalized Search, Writely, and Google Earth. These products use Bigtable for a variety of demanding workloads, which range from throughput-oriented batch-processing jobs to latency-sensitive serving of data to end users. The Bigtable clusters used by these products span a wide range of configurations, from a handful to thousands of servers, and store up to several hundred terabytes of data. 简介在过去的两年半中，我们在Google上设计，实施和部署了一个分布式存储系统来管理结构化数据，称为Bigtable。 Bigtable旨在可靠地扩展到PB级数据和数千台计算机。 Bigtable实现了多个目标：广泛的适用性，可伸缩性，高性能和高可用性。 Bigtable被60多个Google产品和项目所使用，包括Google Analytics（分析），Google Finance，Orkut，个性化搜索，Writely和Google Earth。 这些产品将Bigtable用于各种要求高的工作负载，从面向吞吐量的批处理作业到对延迟敏感的终端用户所享受的数据服务。 这些产品使用的Bigtable集群涵盖了多种配置，从少量服务器到数千个服务器，最多可存储数百TB的数据。 In many ways, Bigtable resembles a database: it shares many implementation strategies with databases. Parallel databases[14] and main-memory databases[13] have achieved scalability and high performance, but Bigtable provides a different interface than such systems. Bigtable does not support a full relational data model; instead, it provides clients with a simple data model that supports dynamic control over data layout and format, and allows clients to reason about the locality properties of the data represented in the underlying storage. Data is indexed using row and column names that can be arbitrary strings. Bigtable also treats data as uninterpreted strings, although clients often serialize various forms of structured and semi-structured data into these strings. Clients can control the locality of their data through careful choices in their schemas. Finally, Bigtable schema parameters let clients dynamically control whether to serve data out of memory or from disk. 在许多方面，Bigtable类似于数据库：它与数据库共享许多实现策略。 并行数据库[14]和主内存数据库[13]已经实现了可伸缩性和高性能，但是Bigtable提供了与此类系统不同的接口。 Bigtable不支持完整的关系数据模型； 相反，它为客户端提供了一个简单的数据模型，该模型支持对数据布局和格式的动态控制，并允许客户端推理存储1在底层的数据的位置属性16。 可以使用任意字符串的行和列名称为数据建立索引。 尽管客户端经常将各种形式的结构化和半结构化数据序列化为这些字符串，但Bigtable还将数据视为未解析2的字符串。客户端可以通过在模式中进行仔细选择来控制其数据的位置。 最后，Bigtable模式参数可让客户端动态控制是从磁盘还是从内存获得数据3。 Section 2 describes the data model in more detail, and Section 3 provides an overview of the client API. Section 4 briefly describes the underlying Google infrastructure on which Bigtable depends. Section 5 describes the fundamentals of the Bigtable implementation, and Section 6 describes some of the refinements that we made to improve Bigtable’s performance. Section 7 provides measurements of Bigtable’s performance. We describe several examples of how Bigtable is used at Google in Section 8, and discuss some lessons we learned in designing and supporting Bigtable in Section 9. Finally, Section 10 describes related work, and Section 11 presents our conclusions. 第2节将更详细地描述数据模型，第3节将概述客户端API，第4节简要介绍了Bigtable所依赖的基础Google架构。 第5节介绍了Bigtable实现的基础知识，第6节介绍了我们为提高Bigtable的性能所做的一些改进。 第7节提供了Bigtable性能的衡量标准。 在第8节中，我们描述了如何在Google中使用Bigtable的几个示例，并在第9节中，讨论了我们在设计和支持Bigtable方面学到的一些教训。最后，第10节描述了相关工作，第11节介绍了我们的结论。 2 Data ModelA Bigtable is a sparse, distributed, persistent multidimensional sorted map. The map is indexed by a row key, column key, and a timestamp; each value in the map is an uninterpreted array of bytes. 一个BigTable是一个稀疏的、分布的、永久的多维有序的映射表（map）。我们采用行键（row key）、列键（column key）和时间戳（timestamp）对映射表（map）进行索引。映射表（map）中的每个值都是未经解析的字节数组。(row:string, column string, time:int64)→stringWe settled on this data model after examining a variety of potential uses of a Bigtable-like system. As one concrete example that drove some of our design decisions, suppose we want to keep a copy of a large collection of web pages and related information that could be used by many different projects; let us call this particular table the Webtable. In Webtable, we would use URLs as row keys, various aspects of web pages as column names, and store the contents of the web pages in the contents: column under the timestamps when they were fetched, as illustrated in Figure 1.在研究了类似Bigtable的系统的各种潜在用途之后，我们选择了此数据模型。 作为推动我们某些设计决策的一个具体示例，假设我们想要保留大量网页和相关信息的副本，这些副本可以由许多不同的项目使用。 让我们将此特定表称为Webtable。 在Webtable中，我们将使用URL作为行键，将网页的各个方面用作列名，并将网页的内容存储在 contents: 列，这些列在时间戳（获取时的时间）底下17，如图1所示。图1 存储了网页数据的Webtable的一个片段行名称是反转的URL，contents列家族包含了网页内容，anchor列家族包含了任何引用这个页面的anchor文本。CNN的主页被Sports Illustrated和MY-look主页同时引用，因此，我们的行包含了名称为 ”anchor:cnnsi.com” 和 ”anchor:my.look.ca” 的列。每个anchor单元格都只有一个版本，contents列有三个版本，分别对应于时间戳t3，t5和t6。#### RowsThe row keys in a table are arbitrary strings (currently up to 64KB in size, although 10-100 bytes is a typical size for most of our users). Every read or write of data under a single row key is atomic (regardless of the number of different columns being read or written in the row), a design decision that makes it easier for clients to reason about the system’s behavior in the presence of concurrent updates to the same row.表中的行键是任意字符串（当前大小最大为64KB，尽管对于大多数用户而言，典型大小是10-100字节）。 单个行键下的每次数据读取或写入都是原子性的（无论该行中读取或写入的不同列的数量如何），该设计决策使客户端在出现并发更新到同一行时更容易推断系统行为。Bigtable maintains data in lexicographic order by row key. The row range for a table is dynamically partitioned. Each row range is called a tablet, which is the unit of distribution and load balancing. As a result, reads of short row ranges are efficient and typically require communication with only a small number of machines. Clients can exploit this property by selecting their row keys so that they get good locality for their data accesses. For example, in Webtable, pages in the same domain are grouped together into contiguous rows by reversing the hostname components of the URLs. For example, we store data for maps.google.com/index.html under the key com.google.maps/index.html. Storing pages from the same domain near each other makes some host and domain analyses more efficient.Bigtable按行键的字典顺序维护数据。表的行区间是动态分区的。每个行区间称为一个Tablet，它是分配和负载平衡的单位。结果，对行的小范围读取（reads of short row ranges，这里short修饰的名词是 ranges 还是 row ，最终根据下文的例子进行反推的）是高效的并且通常仅需要与少量机器通信。客户端可以通过选择行键来利用此属性，以便他们可以很好地进行数据访问。例如，在Webtable中，通过反转URL的主机名部分，可以将同一域中的页面分组为连续的行。例如我们将数据maps.google.com/index.html存储在键com.google.maps/index.html下。 将同一域中的页面彼此靠近存储可以使某些主机和域分析更加高效。#### Column FamiliesColumn keys are grouped into sets called column families, which form the basic unit of access control. All data stored in a column family is usually of the same type (we compress data in the same column family together). A column family must be created before data can be stored under any column key in that family; after a family has been created, any column key within the family can be used. It is our intent that the number of distinct column families in a table be small (in the hundreds at most), and that families rarely change during operation. In contrast, a table may have an unbounded number of columns. 列键被分组成称为列族的集合，这些集合构成访问控制的基本单元。 列族中存储的所有数据通常都是同一类型（我们将同一列族中的数据压缩在一起）。 必须先创建一个列族，然后才能将数据存储在该族中的任何列键下。 创建族后，可以使用族中的任何列键。 我们的目的是使表中不同的列族的数量少（最多数百个），并且在操作过程中族很少改变。 相反，表可能具有无限数量的列。A column key is named using the following syntax: family:qualifier. Column family names must be printable, but qualifiers may be arbitrary strings. An example column family for the Webtable is language, which stores the language in which a web page was written. We use only one column key in the language family, and it stores each web page’s language ID. Another useful column family for this table is anchor; each column key in this family represents a single anchor, as shown in Figure 1. The qualifier is the name of the referring site; the cell contents is the link text.列键使用以下语法命名：family:qualifier。列族（ column family）名称必须是可打印的，但限定词（qualifier）可以是任意字符串。 Webtable的一个示例列族是language，它存储编写网页所用的语言。我们在语言族中仅使用一个列键，并且它存储每个网页的语言ID。此表的另一个有用的列族是锚； 该族中的每个列键都代表一个锚，如图1所示。限定符是引用站点的名称。单元格内容是链接文本。Access control and both disk and memory accounting are performed at the column-family level. In our Webtable example, these controls allow us to manage several different types of applications: some that add new base data, some that read the base data and create derived column families, and some that are only allowed to view existing data (and possibly not even to view all of the existing families for privacy reasons).访问控制以及磁盘和内存统计4 均在列族层次执行。 在我们的Webtable示例中，这些控制（权限）使我们能够管理几种不同类型的应用程序：一些应用程序是添加新的基本数据，一些应用程序是读取基本数据并创建派生的列族，某些应用程序仅被许可查看现有数据（出于隐私原因，甚至可能不允许查看所有现有列族）。#### TimestampsEach cell in a Bigtable can contain multiple versions of the same data; these versions are indexed by timestamp. Bigtable timestamps are 64-bit integers. They can be assigned by Bigtable, in which case they represent “real time” in microseconds, or be explicitly assigned by client applications. Applications that need to avoid collisions must generate unique timestamps themselves. Different versions of a cell are stored in decreasing timestamp order, so that the most recent versions can be read first. Bigtable中的每个单元格可以包含同一数据的多个版本；这些版本通过时间戳索引。 Bigtable时间戳是64位整数。它们可以由Bigtable分配，在这种情况下，它们以微秒为单位表示“真实时间”，也可以由客户端应用程序明确分配。需要避免冲突的应用程序必须自己生成唯一的时间戳。单元格的不同版本以时间戳的降序5存储，因此可以首先读取最新版本。To make the management of versioned data less onerous, we support two per-column-family settings that tell Bigtable to garbage-collect cell versions automatically. The client can specify either that only the last n versions of a cell be kept, or that only new-enough versions be kept (e.g., only keep values that were written in the last seven days).为了减少版本化数据的管理工作，我们支持每个列族的两个设置，这些设置告诉Bigtable自动垃圾回收单元格版本。 客户端可以指定仅保留单元格的最后n个版本，或者仅保留足够新的版本（例如，仅保留最近7天写入的值）。In our Webtable example, we set the timestamps of the crawled pages stored in the contents: column to the times at which these page versions were actually crawled. The garbage-collection mechanism described above lets us keep only the most recent three versions of every page.在我们的Webtable示例中，我们将 content: 列中存储的爬虫网页的时间戳设置为实际爬虫这些页面版本的时间。 上述的垃圾收集机制使我们仅保留每个页面的最新三个版本。### 3 APIThe Bigtable API provides functions for creating and deleting tables and column families. It also provides functions for changing cluster, table, and column family metadata, such as access control rights.Bigtable API提供了用于创建和删除表和列族的功能。它还提供了用于更改集群，表和列族元数据的功能，例如访问控制权限。12345678// Open the tableTable *T = OpenOrDie("/bigtable/web/webtable");// Write a new anchor and delete an old anchorRowMutation r1(T, "com.cnn.www");r1.Set("anchor:www.c-span.org", "CNN");r1.Delete("anchor:www.abc.com");Operation op;Apply(&amp;op, &amp;r1);图 2 写入到BigtableClient applications can write or delete values in Bigtable, look up values from individual rows, or iterate over a subset of the data in a table. Figure 2 shows C++ code that uses a RowMutation abstraction to perform a series of updates. (Irrelevant details were elided to keep the example short.) The call to Apply performs an atomic mutation to the Webtable: it adds one anchor to www.cnn.com and deletes a different anchor.客户端应用程序可以在Bigtable中写入或删除值，可以从各个行中查找值，也可以遍历表中的数据子集。图2显示了使用RowMutation抽象（对象）6来执行一系列更新的 C++ 代码。（省略了详细信息，以使示例简短）对Apply的调用对Webtable进行了原子修改：它将一个锚点添加到 www.cnn.com 并删除另一个锚点。123456789101112Scanner scanner(T);ScanStream *stream;stream = scanner.FetchColumnFamily("anchor");stream-&gt;SetReturnAllVersions();scanner.Lookup("com.cnn.www");for (; !stream-&gt;Done(); stream-&gt;Next()) &#123; printf("%s %s %lld %s\n", scanner.RowName(), stream-&gt;ColumnName(), stream-&gt;MicroTimestamp(), stream-&gt;Value());&#125;图3: 从Bigtable读取数据Figure 3 shows C++ code that uses a Scanner abstraction to iterate over all anchors in a particular row. Clients can iterate over multiple column families, and there are several mechanisms for limiting the rows, columns, and timestamps produced by a scan. For example, we could restrict the scan above to only produce anchors whose columns match the regular expression anchor:*.cnn.com, or to only produce anchors whose timestamps fall within ten days of the current time.图3显示了使用Scanner抽象（对象） 7 对特定行中的所有锚点进行迭代的C ++代码。客户端可以迭代多个列族，并且有几种机制可以限制扫描产生的行，列和时间戳。例如，我们可以将上面的扫描限制为仅生成其列与正则表达式 anchor:*.cnn.com 匹配的锚，或者仅生成其时间戳在当前时间的十天内之内的锚。Bigtable supports several other features that allow the user to manipulate data in more complex ways. First, Bigtable supports single-row transactions, which can be used to perform atomic read-modify-write sequences on data stored under a single row key. Bigtable does not currently support general transactions across row keys, although it provides an interface for batching writes across row keys at the clients. Second, Bigtable allows cells to be used as integer counters. Finally, Bigtable supports the execution of client-supplied scripts in the address spaces of the servers. The scripts are written in a language developed at Google for processing data called Sawzall[28]. At the moment, our Sawzall-based API does not allow client scripts to write back into Bigtable, but it does allow various forms of data transformation, filtering based on arbitrary expressions, and summarization via a variety of operators.Bigtable支持其他几种功能，这些功能允许用户以更复杂的方式操作数据。 首先，Bigtable支持单行事务（single-row transaction），该事务可用于对存储在单个行键下的数据执行原子的 “读-修改-写”（read-modify-write） 序列。 Bigtable目前不支持跨行键的常规事务，尽管它提供了用于在客户端跨行键批处理写入的接口。 其次，Bigtable允许将单元格18用作整数计数器。 最后，Bigtable支持在服务器的地址空间中执行客户端提供的脚本。 这些脚本是用Google开发的一种用于处理数据的语言（称为Sawzall[28]）编写的。 目前，我们基于Sawzall的API不允许客户端脚本写回到Bigtable，但允许多种形式的数据转换，基于任意表达式的过滤以及通过各种运算符的汇总。Bigtable can be used with MapReduce[12], a framework for running large-scale parallel computations developed at Google. We have written a set of wrappers that allow a Bigtable to be used both as an input source and as an output target for MapReduce jobs.Bigtable可与MapReduce[12]结合使用，MapReduce是一种由Google开发的用于运行大规模并行计算的框架。 我们编写了一组包装器（wrappers），这些包装器允许Bigtable用作MapReduce作业的输入源和输出目标。### 4 Building BlocksBigtable is built on several other pieces of Google infrastructure. Bigtable uses the distributed Google File System (GFS) [17] to store log and data files. A Bigtable cluster typically operates in a shared pool of machines that run a wide variety of other distributed applications, and Bigtable processes often share the same machines with processes from other applications. Bigtable depends on a cluster management system for scheduling jobs, managing resources on shared machines, dealing with machine failures, and monitoring machine status.Bigtable建立在Google其他几个基础架构之上。 Bigtable使用分布式Google文件系统（GFS）[17]存储日志和数据文件。 Bigtable集群通常运行在与多种其他分布式应用程序共享的服务器池10中，并且Bigtable进程通常与其他应用程序的进程共享同一台计算机。 Bigtable依靠集群管理系统来调度作业、来管理共享计算机上的资源、来处理计算机故障以及监视计算机状态。The Google SSTable file format is used internally to store Bigtable data. An SSTable provides a persistent, ordered immutable map from keys to values, where both keys and values are arbitrary byte strings. Operations are provided to look up the value associated with a specified key, and to iterate over all key/value pairs in a specified key range. Internally, each SSTable contains a sequence of blocks (typically each block is 64KB in size, but this is configurable). A block index (stored at the end of the SSTable) is used to locate blocks; the index is loaded into memory when the SSTable is opened. A lookup can be performed with a single disk seek: we first find the appropriate block by performing a binary search in the in-memory index, and then reading the appropriate block from disk. Optionally, an SSTable can be completely mapped into memory, which allows us to perform lookups and scans without touching disk.Google SSTable文件格式在内部用于存储Bigtable数据。 SSTable提供了从键到值都可以持久化、有序的、不可变的映射表（map），其中键和值都是任意字节字符串。提供操作以查找与指定键相关联的值，并遍历指定键范围内的所有键/值对。在内部，每个SSTable包含一系列块（通常每个块的大小为64KB，但这是可配置的）。块的索引（存储在SSTable的末尾）用于定位块。当打开SSTable时，索引将加载到内存中。可以使用单次磁盘寻址（ disk seek）执行一次查找：我们首先对内存中的索引执行二分搜索来找到对应的块索引11，然后从磁盘读取相应8的块。可选项是可以将一个SSTable全部映射到内存中，这使我们无需与磁盘进行io12即可执行查找和扫描。Bigtable relies on a highly-available and persistent distributed lock service called Chubby [8]. A Chubby service consists of five active replicas, one of which is elected to be the master and actively serve requests. The service is live when a majority of the replicas are running and can communicate with each other. Chubby uses the Paxos algorithm [9][23] to keep its replicas consistent in the face of failure. Chubby provides a namespace that consists of directories and small files. Each directory or file can be used as a lock, and reads and writes to a file are atomic. The Chubby client library provides consistent caching of Chubby files. Each Chubby client maintains a session with a Chubby service. A client’s session expires if it is unable to renew its session lease within the lease expiration time. When a client’s session expires, it loses any locks and open handles. Chubby clients can also register callbacks on Chubby files and directories for notification of changes or session expiration.Bigtable依赖一个高可用且持久的分布式锁定服务，称为Chubby[8]。Chubby服务由五个活动副本组成，其中一个活动副本被选为主副本，并积极响应请求13。当大部分副本处于运行状态并且能够彼此通信时，这个服务是可用的9。Chubby使用Paxos算法 [9][23] 应对失败时如何保持其副本的一致性。 Chubby提供了一个由目录和小文件组成的命名空间。每个目录或文件都可以用作锁，并且对文件的读写是原子的。 Chubby客户端函数库提供一致的Chubby文件缓存。每个Chubby客户端都维护一个Chubby服务会话（session）。如果客户端的会话（session）无法在租约（lease）到期时间内续签（renew）其会话租约（session lease），则该会话将过期。客户端会话（session）期满后，它将丢失所有锁以及已打开的文件句柄（handle）。Chubby客户端也可以在Chubby文件和目录上注册回调函数（callback），以通知（出现）变化或会话（session）到期。Bigtable uses Chubby for a variety of tasks: to ensure that there is at most one active master at any time; to store the bootstrap location of Bigtable data (see Section 5.1); to discover tablet servers and finalize tablet server deaths (see Section 5.2); to store Bigtable schema information (the column family information for each table); and to store access control lists. If Chubby becomes unavailable for an extended period of time, Bigtable becomes unavailable. We recently measured this effect in 14 Bigtable clusters spanning 11 Chubby instances. The average percentage of Bigtable server hours during which some data stored in Bigtable was not available due to Chubby unavailability (caused by either Chubby outages or network issues) was 0.0047%. The percentage for the single cluster that was most affected by Chubby unavailability was 0.0326%.Bigtable使用Chubby来完成各种任务： 确保任何时候最多一个活跃的master（active master）； 存储Bigtable数据的引导位置（bootstrap location）（请参阅第5.1节）； 发现 Tablet 服务器并确定 Tablet 服务器的死机（请参阅第5.2节）； 存储Bigtable模式（schema）信息（每个表的列族信息）； 存储用于访问控制的信息而组成的列表； 如果Chubby长时间不可用，则Bigtable将不可用。我们最近在跨越11个Chubby实例的14个Bigtable集群中测量了这种影响。由于Chubby不可用（由于Chubby中断或网络问题所致）导致存储在Bigtable服务器上的一些数据无法访问的时间平均占比为0.0047％。受Chubby不可用性影响最大的单个集群上面的数据无法访问的时间占比为0.0326％。 5 ImplementationThe Bigtable implementation has three major components: a library that is linked into every client, one master server, and many tablet servers. Tablet servers can be dynamically added (or removed) from a cluster to accomodate changes in workloads. Bigtable实现具有三个主要组件：一个链接到每个客户端的函数库，一个主服务器（master server）和许多Tablet服务器。可以从集群中动态添加（或删除）Tablet服务器，以适应工作负载的变化。 The master is responsible for assigning tablets to tablet servers, detecting the addition and expiration of tablet servers, balancing tablet-server load, and garbage collection of files in GFS. In addition, it handles schema changes such as table and column family creations. 主服务器（master）负责将Tablet分配给Tablet服务器，检测Tablet服务器的添加和到期，平衡Tablet服务器的负载以及GFS中文件的垃圾回收。此外，它还处理模式（schema）的变化，例如创建表和列族。 Each tablet server manages a set of tablets (typically we have somewhere between ten to a thousand tablets per tablet server). The tablet server handles read and write requests to the tablets that it has loaded, and also splits tablets that have grown too large. 每个Tablet服务器管理一组Tablet（通常每个Tablet服务器有十到一千个Tablet）。Tablet服务器处理对已加载的Tablet的读写请求，并且还会切分太大的Tablet。 As with many single-master distributed storage systems [17][21], client data does not move through the master: clients communicate directly with tablet servers for reads and writes. Because Bigtable clients do not rely on the master for tablet location information, most clients never communicate with the master. As a result, the master is lightly loaded in practice. 与许多单个主服务器（single-master）的分布式存储系统[17][21]一样，客户端数据不会传输到主服务器（master）：客户端直接与Tablet服务器通信以进行读取和写入数据。由于Bigtable客户端不依赖主服务器（master）获取Tablet的位置信息，所以大多数客户端从不与主服务器（master）通信。结果，在实践中主服务器（master）是低负载的。 A Bigtable cluster stores a number of tables. Each table consists of a set of tablets, and each tablet contains all data associated with a row range. Initially, each table consists of just one tablet. As a table grows, it is automatically split into multiple tablets, each approximately 100-200 MB in size by default. Bigtable集群存储许多表。每个表由一组Tablet组成，并且每个Tablet包含了关联一个行区间的所有数据。最初，每个表格仅包含一个Tablet。随着表的增长，它会自动切分成多个Tablet，默认情况下每个Tablet的大小约为100-200 MB。 5.1 Tablet LocationWe use a three-level hierarchy analogous to that of a B+ tree [10] to store tablet location information (Figure 4). 我们使用类似于B+树[10]的三级层次结构来存储Tablet位置信息（图4）。 The first level is a file stored in Chubby that contains the location of the root tablet. The root tablet contains the location of all tablets in a special METADATA table. Each METADATA tablet contains the location of a set of user tablets. The root tablet is just the first tablet in the METADATA table, but is treated specially—it is never split—to ensure that the tablet location hierarchy has no more than three levels. 第一级是存储在Chubby中的文件，它包含Root Tablet的位置。 Root Tablet 包含特殊的 METADATA table 中所有Tablet的位置。 每个METADATA Tablet都包含一组 User Tablets 的位置。 Root Tablet只是METADATA table中的第一个Tablet，但经过特殊处理（从不切分），以确保Tablet位置层次结构不超过三个级别。 The METADATA table stores the location of a tablet under a row key that is an encoding of the tablet’s table identifier and its end row. Each METADATA row stores approximately 1KB of data in memory. With a modest limit of 128 MB METADATA tablets, our three-level location scheme is sufficient to address $2^{34}$ tablets (or $2^{61}$ bytes in 128 MB tablets). METADATA table 存储了某个行键下的Tablet的位置信息，该行键是Tablet表标识符及其最后一行的编码。 每个METADATA行在内存中存储大约1KB的数据。 由于 METADATA Tablet 的 128 MB 这个不大的限制，我们的三级定位方案足以处理 $2^{34}$ 个Tablet（或128 MB Tablet中的 $2^{61}$ 字节）。 译者附 第一级：Chubby 中的一个文件 第二级：METADATA tables（第一个 METADATA table 比较特殊，所以在图中单独画出，但它其实和其他 METADATA table 都属于第二级，即 METADATA tables = 图示中的1st METADATA Tablet (Root Tablet) + Other METADATA Tablets） 第三级：User Tables METADATA 是一个特殊的 Tablet，其中的第一个 Tablet 称为 Root Tablet 。Root Tablet 和 METADATA 内其他 Tablet 不同之处在于：它永远不会分裂，这样就可以保证 Tablet location 层级不会超过三层。 三级间的关系： Chubby 中的文件保存了 Root Tablet 的位置 Root Tablet 保存了 METADATA Tablet 内所有其他 Tablet 的位置 每个 METADATA Tablet（Root Tablet 除外）保存了一组 User Tables 的位置 METADATA 的每行数据在内存中大约占 1KB。而 METADATA Tablet 的大小限制在 128MB，这种三级位置方案就可以存储高达 128MB = $2^{17}$ 1KB，即每个 METADATA Tablet 可以指向 $2^{17}$ 个 User Table，每个 User Table 同样是 128MB 的大小话，就有 $2^{17} 2^{17} = 2^{34}$ 个 Tablet 。 如果每个 Tablet 128 MB 大小，那总数据量就高达 128MB = $2^{27}$ Byte， $2^{34} * 2^{27} = 2^{61}$ Byte，即2000PB The client library caches tablet locations. If the client does not know the location of a tablet, or if it discovers that cached location information is incorrect, then it recursively moves up the tablet location hierarchy. If the client’s cache is empty, the location algorithm requires three network round-trips, including one read from Chubby. If the client’s cache is stale, the location algorithm could take up to six round-trips, because stale cache entries are only discovered upon misses (assuming that METADATA tablets do not move very frequently). Although tablet locations are stored in memory, so no GFS accesses are required, we further reduce this cost in the common case by having the client library prefetch tablet locations: it reads the metadata for more than one tablet whenever it reads the METADATA table. 客户端库缓存Tablet的位置信息。 如果客户端不知道Tablet的位置，或者发现缓存的位置信息不正确，则它将在Tablet位置层级中向上递归14（查找想要的位置信息）。 如果客户的缓存为空，则定位算法需要进行三次网络往返，包括从Chubby中读取一次。 如果客户的缓存过时，则定位算法最多可能需要进行六次往返，因为过时的缓存项仅在未命中时才被发现（假设METADATA Tablet的移动频率不高）。 尽管Tablet位置存储在内存中，所以不需要GFS访问，但在常见情况下，我们通过让客户端库预取Tablet位置来进一步降低了此成本：每当读取METADATA表时，它都会读取一个以上Tablet的元数据。 We also store secondary information in the METADATA table, including a log of all events pertaining to each tablet (such as when a server begins serving it). This information is helpful for debugging and performance analysis. 我们还将辅助信息存储在METADATA表中，包括与每个Tablet有关的所有事件的日志（例如服务器何时开始为其服务）。 此信息有助于调试和性能分析。 5.2 Tablet AssignmentEach tablet is assigned to one tablet server at a time. The master keeps track of the set of live tablet servers, and the current assignment of tablets to tablet servers, including which tablets are unassigned. When a tablet is unassigned, and a tablet server with sufficient room for the tablet is available, the master assigns the tablet by sending a tablet load request to the tablet server. 每个Tablet每次分配到一个Tablet服务器。主服务器跟踪有效的Tablet服务器的集合15以及Tablet到Tablet服务器的当前分配关系，包括未分配的Tablet。当Tablet未分配并且可用的Tablet服务器有足够的空间来容纳Tablet时，主服务器通过向Tablet服务器发送Tablet加载请求来分配Tablet。 Bigtable uses Chubby to keep track of tablet servers. When a tablet server starts, it creates, and acquires an exclusive lock on, a uniquely-named file in a specific Chubby directory. The master monitors this directory (the servers directory) to discover tablet servers. A tablet server stops serving its tablets if it loses its exclusive lock: e.g., due to a network partition that caused the server to lose its Chubby session. (Chubby provides an efficient mechanism that allows a tablet server to check whether it still holds its lock without incurring network traffic.) A tablet server will attempt to reacquire an exclusive lock on its file as long as the file still exists. If the file no longer exists, then the tablet server will never be able to serve again, so it kills itself. Whenever a tablet server terminates (e.g., because the cluster management system is removing the tablet server’s machine from the cluster), it attempts to release its lock so that the master will reassign its tablets more quickly. Bigtable使用Chubby来跟踪Tablet服务器。Tablet服务器启动后，将在特定的Chubby目录中创建一个命名唯一的文件并获这个文件的独占锁。主服务器监控此目录（服务器目录）以发现Tablet服务器。Tablet服务器如果丢失文件的独占锁，则会停止为其Tablet提供服务：例如，由于网络分区导致服务器丢失了Chubby会话。（Chubby提供了一种高效的机制，可让Tablet服务器检查其是否仍然持有独占锁而不会引起网络通信）只要该文件仍然存在，Tablet服务器将尝试重新获取对其文件的独占锁。如果该文件不再存在，则Tablet服务器将永远无法再次提供服务，因此它将自行终止。Tablet服务器终止时（例如，由于集群管理系统正在从集群中删除Tablet服务器的计算机），它将尝试释放它持有的锁，以便主机可以更快地重新分配这个Tablet服务器被分配到的Tablet。 The master is responsible for detecting when a tablet server is no longer serving its tablets, and for reassigning those tablets as soon as possible. To detect when a tablet server is no longer serving its tablets, the master periodically asks each tablet server for the status of its lock. If a tablet server reports that it has lost its lock, or if the master was unable to reach a server during its last several attempts, the master attempts to acquire an exclusive lock on the server’s file. If the master is able to acquire the lock, then Chubby is live and the tablet server is either dead or having trouble reaching Chubby, so the master ensures that the tablet server can never serve again by deleting its server file. Once a server’s file has been deleted, the master can move all the tablets that were previously assigned to that server into the set of unassigned tablets. To ensure that a Bigtable cluster is not vulnerable to networking issues between the master and Chubby, the master kills itself if its Chubby session expires. However, as described above, master failures do not change the assignment of tablets to tablet servers. 主服务器（master ）负责检测Tablet服务器何时不再为其Tablet提供服务，并负责尽快重新分配这些Tablet。为了检测Tablet服务器何时不再为其Tablet提供服务，主服务器（master ）会定期向每个Tablet服务器询问其锁的状态。如果Tablet服务器报告其锁已丢失，或者主服务器（master ）在最后几次尝试期间都无法访问服务器，则主服务器（master ）将尝试获取Chubby所在的服务器的Chubby目录下的文件独占锁。如果主服务器（master ）能够获取锁，则Chubby处于存活的状态，以及如果Tablet服务器死机或者无法访问Chubby，那么主服务器（master ）通过删除Chubby所在的服务器的Chubby目录下的文件来确保Tablet服务器永远不会再次服务。删除Chubby所在的服务器的Chubby目录下的文件后，主服务器（master ）可以将以前分配给处于无效状态的Tablet服务器的所有Tablet移至未分配的Tablet集合中。为了确保Bigtable集群不会受到主服务器（master ）和Chubby之间的网络问题的影响，如果主服务器的Chubby会话到期，则主服务器会自行杀死。但是，如上所述，主服务器（master）设备故障不会更改Tablet到Tablet服务器的分配关系。 When a master is started by the cluster management system, it needs to discover the current tablet assignments before it can change them. The master executes the following steps at startup. (1) The master grabs a unique master lock in Chubby, which prevents concurrent master instantiations. (2) The master scans the servers directory in Chubby to find the live servers. (3) The master communicates with every live tablet server to discover what tablets are already assigned to each server. (4) The master scans the METADATA table to learn the set of tablets. Whenever this scan encounters a tablet that is not already assigned, the master adds the tablet to the set of unassigned tablets, which makesthe tablet eligible for tablet assignment. 当主服务器由集群管理系统启动时，它需要先发现当前的Tablet分配关系，然后才能更改它们。主服务器在启动时执行以下步骤。 （1）主服务器在Chubby中获取唯一的主服务器锁，这可以防止并发的主服务器实例化。（2）主服务器扫描Chubby中的服务器目录以找到有效的Tablet服务器。（3）主服务器与每个有效的Tablet服务器通信，以发现已分配给每个服务器的Tablet。（4）主服务器扫描METADATA table获知Tablet集合。每当此扫描遇到尚未分配的Tablet时，主服务器就会将该Tablet添加到未分配的Tablet集合中，这使该Tablet有资格进行Tablet分配。 One complication is that the scan of the METADATA table cannot happen until the METADATA tablets have been assigned. Therefore, before starting this scan (step 4), the master adds the root tablet to the set of unassigned tablets if an assignment for the root tablet was not discovered during step 3. This addition ensures that the root tablet will be assigned. Because the root tablet contains the names of all METADATA tablets, the master knows about all of them after it has scanned the root tablet. 一种复杂的情况是，在分配 METADATA Tablet 之前，无法进行 METADATA table 的扫描。因此，在开始此扫描（步骤4）之前，如果在步骤3中未找到针对Root Tablet的分配，则主服务器会将Root Tablet添加到未分配Tablet的集合中。此添加操作确保了将对Root Tablet进行分配。由于Root Tablet包含所有METADATA Tablet的名称，因此主服务器在扫描了Root Tablet之后便知道了所有这些名称。 译者附在扫描 METADATA Tablet 之前，必须保证 METADATA table 自己已经被分配出去了。因此，如果在步骤 3 中发现 Root Tablet 还没有被分配出去，那主服务器就要先将它放到 未分配 Tablet 集合，然后去执行步骤 4。 这样就保证了 Root Tablet 将会被分配出去。 The set of existing tablets only changes when a table is created or deleted, two existing tablets are merged to form one larger tablet, or an existing tablet is split into two smaller tablets. The master is able to keep track of these changes because it initiates all but the last. Tablet splits are treated specially since they are initiated by a tablet server. The tablet server commits the split by recording information for the new tablet in the METADATA table. When the split has committed, it notifies the master. In case the split notification is lost (either because the tablet server or the master died), the master detects the new tablet when it asks a tablet server to load the tablet that has now split. The tablet server will notify the master of the split, because the tablet entry it finds in the METADATA table will specify only a portion of the tablet that the master asked it to load. 现有的Tablet集合，只有在以下情形才会发生改变： （1）当一个Tablet被创建或删除； （2）对两个现有的Tablet进行合并得到一个更大的Tablet； （3）一个现有的tablet被切分成两个较小的Tablet。 主服务器能够跟踪这些变化，因为它负责启动除最后一次以外的所有操作。Tablet切分操作是由Tablet服务器启动的，因此受到特殊对待。Tablet服务器通过在 METADATA table 中记录新Tablet的信息来提交切分操作。提交切分操作后，它将通知主服务器。万一切分事件通知丢失（由于Tablet服务器或主服务器死机），则主服务器在要求Tablet服务器加载现在已切分的Tablet时，会检测到新的Tablet。Tablet服务器会把切分操作通知主服务器，因为它在 METADATA table 中查到的Tablet条目将仅指定一部分的Tablet，而Tablet是主服务器要求Tablet服务器加载的。 译者附如果通知丢失（由于Tablet服务器或主服务器挂掉），主服务器会在它下次要求一个Tablet server 加载 Tablet 时发现。这个 Tablet 服务器会将这次切分事件通知给主服务器，因为“Tablet服务器通过在 METADATA table 中记录新Tablet的信息来提交切分操作。提交切分操作后，它将通知主服务器”。所以它在 METADATA table 中发现的 Tablet 项只覆盖主服务器要求它加载的 Tablet 的了一部分。 5.3 Tablet Serving The persistent state of a tablet is stored in GFS, as illustrated in Figure 5. Updates are committed to a commit log that stores redo records. Of these updates, the recently committed ones are stored in memory in a sorted buffer called a memtable; the older updates are stored in a sequence of SSTables. To recover a tablet, a tablet server reads its metadata from the METADATA table. This metadata contains the list of SSTables that comprise a tablet and a set of a redo points, which are pointers into any commit logs that may contain data for the tablet. The server reads the indices of the SSTables into memory and reconstructs the memtable by applying all of the updates that have committed since the redo points. Tablet的持久化状态存储在GFS中，如图5所示。更新被提交（commit）到一个提交日志（commit log），这些日志存储着重做的记录（redo records）。在这些更新当中，最近提交的更新被存储到内存当中的一个被称为memtable的排序缓冲区，比较老的更新被存储在一系列SSTable中。为了恢复Tablet，Tablet服务器从 METADATA table 读取其元数据。该元数据包含SSTables列表，该SSTables包含一个Tablet和一个重做点（redo point）的集合 ，这些重做点（redo point）是指向任何可能包含该Tablet数据的提交日志的指针。服务器将SSTables的索引读入内存，并通过应用自重做点以来已提交的所有更新来重建memtable。 When a write operation arrives at a tablet server, the server checks that it is well-formed, and that the sender is authorized to perform the mutation. Authorization is performed by reading the list of permitted writers from a Chubby file (which is almost always a hit in the Chubby client cache). A valid mutation is written to the commit log. Group commit is used to improve the throughput of lots of small mutations [13][16]. After the write has been committed, its contents are inserted into the memtable. 当写操作到达Tablet服务器时，服务器将检查其格式是否正确，以及发送方是否有权执行这个更改（mutation）。通过从Chubby文件中读取允许的作者列表来执行授权（这在Chubby客户端缓存中几乎总是命中）。有效的更改（mutation）将写入提交日志（commit log）。整组提交（group commit）用于提高许多小更改的吞吐量 [13][16]。提交写入后，其内容将插入到memtable中。 When a read operation arrives at a tablet server, it is similarly checked for well-formedness and proper authorization. A valid read operation is executed on a merged view of the sequence of SSTables and the memtable. Since the SSTables and the memtable are lexicographically sorted data structures, the merged view can be formed efficiently. Incoming read and write operations can continue while tablets are split and merged. 当读操作到达Tablet服务器时，同样会检查其格式是否正确以及是否获得适当的授权。在SSTables和memtable序列的合并视图上执行有效的读取操作。由于SSTables和memtable是按字典顺序排序的数据结构，因此可以有效地形成合并视图。切分和合并Tablet时，传入的读写操作可以继续。 5.4 CompactionsAs write operations execute, the size of the memtable increases. When the memtable size reaches a threshold, the memtable is frozen, a new memtable is created, and the frozen memtable is converted to an SSTable and written to GFS. This minor compaction process has two goals: it shrinks the memory usage of the tablet server, and it reduces the amount of data that has to be read from the commit log during recovery if this server dies. Incoming read and write operations can continue while compactions occur. 随着写操作的执行，memtable的大小增加。 当memtable大小达到阈值时，该memtable被冻结，创建新的memtable，并将冻结的memtable转换为SSTable并写入GFS。 这个次要的压缩过程有两个目标：减少Tablet服务器的内存使用量，并且如果该服务器死机，那么在恢复期间，压缩将减少必须从提交日志中读取的数据量。 发生压缩时，传入的读取和写入操作可以继续。 Every minor compaction creates a new SSTable. If this behavior continued unchecked, read operations might need to merge updates from an arbitrary number of SSTables. Instead, we bound the number of such files by periodically executing a merging compaction in the background. A merging compaction reads the contents of a few SSTables and the memtable, and writes out a new SSTable. The input SSTables and memtable can be discarded as soon as the compaction has finished. A merging compaction that rewrites all SSTables into exactly one SSTable is called a major compaction. 每次minor compaction（小型压缩）都会创建一个新的SSTable。 如果此行为持续未经检查，则读操作可能需要合并任意数量的SSTables中的更新。 相反，我们通过在后台定期执行merging compaction（合并压缩）来限制此类文件的数量。 合并压缩读取一些SSTables和memtable的内容，并输出新的SSTable。 压缩完成后，可以立即丢弃输入的SSTables和memtable。 将所有SSTable重写为一个SSTable的合并压缩称为major compaction（大型压缩）。 SSTables produced by non-major compactions can contain special deletion entries that suppress deleted data in older SSTables that are still live. A major compaction, on the other hand, produces an SSTable that contains no deletion information or deleted data. Bigtable cycles through all of its tablets and regularly applies major compactions to them. These major compactions allow Bigtable to reclaim resources used by deleted data, and also allow it to ensure that deleted data disappears from the system in a timely fashion, which is important for services that store sensitive data. 由 non-major compaction（非大型压缩）产生的SSTable可以包含特殊的删除条目（这里删除条目视为存储着：起到删除功能的指令，然而执行指令在：major compaction阶段），这些条目用于删除掉仍然存在于旧SSTable中逻辑上视为已删除的数据（逻辑上视为已删除的数据：客户端无法读取这些数据，即对客户端不可见，然而磁盘上这些数据还在。逻辑上已经不存在，物理上还存在）。 另一方面，major compaction（大型压缩）会产生一个SSTable，该表不包含删除信息或已删除的数据。 Bigtable会遍历其所有Tablet，并定期对其应用major compaction（大型压缩）。 这些major compaction（大型压缩）使Bigtable可以回收已删除数据所使用的资源，还可以确保Bigtable及时地从系统中删除已删除的数据，这对于存储敏感数据的服务很重要。 6 RefinementsThe implementation described in the previous section required a number of refinements to achieve the high performance, availability, and reliability required by our users. This section describes portions of the implementation in more detail in order to highlight these refinements.上一节中描述的实现需要大量改进，以实现我们的用户所需的高性能，可用性和可靠性。 本节将更详细地描述实现的各个部分，以突出显示这些改进。 Locality groupsClients can group multiple column families together into a locality group. A separate SSTable is generated for each locality group in each tablet. Segregating column families that are not typically accessed together into separate locality groups enables more efficient reads. For example, page metadata in Webtable (such as language and checksums) can be in one locality group, and the contents of the page can be in a different group: an application that wants to read the metadata does not need to read through all of the page contents. 客户端可以将多个列族组合到一个 locality group 中。为每个Tablet中的每个位置组生成一个单独的SSTable。将通常不一起访问的列族隔离到单独的 locality group 中，可以提高读取效率。例如，Webtable中的页面元数据（例如语言以及校验和）可以在一个 locality group 中，而页面的内容可以在另一个组中：想要读取元数据的应用程序不需要通读所有页面内容。 In addition, some useful tuning parameters can be specified on a per-locality group basis. For example, a locality group can be declared to be in-memory. SSTables for in-memory locality groups are loaded lazily into the memory of the tablet server. Once loaded, column families that belong to such locality groups can be read without accessing the disk. This feature is useful for small pieces of data that are accessed frequently: we use it internally for the location column family in the METADATA table. 另外，可以在每个 locality group 的基础上指定一些有用的调整参数。例如，可以将一个 locality group 声明为内存中。内存中 locality group 的SSTable延迟加载到Tablet服务器的内存中。一旦加载后，无需访问磁盘即可读取属于此类 locality group 的列族。此功能对经常访问的小数据很有用：我们在内部将其用于METADATA表中的location列族。 译者附主要是根据数据访问的局部性原理与在操作系统中内存页的缓存算法是同理。 CompressionClients can control whether or not the SSTables for a locality group are compressed, and if so, which compression format is used. The user-specified compression format is applied to each SSTable block (whose size is controllable via a locality group specific tuning parameter). Although we lose some space by compressing each block separately, we benefit in that small portions of an SSTable can be read without decompressing the entire file. Many clients use a two-pass custom compression scheme. The first pass uses Bentley and McIlroy’s scheme [6], which compresses long common strings across a large window. The second pass uses a fast compression algorithm that looks for repetitions in a small 16 KB window of the data. Both compression passes are very fast—they encode at 100–200 MB/s, and decode at 400–1000 MB/s on modern machines. Even though we emphasized speed instead of space reduction when choosing our compression algorithms, this two-pass compression scheme does surprisingly well. 客户端可以控制是否压缩 locality group 的SSTable，以及如果压缩，则使用哪种压缩格式。用户指定的压缩格式将应用于每个SSTable块（其大小可通过 locality group 的特定的调整参数来控制）。尽管我们通过分别压缩每个块而损失了一些空间，但我们的好处是因为：可以读取SSTable的一小部分而无需解压缩整个文件。许多客户端使用两阶段自定义压缩方案。第一阶段使用Bentley和McIlroy的方案[6]，该方案在一个大窗口中压缩长的公共字符串。第二阶段使用快速压缩算法，该算法在一个小的16 KB数据窗口中查找重复项。两种压缩过程都非常快——在现代机器上，它们的编码速度为100-200 MB/s，解码速度为 400-1000 MB/s。尽管在选择压缩算法时我们强调速度而不是减少空间，但这种两阶段压缩方案的效果出奇地好。 For example, in Webtable, we use this compression scheme to store Web page contents. In one experiment,we stored a large number of documents in a compressed locality group. For the purposes of the experiment, we limited ourselves to one version of each document instead of storing all versions available to us. The scheme achieved a 10-to-1 reduction in space. This is much better than typical Gzip reductions of 3-to-1 or 4-to-1 on HTML pages because of the way Webtable rows are laid out: all pages from a single host are stored close to each other. This allows the Bentley-McIlroy algorithm to identify large amounts of shared boilerplate in pages from the same host. Many applications, not just Webtable, choose their row names so that similar data ends up clustered, and therefore achieve very good compression ratios. Compression ratios get even better when we store multiple versions of the same value in Bigtable. 例如，在Webtable中，我们使用这种压缩方案来存储Web页面内容。在一个实验中，我们将大量文档存储在一个压缩的 locality group 中。为了进行实验，我们将自己限制为每个文档的一个版本，而不是存储所有可用的版本。该方案使空间减少了10比1。由于Webtable行的布局方式，这比HTML页面上通常的Gzip压缩（3比1或4比1）要好得多：来自单个主机的所有页面都存储得彼此靠近。这使Bentley-McIlroy算法可以识别来自同一主机的页面中的大量共享样板。许多应用程序（不仅是Webtable）都选择其行名致使相似的数据最终聚集在一起，因此实现了很好的压缩率。当我们在Bigtable中存储相同值的多个版本时，压缩率甚至会更高。 Caching for read performanceTo improve read performance, tablet servers use two levels of caching. The Scan Cache is a higher-level cache that caches the key-value pairs returned by the SSTable interface to the tablet server code. The Block Cache is a lower-level cache that caches SSTables blocks that were read from GFS. The Scan Cache is most useful for applications that tend to read the same data repeatedly. The Block Cache is useful for applications that tend to read data that is close to the data they recently read (e.g., sequential reads, or random reads of different columns in the same locality group within a hot row). 为了提高读取性能，Tablet服务器使用两个级别的缓存。 Scan Cache是一个更高层次的缓存，它将SSTable接口返回的键值对缓存到Tablet服务器代码。 Block Cache是较低层次的缓存，它缓存从GFS读取的SSTables块。 Scan Cache对于倾向于重复读取相同数据的应用程序最有用。 对于倾向于读取与其最近读取的数据接近的数据的应用程序（例如，顺序读取或对热点行中同一个 locality group 中不同列的随机读取），Block Cache非常有用。 Bloom filtersAs described in Section 5.3, a read operation has to read from all SSTables that make up the state of a tablet. If these SSTables are not in memory, we may end up doing many disk accesses. We reduce the number of accesses by allowing clients to specify that Bloom filters [7] should be created for SSTables in a particular locality group. A Bloom filter allows us to ask whether an SSTable might contain any data for a specified row/column pair. For certain applications, a small amount of tablet server memory used for storing Bloom filters drastically reduces the number of disk seeks required for read operations. Our use of Bloom filters also implies that most lookups for non-existent rows or columns do not need to touch disk. 如第5.3节所述，读取操作必须从构成Tablet状态的所有SSTable中读取。如果这些SSTable不在内存中，我们可能最终会进行许多磁盘访问。通过允许客户端指定应为特定 locality group 中的SSTable创建Bloom过滤器[7]，我们减少了访问次数。 布隆过滤器允许我们询问SSTable是否可以包含指定行/列对的任何数据。对于某些应用程序，用于存储布隆过滤器的少量Tablet服务器的内存会大大减少读取操作所需的磁盘搜寻次数。 我们对Bloom过滤器的使用还意味着对于不存在的行或列的大多数查找都不需要接触磁盘。 Commit-log implementationIf we kept the commit log for each tablet in a separate log file, a very large number of files would be written concurrently in GFS. Depending on the underlying file system implementation on each GFS server, these writes could cause a large number of disk seeks to write to the different physical log files. In addition, having separate log files per tablet also reduces the effectiveness of the group commit optimization, since groups would tend to be smaller. To fix these issues, we append mutations to a single commit log per tablet server, co-mingling mutations for different tablets in the same physical log file [18][20]. 如果我们将每个Tablet的提交日志保存在单独的日志文件中，则会在GFS中同时写入大量文件。根据每个GFS服务器上基础文件系统的实现，这些写操作可能导致大量磁盘搜索以写到不同的物理日志文件。此外，每个Tablet使用单独的日志文件还会降低整组提交（ group commit ）优化的效率，因为组的规模往往较小。为了解决这些问题，我们将数据的变化记录（mutation）追加到每个Tablet服务器的单个提交日志中，将不同Tablet的变化记录（mutation）混合在同一物理日志文件中 [18][20]。 Using one log provides significant performance benefits during normal operation, but it complicates recovery. When a tablet server dies, the tablets that it served will be moved to a large number of other tablet servers: each server typically loads a small number of the original server’s tablets. To recover the state for a tablet, the new tablet server needs to reapply the mutations for that tablet from the commit log written by the original tablet server. However, the mutations for these tablets were co-mingled in the same physical log file. One approach would be for each new tablet server to read this full commit log file and apply just the entries needed for the tablets it needs to recover. However, under such a scheme, if 100 machines were each assigned a single tablet from a failed tablet server, then the log file would be read 100 times (once by each server). 在正常操作期间，使用一个日志可以显著提高性能，但是会使恢复变得复杂。当Tablet服务器死亡时，其所服务的Tablet将被移至大量其他Tablet服务器：每个服务器通常会加载少量原始服务器的Tablet。要恢复Tablet的状态，新的Tablet服务器需要从原始Tablet服务器写入的提交日志中重新应用该Tablet的变化日志。但是，这些Tablet的变化日志被混合在同一物理日志文件中。一种方法是让每个新的Tablet服务器读取此完整的提交日志文件，并仅应用其需要恢复的Tablet所需的条目。但是，在这种方案下，如果从故障的Tablet服务器中分别为100台计算机分配了一个Tablet，那么日志文件将被读取100次（每个服务器一次）。 We avoid duplicating log reads by first sorting the commit log entries in order of the keys &lt;htable; row name; log sequence number&gt;. In the sorted output, all mutations for a particular tablet are contiguous and can therefore be read efficiently with one disk seek followed by a sequential read. To parallelize the sorting, we partition the log file into 64 MB segments, and sort each segment in parallel on different tablet servers. This sorting process is coordinated by the master and is initiated when a tablet server indicates that it needs to recover mutations from some commit log file. 我们通过以 (table; row name; log sequence number) 为键对提交日志条目进行排序来避免重复的日志读取。在排序的输出中，特定Tablet的所有mutation（数据的变化）都是连续的，因此可以通过一个磁盘搜索有效读取，然后顺序读取。为了并行化排序，我们将日志文件划分为64 MB的分段，然后在不同的Tablet服务器上并行地对每个分段进行排序。此排序过程由主服务器（master）协调，并在Tablet服务器指示需要从某些提交日志文件中恢复mutation（数据的更改）时启动。 Writing commit logs to GFS sometimes causes performance hiccups for a variety of reasons (e.g., a GFS server machine involved in the write crashes, or the network paths traversed to reach the particular set of three GFS servers is suffering network congestion, or is heavily loaded). To protect mutations from GFS latency spikes, each tablet server actually has two log writing threads, each writing to its own log file; only one of these two threads is actively in use at a time. If writes to the active log file are performing poorly, the log file writing is switched to the other thread, and mutations that are in the commit log queue are written by the newly active log writing thread. Log entries contain sequence numbers to allow the recovery process to elide duplicated entries resulting from this log switching process. 将提交日志写入GFS有时会由于各种原因而导致性能下降（例如，写入时发生崩溃的GFS服务器计算机，或用来穿越以便到达特定的三个GFS服务器集的网络路径正遭受网络拥塞或负载过重） 。为了保护变化免受GFS延迟高峰的影响，每个Tablet服务器实际上都有两个日志写入线程（一个是被激活也就是正在使用的线程，一个是备用线程），每个线程都写入自己的日志文件。一次仅积极使用这两个线程之一。如果对激活的（active 有些人翻译：活跃的）日志文件的写入性能不佳，则日志文件的写入将切换到另一个线程，并且提交日志队列中的数据变化记录将由新激活的日志写线程进行写入。日志条目包含序列号，以允许恢复过程清除此日志切换过程产生的重复条目。 Speeding up tablet recoveryIf the master moves a tablet from one tablet server to another, the source tablet server first does a minor compaction on that tablet. This compaction reduces recovery time by reducing the amount of uncompacted state in the tablet server’s commit log. After finishing this compaction, the tablet server stops serving the tablet. Before it actually unloads the tablet, the tablet server does another (usually very fast) minor compaction to eliminate any remaining uncompacted state in the tablet server’s log that arrived while the first minor compaction was being performed. After this second minor compaction is complete, the tablet can be loaded on another tablet server without requiring any recovery of log entries. 如果主服务器（master）将 Tablet 从一台 Tablet 服务器移动到另一台 Tablet 服务器，则源 Tablet 服务器首先对该 Tablet 进行 minor compaction（小型压缩）。 这种压缩通过减少 Tablet 服务器的提交日志中未压缩状态的数量来减少恢复时间。 完成这次压缩后，Tablet 服务器将停止为 Tablet 提供服务。 在实际卸载 Tablet 之前，Tablet 服务器会进行另一次（通常非常快） minor compaction（小型压缩）来消除执行第一次 minor compaction（小型压缩）时到达 Tablet 服务器的日志当中任何剩余的未压缩状态。 在完成第二次 minor compaction（小型压缩）后，可将 Tablet 加载到另一台 Tablet 服务器上，而无需恢复日志条目。 Exploiting immutabilityBesides the SSTable caches, various other parts of the Bigtable system have been simplified by the fact that all of the SSTables that we generate are immutable. For example, we do not need any synchronization of accesses to the file system when reading from SSTables. As a result, concurrency control over rows can be implemented very efficiently. The only mutable data structure that is accessed by both reads and writes is the memtable. To reduce contention during reads of the memtable, we make each memtable row copy-on-write and allow reads and writes to proceed in parallel. 除了SSTable缓存外，我们生成的所有SSTable都是不可变的，从而简化了Bigtable系统的其他各个部分。例如，当从SSTables读取数据时，我们不需要对文件系统的访问进行任何同步。结果，可以非常有效地实现对行的并发控制。读取和写入均访问的唯一可变数据结构是memtable。为了减少在读取memtable期间的竞争，我们使每个memtable的行使用写时复制的策略，并允许读取和写入并行进行。 Since SSTables are immutable, the problem of permanently removing deleted data is transformed to garbage collecting obsolete SSTables. Each tablet’s SSTables are registered in the METADATA table. The master removes obsolete SSTables as a mark-and-sweep garbage collection [25] over the set of SSTables, where the METADATA table contains the set of roots. Finally, the immutability of SSTables enables us to split tablets quickly. Instead of generating a new set of SSTables for each child tablet, we let the child tablets share the SSTables of the parent tablet. 由于SSTable是不可变的，因此永久删除已删除数据（前面讲过的发出删除指令，但未被执行的数据）的问题被转换为垃圾收集过期的SSTable。每个Tablet的SSTables都注册在 METADATA table 中。主服务器（master）删除过时的SSTables作为SSTables集合上的标记再清除式的垃圾收集[25]，其中 METADATA table 包含根集合（按照前文：METADATA table 记录了这些 SSTable 的对应的 tablet 的 root）。最后，SSTables的不变性使我们能够快速拆分Tablet。我们不必为每个子 Tablet 生成一组新的SSTable，而是让子 Tablet 共享 Tablet 的SSTable。 7 Performance EvaluationWe set up a Bigtable cluster with N tablet servers to measure the performance and scalability of Bigtable as N is varied. The tablet servers were configured to use 1 GB of memory and to write to a GFS cell consisting of 1786 machines with two 400 GB IDE hard drives each. N client machines generated the Bigtable load used for these tests. (We used the same number of clients as tablet servers to ensure that clients were never a bottleneck.) Each machine had two dual-core Opteron 2 GHz chips, enough physical memory to hold the working set of all running processes, and a single gigabit Ethernet link. The machines were arranged in a two-level tree-shaped switched network with approximately 100-200 Gbps of aggregate bandwidth available at the root. All of the machines were in the same hosting facility and therefore the round-trip time between any pair of machines was less than a millisecond. 我们建立了一个由N台Tablet服务器组成的Bigtable集群，以随着 N 的变化来衡量Bigtable的性能和可扩展性。Tablet服务器配置为使用1 GB的内存，并写入由1786台计算机组成的GFS单元，每台计算机具有两个400 GB的IDE硬盘驱动器。 N个客户端计算机生成了用于这些测试的Bigtable负载。（我们使用与Tablet服务器相同数量的客户端，以确保客户端永远不会成为瓶颈）每台机器都具有两个双核Opteron 2 GHz芯片，足够的物理内存来容纳所有正在运行的进程的工作集以及一个 1Gbp/s 以太网链路。这些机器被安排在两级树形交换网络（two-level tree-shaped switched network）中，网络根节点大约有100-200 Gbps的总带宽。所有机器都位于同一托管设施中，因此任何两对机器之间的往返时间均不到一毫秒。 The tablet servers and master, test clients, and GFS servers all ran on the same set of machines. Every machine ran a GFS server. Some of the machines also ran either a tablet server, or a client process, or processes from other jobs that were using the pool at the same time as these experiments. Tablet服务器以及主服务器，测试客户端和GFS服务器都在同一组计算机上运行。每台机器都运行GFS服务器。其中一些机器还运行了Tablet服务器或客户端进程，或者运行了与这些实验同时使用这些机器池（根据本文第四节第一段推测 “the pool” 翻译为：机器池）的其他作业的进程。 R is the distinct number of Bigtable row keys involved in the test. R was chosen so that each benchmark read or wrote approximately 1 GB of data per tablet server. R 是测试中涉及的Bigtable不重复行键的数量。选择R是为了使每个基准测试中每个Tablet服务器读取或写入大约1 GB的数据。 The sequential write benchmark used row keys with names 0 to R - 1. This space of row keys was partitioned into 10N equal-sized ranges. These ranges were assigned to the N clients by a central scheduler that as signed the next available range to a client as soon as the client finished processing the previous range assigned to it. This dynamic assignment helped mitigate the effects of performance variations caused by other processes running on the client machines. We wrote a single string under each row key. Each string was generated randomly and was therefore uncompressible. In addition, strings under different row key were distinct, so no cross-row compression was possible. The random write benchmark was similar except that the row key was hashed modulo R immediately before writing so that the write load was spread roughly uniformly across the entire row space for the entire duration of the benchmark. 顺序写基准测试使用名称为 0 到 R - 1 的行键。此行键空间被划分为 10N 个相等大小的区间。这些区间由中央调度程序分配给N个客户端，该中央调度程序在客户端完成对分配给它的先前区间的处理后立即将下一个可用区间分配给客户端。这种动态分配有助于减轻由客户端计算机上运行的其他进程引起的性能变化的影响。我们在每个行键下写了一个字符串。每个字符串都是随机生成的，因此不可压缩的。另外，不同行键下的字符串是不同的，因此不可能进行跨行压缩。随机写基准测试类似于顺序写基准测试，不同的是在写入之前立即对行密钥进行了模R哈希运算，以便在基准测试的整个期间，写入负载大致均匀地分布在整个行空间中。 The sequential read benchmark generated row keys in exactly the same way as the sequential write benchmark, but instead of writing under the row key, it read the string stored under the row key (which was written by an earlier invocation of the sequential write benchmark). Similarly, the random read benchmark shadowed the operation of the random write benchmark. 顺序读基准测试产生的行密钥与顺序写入基准完全相同，但它不是在行密钥下写入，而是读取存储在行密钥下的字符串（该字符串是由顺序写基准测试的较早时候调用写入的） 。同样，随机读基准测试与随机写基准测试的操作一样。 The scan benchmark is similar to the sequential read benchmark, but uses support provided by the Bigtable API for scanning over all values in a row range. Using a scan reduces the number of RPCs executed by the benchmark since a single RPC fetches a large sequence of values from a tablet server. 扫描基准测试（scan benchmark）类似于顺序读基准测试，但是使用Bigtable API提供的支持来扫描行区间内的所有值。使用扫描减少了基准测试执行的RPC数量，因为单个RPC从Tablet服务器中提取了大量的值。 The random reads (mem) benchmark is similar to the random read benchmark, but the locality group that contains the benchmark data is marked as in-memory, and therefore the reads are satisfied from the tablet server’s memory instead of requiring a GFS read. For just this benchmark, we reduced the amount of data per tablet server from 1 GB to 100 MB so that it would fit comfortably in the memory available to the tablet server. 随机读（mem）基准测试类似于随机读基准测试，但是包含基准数据的 locality group 被标记为内存中，因此可以从Tablet服务器的内存中读取数据，而无需进行GFS读取。对于该基准测试，我们将每个Tablet服务器的数据量从1 GB减少到100 MB，以便可以合适地容纳在Tablet服务器可用的内存中。 Figure 6 shows two views on the performance of our benchmarks when reading and writing 1000-byte values to Bigtable. The table shows the number of operations per second per tablet server; the graph shows the aggregate number of operations per second. 图6显示了在向 Bigtable 读取和写入 1000MB/S 时基准测试性能的两个视图。该表显示了每台Tablet服务器每秒的操作数；该图显示了每秒的总操作数。 Single tablet-server performanceLet us first consider performance with just one tablet server. Random reads are slower than all other operations by an order of magnitude or more. Each random read involves the transfer of a 64 KB SSTable block over the network from GFS to a tablet server, out of which only a single 1000-byte value is used. The tablet server executes approximately 1200 reads per second, which translates into approximately 75 MB/s of data read from GFS. This bandwidth is enough to saturate the tablet server CPUs because of overheads in our networking stack, SSTable parsing, and Bigtable code, and is also almost enough to saturate the network links used in our system. Most Bigtable applications with this type of an access pattern reduce the block size to a smaller value, typically 8KB. 首先让我们考虑一台Tablet服务器的性能。随机读取的速度比所有其他操作慢一个数量级或更多。每次随机读取都涉及通过网络将64 KB SSTable块从GFS传输到Tablet服务器，其中仅使用一个1000字节的值。Tablet服务器每秒执行大约1200次读取，这意味着从GFS读取的数据大约为75 MB/s (1200 * 64 KB / 1024 = 75MB/s)。由于网络堆栈，SSTable解析和Bigtable代码的开销，该带宽足以使 Tablet 服务器的CPU饱和，也几乎足以使系统中使用的网络链路饱和。具有这种访问模式的大多数Bigtable应用程序将块大小减小为一个较小的值，通常为8KB。 Random reads from memory are much faster since each 1000-byte read is satisfied from the tablet server’s local memory without fetching a large 64 KB block from GFS. 从内存中进行随机读取的速度要快得多，因为每次从Tablet服务器的本地内存读取 1000B 即可满足需要，而无需从GFS提取大的 64 KB块。 Random and sequential writes perform better than random reads since each tablet server appends all incoming writes to a single commit log and uses group commit to stream these writes efficiently to GFS. There is no significant difference between the performance of random writes and sequential writes; in both cases, all writes to the tablet server are recorded in the same commit log. 随机和顺序写入的性能要优于随机读取，因为每个Tablet服务器会将所有传入的写入都追加到单个提交日志中，并使用整组提交（group commit）将这些写入高效地流式传输到GFS。随机写入和顺序写入的性能之间没有显着差异。在这两种情况下，对Tablet服务器的所有写入都记录在同一提交日志中。 Sequential reads perform better than random reads since every 64 KB SSTable block that is fetched from GFS is stored into our block cache, where it is used to serve the next 64 read requests. 顺序读取的性能要优于随机读取，因为从GFS提取的每个64 KB SSTable块都存储在我们的块缓存中，用于满足接下来的64个读取请求。 Scans are even faster since the tablet server can return a large number of values in response to a single client RPC, and therefore RPC overhead is amortized over a large number of values. 由于Tablet服务器可以在响应单个客户端RPC时返回大量值，因此 scan 速度甚至更快，因此RPC开销将在大量值上摊销。 ScalingAggregate throughput increases dramatically, by over a factor of a hundred, as we increase the number of tablet servers in the system from 1 to 500. For example, the performance of random reads from memory increases by almost a factor of 300 as the number of tablet server increases by a factor of 500. This behavior occurs because the bottleneck on performance for this benchmark is the individual tablet server CPU. 随着我们将系统中Tablet服务器的数量从1个增加到500个，总的吞吐量急剧增加了一百倍。例如，随着内存数量的增加，从内存中随机读取的性能几乎提高了300倍。Tablet服务器增加了500倍。之所以发生这种现象，是因为该基准测试的性能瓶颈是各个Tablet服务器CPU。 However, performance does not increase linearly. For most benchmarks, there is a significant drop in per-server throughput when going from 1 to 50 tablet servers. This drop is caused by imbalance in load in multiple server configurations, often due to other processes contending for CPU and network. Our load balancing algorithm attempts to deal with this imbalance, but cannot do a perfect job for two main reasons: rebalancing is throttled to reduce the number of tablet movements (a tablet is unavailable for a short time, typically less than one second, when it is moved), and the load generated by our benchmarks shifts around as the benchmark progresses. 但是，性能不会线性增加。对于大多数基准测试，当从1台Tablet服务器增加到50台Tablet服务器时，每台服务器的吞吐量将大幅下降。这种下降是由于多个服务器配置中的负载不平衡而引起的，通常是由于其他争用CPU和网络的进程所致。我们的负载平衡算法试图解决这种不平衡问题，但由于两个主要原因而无法做到完美：限制重新平衡以减少Tablet的移动次数（Tablet在短时间内无法使用，通常少于一秒钟，移动），并且随着基准测试的进行，由基准测试产生的负载也会发生变化。 The random read benchmark shows the worst scaling (an increase in aggregate throughput by only a factor of 100 for a 500-fold increase in number of servers). This behavior occurs because (as explained above) we transfer one large 64KB block over the network for every 1000- byte read. This transfer saturates various shared 1 Gigabit links in our network and as a result, the per-server throughput drops significantly as we increase the number of machines. 随机读基准测试显示最差的扩展性（服务器数量增加500倍时，总吞吐量仅增加100倍）。发生这种现象的原因是（如上所述），每读取1000字节，我们就会通过网络传输一个 64 KB的大块。这种转移使我们网络中的各种共享 1 Gigabit 链路饱和，结果，随着计算机数量的增加，每服务器的吞吐量显着下降。 8 Real Applications As of August 2006, there are 388 non-test Bigtable clusters running in various Google machine clusters, with a combined total of about 24,500 tablet servers. Table 1 shows a rough distribution of tablet servers per cluster. Many of these clusters are used for development purposes and therefore are idle for significant periods. One group of 14 busy clusters with 8069 total tablet servers saw an aggregate volume of more than 1.2 million requests per second, with incoming RPC traffic of about 741 MB/s and outgoing RPC traffic of about 16 GB/s. 截至2006年8月，在各种Google机器集群中运行着388个非测试版Bigtable集群，总共约有24,500台Tablet服务器。表1显示了每个集群的Tablet服务器的大致分布。这些集群中的许多集群都用于开发目的，因此在相当长的一段时间内都处于空闲状态。一组14个繁忙的集群（总共8069个Tablet服务器）每秒总计收到超过120万个请求，其中传入RPC流量约为 741 MB/s，传出RPC流量约为 16 GB/s。 Table 2 provides some data about a few of the tables currently in use. Some tables store data that is served to users, whereas others store data for batch processing; the tables range widely in total size, average cell size, percentage of data served from memory, and complexity of the table schema. In the rest of this section, we briefly describe how three product teams use Bigtable. 表2提供了一些有关当前使用的表的数据。有些表存储提供给用户的数据，而另一些表则存储用于批处理的数据。这些表在总大小，平均单元大小，从内存提供的数据百分比以及表模式的复杂性方面分布的范围很广。在本节的其余部分，我们简要描述三个产品团队如何使用Bigtable。 8.1 Google AnalyticsGoogle Analytics (analytics.google.com) is a service that helps webmasters analyze traffic patterns at their web sites. It provides aggregate statistics, such as the number of unique visitors per day and the page views per URL per day, as well as site-tracking reports, such as the percentage of users that made a purchase, given that they earlier viewed a specific page. Google Analytics（分析）（analytics.google.com）是一项服务，可帮助网站管理员分析其网站上的流量模式。它提供了汇总统计信息，例如每天，身份不重复的访客数量和每个URL每天的页面浏览量，以及网站跟踪报告，例如在先前查看特定页面的情况下进行购买的用户所占的百分比。 To enable the service, webmasters embed a small JavaScript program in their web pages. This program is invoked whenever a page is visited. It records various information about the request in Google Analytics, such as a user identifier and information about the page being fetched. Google Analytics summarizes this data and makes it available to webmasters. 为了启用该服务，网站管理员将一个小的JavaScript程序嵌入其网页中。每当访问页面时都会调用此程序。它在Google Analytics（分析）中记录有关请求的各种信息，例如用户标识符和有关正在获取的页面的信息。 Google Analytics（分析）会汇总这些数据并将其提供给网站管理员。 We briefly describe two of the tables used by Google Analytics. The raw click table (˜200 TB) maintains a row for each end-user session. The row name is a tuple containing the website’s name and the time at which the session was created. This schema ensures that sessions that visit the same web site are contiguous, and that they are sorted chronologically. This table compresses to 14% of its original size. 我们简要介绍了Google Analytics（分析）使用的两个表格。原始点击表（约200 TB）为每个终端用户会话维护一行。行名称是一个元组，其中包含网站的名称和创建会话的时间。此模式可确保访问同一网站的会话是连续的，并且可以按时间顺序对其进行排序。该表压缩到其原始大小的14％。 The summary table (˜20 TB) contains various predefined summaries for each website. This table is generated from the raw click table by periodically scheduled MapReduce jobs. Each MapReduce job extracts recent session data from the raw click table. The overall system’s throughput is limited by the throughput of GFS. This table compresses to 29% of its original size. 摘要表（约20 TB）包含每个网站的各种预定义摘要。该表是通过定期计划的MapReduce作业从原始点击表生成的。每个MapReduce作业都会从原始点击表中提取最近的会话数据。整个系统的吞吐量受GFS吞吐量的限制。该表压缩到其原始大小的29％。 8.2 Google EarthGoogle operates a collection of services that provide users with access to high-resolution satellite imagery of the world’s surface, both through the web-based Google Maps interface (maps.google.com) and through the Google Earth (earth.google.com) custom client software. These products allow users to navigate across the world’s surface: they can pan, view, and annotate satellite imagery at many different levels of resolution. This system uses one table to preprocess data, and a different set of tables for serving client data. Google提供了一系列服务，可通过基于Web的Google Maps界面（maps.google.com）和Google Earth（earth.google.com）自定义客户端软件向用户提供世界地面的高分辨率卫星图像。这些产品使用户可以在整个地球表面导航：他们可以以许多不同的分辨率摇动拍摄，查看和注释卫星图像。该系统使用一个表预处理数据，并使用一组不同的表来提供客户端数据。 The preprocessing pipeline uses one table to store raw imagery. During preprocessing, the imagery is cleaned and consolidated into final serving data. This table contains approximately 70 terabytes of data and therefore is served from disk. The images are efficiently compressed already, so Bigtable compression is disabled. 预处理管道使用一张表存储原始图像。在预处理期间，图像将被清理并合并为最终投放数据。该表包含大约70 TB的数据，因此是从磁盘提供的。图像已被高效压缩，因此已禁用Bigtable压缩。 Each row in the imagery table corresponds to a single geographic segment. Rows are named to ensure that adjacent geographic segments are stored near each other. The table contains a column family to keep track of the sources of data for each segment. This column family has a large number of columns: essentially one for each raw data image. Since each segment is only built from a few images, this column family is very sparse. The preprocessing pipeline relies heavily on MapReduce over Bigtable to transform data. The overall system processes over 1 MB/sec of data per tablet server during some of these MapReduce jobs. 图像表格中的每一行都对应一个地理区域。 对行进行命名以确保相邻的地理段彼此相邻存储。 该表包含一个列族，以跟踪每个段的数据源。 该列族有大量列：基本上每个原始数据图像（raw data image）都有一列。 由于每个段仅由几个图像构成，因此该列族非常稀疏。 预处理管道严重依赖BigTable上的MapReduce来转换数据。 在其中的某些MapReduce作业中，整个系统每台Tablet服务器处理超过 1 MB/s 的数据。 The serving system uses one table to index data stored in GFS. This table is relatively small (˜500 GB), but it must serve tens of thousands of queries per second per datacenter with low latency. As a result, this table is hosted across hundreds of tablet servers and contains in-memory column families. 服务系统使用一张表索引存储在GFS中的数据。该表相对较小（约500 GB），但每个数据中心每秒必须处理数万个查询，且延迟低。结果，该表托管在数百台Tablet服务器中，并包含内存列族。 8.3 Personalized SearchPersonalized Search (www.google.com/psearch) is an opt-in service that records user queries and clicks across a variety of Google properties such as web search, images, and news. Users can browse their search histories to revisit their old queries and clicks, and they can ask for personalized search results based on their historical Google usage patterns. 个性化搜索（www.google.com/psearch）是一项选择性服务，可记录用户对各种Google属性（例如网络搜索，图片和新闻）的查询和点击。用户可以浏览其搜索历史记录以重新访问其以前的查询和点击，还可以根据其Google的历史使用模式（pattern：模型，模式）来请求个性化搜索结果。 Personalized Search stores each user’s data in Bigtable. Each user has a unique userid and is assigned a row named by that userid. All user actions are stored in a table. A separate column family is reserved for each type of action (for example, there is a column family that stores all web queries). Each data element uses as its Bigtable timestamp the time at which the corresponding user action occurred. Personalized Search generates user profiles using a MapReduce over Bigtable. These user profiles are used to personalize live search results. 个性化搜索将每个用户的数据存储在Bigtable中。每个用户都有一个唯一的用户ID，并分配有一个由该用户ID命名的行。所有用户操作（action）都存储在一个表中。每种操作（action）类型都保留一个单独的列族（例如，有一个列系列存储所有Web查询）。每个数据元素都将发生相应用户操作的时间用作其Bigtable时间戳。个性化搜索使用BigTable上的MapReduce生成用户个人资料（user profiles）。这些用户个人资料用于个性化实时搜索结果。 The Personalized Search data is replicated across several Bigtable clusters to increase availability and to reduce latency due to distance from clients. The Personalized Search team originally built a client-side replication mechanism on top of Bigtable that ensured eventual consistency of all replicas. The current system now uses a replication subsystem that is built into the servers. 个性化搜索数据可在多个Bigtable集群之间复制，以提高可用性并减少由于与客户端之间的距离而引起的延迟。个性化搜索团队最初在Bigtable之上构建了一个客户端复制机制，以确保所有副本的最终一致性。现在，当前系统使用服务器内置的复制子系统。 The design of the Personalized Search storage system allows other groups to add new per-user information in their own columns, and the system is now used by many other Google properties that need to store per-user configuration options and settings. Sharing a table amongst many groups resulted in an unusually large number of column families. To help support sharing, we added a simple quota mechanism to Bigtable to limit the storage consumption by any particular client in shared tables; this mechanism provides some isolation between the various product groups using this system for per-user information storage. 个性化搜索存储系统的设计允许其他组在其自己的列中添加新的每个用户信息，并且该系统现在已由许多其他需要存储每个用户配置选项和设置的Google属性所使用。在许多组之间共享一张表导致了异常多的列族。为了帮助支持共享，我们在Bigtable中添加了一个简单的配额机制，以限制共享表中任何特定客户端的存储消耗。这种机制使用此系统为每个用户的信息存储提供了各种产品组之间的隔离。 9 LessonsIn the process of designing, implementing, maintaining, and supporting Bigtable, we gained useful experience and learned several interesting lessons. 在设计，实施，维护和支持Bigtable的过程中，我们获得了有益的经验并吸取了一些有趣的经验教训。 One lesson we learned is that large distributed systems are vulnerable to many types of failures, not just the standard network partitions and fail-stop failures assumed in many distributed protocols. For example, we have seen problems due to all of the following causes: memory and network corruption, large clock skew, hung machines, extended and asymmetric network partitions, bugs in other systems that we are using (Chubby for example), overflow of GFS quotas, and planned and unplanned hardware maintenance. As we have gained more experience with these problems, we have addressed them by changing various protocols. For example, we added check summing to our RPC mechanism. We also handled some problems by removing assumptions made by one part of the system about another part. For example, we stopped assuming a given Chubby operation could return only one of a fixed set of errors. 我们 吸取的教训是，大型分布式系统容易遭受多种类型的故障，而不仅仅是许多分布式协议中假定的标准网络分区和出错后停止服务（fail-stop failures）。例如，由于以下所有原因，我们发现了问题： 内存和网络损坏 很大的时钟偏差（clock skew） 停止响应的机器 扩展 非对称的网络分区 我们正在使用的其他系统中的错误（例如，Chubby） GFS溢出配额 计划和计划外的硬件维护 随着我们在这些问题上获得更多经验，我们已通过更改各种协议来解决这些问题。例如，我们在RPC机制中添加了校验和。我们还通过消除系统某个部分对另一部分所做的假设来处理一些问题。例如，我们停止假设给定的Chubby操作只能返回一组固定错误中的一个。 总结非预期的故障源远比你想象中多 Another lesson we learned is that it is important to delay adding new features until it is clear how the new features will be used. For example, we initially planned to support general-purpose transactions in our API. Because we did not have an immediate use for them, however, we did not implement them. Now that we have many real applications running on Bigtable, we have been able to examine their actual needs, and have discovered that most applications require only single-row transactions. Where people have requested distributed transactions, the most important use is for maintaining secondary indices, and we plan to add a specialized mechanism to satisfy this need. The new mechanism will be less general than distributed transactions, but will be more efficient (especially for updates that span hundreds of rows or more) and will also interact better with our scheme for optimistic cross-data-center replication. 我们吸取的 另一个教训是，重要的是延迟添加新特性直到明确如何使用新特性。例如，我们最初计划在我们的API中支持通用事务（general-purpose transaction）。因为我们没有立即使用它们，所以我们没有实现它们。现在，我们在Bigtable上运行了许多真实的应用程序，我们已经能够检查它们的实际需求，并且发现大多数应用程序仅需要单行事务。当人们要求进行分布式交易时，最重要的用途是维护二级索引，我们计划添加一种专门的机制来满足这一需求。新机制将不如分布式事务通用，但效率更高（特别是对于跨越数百行或更多行的更新），并且还将与我们的乐观跨数据中心复制方案更好地交互。 总结避免过早添加使用场景不明确的新特性 A practical lesson that we learned from supporting Bigtable is the importance of proper system-level monitoring (i.e., monitoring both Bigtable itself, as well as the client processes using Bigtable). For example, we extended our RPC system so that for a sample of the RPCs, it keeps a detailed trace of the important actions done on behalf of that RPC. This feature has allowed us to detect and fix many problems such as lock contention on tablet data structures, slow writes to GFS while committing Bigtable mutations, and stuck accesses to the METADATA table when METADATA tablets are unavailable. Another example of useful monitoring is that every Bigtable cluster is registered in Chubby. This allows us to track down all clusters, discover how big they are, see which versions of our software they are running, how much traffic they are receiving, and whether or not there are any problems such as unexpectedly large latencies. 我们从支持Bigtable中学到的实践经验是正确进行系统级监视的重要性（即监视Bigtable本身以及使用Bigtable的客户端进程）。例如，我们扩展了RPC系统，以便对于RPC的抽样，它可以详细记录代表该RPC进行的重要操作。此功能使我们能够检测并修复许多问题，例如Tablet数据结构上的锁争用，在提交Bigtable更改时缓慢写入GFS以及在 METADATA Tablet 不可用时卡住对 METADATA table 的访问。有用监视的另一个示例是，每个Bigtable集群都在Chubby中注册。这使我们能够跟踪所有集群，发现它们有多大，查看它们正在运行的软件版本，正在接收多少流量，以及是否存在诸如意外的长延迟之类的问题。 总结合理的系统级监控非常重要。 The most important lesson we learned is the value of simple designs. Given both the size of our system (about 100,000 lines of non-test code), as well as the fact that code evolves over time in unexpected ways, we have found that code and design clarity are of immense help in code maintenance and debugging. One example of this is our tablet-server membership protocol. Our first protocol was simple: the master periodically issued leases to tablet servers, and tablet servers killed themselves if their lease expired. Unfortunately, this protocol reduced availability significantly in the presence of network problems, and was also sensitive to master recovery time. We redesigned the protocol several times until we had a protocol that performed well. However, the resulting protocol was too complex and depended on the behavior of Chubby features that were seldom exercised by other applications. We discovered that we were spending an inordinate amount of time debugging obscure corner cases, not only in Bigtable code, but also in Chubby code. Eventually, we scrapped this protocol and moved to a newer simpler protocol that depends solely on widely-used Chubby features. 我们 学到的最重要的一课是简单设计的价值。考虑到系统的大小（大约100,000行非测试代码），以及代码会以意想不到的方式随时间变化的事实，我们 发现代码和设计的清晰性对代码维护和调试有极大的帮助。我们的Tablet服务器成员身份协议就是一个例子。我们的第一个协议很简单：主服务器定期向Tablet服务器发布租约，而Tablet服务器在租约到期时会自杀。不幸的是，该协议在存在网络问题的情况下大大降低了可用性，并且对主服务器的恢复时间也很敏感。我们多次对协议进行了重新设计，直到有了一个性能良好的协议。但是，最终的协议太复杂了，取决于其他应用程序很少使用的Chubby功能的行为。我们发现，不仅在Bigtable代码中，而且在Chubby代码中，我们花费大量时间调试晦涩难解的案例。最终，我们放弃了该协议，转而使用仅依赖于广泛使用的Chubby特性的更新的更简单协议。 总结保持设计的简洁 10 Related WorkThe Boxwood project [24] has components that overlap in some ways with Chubby, GFS, and Bigtable, since it provides for distributed agreement, locking, distributed chunk storage, and distributed B-tree storage. In each case where there is overlap, it appears that the Boxwood’s component is targeted at a somewhat lower level than the corresponding Google service. The Boxwood project’s goal is to provide infrastructure for building higher-level services such as file systems or databases, while the goal of Bigtable is to directly support client applications that wish to store data. Boxwood项目[24]具有与Chubby，GFS和Bigtable在某些方面重叠的组件，因为它提供了分布式协议，锁，分布式块存储和分布式B树存储。在每种情况下，如果出现重叠，则Boxwood的组件似乎定位在比相应Google服务更低的级别上。 Boxwood项目的目标是为构建高级服务（例如文件系统或数据库）提供基础结构，而Bigtable的目标是直接支持希望存储数据的客户端应用程序。 Many recent projects have tackled the problem of providing distributed storage or higher-level services over wide area networks, often at “Internet scale.” This includes work on distributed hash tables that began with projects such as CAN [29], Chord [32], Tapestry [37], and Pastry [30]. These systems address concerns that do not arise for Bigtable, such as highly variable bandwidth, untrusted participants, or frequent reconfiguration; decentralized control and Byzantine fault tolerance are not Bigtable goals. 许多最近的项目解决了通常在“ Internet规模”上通过广域网提供分布式存储或更高级别服务的问题。这包括以CAN [29]，Chord [32]，Tapestry [37] 和 Pastry [30] 等项目开头的分布式哈希表的工作。这些系统解决了Bigtable不会出现的问题，例如带宽可变，参与者不受信任或频繁重新配置。分散控制和拜占庭容错并不是Bigtable的目标。 In terms of the distributed data storage model that one might provide to application developers, we believe the key-value pair model provided by distributed B-trees or distributed hash tables is too limiting. Key-value pairs are a useful building block, but they should not be the only building block one provides to developers. The model we chose is richer than simple key-value pairs, and supports sparse semi-structured data. Nonetheless, it is still simple enough that it lends itself to a very efficient flat-file representation, and it is transparent enough (via locality groups) to allow our users to tune important behaviors of the system. 就可能会提供给应用程序开发人员的分布式数据存储模型而言，我们认为由分布式B树或分布式哈希表提供的键值对模型过于局限。键值对是一个有用的构建块，但它们不应成为一个唯一提供给开发人员的构建块。我们选择的模型比简单的键/值对丰富，并且支持稀疏的半结构化数据。尽管如此，它仍然非常简单，可以使其非常有效地使用 flate file 表示，并且它（通过locality group）足够透明以允许我们的用户调整系统的重要行为。 译者附flat file: n. a file consisting of records of a single record type in which there is no embedded structure information that governs relationships between records. 由单一记录类型的记录组成的文件，其中没有控制记录之间关系的嵌入式结构信息。 —— 《微软计算机词典》 Several database vendors have developed parallel databases that can store large volumes of data. Oracle’s Real Application Cluster database [27] uses shared disks to store data (Bigtable uses GFS) and a distributed lock manager (Bigtable uses Chubby). IBM’s DB2 Parallel Edition [4] is based on a shared-nothing [33] architecture similar to Bigtable. Each DB2 server is responsible for a subset of the rows in a table which it stores in a local relational database. Both products provide a complete relational model with transactions. 几个数据库供应商已经开发了可以存储大量数据的并行数据库。 Oracle 的 Real Application Cluster 数据库[27]使用共享磁盘存储数据（Bigtable使用GFS）和分布式锁管理器（Bigtable使用Chubby）。 IBM的DB2并行版[4]基于类似于Bigtable的无共享[33]架构。每个DB2服务器负责存储在本地关系数据库中的表中行的子集。两种产品都提供了完整的交易关系模型。 Bigtable locality groups realize similar compression and disk read performance benefits observed for other systems that organize data on disk using column-based rather than row-based storage, including C-Store [1][34] and commercial products such as Sybase IQ [15][36], SenSage [31], KDB+ [22], and the ColumnBM storage layer in MonetDB/X100 [38]. Another system that does vertical and horizontal data partioning into flat files and achieves good data compression ratios is AT&amp;T’s Daytona database [19]. Locality groups do not support CPUcache-level optimizations, such as those described by Ailamaki [2]. 对于使用基于列而不是基于行的存储在磁盘上组织数据的其他系统，Bigtable locality group 实现了类似的压缩和磁盘读取性能优势，包括C-Store [1][34] 和Sybase IQ [15][36]，SenSage [31]，KDB + [22] 和MonetDB / X100 [38] 中的ColumnBM存储层。 AT＆T的Daytona数据库[19]是将垂直和水平数据分成 flat file 并实现良好数据压缩率的另一个系统。 locality group 不支持 CPU 缓存级别的优化，例如 Ailamaki [2] 所描述的那些。 The manner in which Bigtable uses memtables and SSTables to store updates to tablets is analogous to the way that the Log-Structured Merge Tree [26] stores updates to index data. In both systems, sorted data is buffered in memory before being written to disk, and reads must merge data from memory and disk. Bigtable 使用 memtable 和 SSTables 将更新存储到Tablet的方式类似于 Log-Structured Merge Tree [26] 存储更新到索引数据的方式。在这两个系统中，已排序的数据在写入磁盘之前都要先在内存中进行缓冲，并且读取操作必须合并内存和磁盘中的数据。 C-Store and Bigtable share many characteristics: both systems use a shared-nothing architecture and have two different data structures, one for recent writes, and one for storing long-lived data, with a mechanism for moving data from one form to the other. The systems differ significantly in their API: C-Store behaves like a relational database, whereas Bigtable provides a lower level read and write interface and is designed to support many thousands of such operations per second per server. C-Store is also a “read-optimized relational DBMS”, whereas Bigtable provides good performance on both read-intensive and write-intensive applications. C-Store和Bigtable具有许多特征：这两个系统都使用无共享架构，并且具有两种不同的数据结构，一种用于最近的写入，一种用于存储长期存在的数据，其机制是将数据从一种形式转移到另一种形式。这些系统的API显着不同：C-Store的行为类似于关系数据库，而Bigtable提供了较低级别的读写接口，并且旨在每服务器每秒支持数千个此类操作。 C-Store也是“读取优化的关系DBMS”，而Bigtable在读取密集型和写入密集型应用程序上均提供了良好的性能。 Bigtable’s load balancer has to solve some of the same kinds of load and memory balancing problems faced by shared-nothing databases (e.g., [11][35]). Our problem is somewhat simpler: (1) we do not consider the possibility of multiple copies of the same data, possibly in alternate forms due to views or indices; (2) we let the user tell us what data belongs in memory and what data should stay on disk, rather than trying to determine this dynamically; (3) we have no complex queries to execute or optimize. Bigtable的负载平衡器必须解决无共享数据库（例如 [11][35]）面临的某些相同类型的负载和内存平衡问题。我们的问题稍微简单一些： （1）我们不考虑同一数据的多个副本的可能性，这些副本可能由于视图或索引而以其他形式出现； （2）让用户告诉我们哪些数据属于内存，哪些数据应保留在磁盘上，而不是试图动态地确定它； （3）我们没有执行或优化的复杂查询； 11 ConclusionsWe have described Bigtable, a distributed system for storing structured data at Google. Bigtable clusters have been in production use since April 2005, and we spent roughly seven person-years on design and implementation before that date. As of August 2006, more than sixty projects are using Bigtable. Our users like the performance and high availability provided by the Bigtable implementation, and that they can scale the capacity of their clusters by simply adding more machines to the system as their resource demands change over time. 我们已经介绍了Bigtable，这是一个用于在Google存储结构化数据的分布式系统。自2005年4月以来，Bigtable集群已投入生产使用，在此日期之前，我们在设计和实施上花费了大约7人年的时间。截至2006年8月，超过60个项目正在使用Bigtable。我们的用户喜欢Bigtable实施提供的性能和高可用性，他们可以通过随资源需求随时间的变化向系统中添加更多计算机，从而扩展集群的容量。 Given the unusual interface to Bigtable, an interesting question is how difficult it has been for our users to adapt to using it. New users are sometimes uncertain of how to best use the Bigtable interface, particularly if they are accustomed to using relational databases that support general-purpose transactions. Nevertheless, the fact that many Google products successfully use Bigtable demonstrates that our design works well in practice. 鉴于Bigtable具有非同寻常的界面，一个有趣的问题是，我们的用户适应使用它有多困难。新用户有时不确定如何最好地使用Bigtable接口，特别是如果他们习惯于使用支持通用事务的关系数据库时。不过，许多Google产品成功使用Bigtable的事实表明我们的设计在实践中效果很好。 We are in the process of implementing several additional Bigtable features, such as support for secondary indices and infrastructure for building cross-data-center replicated Bigtables with multiple master replicas. We have also begun deploying Bigtable as a service to product groups, so that individual groups do not need to maintain their own clusters. As our service clusters scale, we will need to deal with more resource-sharing issues within Bigtable itself [3], [5]. 我们正在实现几个其他Bigtable功能，例如对二级索引的支持以及用于构建具有多个主副本的跨数据中心复制Bigtable的基础结构。我们也已开始将Bigtable作为服务部署到产品组，以便各个组不需要维护自己的集群。随着我们服务集群的扩展，我们将需要在Bigtable自身内部处理更多的资源共享问题 [3], [5]。 Finally, we have found that there are significant advantages to building our own storage solution at Google. We have gotten a substantial amount of flexibility from designing our own data model for Bigtable. In addition, our control over Bigtable’s implementation, and the other Google infrastructure upon which Bigtable depends, means that we can remove bottlenecks and inefficiencies as they arise. 最后，我们发现在Google建立自己的存储解决方案具有明显的优势。通过为Bigtable设计我们自己的数据模型，我们获得了很大的灵活性。此外，我们对Bigtable的实施以及Bigtable依赖的其他Google基础架构的控制权意味着我们可以消除瓶颈和效率低下的情况。 AcknowledgementsWe thank the anonymous reviewers, David Nagle, and our shepherd Brad Calder, for their feedback on this paper. The Bigtable system has benefited greatly from the feedback of our many users within Google. In addition,we thank the following people for their contributions to Bigtable: Dan Aguayo, Sameer Ajmani, Zhifeng Chen, Bill Coughran, Mike Epstein, Healfdene Goguen, Robert Griesemer, Jeremy Hylton, Josh Hyman, Alex Khesin, Joanna Kulik, Alberto Lerner, Sherry Listgarten, Mike Maloney, Eduardo Pinheiro, Kathy Polizzi, Frank Yellin, and Arthur Zwiegincew. 我们感谢匿名审稿人David Nagle和我们的牧羊人Brad Calder对本文的反馈。 Bigtable系统得益于Google众多用户的反馈。 此外，我们感谢以下人员对Bigtable的贡献：Dan Aguayo，Sameer Ajmani，Zhifeng Chen，Bill Coughran，Mike Epstein，Healfdene Goguen，Robert Griesemer，Jeremy Hylton，Josh Hyman，Alex Khesin，Joanna Kulik，Alberto Lerner， Sherry Listgarten，Mike Maloney，Eduardo Pinheiro，Kathy Polizzi，Frank Yellin和Arthur Zwiegincew。 References[1] ABADI, D. J., MADDEN, S. R., AND FERREIRA, M. C. Integrating compression and execution in column oriented database systems. Proc. of SIGMOD (2006). [2] AILAMAKI, A., DEWITT, D. J., HILL, M. D., AND SKOUNAKIS, M. Weaving relations for cache performance. In The VLDB Journal (2001), pp. 169-180. [3] BANGA, G., DRUSCHEL, P., AND MOGUL, J. C. Resource containers: A new facility for resource management in server systems. In Proc. of the 3rd OSDI (Feb. 1999), pp. 45-58. [4] BARU, C. K., FECTEAU, G., GOYAL, A., HSIAO, H., JHINGRAN, A., PADMANABHAN, S., COPELAND,G. P., AND WILSON, W. G. DB2 parallel edition. IBM Systems Journal 34, 2 (1995), 292-322. [5] BAVIER, A., BOWMAN, M., CHUN, B., CULLER, D., KARLIN, S., PETERSON, L., ROSCOE, T., SPALINK, T., AND WAWRZONIAK, M. Operating system support for planetary-scale network services. In Proc. of the 1st NSDI(Mar. 2004), pp. 253-266. [6] BENTLEY, J. L., AND MCILROY, M. D. Data compression using long common strings. In Data Compression Conference (1999), pp. 287-295. [7] BLOOM, B. H. Space/time trade-offs in hash coding with allowable errors. CACM 13, 7 (1970), 422-426. [8] BURROWS, M. The Chubby lock service for loosely coupled distributed systems. In Proc. of the 7th OSDI (Nov. 2006). [9] CHANDRA, T., GRIESEMER, R., AND REDSTONE, J. Paxos made live ? An engineering perspective. In Proc. of PODC (2007).[10] COMER, D. Ubiquitous B-tree. Computing Surveys 11, 2 (June 1979), 121-137. [11] COPELAND, G. P., ALEXANDER, W., BOUGHTER, E. E., AND KELLER, T. W. Data placement in Bubba. In Proc. of SIGMOD (1988), pp. 99-108. [12] DEAN, J., AND GHEMAWAT, S. MapReduce: Simplified data processing on large clusters. In Proc. of the 6th OSDI (Dec. 2004), pp. 137-150. [13] DEWITT, D., KATZ, R., OLKEN, F., SHAPIRO, L., STONEBRAKER, M., AND WOOD, D. Implementation techniques for main memory database systems. In Proc. of SIGMOD (June 1984), pp. 1-8. [14] DEWITT, D. J., AND GRAY, J. Parallel database systems: The future of high performance database systems. CACM 35, 6 (June 1992), 85-98. [15] FRENCH, C. D. One size ts all database architectures do not work for DSS. In Proc. of SIGMOD (May 1995), pp. 449-450. [16] GAWLICK, D., AND KINKADE, D. Varieties of concurrency control in IMS/VS fast path. Database Engineering Bulletin 8, 2 (1985), 3-10. [17] GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google file system. In Proc. of the 19th ACM SOSP (Dec.2003), pp. 29-43. [18] GRAY, J. Notes on database operating systems. In Operating Systems ? An Advanced Course, vol. 60 of Lecture Notes in Computer Science. Springer-Verlag, 1978. [19] GREER, R. Daytona and the fourth-generation language Cymbal. In Proc. of SIGMOD (1999), pp. 525-526. [20] HAGMANN, R. Reimplementing the Cedar file system using logging and group commit. In Proc. of the 11th SOSP (Dec. 1987), pp. 155-162. [21] HARTMAN, J. H., AND OUSTERHOUT, J. K. The Zebra striped network file system. In Proc. of the 14th SOSP(Asheville, NC, 1993), pp. 29-43. [22] KX.COM. kx.com/products/database.php. Product page. [23] LAMPORT, L. The part-time parliament. ACM TOCS 16,2 (1998), 133-169. [24] MACCORMICK, J., MURPHY, N., NAJORK, M., THEKKATH, C. A., AND ZHOU, L. Boxwood: Abstractions as the foundation for storage infrastructure. In Proc. of the 6th OSDI (Dec. 2004), pp. 105-120. [25] MCCARTHY, J. Recursive functions of symbolic expressions and their computation by machine. CACM 3, 4 (Apr. 1960), 184-195. [26] O’NEIL, P., CHENG, E., GAWLICK, D., AND O’NEIL, E. The log-structured merge-tree (LSM-tree). Acta Inf. 33, 4 (1996), 351-385. [27] ORACLE.COM. www.oracle.com/technology/products/database/clustering/index.html. Product page. [28] PIKE, R., DORWARD, S., GRIESEMER, R., AND QUINLAN, S. Interpreting the data: Parallel analysis with Sawzall. Scientific Programming Journal 13, 4 (2005), 227-298. [29] RATNASAMY, S., FRANCIS, P., HANDLEY, M., KARP, R., AND SHENKER, S. A scalable content-addressable network. In Proc. of SIGCOMM (Aug. 2001), pp. 161-172. [30] ROWSTRON, A., AND DRUSCHEL, P. Pastry: Scalable, distributed object location and routing for largescale peer-to-peer systems. In Proc. of Middleware 2001(Nov. 2001), pp. 329-350. [31] SENSAGE.COM. sensage.com/products-sensage.htm. Product page. [32] STOICA, I., MORRIS, R., KARGER, D., KAASHOEK, M. F., AND BALAKRISHNAN, H. Chord: A scalable peer-to-peer lookup service for Internet applications. In Proc. of SIGCOMM (Aug. 2001), pp. 149-160. [33] STONEBRAKER, M. The case for shared nothing. Database Engineering Bulletin 9, 1 (Mar. 1986), 4-9. [34] STONEBRAKER,M., ABADI, D. J., BATKIN, A., CHEN, X., CHERNIACK, M., FERREIRA, M., LAU, E., LIN, A., MADDEN, S., O’NEIL, E., O’NEIL, P., RASIN, A., TRAN, N., AND ZDONIK, S. C-Store: A columnoriented DBMS. In Proc. of VLDB (Aug. 2005), pp. 553-564. [35] STONEBRAKER, M., AOKI, P. M., DEVINE, R., LITWIN, W., AND OLSON, M. A. Mariposa: A new architecture for distributed data. In Proc. of the Tenth ICDE(1994), IEEE Computer Society, pp. 54-65. [36] SYBASE.COM. www.sybase.com/products/databaseservers/sybaseiq. Product page. [37] ZHAO, B. Y., KUBIATOWICZ, J., AND JOSEPH, A. D. Tapestry: An infrastructure for fault-tolerant wide-area location and routing. Tech. Rep. UCB/CSD-01-1141, CS Division, UC Berkeley, Apr. 2001. [38] ZUKOWSKI, M., BONCZ, P. A., NES, N., AND HEMAN, S. MonetDB/X100 ?A DBMS in the CPU cache. IEEE Data Eng. Bull. 28, 2 (2005), 17-22. 翻译参考： Google Bigtable (中文版) BIGTABLE中文版论文 [译] [论文] Bigtable: A Distributed Storage System for Structured Data (OSDI 2006) 深入浅出BigTable BigTable论文阅读 论文总结BigTable 推演过程 BigTable架构 SSTable参考： Log Structured Merge Trees(LSM) 原理 SSTable 原理 Leveled Compaction · facebook/rocksdb Wiki 1.原文 ：reason about the locality properties of the data represented in the underlying storage. ↩2.原文：uninterpreted strings. ↩3.原文：whether to serve data out of memory or from disk. ↩4.原文: and memory accounting. ↩5.原文: in decreasing timestamp order. ↩6.原文: Figure 2 shows C++ code that uses a RowMutation abstraction to perform a series of updates. ↩7.原文: C++ code that uses a Scanner abstraction to iterate over all anchors in a particular row. ↩8.原文: and then reading the appropriate block from disk. ↩9.原文: The service is live. ↩10.原文: A Bigtable cluster typically operates in a shared pool of machines. ↩11.原文: we first find the appropriate block by performing a binary search in the in-memory index. ↩12.原文: without touching disk. ↩13.原文: which is elected to be the master and actively serve requests. ↩14.原文: it recursively moves up the tablet location hierarchy. ↩15.原文: live tablet sever有些人翻译为：存活的Tablet服务器，我觉得不够贴切，因为机器只要还在运行，那么我们认为它是活着的（就像生物的生命一样），但是有木有效是看机器运行过程的自身资源符不符合相关服务的规定，这个是动态的，在特定时刻的一台机器对有些服务而言是有效的，对有其他服务言可能是无效的。当然有些人觉得也可以这样解释：在某个特定时刻，针对有些服务而言，某台机器是存活的，然而对其他服务而言是已经死了（down， death），但是这样解释更像是强行与 live 的意思靠拢。 ↩16.位置属性可以这样理解，比如树状结构，具有相同前缀的数据的存放位置接近。在读取的时候，可以把这些数据一次读取出来，联想到数据的局部性原理。 ↩17.获取该网页的时间戳作为标识：即按照获取时间不同，存储了多个版本的网页数据。 ↩18.原文：cell，这里指的是table中一个cell. ↩]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>distributed system</tag>
        <tag>papers</tag>
        <tag>google</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CAP定理图示与Raft各种场景演示]]></title>
    <url>%2F2019%2F12%2F11%2Fa-brief-introduction-of-Paxos-Raft-ZAB%2F</url>
    <content type="text"><![CDATA[本文主要着重于CAP定理和raft各种场景演示 CAP 定理根据加州大学伯克利分校计算机科学家Eric Brewer说法，该定理于1998年秋季首次出现。该定理于1999年作为CAP原理发表，并由Brewer在2000 年的分布式原理研讨会上提出计算（PODC）。2002年，麻省理工学院的塞斯·吉尔伯特（Seth Gilbert ) 和 南希·林奇（Nancy Lynch）发表了布鲁尔猜想的正式证明，使之成为一个定理。 CAP定理指出分布式计算机系统不可能同时提供以下三个保证： Consistency 一致性（每次读取都会收到最新的写入或错误） Availability 可用性（保证每个请求都收到有关成功还是失败的响应），但不能保证它包含最新的写入 Partition tolerance 分区容错性（尽管任意消息丢失或系统部分出现故障，系统仍可继续运行） CAP定理证明详细查看：Gilbert和Lynch的论文，以下翻译自图解CAP定理： 分布式系统让我们考虑一个非常简单的分布式系统。我们的系统由两个服务器 $G_1$ 和 $G_2$ 组成。这两个服务器都跟踪相同的变量 $v_0$，其初始值为 $v_0$。$G_1$ 和 $G_2$ 可以相互通信，也可以与外部客户端通信。这是我们的系统的布局。 客户端可以请求从任何服务器进行写入和读取。服务器收到请求后，将执行所需的任何计算，然后响应客户端。例如，这是写的样子。 这就是读取的情况。 现在我们已经建立了系统，接下来让我们回顾一下对于系统一致性，可用性和分区容错性意味着什么。 一致性 在写操作完成之后开始的任何读操作必须返回该值，或者以后的写操作的结果 —— Gilbert，Lynch 在一致的系统中，客户端将值写入任何服务器并获得响应后，它希望从其读取的任何服务器取回该值（或更新的值）。 这是一个不一致的系统的示例。 我们的客户端写 $v_1$ 至 $G_1$ 和 $G_1$ 确认，但是当它从 $G_2$ 读取时 $G_2$，它将获取旧的数据：$v_0$。 另一方面，这是一个一致的系统的示例。 在这个系统中，在向客户端发送确认之前 $G_1$ 将其值复制到 $G_2$ 。因此，当客户端从 $G_2$读取 ，它获取 $v$ 的最新值： $v_1$ 。 可用性 系统中非故障节点收到的每个请求都必须得到响应 —— Gilbert，Lynch 在可用的系统中，如果我们的客户端向服务器发送请求并且服务器没有崩溃，则服务器最终必须响应客户端。不允许服务器忽略客户端的请求。 分区容错性 网络将被允许任意丢失从一个节点发送到另一节点的许多消息 —— Gilbert，Lynch 这意味着 $G_1$ 和 $G_2$ 互相发送的任何消息能被删除。如果所有消息都被丢弃，那么我们的系统将如下所示（注：初始值为 $v_0$ ）。 为了达到分区容错性，我们的系统必须能够在任意网络分区下正常运行。 证明现在我们已经了解了一致性，可用性和分区容错性的概念，我们可以证明一个系统不能同时拥有这三个。 对于这个矛盾，假设确实存在一个一致，可用且分区容错的系统。我们要做的第一件事是对系统进行分区。看起来像这样。 接下来，我们有客户端要求 $v_1$ 写入 $G_1$。由于我们的系统可用，因此 $G_1$ 必须响应。由于网络是分区的，因此 $G_1$ 无法将其数据复制到 $G_2$。Gilbert 和 Lynch 将此执行阶段称为 $\alpha_1$。 译者附原文：”Since the network is partitioned, however, $G_1$ cannot replicate its data to $G_2$.” 网络被分区，比如：中美海底电缆火山喷发断掉了，淘宝电缆被施工方不小心挖掉了，这时候，对于前者美国的所有服务器和中国的所有服务器都是对外可用的，国家内的服务器节点都是互通的，但是中美之间的服务器是不通的，虽然开始阶段所有各个服务器节点都是互通的，那么这时候就发生了网络分区。对于后者，国内的情况也同理，各个大区之间也有可能发生网络分区，举例：华东，华北，华南，西北，西南等等。 接下来，我们让客户端向 $G_2$ 发出读取请求。同样，由于我们的系统可用，因此 $G_2$ 必须响应。由于网络是分区的，因此 $G_2$ 无法从 $G_1$ 更新其值。返回 $v_0$。Gilbert 和 Lynch 将此执行阶段称为 $\alpha_2$。 客户端已经将 $v_1$ 写至 $G_1$ 之后，$G_2$ 返回 $v_0$ 给客户端。这是不一致的。 我们假设存在一个一致的，可用的，分区容错的系统，但是我们只是展示了存在任何此类系统执行的情况，其中该系统的行为不一致。因此，不存在这样的系统。 译者附由于发生了网络分区，此时只能在可用性和一致性之间做一个取舍，比如上文提到的海底电缆断掉，那么为了一致性，停掉中国或者美国服务器节点提供的对外服务，这时候可能所有原本请求国内的服务器节点都得转向去请求美国的服务器节点，但是这时候就降低了可用性，比如：请求的延迟。那么另外一种情况，保持中美服务器节点继续对外服务，那么可用性没有变化，但是就破坏了一致性，因为网络分区以后中美内部服务器节点行为不一致，这样给系统留下的影响（数据，及其间接衍生物：比如算法迭代更新等）是不一样的。 什么是一致性 弱一致性 最终一致性（无法实时获取最新更新的数据，但是一段时间过后，数据是一致的） DNS(Domain Name System) Gossip(Cassandra的通信协议) 优先保证AP（Availability, Partition tolerance）的 CouchDB，Cassandra，DynamoDB，Riak 强一致性 同步 Paxos Raft（multi-paxos） ZAB（multi-paxos） 定义问题 数据不能存在单个节点上，防止单点故障。 分布式系统对fault tolerance的一般解决方案是state machine replication。 本文主题是 state machine replication的共识（consensus）算法。paxos其实是一个共识算法。 系统的最终一致性，不仅需要达成共识，还会取决于客户端（client）的行为，后文将详细说明。 一致性模拟强一致性算法主从同步复制 只有主节点（master）接受客户端（client）请求。 由主节点复制日志到多个从节点（slave）。 主节点（master）等待直到所有从节点（slave）返回成功，才能向客户端返回写成功。 缺点： 任意一个节点失败（master或者某一个slave阻塞）都将导致整个集群不可用，虽然保证了强一致性（Strong Consistency），但是却大大降低了可用性（Weak Availability）。 多数派每次写都保证写入大于 N/2 个节点，每次读都保证大于 N/2 个节点中读，总共 N 个节点。 主要是为了解决主从同步复制中“所有”节点都得处于正常运转。缺点：在并发环境下，无法保证系统正确性，顺序非常重要。例如以下场景： PaxosPaxos算法是 Lesile Lamport（Latex发明者）提出的一种基于消息传递的分布式一致性算法，于1998年在《The Part-Time Parliament》论文中首次公开，使其获得2013年图灵奖。 最初的描述使用希腊的一个小岛Paxos作为比喻，描述了Paxos小岛中通过决议的流程，并以此命名这个算法，但是这个描述理解起来比较有挑战性。 为描述Paxos算法，Lamport虚拟了一个叫做Paxos的希腊小岛，这个小岛按照议会民主制的政治模式制定法律，但是没有人愿意将自己的全部时间精力放在这种事上。所以无论是议员，议长或者传递纸条的服务员都不能承诺别人需要时一定会出现，也无法承诺批准决议或者传递消息的时间。 Basic Paxos角色介绍（roles） client：系统外部角色，请求发起者。像民众。 proposer：接受client请求，向集群提出提案（proposal）。并在冲突发生时，起到冲突调节的作用。像议员，替民众提出提案。 acceptor (voter)：提案的投票和接收者，只有在达到法定人数（Quorum，一般即为 majority 多数派）时，提案者提出的提案才会最终被接受。像国会。 learner：提议接收者，backup，备份，对集群一致性没什么影响。像记录员。 2阶段（phases）的步骤在这个部分先大致过一遍流程，细节后文说明，形象化描述先把流程走通。 Phase 1a: Prepare proposer 提出一个提案，编号为N，此 N 大于这个proposer之前提出的提案编号。向所有 acceptor 请求接受。 Phase 1b: Promise 如果 N 大于acceptor之前接受的任何提案编号则接受，否则认为此提案是已经提出过的旧提案，直接拒绝。如果 promise 阶段达到了 quorum（法定人数），proposer 这一步成功否则失败。 Phase 2a: Accept promise 阶段成功以后，proposer 进一步发出accept请求，此请求包含提案编号 （N），以及提案内容（V）。 Phase 2b: Accepted 如果此acceptor在此期间没有收到任何编号大于 N 的提案否则忽略，且接受的acceptor达到法定人数，则接受此提案内容。因此在这个阶段，编号N的提案也可能失效。 所以由以上可知：跟现实生活中不太一样的地方在于acceptor（voter）不在乎提案内容，在乎提案编号。 图示流程Basic Paxos when an Acceptor fails 在下图中，有1个client，1个proposer，3个acceptor（即法定人数为3）和2个 learner（由2条垂直线表示）。该图表示第一轮成功的情况（即网络中没有进程失败）。 123456789Client Proposer Acceptor Learner | | | | | | | X--------&gt;| | | | | | Request | X---------&gt;|-&gt;|-&gt;| | | Prepare(1) | |&lt;---------X--X--X | | Promise(1,&#123;Va,Vb,Vc&#125;) | X---------&gt;|-&gt;|-&gt;| | | Accept!(1,V) | |&lt;---------X--X--X------&gt;|-&gt;| Accepted(1,V) |&lt;---------------------------------X--X Response | | | | | | | 这里 V 是最后的 {Va, Vb, Vc}. Basic Paxos when an Acceptor fails 12345678910Client Proposer Acceptor Learner | | | | | | | X--------&gt;| | | | | | Request | X---------&gt;|-&gt;|-&gt;| | | Prepare(1) | | | | ! | | !! FAIL !! | |&lt;---------X--X | | Promise(1,&#123;Va, Vb, null&#125;) | X---------&gt;|-&gt;| | | Accept!(1,V) | |&lt;---------X--X---------&gt;|-&gt;| Accepted(1,V) |&lt;---------------------------------X--X Response | | | | | | Basic Paxos when a Proposer fails 在这种情况下，proposer 在提出值之后但在达成协议之前失败。具体来说，它在Accept消息的中间失败，因此只有一个Acceptor接收到该值。此时由新的proposer（即图中的 NEW LEADER，选举出来的，怎么选举看后面详细分析）。请注意，在这种情况下有2轮（轮从上到下垂直进行）。 12345678910111213141516Client Proposer Acceptor Learner | | | | | | | X-----&gt;| | | | | | Request | X------------&gt;|-&gt;|-&gt;| | | Prepare(1) | |&lt;------------X--X--X | | Promise(1,&#123;Va, Vb, Vc&#125;) | | | | | | | | | | | | | | !! Leader fails during broadcast !! | X------------&gt;| | | | | Accept!(1,V) | ! | | | | | | | | | | | | !! NEW LEADER !! | X---------&gt;|-&gt;|-&gt;| | | Prepare(2) | |&lt;---------X--X--X | | Promise(2,&#123;V, null, null&#125;) | X---------&gt;|-&gt;|-&gt;| | | Accept!(2,V) | |&lt;---------X--X--X------&gt;|-&gt;| Accepted(2,V) |&lt;---------------------------------X--X Response | | | | | | | 新的提案人重新提出此前失败的提案，但是此时提案编号已经增大。 Basic Paxos when a redundant learner fails 在以下情况下，（冗余的）学习者之一失败，但是Basic Paxos协议仍然成功。 12345678910Client Proposer Acceptor Learner | | | | | | | X--------&gt;| | | | | | Request | X---------&gt;|-&gt;|-&gt;| | | Prepare(1) | |&lt;---------X--X--X | | Promise(1,&#123;Va,Vb,Vc&#125;) | X---------&gt;|-&gt;|-&gt;| | | Accept!(1,V) | |&lt;---------X--X--X------&gt;|-&gt;| Accepted(1,V) | | | | | | ! !! FAIL !! |&lt;---------------------------------X Response | | | | | | Basic Paxos when multiple Proposers conflict 潜在问题：多个proposer竞争地提出各自提案，比如一个proposer，假设叫 Mike，提出提案的时候，正在处理第二个阶段却被另一个叫 Tom 的 proposer，打断（即这个提案失效了），因为 Tom 提出更大提案编号的提案，然后被打断的 Mike 重新提出提案，这时刚好也打断了 Tom 提出的提案，而这时候 Tom 的提案刚好也进行到了第二阶段，然后循环反复。这个现象称为：活锁（liveness）或 dueling（竞争） 1234567891011121314151617181920212223242526Client Proposer Acceptor Learner | | | | | | | X-----&gt;| | | | | | Request | X------------&gt;|-&gt;|-&gt;| | | Prepare(1) | |&lt;------------X--X--X | | Promise(1,&#123;null,null,null&#125;) | ! | | | | | !! LEADER FAILS | | | | | | | !! NEW LEADER (knows last number was 1) | X---------&gt;|-&gt;|-&gt;| | | Prepare(2) | |&lt;---------X--X--X | | Promise(2,&#123;null,null,null&#125;) | | | | | | | | !! OLD LEADER recovers | | | | | | | | !! OLD LEADER tries 2, denied | X------------&gt;|-&gt;|-&gt;| | | Prepare(2) | |&lt;------------X--X--X | | Nack(2) | | | | | | | | !! OLD LEADER tries 3 | X------------&gt;|-&gt;|-&gt;| | | Prepare(3) | |&lt;------------X--X--X | | Promise(3,&#123;null,null,null&#125;) | | | | | | | | !! NEW LEADER proposes, denied | | X---------&gt;|-&gt;|-&gt;| | | Accept!(2,Va) | | |&lt;---------X--X--X | | Nack(3) | | | | | | | | !! NEW LEADER tries 4 | | X---------&gt;|-&gt;|-&gt;| | | Prepare(4) | | |&lt;---------X--X--X | | Promise(4,&#123;null,null,null&#125;) | | | | | | | | !! OLD LEADER proposes, denied | X------------&gt;|-&gt;|-&gt;| | | Accept!(3,Vb) | |&lt;------------X--X--X | | Nack(4) | | | | | | | | ... and so on ... Multi PaxosBasic Paxos 除了活锁（liveness），还有2轮RPC效率低下且难以实现的问题。 Leader这是新的概念，是“唯一”的proposer，所有请求都要经过此 leader。 Multi-Paxos without failures 123456789Client Proposer Acceptor Learner | | | | | | | --- First Request --- X--------&gt;| | | | | | Request | X---------&gt;|-&gt;|-&gt;| | | Prepare(N) | |&lt;---------X--X--X | | Promise(N,I,&#123;Va,Vb,Vc&#125;) | X---------&gt;|-&gt;|-&gt;| | | Accept!(N,I,V) where V = last of (Va, Vb, Vc) | |&lt;---------X--X--X------&gt;|-&gt;| Accepted(N,I,V) |&lt;---------------------------------X--X Response | | | | | | | N 表示竞选出来的第 N 任 leader， I 表示第 I 个提案 在这种情况下，新的提案过来，由于使用相同的且唯一的 leader，因此Basic Paxos中包含“Prepare”和“Promise”子阶段的阶段一都可以被跳过。 1234567Client Proposer Acceptor Learner | | | | | | | --- Following Requests --- X--------&gt;| | | | | | Request | X---------&gt;|-&gt;|-&gt;| | | Accept!(N,I+1,W) | |&lt;---------X--X--X------&gt;|-&gt;| Accepted(N,I+1,W) |&lt;---------------------------------X--X Response | | | | | | | 其实Basic Paxos中 prepare 和 promise 阶段可以认为是多个proposer在申请相应编号的提案权，所以会出现活锁（liveness），而在 Multi-Paxos 中由于只有唯一的被竞选出来的leader有提案权，所以就可以省去了阶段一。 角色精简的 Multi-Paxos Multi-Paxos的常见部署包括将 proposer，acceptor 和 learner 的角色精简为为“server”。因此，最后只有“client”和“server”。 123456789Client Servers | | | | --- First Request --- X--------&gt;| | | Request | X-&gt;|-&gt;| Prepare(N) | |&lt;-X--X Promise(N, I, &#123;Va, Vb&#125;) | X-&gt;|-&gt;| Accept!(N, I, Vn) | X&lt;&gt;X&lt;&gt;X Accepted(N, I) |&lt;--------X | | Response | | | | 角色精简且 leader 稳定的 Multi-Paxos 因此后面来的提案，就可以简化流程了。 123456Client Servers X--------&gt;| | | Request | X-&gt;|-&gt;| Accept!(N,I+1,W) | X&lt;&gt;X&lt;&gt;X Accepted(N,I+1) |&lt;--------X | | Response | | | | 这时候，可以明显看出，只要 leader 稳定，没有经常竞选 leader，那么服务器之间的请求（RPC：远程过程调用）减少了，相应效率也提高了。 Fast PaxosRaft3个子问题 Leader election Log Replication Safety 重新定义角色任意一个节点可以在不同时期扮演一下三个角色中的一个，因此这里的角色理解可以为状态（state）： Leader Follower Candidate 原理的动画解释初始时，集群中所有节点，都是 follower 状态。 如果 follower 没有收到 leader 的来信，那么他们可以成为 candidate，怎么成为candidate后文会说明。 说明： term：表示任期，表示节点处在第几任的leader管辖下） Vote Count : 投票计数。 然后，candidate 从其他节点请求投票，其他节点会用投票进行回复。 如果 candidate 从多数（majority）节点中获得选票，它将成为 leader。这个过程称为领导人选举（Leader election）。下面是这个过程的细节： 在Raft中，有两个超时设置可控制选举。首先是选举超时（election timeout）。选举超时是指 follower 成为 candidate 之前所等待的时间。选举超时被随机分配在150毫秒至300毫秒之间。选举超时后，follower 将成为 candidate，开始新的选举任期（term），对其进行投票（ballot），然后将“请求投票”消息发送给其他节点。如果接收节点在此期限内尚未投票，则它将为 candidate 投票，节点将重置其选举超时。一旦 candidate 获得多数票（majority），便成为 leader。leader 开始向其 follower 发送“添加条目”消息。这些消息将按心跳超时（heartbeat timeout）指定的时间间隔发送，然后 follower 响应每个追加条目消息。此选举任期将持续到 follower 停止接收心跳并成为 candidate 为止。 让我们停止 leader 并观察再次选举，节点B现在是第2届的 leader。需要多数表决才能保证每个任期只能选举一名leader。如果两个节点同时成为 candidate，则“可能”会发生投票表决的分裂。 让我们看一个投票表决的分裂的例子。两个节点都开始以相同的任期进行选举，并且每个节点都已经选举超时，并且先获得一个follower。现在，每个 candidate 都有2票，并且在这个任期中将无法获得更多选票。节点将等待新的选举，然后重试。节点A在第5届中获得了多数选票，因此成为 leader。 系统的所有更改现在都通过领导者。每次更改都将添加为节点日志中的条目。该日志条目当前未提交（uncommitted），因此不会更新节点的值。 要提交条目，节点首先将其复制到 follower 节点上， 然后 leader 等待，直到大多数（majority）节点都写入了条目，在这期间leader不断发送给follower心跳包，一方面确定集群中各节点是否存活，另一方面也可以知道follower是否写入了条目；现在，该条目已提交到 leader 节点上，并且节点状态为“ 5”；leader 节点然后通知 follower 节点该条目已提交（commited）；现在，集群已就系统状态达成共识（consensus），然后最后再响应给客户端表示成功。 此过程称为日志复制（Log Replication）。 面对网络分区，日志甚至可以保持一致。让我们添加一个分区以将A＆B与C，D＆E分开。由于有了这个分区，我们现在拥有两个术语不同的 leader。让我们添加另一个客户端，并尝试更新两个 leader。一个客户端将尝试将节点B的值设置为“ 3”。节点B无法复制为多数，因此其日志条目保持未提交状态。另一个客户端将尝试将节点E的值设置为“ 8”。这将成功，因为它可以复制到大多数。现在，让我们修复网络分区。节点B将看到更高的任期（term）并退出。节点A和B都将回滚其未提交的条目并匹配新 leader 的日志。现在，我们的日志在整个集群中是一致的。 这里验证上文 CAP定理中的保证了强一致性和可用性（网络分区以后，分区两端还在使用），却牺牲分区容错性，因为 “SET 3” 被删除了，没有写到集群中，所以并没有完全正确。 场景测试由以上可知保证一致性并不能代表完全正确。接下来又一个例子将会说明，并到 https://raft.github.io/ 网站进行验证。 假设集群共有5个节点，Client 写请求，leader向follower同步日志，此时急集群中有3个节点失败，2个结点存活，对于客户端得到结果有3种情况。 unknown（Timeout） （超时后）成功 client 发送给 S5 的请求刚开始，由于只有2个节点存活，因此S4，S5的第二条日志只是虚线（表示未提交），client 此时并不知道，是否请求成功。随后其他节点相继恢复服务，同步了S5的日志，最终client第二条日志写入成功。 （超时后）失败 client 发送给 S4 的请求刚开始，由于只有2个节点存活，因此S4，S5的索引为2的日志（即第二条内容为2的日志）只是虚线（表示未提交），client 此时并不知道，是否请求成功。此时如果S4，S5停止服务，但是S1，S2，S3恢复服务并且S3为leader，有client向S3发送了另一条请求，而后S4，S5恢复服务，那么此时将抹掉S4，S5的第二条日志。索引为2的日志内容统一为：4，即依然保证集群一致性。 对于client请求响应是unkown（Timeout），客户端可以跟集群配合来增强可用性，比如：重试机制，但同时带来了副作用——可能重复写入（在超时后成功却重试了）。 ZAB基本上与raft一样。不同点在于名词叫法上不同：ZAB将某一个leader的周期称为 epoch 而不是 term，实现上的不同：raft日志是连续的，心跳方向为leader-&gt;follower，而ZAB相反。 相关项目实现 Zookeeper（ZAB的实现） etcd（raft的实现） 巨人的肩膀 wiki CAP theorem an_illustrated_proof_of_the_cap_theorem The CAP Theorem | Learn Cassandra Gilbert and Lynch’s specification and proof of the CAP Theorem Raft : Understandable Distributed Consensus https://raft.github.io/]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>distributed-system</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Charpter 19 Performance Tuning]]></title>
    <url>%2F2019%2F08%2F13%2FChapter19_Performance-Tuning(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 19 Performance Tuning 性能调优Chapter 18 covered the Spark user interface (UI) and basic first-aid for your Spark Application. Using the tools outlined in that chapter, you should be able to ensure that your jobs run reliably. However, sometimes you’ll also need them to run faster or more efficiently for a variety of reasons. That’s what this chapter is about. Here, we present a discussion of some of the performance choices that are available to make your jobs run faster. 第18章介绍了Spark用户界面（UI）和 Spark应用程序的基本急救。使用这一章中概述的工具，您应该能够确保工作可靠地运行。但是，有时出于各种原因，您还需要它们更快或更高效地运行。这就是本章的内容。在这里，我们将讨论一些性能选择，这些选择可以使您的工作运行得更快。 Just as with monitoring, there are a number of different levels that you can try to tune at. For instance, if you had an extremely fast network, that would make many of your Spark jobs faster because shuffles are so often one of the costlier steps in a Spark job. Most likely, you won’t have much ability to control such things; therefore, we’re going to discuss the things you can control through code choices or configuration. 正如监视一样，您可以尝试调到许多不同的级别。例如，如果你有一个非常快速的网络，这将使你的许多 Spark 工作更快，因为 shuffle（洗牌）往往是 Spark 工作成本更高的步骤之一。最有可能的是，您没有太多的能力来控制这些事情；因此，我们将讨论通过代码选择或配置可以控制的事情 There are a variety of different parts of Spark jobs that you might want to optimize, and it’s valuable to be specific. Following are some of the areas: Spark作业有许多不同的部分，您可能希望对其进行优化，具体来说很有价值。以下是一些领域： Code-level design choices (e.g., RDDs versus DataFrames) 代码级设计选择（例如，RDD与数据帧） Data at rest 非运行时数据（未流通的数据） Joins 连接 Aggregations 聚合 Data in flight 程序运行中的数据 Individual application properties 单个应用程序属性 Inside of the Java Virtual Machine (JVM) of an executor 执行器的Java虚拟机（JVM）的内部 Worker nodes 工作节点 Cluster and deployment properties 集群和部署属性 This list is by no means exhaustive, but it does at least ground the conversation and the topics that we cover in this chapter. Additionally, there are two ways of trying to achieve the execution characteristics that we would like out of Spark jobs. We can either do so indirectly by setting configuration values or changing the runtime environment. These should improve things across Spark Applications or across Spark jobs. Alternatively, we can try to directly change execution characteristic or design choices at the individual Spark job, stage, or task level. These kinds of fixes are very specific to that one area of our application and therefore have limited overall impact. There are numerous things that lie on both sides of the indirect versus direct divide, and we will draw lines in the sand accordingly. 这个列表决不是详尽的，但它至少为我们在本章中讨论的话题打下了基础。此外，有两种方法可以尝试实现我们希望的无 Spark 作业的执行特性。我们可以通过设置配置值或更改运行时环境来间接地这样做。这些应该可以改善Spark应用程序或Spark作业的性能。或者，我们可以尝试在单个Spark作业、阶段或任务级别直接更改执行特性或设计选择。这些类型的修复程序非常特定于我们应用程序的某个领域，因此总体影响有限。间接与直接之间分水岭的两边有许多东西，我们将区分出来。 One of the best things you can do to figure out how to improve performance is to implement good monitoring and job history tracking. Without this information, it can be difficult to know whether you’re really improving job performance. 要想了解如何提高绩效，最好的方法之一就是实施良好的监控和工作历史跟踪。没有这些信息，很难知道你是否真的在提高工作绩效。 Indirect Performance Enforcement 间接的性能加强As discussed, there are a number of indirect enhancements that you can perform to help your Spark jobs run faster. We’ll skip the obvious ones like “improve your hardware” and focus more on the things within your control. 如前所述，您可以执行一些间接增强，以帮助 Spark 作业运行得更快。我们将跳过“改进硬件”这类明显的问题，更多地关注您控制范围内的事情。 Design Choices 设计选择Although good design choices seem like a somewhat obvious way to optimize performance, we often don’t prioritize this step in the process. When designing your applications, making good design choices is very important because it not only helps you to write better Spark applications but also to get them to run in a more stable and consistent manner over time and in the face of external changes or variations. We’ve already discussed some of these topics earlier in the book, but we’ll summarize some of the fundamental ones again here. 尽管良好的设计选择似乎是优化性能的一种比较明显的方法，但我们通常不会在流程中优先考虑这一步骤。在设计应用程序时，做出良好的设计选择是非常重要的，因为这不仅有助于编写更好的Spark应用程序，而且有助于使它们在一段时间内以更稳定和一致的方式运行，并在面对外部变化运行。我们已经在书的前面讨论了其中的一些主题，但是我们将在这里再次总结一些基本的主题。 Scala versus Java versus Python versus R 语言的选择This question is nearly impossible to answer in the general sense because a lot will depend on your use case. For instance, if you want to perform some single-node machine learning after performing a large ETL job, we might recommend running your Extract, Transform, and Load (ETL) code as SparkR code and then using R’s massive machine learning ecosystem to run your single-node machine learning algorithms. This gives you the best of both worlds and takes advantage of the strength of R as well as the strength of Spark without sacrifices. As we mentioned numerous times, Spark’s Structured APIs are consistent across languages in terms of speed and stability. That means that you should code with whatever language you are most comfortable using or is best suited for your use case. 这个问题几乎不可能从一般情况下回答，因为很大程度上取决于您的使用案例。例如，如果您想在执行大型ETL作业后执行一些单节点机器学习，我们可能建议运行提取、转换和加载（ETL）代码作为 SparkR 代码，然后使用R的大型机器学习生态系统来运行单节点机器学习算法。这给了你两个世界中最好的资源，利用 R 和 Spark 的强大力量且不带（性能上的）牺牲。正如我们多次提到的，Spark的结构化API在速度和稳定性方面跨语言保持一致。这意味着您应该使用最适合您使用或最适合您的用例的任何语言进行编码。 Things do get a bit more complicated when you need to include custom transformations that cannot be created in the Structured APIs. These might manifest themselves as RDD transformations or user defined functions (UDFs). If you’re going to do this, R and Python are not necessarily the best choice simply because of how this is actually executed. It’s also more difficult to provide stricter guarantees of types and manipulations when you’re defining functions that jump across languages. We find that using Python for the majority of the application, and porting some of it to Scala or writing specific UDFs in Scala as your application evolves, is a powerful technique—it allows for a nice balance between overall usability, maintainability, and performance. 当需要包含无法在结构化API中创建的自定义转换时，情况确实会变得更加复杂。这些可能表现为RDD转换或用户自定义函数（UDF）。如果要这样做，R 和 Python不一定是最佳选择，仅仅因为它实际上是如何执行的。在定义跨语言的函数时，更难对类型和操作提供更严格的保证。我们发现，对大多数应用程序使用 Python，随着应用程序的发展，将其中的一部分移植到 Scala 或者在 Scala 中编写特定的 UDF，这是一种强大的技术，它允许在总体可用性、可维护性和性能之间实现良好的平衡。 DataFrames versus SQL versus Datasets versus RDDs 不同层级的API选择This question also comes up frequently. The answer is simple. Across all languages, DataFrames, Datasets, and SQL are equivalent in speed. This means that if you’re using DataFrames in any of these languages, performance is equal. However, if you’re going to be defining UDFs, you’ll take a performance hit writing those in Python or R, and to some extent a lesser performance hit in Java and Scala. If you want to optimize for pure performance, it would behoove you to try and get back to DataFrames and SQL as quickly as possible. Although all DataFrame, SQL, and Dataset code compiles down to RDDs, Spark’s optimization engine will write “better” RDD code than you can manually and certainly do it with orders of magnitude less effort. Additionally, you will lose out on new optimizations that are added to Spark’s SQL engine every release. 这个问题也经常出现。答案很简单。在所有语言中，DataFrames、Datasets 和 SQL 在速度上是等效的。这意味着，如果您使用这些语言中的任何一种，那么性能都是相同的。但是，如果你要定义UDFs，你将在Python或R中进行性能损失，在一定程度上，Java和Scala的性能下降更少一些。如果您希望优化以获得纯粹的性能，那么应该尝试尽快返回到 DataFrames 和 SQL。尽管所有的数据框架、SQL和数据集代码都编译成RDD，但Spark的优化引擎将编写“更好”的RDD代码，这比您可以手动编写的代码要好，而且肯定要花费更少的工作量。此外，在每一个版本中，您都将失去添加到Spark的SQL引擎中的新优化。 Lastly, if you want to use RDDs, we definitely recommend using Scala or Java. If that’s not possible, we recommend that you restrict the “surface area” of RDDs in your application to the bare minimum. That’s because when Python runs RDD code, it’s serializes a lot of data to and from the Python process. This is very expensive to run over very big data and can also decrease stability. 最后，如果您想使用RDDs，我们绝对推荐使用Scala或Java。如果这不可能，我们建议您将应用程序中RDD的使用范围限制为最小值。这是因为当Python运行RDD代码时，它会在Python进程之间序列化大量数据。这是非常昂贵的运行非常大的数据，也可以降低稳定性。 Although it isn’t exactly relevant to performance tuning, it’s important to note that there are also some gaps in what functionality is supported in each of Spark’s languages. We discussed this in Chapter 16. 尽管它与性能调优并不完全相关，但需要注意的是，在Spark的每种语言中，在支持哪些功能方面也存在一些差距。我们在第16章讨论了这一点。 Shuffle Configurations 洗牌的配置Configuring Spark’s external shuffle service (discussed in Chapters 16 and 17) can often increase performance because it allows nodes to read shuffle data from remote machines even when the executors on those machines are busy (e.g., with garbage collection). This does come at the cost of complexity and maintenance, however, so it might not be worth it in your deployment. Beyond configuring this external service, there are also a number of configurations for shuffles, such as the number of concurrent connections per executor, although these usually have good defaults. In addition, for RDD-based jobs, the serialization format has a large impact on shuffle performance—always prefer Kryo over Java serialization, as described in “Object Serialization in RDDs”. 配置Spark的外部shuffle服务（在第16和17章中讨论）通常可以提高性能，因为它允许节点从远程机器读取shuffle数据，即使这些机器上的执行器很忙（例如，垃圾回收）。然而，这是以复杂性和维护为代价的，因此在您的部署中可能不值得这样做。除了配置这个外部服务之外，还有许多配置用于 shuffle，例如每个 executor 的并发连接数，尽管这些配置通常具有良好的默认值。此外，对于基于 RDD 的作业，序列化格式对 shuffle 性能的影响很大，在 Java 序列化中总是首选 Kryo，如“RDDS中的对象序列化”中所描述的。 Furthermore, for all jobs, the number of partitions of a shuffle matters. If you have too few partitions, then too few nodes will be doing work and there may be skew, but if you have too many partitions, there is an overhead to launching each one that may start to dominate. Try to aim for at least a few tens of megabytes of data per output partition in your shuffle. 此外，对于所有作业，shuffle 的分区数都很重要。如果分区太少，那么节点就太少，可能会出现数据倾斜，但是如果分区太多，那么启动每个分区的开销就会开始占主导地位。试着在shuffle时，每个输出分区至少要有几十兆的数据。 Memory Pressure and Garbage Collection 性能压力和垃圾回收During the course of running Spark jobs, the executor or driver machines may struggle to complete their tasks because of a lack of sufficient memory or “memory pressure.” This may occur when an application takes up too much memory during execution or when garbage collection runs too frequently or is slow to run as large numbers of objects are created in the JVM and subsequently garbage collected as they are no longer used. One strategy for easing this issue is to ensure that you’re using the Structured APIs as much as possible. These will not only increase the efficiency with which your Spark jobs will execute, but it will also greatly reduce memory pressure because JVM objects are never realized and Spark SQL simply performs the computation on its internal format. 在运行spark作业的过程中，由于内存不足或“内存压力”，executor 或 driver 机器可能难以完成其任务。当应用程序在执行期间占用太多内存或垃圾回收运行太频繁时，可能会发生这种情况。或者运行起来很慢，因为在JVM中创建了大量的对象，然后由于不再使用这些对象而被垃圾回收。缓解这个问题的一个策略是确保尽可能多地使用结构化API。这些不仅可以提高Spark作业的执行效率，而且还可以极大地降低内存压力，因为JVM对象从未实现，Spark SQL只是对其内部格式执行计算。 The Spark documentation includes some great pointers on tuning garbage collection for RDD andUDF based applications, and we paraphrase the following sections from that information. spark文档包含一些基于RDD和UDF的应用程序优化垃圾回收的重要建议，我们从这些信息中解释了以下部分。 Measuring the impact of garbage collection 衡量垃圾回收的影响The first step in garbage collection tuning is to gather statistics on how frequently garbage collection occurs and the amount of time it takes. You can do this by adding -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps to Spark’s JVM options using the spark.executor.extraJavaOptions configuration parameter. The next time you run your Spark job, you will see messages printed in the worker’s logs each time a garbage collection occurs. These logs will be on your cluster’s worker nodes (in the stdout files in their work directories), not in the driver. GC调优的第一步是垃圾回收发生的频率和 GC 花费的时间。你可以通过配置参数：spark.executor.extraJavaOptions 添加 -verbose：gc -XX:+ PrintGCDetails -XX:+ PrintGCTimeStamps 到 Spark 的 Java 虚拟机选项来完成。下次你运行Spark作业时，每次发生垃圾回收时都会在 worker 的日志中看到消息。请注意，这些日志将位于集群的 worker 节点上（在其工作目录中的 stdout 文件中），而不是在驱动程序上。 译者注：官网例子： 123./bin/spark-submit --name "My app" --master local[4] --conf spark.eventLog.enabled=false--conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" myApp.jar Garbage collection tuning 垃圾回收的调试To further tune garbage collection, you first need to understand some basic information about memory management in the JVM: Java heap space is divided into two regions: Young and Old. The Young generation is meant to hold short-lived objects whereas the Old generation is intended for objects with longer lifetimes. 为了进一步调整垃圾回收，首先需要了解 JVM 中内存管理的一些基本信息：Java 堆空间被分为两个区域：Young 和 Old。年轻一代的目的是持有生命周期短的对象，而老一代的目的在于生命周期较长的对象。 The Young generation is further divided into three regions: Eden, Survivor1, and Survivor2. Here’s a simplified description of the garbage collection procedure: 年轻一代被进一步划分为三个区域：Eden, Survivor1 和 Survivor2。以下是垃圾回收过程的简化描述： When Eden is full, a minor garbage collection is run on Eden and objects that are alive from Eden and Survivor1 are copied to Survivor2. 当 Eden 已满时，会在 Eden 上运行一个小量的垃圾回收，来自 Eden 和 Survivor1 的活动对象会被复制到Survivor2。 The Survivor regions are swapped. Survivor 区域交换。 If an object is old enough or if Survivor2 is full, that object is moved to Old. 如果对象足够旧，或者 Survivor2 已满，则该对象将移动到 Old。 Finally, when Old is close to full, a full garbage collection is invoked. This involves tracing through all the objects on the heap, deleting the unreferenced ones, and moving the others to fill up unused space, so it is generally the slowest garbage collection operation. 最后，当 Old 接近满的时候，将调用完全的垃圾回收 。这涉及到跟踪堆上的所有对象，删除未引用的对象，并移动其他对象以填充未使用的空间，因此它通常是最慢的垃圾回收操作。 The goal of garbage collection tuning in Spark is to ensure that only long-lived cached datasets are stored in the Old generation and that the Young generation is sufficiently sized to store all short-lived objects. This will help avoid full garbage collections to collect temporary objects created during task execution. Here are some steps that might be useful. Spark中垃圾回收调优的目标是确保只有生命周期长的缓存数据集存储在老年代中，并且年轻一代的大小足以存储所有生命周期短的对象。这将有助于避免垃圾回收，以回收在任务执行期间创建的临时对象。 以下是一些可能有用的步骤。 Gather garbage collection statistics to determine whether it is being run too often. If a full garbage collection is invoked multiple times before a task completes, it means that there isn’t enough memory available for executing tasks, so you should decrease the amount of memory Spark uses for caching (spark.memory.fraction). 收集垃圾回收的统计信息以确定它是否运行得太频繁。如果在任务完成之前多次调用完全的垃圾回收，这意味着没有足够的内存用于执行任务，因此应该减少Spark用于缓存的内存量（spark.memory.fraction）。 If there are too many minor collections but not many major garbage collections, allocating more memory for Eden would help. You can set the size of the Eden to be an over-estimate of how much memory each task will need. If the size of Eden is determined to be E, you can set the size of the Young generation using the option -Xmn=4/3*E. (The scaling up by 4/3 is to account for space used by survivor regions, as well.) 如果有太多的小量的垃圾回收，但没有太多大量的垃圾回收，为 Eden 分配更多的内存会有所帮助。您可以将 Eden 的大小设置为对每个任务需要多少内存的过度估计（有冗余空间）。如果 Eden（伊甸园）的大小确定为 E，您可以使用选项 -xmn=4/3*E 设置年轻一代的大小（放大4/3也是为了说明 survivor 区域使用的空间）。 As an example, if your task is reading data from HDFS, the amount of memory used by the task can be estimated by using the size of the data block read from HDFS. Note that the size of a decompressed block is often two or three times the size of the block. So if you want to have three or four tasks’ worth of working space, and the HDFS block size is 128 MB, we can estimate size of Eden to be $43128$ MB. 例如，如果您的任务是从 HDFS 读取数据，那么可以使用从 HDFS 读取的数据块的大小来估计任务使用的内存量。请注意，解压块的大小通常是块大小的两到三倍。因此，如果您想要三个或四个任务的工作空间，并且 HDFS块大小为 $128$ MB，我们可以估计 Eden 的大小为 $4 3 128$ MB。 Try the G1GC garbage collector with -XX:+UseG1GC. It can improve performance in some situations in which garbage collection is a bottleneck and you don’t have a way to reduce it further by sizing the generations. Note that with large executor heap sizes, it can be important to increase the G1 region size with -XX:G1HeapRegionSize . 使用 -xx:+useg1gc 尝试 G1GC 垃圾回收器。在垃圾回收是一个瓶颈的情况下，并且您没有办法通过调整代的大小进一步降低它，这样做可以提高性能。请注意，拥有大的 executor 堆大小，使用 -xx:g1HeapRegionSize 增加G1区大小可能很重要。 Monitor how the frequency and time taken by garbage collection changes with the new settings. Our experience suggests that the effect of garbage collection tuning depends on your application and the amount of memory available. There are many more tuning options described online, but at a high level, managing how frequently full garbage collection takes place can help in reducing the overhead. You can specify garbage collection tuning flags for executors by setting spark.executor.extraJavaOptions in a job’s configuration. 监视垃圾回收所用的频率和时间如何随新设置的变化而变化。我们的经验表明，垃圾回收调优的效果取决于您的应用程序和可用内存量。线上描述了更多的调优选项，但从较高的层次来说，管理完全的垃圾回收的频率有助于减少开销。通过在作业配置中设置 spark.executor.extraJavaOptions，可以为 executor 指定垃圾回收的优化标志。 Direct Performance Enhancements 直接性能增强In the previous section, we touched on some general performance enhancements that apply to all jobs. Be sure to skim the previous couple of pages before jumping to this section and the solutions here. These solutions here are intended as “band-aids” of sorts for issues with specific stages or jobs, but they require inspecting and optimizing each stage or job separately. 在前一节中，我们讨论了一些适用于所有作业的通用的性能增强。在跳到本节和这里的解决方案之前，一定要浏览前面的几页。这里的这些解决方案是针对特定阶段或工作的各种问题的“创可贴”，但它们需要分别检查和优化每个阶段或工作。 Parallelism 并行主义The first thing you should do whenever trying to speed up a specific stage is to increase the degree of parallelism. In general, we recommend having at least two or three tasks per CPU core in your cluster if the stage processes a large amount of data. You can set this via the spark.default.parallelism property as well as tuning the spark.sql.shuffle.partitions according to the number of cores in your cluster. 当您试图加速某个特定阶段时，首先应该做的是提高并行度。通常，如果阶段处理大量数据，我们建议集群中每个CPU核心至少有两到三个任务。您可以通过spark.default.parallelism属性进行设置，也可以根据集群中核心的数量来调整 spark.sql.shuffle.partitions。 Improved Filtering 改进过滤Another frequent source of performance enhancements is moving filters to the earliest part of your Spark job that you can. Sometimes, these filters can be pushed into the data sources themselves and this means that you can avoid reading and working with data that is irrelevant to your end result. Enabling partitioning and bucketing also helps achieve this. Always look to be filtering as much data as you can early on, and you’ll find that your Spark jobs will almost always run faster. 性能增强的另一个常见来源是将过滤器移动到Spark作业的最早部分。有时，这些过滤器可以被推入数据源本身，这意味着您可以避免读取和处理与最终结果无关的数据。启用分区和bucketing也有助于实现这一点。 总是尽可能早地过滤数据，你会发现你的Spark作业几乎总是运行得更快。 Repartitioning and Coalescing 重分区和联合Repartition calls can incur a shuffle. However, doing some can optimize the overall execution of a job by balancing data across the cluster, so they can be worth it. In general, you should try to shuffle the least amount of data possible. For this reason, if you’re reducing the number of overall partitions in a DataFrame or RDD, first try coalesce method, which will not perform a shuffle but rather merge partitions on the same node into one partition. The slower repartition method will also shuffle data across the network to achieve even load balancing. Repartitions can be particularly helpful when performing joins or prior to a cache call. Remember that repartitioning is not free, but it can improve overall application performance and parallelism of your jobs. 重新分区调用可能导致混乱。但是，通过在集群中平衡数据，可以优化作业的整体执行，因此它们是值得的。一般来说，您应该尽量减少数据量。因此，如果要减少 DataFrame 或 RDD 中的整体分区数，请首先尝试 coalesce 方法，它不会执行洗牌（shuffle），而是将同一节点上的分区合并到一个分区中。较慢的重新分区方法还将在网络上洗牌数据，以实现均匀的负载平衡。重新分区在执行连接或在缓存调用之前特别有用。记住，重新分区不是免费的，但它可以提高应用程序的整体性能和作业的并行性。 Custom partitioning 自定义分区器If your jobs are still slow or unstable, you might want to explore performing custom partitioning at the RDD level. This allows you to define a custom partition function that will organize the data across the cluster to a finer level of precision than is available at the DataFrame level. This is very rarely necessary, but it is an option. For more information, see Part III. 如果您的作业仍然很慢或不稳定，您可能希望探索在RDD级别执行自定义分区。这允许您定义一个自定义分区函数，该函数将跨集群组织数据，使其达到比 DataFrame 级别更高的精度级别。这是很少必要的，但它是一种选择。更多信息，见第三部分。 User-Defined Functions (UDFs) 用户定义函数（UDF）In general, avoiding UDFs is a good optimization opportunity. UDFs are expensive because they force representing data as objects in the JVM and sometimes do this multiple times per record in a query. You should try to use the Structured APIs as much as possible to perform your manipulations simply because they are going to perform the transformations in a much more efficient manner than you can do in a high-level language. There is also ongoing work to make data available to UDFs in batches, such as the Vectorized UDF extension for Python that gives your code multiple records at once using a Pandas data frame. We discussed UDFs and their costs in Chapter 18. 一般来说，避免UDF是一个很好的优化机会。UDF之所以昂贵，是因为它们强制将数据表示为JVM中的对象，并且有时在查询中对每条记录执行多次这样的操作。您应该尽可能多地使用结构化API来执行操作，因为它们将以比高级语言更高效的方式执行转换。还有一些正在进行的工作是批量向UDF提供数据，例如针对Python的矢量化UDF扩展，它使用PANDAS数据帧一次为代码提供多个记录。我们在第18章讨论了UDF及其成本。 Temporary Data Storage (Caching) 临时数据存储In applications that reuse the same datasets over and over, one of the most useful optimizations is caching. Caching will place a DataFrame, table, or RDD into temporary storage (either memory or disk) across the executors in your cluster, and make subsequent reads faster. Although caching might sound like something we should do all the time, it’s not always a good thing to do. That’s because caching data incurs a serialization, deserialization, and storage cost. For example, if you are only going to process a dataset once (in a later transformation), caching it will only slow you down. The use case for caching is simple: as you work with data in Spark, either within an interactive session or a standalone application, you will often want to reuse a certain dataset (e.g., a DataFrame or RDD). For example, in an interactive data science session, you might load and clean your data and then reuse it to try multiple statistical models. Or in a standalone application, you might run an iterative algorithm that reuses the same dataset. You can tell Spark to cache a dataset using the cache method on DataFrames or RDDs. 在反复重用相同数据集的应用程序中，最有用的优化之一是缓存。缓存将把一个DataFrame、表或RDD放到集群中执行器的临时存储器（内存或磁盘）中，并使后续的读取速度更快。尽管缓存听起来像是我们一直应该做的事情，但这并不总是一件好事。这是因为缓存数据会导致序列化、反序列化和存储成本。 例如，如果您只处理一次数据集（在以后的转换中），缓存它只会减慢您的速度。缓存的用例很简单：当您在Spark中处理数据时，无论是在交互会话中还是在独立的应用程序中，您通常都希望重用某个数据集（例如，DataFrame 或 RDD）。 例如，在交互式数据科学会话中，您可以加载和清理数据，然后重新使用它来尝试多个统计模型。或者在独立的应用程序中，您可以运行一个重复使用相同数据集的迭代算法。您可以告诉Spark在 DataFrame 或 RDD 上使用cache方法缓存数据集。 Caching is a lazy operation, meaning that things will be cached only as they are accessed. The RDD API and the Structured API differ in how they actually perform caching, so let’s review the gory details before going over the storage levels. When we cache an RDD, we cache the actual, physical data (i.e., the bits). The bits. When this data is accessed again, Spark returns the proper data. This is done through the RDD reference. However, in the Structured API, caching is done based on the physical plan. This means that we effectively store the physical plan as our key (as opposed to the object reference) and perform a lookup prior to the execution of a Structured job. This can cause confusion because sometimes you might be expecting to access raw data but because someone else already cached the data, you’re actually accessing their cached version. Keep that in mind when using this feature. 缓存是一个懒惰的操作，这意味着只有数据被访问时才会对其进行缓存。RDD API和结构化API在实际执行缓存的方式上有所不同， 因此，在讨论存储级别之前，让我们先回顾一下详细信息。当我们缓存一个RDD时，我们缓存实际的物理数据（也就是：比特）。当再次访问此数据时，spark返回正确的数据。这是通过RDD引用完成的。但是，在结构化API中，缓存是基于物理计划完成的。这意味着我们有效地将物理计划存储为键（而不是对象引用），并在执行结构化作业之前执行查找。这可能会导致混淆，因为有时您可能希望访问原始数据，但因为其他人已经缓存了数据，所以实际上您正在访问他们的缓存版本。使用此功能时请记住这一点。 There are different storage levels that you can use to cache your data, specifying what type of storage to use. Table 19-1 lists the levels. 您可以使用不同的存储级别来缓存数据，指定要使用的存储类型。表19-1列出了各等级。 Table 19-1. Data cache storage levels 数据缓存级别 Storage level Meaning Meaning MEMORY_ONLY Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they’re needed. This is the default level. 将RDD存储为JVM中的反序列化Java对象。如果RDD不在内存中，那么某些分区将不会被缓存，并且将在每次需要时即时重新计算。这是默认级别。 MEMORY_AND_DISK Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don’t fit on disk, and read them from there when they’re needed.将RDD存储为JVM中的反序列化Java对象。如果RDD不适合内存，请将不适合的分区存储在磁盘上，并在需要时从磁盘上读取它们。 MEMORY_ONLY_SER(Java and Scala) Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.将RDD存储为序列化的Java对象（每个分区的一个字节数组）。这通常比反序列化对象更节省空间，尤其是在使用快速序列化程序时，但读取时CPU占用更大。 DISK_ONLY Store the RDD partitions only on disk.仅将RDD分区存储在磁盘上。 MEMORY_ONLY_2,MEMORY_AND_DISK_2,etc. Same as the previous levels, but replicate each partition on two cluster nodes.与前面的级别相同，但在两个集群节点上复制每个分区。 OFF_HEAP(experimental) Similar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to be enabled.类似于只存储数据，但将数据存储在堆外内存中。这需要启用堆外内存。 For more information on these options, take a look at “Configuring Memory Management”. 有关这些选项的详细信息，请参阅“配置内存管理”。 Figure 19-1 presents a simple illustrations of the process. 图19-1给出了该过程的简单说明。 We load an initial DataFrame from a CSV file and then derive some new DataFrames from it using transformations. We can avoid having to recompute the original DataFrame (i.e., load and parse the CSV file) many times by adding a line to cache it along the way. 我们从csv文件加载一个初始数据帧，然后使用转换从中派生一些新的数据帧。我们可以避免多次重新计算原始数据帧（即加载和解析csv文件），方法是添加一行缓存它。 Now let’s walk through the code: 现在让我们完整地学习一个代码： 123456789# in Python# Original loading code that does *not* cache DataFrameDF1 = spark.read.format("csv")\.option("inferSchema", "true")\.option("header", "true")\.load("/data/flight-data/csv/2015-summary.csv")DF2 = DF1.groupBy("DEST_COUNTRY_NAME").count().collect()DF3 = DF1.groupBy("ORIGIN_COUNTRY_NAME").count().collect()DF4 = DF1.groupBy("count").count().collect() You’ll see here that we have our “lazily” created DataFrame (DF1), along with three other DataFrames that access data in DF1. All of our downstream DataFrames share that common parent (DF1) and will repeat the same work when we perform the preceding code. In this case, it’s just reading and parsing the raw CSV data, but that can be a fairly intensive process, especially for large datasets. 在这里，您将看到我们的“惰性”创建的数据帧（df1），以及其他三个访问df1中数据的数据帧。我们所有的下游数据帧都共享这个公共父级（DF1），并且在执行前面的代码时将重复相同的工作。在这种情况下，它只是读取和解析原始的csv数据，但这可能是一个相当密集的过程，特别是对于大型数据集。 On my machine, those commands take a second or two to run. Luckily caching can help speed things up. When we ask for a DataFrame to be cached, Spark will save the data in memory or on disk the first time it computes it. Then, when any other queries come along, they’ll just refer to the one stored in memory as opposed to the original file. You do this using the DataFrame’s cache method: 在我的机器上，这些命令需要一两秒钟才能运行。幸运的是，缓存可以帮助加快速度。当我们请求缓存一个 DataFrame 时，spark将在第一次计算数据时将数据保存在内存或磁盘上。然后，当出现任何其他查询时，它们只引用存储在内存中的查询，而不是原始文件。使用 DataFrame 的缓存方法执行此操作： 12DF1.cache()DF1.count() We used the count above to eagerly cache the data (basically perform an action to force Spark to store it in memory), because caching itself is lazy—the data is cached only on the first time you run an action on the DataFrame. Now that the data is cached, the previous commands will be faster, as we can see by running the following code: 我们使用上面的计数来急切地缓存数据（基本上是执行一个操作来强制 spark 将其存储在内存中），因为缓存本身是懒惰的，数据只在您第一次在 DataFrame 上运行一个操作时缓存。现在缓存了数据，前面的命令将更快，我们可以通过运行以下代码看到这一点： 1234# in PythonDF2 = DF1.groupBy("DEST_COUNTRY_NAME").count().collect()DF3 = DF1.groupBy("ORIGIN_COUNTRY_NAME").count().collect()DF4 = DF1.groupBy("count").count().collect() When we ran this code, it cut the time by more than half! This might not seem that wild, but picture a large dataset or one that requires a lot of computation to create (not just reading in a file). The savings can be immense. It’s also great for iterative machine learning workloads because they’ll often need to access the same data a number of times, which we’ll see shortly. 当我们运行这个代码时，它将时间缩短了一半以上！这看起来并不是那么疯狂，但想象一下一个大型数据集或需要大量计算才能创建的数据集（不仅仅是在文件中读取）。节省的钱可能是巨大的。这对于迭代机器学习工作负载也很好，因为它们通常需要多次访问相同的数据，稍后我们将看到。 The cache command in Spark always places data in memory by default, caching only part of the dataset if the cluster’s total memory is full. For more control, there is also a persist method that takes a StorageLevel object to specify where to cache the data: in memory, on disk, or both. Spark中的cache命令在默认情况下总是将数据放在内存中，如果集群的总内存已满，则只缓存数据集的一部分。为了获得更多的控制权，还有一个持久化方法，它使用一个 StorageLevel 对象来指定数据的缓存位置：内存中、磁盘上，或者两者兼而有之。 Joins 连接Joins are a common area for optimization. The biggest weapon you have when it comes to optimizing joins is simply educating yourself about what each join does and how it’s performed. This will help you the most. Additionally, equi-joins are the easiest for Spark to optimize at this point and therefore should be preferred wherever possible. Beyond that, simple things like trying to use the filtering ability of inner joins by changing join ordering can yield large speedups. Additionally, using broadcast join hints can help Spark make intelligent planning decisions when it comes to creating query plans, as described in Chapter 8. Avoiding Cartesian joins or even full outer joins is often low-hanging fruit for stability and optimizations because these can often be optimized into different filtering style joins when you look at the entire data flow instead of just that one particular job area. 连接是优化的一个常见领域。在优化连接时，您拥有的最大武器就是简单地向自己介绍每个连接的作用和执行方式。这对你的帮助最大。此外，equi-joins 对于spark来说是最容易在这一点上进行优化的，因此在可能的情况下应首选。除此之外，通过改变连接顺序来尝试使用内部连接的过滤能力等简单的事情可以产生很大的加速。此外，使用广播连接提示可以帮助Spark在创建查询计划时做出智能规划决策，如第8章所述。避免笛卡尔连接，甚至是完全的外部连接，对于稳定性和优化来说通常都是容易获得的成果，因为当您查看整个数据流而不仅仅是一个特定的工作区域时，这些连接常常可以优化为不同的过滤类型的连接。 Lastly, following some of the other sections in this chapter can have a significant effect on joins. For example, collecting statistics on tables prior to a join will help Spark make intelligent join decisions. Additionally, bucketing your data appropriately can also help Spark avoid large shuffles when joins are performed. 最后，遵循本章中的一些其他部分可以对连接产生显著的影响。例如，在连接之前收集表的统计信息将有助于Spark 做出智能连接决策。此外，适当地将数据进行分桶还可以帮助 Spark 在执行连接时避免大的洗牌（shuffle）。 Aggregations 聚合For the most part, there are not too many ways that you can optimize specific aggregations beyond filtering data before the aggregation having a sufficiently high number of partitions. However, if you’re using RDDs, controlling exactly how these aggregations are performed ( e.g., using reduceByKeywhen possible over groupByKey ) can be very helpful and improve the speed and stability of your code. 在大多数情况下，除了在聚合具有足够多的分区之前过滤数据之外，没有太多方法可以优化特定聚合。但是，如果您使用的是 RDD，那么准确地控制这些聚合的执行方式（例如，在可能的情况下使用 reduceByKey 而不是groupByKey）会非常有帮助，并且可以提高代码的速度和稳定性。 Broadcast Variables 广播变量We touched on broadcast joins and variables in previous chapters, and these are a good option for optimization. The basic premise is that if some large piece of data will be used across multiple UDF calls in your program, you can broadcast it to save just a single read-only copy on each node and avoid re-sending this data with each job. For example, broadcast variables may be useful to save a lookup table or a machine learning model. You can also broadcast arbitrary objects by creating broadcast variables using your SparkContext, and then simply refer to those variables in your tasks, as we discussed in Chapter 14. 我们在前面的章节中讨论了广播连接和变量，这些是一个很好的优化选择。基本前提是，如果在程序中的多个UDF调用之间使用一些大的数据块，您可以广播它以在每个节点上只保存一个只读副本，并避免在每个作业中重新发送这些数据。例如，广播变量对于保存查找表或机器学习模型可能很有用。您还可以通过使用 SparkContext 创建广播变量来广播任意对象，然后在任务中简单地引用这些变量，如我们在第14章中所讨论的。 Conclusion 结论There are many different ways to optimize the performance of your Spark Applications and make them run faster and at a lower cost. In general, the main things you’ll want to prioritize are (1) reading as little data as possible through partitioning and efficient binary formats, (2) making sure there is sufficient parallellism and no data skew on the cluster using partitioning, and (3) using high-level APIs such as the Structured APIs as much as possible to take already optimized code. As with any other software optimization work, you should also make sure you are optimizing the right operations for your job: the Spark monitoring tools described in Chapter 18 will let you see which stages are taking the longest time and focus your efforts on those. Once you have identified the work that you believe can be optimized, the tools in this chapter will cover the most important performance optimization opportunities for the majority of users. 有许多不同的方法来优化Spark应用程序的性能，使其以更低的成本更快地运行。一般来说，您要优先考虑的主要事情是 （1）通过分区和有效的二进制格式读取尽可能少的数据 （2）确保在使用分区的集群上有足够的并行性和没有数据倾斜 （3）尽可能多地使用 high-level APIs去采用已经优化过的代码，例如：结构化的API 与任何其他软件优化工作一样，您还应该确保为您的工作优化了正确的操作：第18章中描述的 Spark 监控工具将让您了解哪些阶段花费的时间最长，并将您的精力集中在这些阶段上。一旦确定了您认为可以优化的工作，本章中的工具将为大多数用户提供最重要的性能优化机会。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter18_Monitoring-and-Debugging]]></title>
    <url>%2F2019%2F08%2F10%2FChapter18_Monitoring-and-Debugging(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 18 Monitoring and Debugging 监控与调试This chapter covers the key details you need to monitor and debug your Spark Applications. To do this, we will walk through the Spark UI with an example query designed to help you understand how to trace your own jobs through the execution life cycle. The example we’ll look at will also help you understand how to debug your jobs and where errors are likely to occur. 本章介绍了监视和调试Spark应用程序所需的关键详细信息。为此，我们将使用一个示例查询来浏览Spark UI，该查询旨在帮助您了解如何在执行生命周期中跟踪自己的作业。我们将看到的示例还将帮助您了解如何调试作业以及可能发生错误的位置。 The Monitoring Landscape 监控的宏观图At some point, you’ll need to monitor your Spark jobs to understand where issues are occuring in them. It’s worth reviewing the different things that we can actually monitor and outlining some of the options for doing so. Let’s review the components we can monitor (see Figure 18-1). 在某些时候，您需要监视您的Spark作业，以了解其中发生问题的位置。值得回顾一下我们可以实际监控的不同内容，并概述了这样做的一些选项。让我们回顾一下我们可以监控的组件（参见图18-1，在下文）。 Spark Applications and Jobs Spark应用程序和作业 The first thing you’ll want to begin monitoring when either debugging or just understanding better how your application executes against the cluster is the Spark UI and the Spark logs. These report information about the applications currently running at the level of concepts in Spark, such as RDDs and query plans. We talk in detail about how to use these Spark monitoring tools throughout this chapter. 在调试或只是更好地了解在集群背景下应用程序执行的方式时，您首先想要开始监视的是Spark UI和Spark日志。这些报告有关当前在Spark中概念级别运行的应用程序的信息，例如RDD和查询计划。我们将在本章中详细讨论如何使用这些Spark监视工具。 JVM Spark runs the executors in individual Java Virtual Machines (JVMs). Therefore, the next level of detail would be to monitor the individual virtual machines (VMs) to better understand how your code is running. JVM utilities such as jstack for providing stack traces, jmap for creating heapdumps, jstat for reporting time–series statistics, and jconsole for visually exploring various JVM properties are useful for those comfortable with JVM internals. You can also use a tool like jvisualvm to help profile Spark jobs. Some of this information is provided in the Spark UI, but for very low-level debugging, the aforementioned tools can come in handy. Spark 在各个 Java 虚拟机（JVM）中运行执行程序（executor）。因此，下一级详细信息将是监视单个虚拟机（VM）以更好地了解代码的运行方式。 JVM实用程序（如用于提供堆栈跟踪的 jstack ，用于创建 heapdumps 的 jmap，用于报告时间序列统计信息的 jstat）以及用于可视化地探索各种 JVM 属性的 jconsole， 这些对于那些熟悉 JVM 内部的人来说非常有用。您还可以使用 jvisualvm 之类的工具来帮助分析 Spark 作业。其中一些信息在 Spark UI 中提供，但对于非常低级的调试，上述工具可以派上用场。 OS/Machine The JVMs run on a host operating system (OS) and it’s important to monitor the state of those machines ensure that they are healthy. This includes monitoring things like CPU, network, and I/O. These are often reported in cluster-level monitoring solutions; however, there are more specific tools that you can use, including dstat, iostat, and iotop. JVM 在主机操作系统（OS）上运行，监控这些机器的状态以确保它们是健康的非常重要。这包括监视 CPU，网络和 I/O 等内容。这些通常在集群级监控解决方案中报告；但是，您可以使用更多特定工具，包括 dstat，iostat 和iotop。 Cluster Naturally, you can monitor the cluster on which your Spark Application(s) will run. This might be a YARN, Mesos, or standalone cluster. Usually it’s important to have some sort of monitoring solution here because, somewhat obviously, if your cluster is not working, you should probably know pretty quickly. Some popular cluster-level monitoring tools include Ganglia and Prometheus. 当然，您可以监视运行 Spark 应用程序的集群。这可能是 YARN，Mesos 或独立集群。通常，在这里使用某种监控解决方案很重要，因为很明显，如果您的集群不工作，您应该很快就会知道。一些流行的集群级监控工具包括Ganglia 和 Prometheus。 What to Monitor 要监控什么After that brief tour of the monitoring landscape, let’s discuss how we can go about monitoring and debugging our Spark Applications. There are two main things you will want to monitor: the processes running your application (at the level of CPU usage, memory usage, etc.), and the query execution inside it (e.g., jobs and tasks). 在简要介绍了监控环境之后，让我们讨论如何监控和调试我们的 Spark 应用程序。您需要监视两个主要内容：运行应用程序的进程（在 CPU 使用情况，内存使用情况的等级）以及在其中执行的查询（例如，作业和任务）。 Driver and Executor Processes 驱动程序和执行程序进程When you’re monitoring a Spark application, you’re definitely going to want to keep an eye on the driver. This is where all of the state of your application lives, and you’ll need to be sure it’s running in a stable manner. If you could monitor only one machine or a single JVM, it would definitely be the driver. With that being said, understanding the state of the executors is also extremely important for monitoring individual Spark jobs. To help with this challenge, Spark has a configurable metrics system based on the Dropwizard Metrics Library. The metrics system is configured via a configuration file that Spark expects to be present at $SPARK_HOME/conf/metrics.properties. A custom file location can be specified by changing the spark.metrics.conf configuration property. These metrics can be output to a variety of different sinks, including cluster monitoring solutions like Ganglia. 当您监控Spark应用程序时，您肯定会想要关注驱动程序（driver）。这是您的应用程序的所有状态所在的位置，您需要确保它以稳定的方式运行。如果您只能监控一台机器或一台 JVM，那肯定是驱动程序（driver）。话虽如此，了解执行程序（executor）的状态对于监视各个 Spark 作业也非常重要。为了应对这一挑战，Spark 拥有一个基于 Dropwizard Metrics 库的可配置衡量系统。衡量系统通过 Spark 预期出现在 $SPARK_HOME/conf/metrics.properties 中的配置文件进行配置。可以通过更改 spark.metrics.conf 配置属性来指定自定义文件位置。这些指标可以输出到各种不同的接收器，包括像 Ganglia 这样的集群监控解决方案。 Queries, Jobs, Stages, and Tasks 查询，作业，阶段和任务Although the driver and executor processes are important to monitor, sometimes you need to debug what’s going on at the level of a specific query. Spark provides the ability to dive into queries, jobs, stages, and tasks. (We learned about these in Chapter 15.) This information allows you to know exactly what’s running on the cluster at a given time. When looking for performance tuning or debugging, this is where you are most likely to start. Now that we know what we want to monitor, let’s look at the two most common ways of doing so: the Spark logs and the Spark UI. 虽然驱动程序（driver）和执行程序（executor）进程对于监视很重要，但有时您需要调试特定查询级别的进程。 Spark 提供了深入查询，工作，阶段和任务的能力。 （我们在第15章中了解了这些内容。）此信息可让您准确了解在给定时间情况下集群上正在运行的内容。在寻找性能调优或调试时，这是您最有可能开始的地方。现在我们知道了我们想要监控的内容，让我们看看这两种最常见的方式：Spark 日志和 Spark UI 。 Spark Logs Spark日志One of the most detailed ways to monitor Spark is through its log files. Naturally, strange events in Spark’s logs, or in the logging that you added to your Spark Application, can help you take note of exactly where jobs are failing or what is causing that failure. If you use the application template provided with the book, the logging framework we set up in the template will allow your application logs to show up along Spark’s own logs, making them very easy to correlate. One challenge, however, is that Python won’t be able to integrate directly with Spark’s Java-based logging library. Using Python’s logging module or even simple print statements will still print the results to standard error, however, and make them easy to find. 监视 Spark 的最详细方法之一是通过其日志文件。当然，Spark 的日志中或您添加到 Spark 应用程序的日志记录中的奇怪事件可以帮助您准确记录作业失败的原因或导致失败的原因。如果您使用本书提供的应用程序模板，我们在模板中设置的日志记录框架将允许您的应用程序日志显示在 Spark 自己的日志中，使它们非常容易关联。然而，一个挑战是 Python 无法直接与 Spark 的基于 Java 的日志库集成。但是，使用 Python 的日志记录模块甚至简单的打印语句仍然会将结果打印到标准错误，并使它们易于查找。 To change Spark’s log level, simply run the following command : 要更改 Spark 的日志级别，只需运行以下命令 : spark.sparkContext.setLogLevel(&quot;INFO&quot;) This will allow you to read the logs, and if you use our application template, you can log your own relevant information along with these logs, allowing you to inspect both your own application and Spark. The logs themselves will be printed to standard error when running a local mode application, or saved to files by your cluster manager when running Spark on a cluster. Refer to each cluster manager’s documentation about how to find them—typically, they are available through the cluster manager’s web UI. 这将允许您阅读日志，如果您使用我们的应用程序模板，您可以记录您自己的相关信息以及这些日志，允许您检查自己的应用程序和 Spark。运行本地模式应用程序时，日志本身将打印为标准错误，或者在集群上运行 Spark 时由集群管理器保存到文件。请参阅每个集群管理器的文档，了解如何查找它们——通常，它们可通过集群管理器的Web UI 获得。 You won’t always find the answer you need simply by searching logs, but it can help you pinpoint the given problem that you’re encountering and possibly add new log statements in your application to better understand it. It’s also convenient to collect logs over time in order to reference them in the future. For instance, if your application crashes, you’ll want to debug why, without access to the now crashed application. You may also want to ship logs off the machine they were written on to hold onto them if a machine crashes or gets shut down (e.g., if running in the cloud). 您不会总是通过搜索日志找到所需的答案，但它可以帮助您查明您遇到的给定问题，并可能在您的应用程序中添加新的日志语句以更好地理解它。随着时间的推移收集日志以便将来引用它们也很方便。例如，如果您的应用程序崩溃，您将需要调试原因，而无需访问现在崩溃的应用程序。如果计算机崩溃或关闭（例如，如果在云中运行），您可能还希望将日志从他们写入的计算机上发送到其上。 The Spark UIThe Spark UI provides a visual way to monitor applications while they are running as well as metrics about your Spark workload, at the Spark and JVM level. Every SparkContext running launches a web UI, by default on port 4040, that displays useful information about the application. When you run Spark in local mode, for example, just navigate to http://localhost:4040 to see the UI when running a Spark Application on your local machine. If you’re running multiple applications, they will launch web UIs on increasing port numbers (4041, 4042, …). Cluster managers will also link to each application’s web UI from their own UI. Spark UI 提供了一种可视化方式，用于在运行时监视应用程序，以及 Spark 和 JVM 级别的 Spark 工作负载指标。每个运行的 SparkContext 都会在端口 4040 上默认启动 Web UI，该UI显示有关应用程序的有用信息。例如，在本地模式下运行 Spark 时，只需导航到 http://localhost:4040，即可在本地计算机上运行 Spark 应用程序时查看UI 。如果您正在运行多个应用程序，他们将在增加端口号（4041,4042，…）时启动Web UI。集群管理器还将从其自己的 UI 链接到每个应用程序的 Web UI。 Figure 18-2 shows all of the tabs available in the Spark UI. 图18-2 显示了 Spark UI 中可用的所有选项卡。 These tabs are accessible for each of the things that we’d like to monitor. For the most part, each of these should be self-explanatory : 这些选项卡可供我们要监控的每个事项访问。在大多数情况下，每一个都应该是不言自明的： The Jobs tab refers to Spark jobs. “作业”选项卡指的是Spark作业。 The Stages tab pertains to individual stages (and their relevant tasks). 阶段选项卡适用于各个阶段（及其相关任务）。 The Storage tab includes information and the data that is currently cached in our Spark Application. “存储”选项卡包含当前在我们的 Spark 应用程序中缓存的信息和数据。 The Environment tab contains relevant information about the configurations and current settings of the Spark application. “环境”选项卡包含有关 Spark 应用程序的配置和当前设置的相关信息。 The SQL tab refers to our Structured API queries (including SQL and DataFrames). SQL选项卡引用我们的结构化API查询（包括 SQL 和 DataFrames）。 The Executors tab provides detailed information about each executor running our application. Executors选项卡提供有关运行我们的应用程序的每个执行程序（executor）的详细信息。 Let’s walk through an example of how you can drill down into a given query. Open a new Spark shell, run the following code, and we will trace its execution through the Spark UI: 让我们来看一个如何深入查看给定查询的示例。打开一个新的 Spark shell，运行以下代码，我们将通过 Spark UI 跟踪它的执行： 123456789# in Pythonspark.read\.option("header", "true")\.csv("/data/retail-data/all/online-retail-dataset.csv")\.repartition(2)\.selectExpr("instr(Description, 'GLASS') &gt;= 1 as is_glass")\.groupBy("is_glass")\.count()\.collect() This results in three rows of various values. The code kicks off a SQL query, so let’s navigate to the SQL tab, where you should see something similar to Figure 18-3. 这导致不同值的三行。 代码开始启动 SQL 查询，所以让我们导航到 SQL 选项卡，在那里你应该看到类似于图 18-3 的内容。 The first thing you see is aggregate statistics about this query: 您看到的第一件事是关于此查询的汇总统计信息： Submitted Time: 2017/04/08 16:24:41Duration: 2 sSucceeded Jobs: 2 These will become important in a minute, but first let’s take a look at the Directed Acyclic Graph (DAG) of Spark stages. Each blue box in these tabs represent a stage of Spark tasks. The entire group of these stages represent our Spark job. Let’s take a look at each stage in detail so that we can better understand what is going on at each level, starting with Figure 18-4. 这些将马上变得重要，但首先让我们来看看 Spark 阶段的有向无环图（DAG）。 这些选项卡中的每个蓝色框表示 Spark 任务的一个阶段。 这些阶段的整个组代表我们的 Spark 工作。 让我们详细了解每个阶段，以便我们可以更好地了解每个级别的情况，从图 18-4 开始。 The box on top, labeled WholeStateCodegen, represents a full scan of the CSV file. The box below that represents a shuffle that we forced when we called repartition. This turned our original dataset (of a yet to be specified number of partitions) into two partitions. 标记为 WholeStateCodegen 的顶部框表示 CSV 文件的完整扫描。 下面的框表示我们在调用重新分区时强制进行的随机洗牌(shuffle)。 这将我们的原始数据集（尚未指定的分区数）转换为两个分区。 The next step is our projection (selecting/adding/filtering columns) and the aggregation. Notice that in Figure 18-5 the number of output rows is six. This conveniently lines up with the number of output rows multiplied by the number of partitions at aggregation time. This is because Spark performs an aggregation for each partition (in this case a hash-based aggregation) before shuffling the data around in preparation for the final stage. 下一步是我们的投影（选择/添加/过滤列）和聚合。 请注意，在图18-5中，输出行数为6（final output * 2 = 6）。 这方便与输出行的数量乘以聚合时的分区数对齐。 这是因为 Spark 在为最终阶段做准备之前对数据进行洗牌(shuffle)之前，为每个分区执行聚合（在这种情况下是基于散列（hash-based） 的聚合）。 The last stage is the aggregation of the subaggregations that we saw happen on a per-partition basis in the previous stage. We combine those two partitions in the final three rows that are the output of our total query (Figure 18-6)。 最后一个阶段是我们在前一阶段基于每个分区发生的子聚合的聚合。 我们将这两个分区组合在最后三行中，这三行是我们总查询的输出（图18-6）。 Let’s look further into the job’s execution. On the Jobs tab, next to Succeeded Jobs, click 2. As Figure 18-7 demonstrates, our job breaks down into three stages (which corresponds to what we saw on the SQL tab). 让我们进一步了解作业的执行情况。 在 Jobs 选项卡上，单击 Succeeded Jobs，单击2. 如图18-7所示，我们的工作分为三个阶段（与我们在SQL选项卡上看到的相对应）。 These stages have more or less the same information as what’s shown in Figure 18-6, but clicking the label for one of them will show the details for a given stage. In this example, three stages ran, with eight, two, and then two hundred tasks each. Before diving into the stage detail, let’s review why this is the case. 这些阶段或多或少与图18-6中显示的信息相同，但单击其中一个阶段的标签将显示给定阶段的详细信息。 在这个例子中，运行了三个阶段，每个阶段分别有八个，两个，然后是两百个任务。 在深入了解阶段细节之前，让我们回顾一下为什么会这样。 The first stage has eight tasks. CSV files are splittable, and Spark broke up the work to be distributed relatively evenly between the different cores on the machine. This happens at the cluster level and points to an important optimization: how you store your files. The following stage has two tasks because we explicitly called a repartition to move the data into two partitions. The last stage has 200 tasks because the default shuffle partitions value is 200. 第一阶段有八个任务。 CSV 文件是可拆分的，Spark 分解了工作使其相对均匀分布在机器上不同核心之间。 这发生在集群级别，并指向一个重要的优化：如何存储文件。 下一个阶段有两个任务，因为我们显式调用了重新分区以将数据移动到两个分区中。 最后一个阶段有200个任务，因为默认的 shuffle 分区值为 200。 Now that we reviewed how we got here, click the stage with eight tasks to see the next level of detail, as shown in Figure 18-8. 现在我们回顾了我们如何到达这里，单击具有八个任务的阶段以查看下一个详细级别，如图18-8所示。 Spark provides a lot of detail about what this job did when it ran. Toward the top, notice the Summary Metrics section. This provides a synopsis of statistics regarding various metrics. What you want to be on the lookout for is uneven distributions of the values (we touch on this in Chapter 19). In this case, everything looks very consistent; there are no wide swings in the distribution of values. In the table at the bottom, we can also examine on a per-executor basis (one for every core on this particular machine, in this case). This can help identify whether a particular executor is struggling with its workload. Spark 提供了很多关于这项工作在运行时所做些什么的细节。在顶部，请注意“摘要衡量标准（Summary Metrics）”部分。这提供了有关各种指标的统计数据的概要。你想要注意的是值的不均匀分布（我们在第19章中讨论）。在这种情况下，一切看起来都非常一致; 值的分布没有大幅波动。在底部的表中，我们还可以基于每个执行程序（executor）进行检查（在这种情况下，该特定计算机上的每个核心都有一个）。这有助于确定特定执行程序（executor）是否在努力应对其工作量。 Spark also makes available a set of more detailed metrics, as shown in Figure 18-8, which are probably not relevant to the large majority of users. To view those, click Show Additional Metrics, and then either choose (De)select All or select individual metrics, depending on what you want to see. Spark还提供了一组更详细的指标，如图18-8所示，这些指标可能与绝大多数用户无关。要查看这些，请单击“显示其他衡量标准”，然后选择（取消）选择“全部”或选择单个衡量标准，具体取决于您要查看的内容。 You can repeat this basic analysis for each stage that you want to analyze. We leave that as an exercise for the reader. 您可以为要分析的每个阶段重复此基本分析。我们把它作为读者的练习。 Other Spark UI tabs 其他Spark UI选项卡The remaining Spark tabs, Storage, Environment, and Executors, are fairly self-explanatory. The Storage tab shows information about the cached RDDs/DataFrames on the cluster. This can help you see if certain data has been evicted from the cache over time. The Environment tab shows you information about the Runtime Environment, including information about Scala and Java as well as the various Spark Properties that you configured on your cluster. 其余的 Spark 选项卡，存储，环境和执行程序（executor），都是不言自明的。 “存储”选项卡显示有关集群上缓存的 RDD / DataFrame 的信息。这可以帮助您查看某些数据是否随着时间的推移从缓存中逐出。 “环境”选项卡显示有关运行时环境的信息，包括有关 Scala 和 Java 的信息以及您在集群上配置的各种Spark属性。 Configuring the Spark user interface 配置Spark用户界面There are a number of configurations that you can set regarding the Spark UI. Many of them are networking configurations such as enabling access control. Others let you configure how the Spark UI will behave (e.g., how many jobs, stages, and tasks are stored). Due to space limitations, we cannot include the entire configuration set here. Consult the relevant table on Spark UI Configurations in the Spark documentation. 您可以设置有关 Spark UI 的许多配置。其中许多是网络配置，例如启用访问控制。其他允许您配置 Spark UI 的行为方式（例如，存储了多少个作业，阶段和任务）。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中 Spark UI配置 的相关表。 Spark REST APIIn addition to the Spark UI, you can also access Spark’s status and metrics via a REST API. This is is available at http://localhost:4040/api/v1 and is a way of building visualizations and monitoring tools on top of Spark itself. For the most part this API exposes the same information presented in the web UI, except that it doesn’t include any of the SQL-related information. This can be a useful tool if you would like to build your own reporting solution based on the information available in the Spark UI. Due to space limitations, we cannot include the list of API endpoints here. Consult the relevant table on REST API Endpoints in the Spark documentation. 除了 Spark UI，您还可以通过 REST API 访问Spark的状态和指标。这可以在 http://localhost:4040/api/v1 上获得，它是一种在Spark本身之上构建可视化和监视工具的方法。在大多数情况下，此API公开Web UI中显示的相同信息，但它不包含任何与SQL相关的信息。如果您希望根据Spark UI中提供的信息构建自己的报告解决方案，这可能是一个有用的工具。由于篇幅限制，我们无法在此处包含API端点列表。请参阅Spark文档中有关REST API端点的相关表。 Spark UI History Server SparkUI历史记录服务器Normally, the Spark UI is only available while a SparkContext is running, so how can you get to it after your application crashes or ends? To do this, Spark includes a tool called the Spark History Server that allows you to reconstruct the Spark UI and REST API, provided that the application was configured to save an event log. You can find up-to-date information about how to use this tool in the Spark documentation. 通常，Spark UI 仅在 SparkContext 运行时可用，因此在应用程序崩溃或结束后如何才能访问它？为此，Spark包含一个名为 Spark History Server 的工具，允许您重建 Spark UI 和 REST API，前提是应用程序已配置为保存事件日志。您可以在Spark文档中找到有关如何使用此工具的最新信息。 To use the history server, you first need to configure your application to store event logs to a certain location. You can do this by by enabling and the event log location with the configuration spark.eventLog.dir. Then, once you have stored the events, you can run the history server as a standalone application, and it will automatically reconstruct the web UI based on these logs. Some cluster managers and cloud services also configure logging automatically and run a history server by default. 要使用历史记录服务器，首先需要配置应用程序以将事件日志存储到特定位置。您可以通过启用spark.eventLog.enabled 和配置 spark.eventLog.dir 的事件日志位置来完成此操作。然后，一旦存储了事件，就可以将历史服务器作为独立应用程序运行，它将根据这些日志自动重建Web UI。某些集群管理器和云服务还会自动配置日志记录并默认运行历史记录服务器。 There are a number of other configurations for the history server. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Spark History Server Configurations in the Spark documentation. 历史服务器还有许多其他配置。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中有关Spark History Server配置的相关表。 Debugging and Spark First Aid 调试和Spark急救The previous sections defined some core “vital signs”—that is, things that we can monitor to check the health of a Spark Application. For the remainder of the chapter we’re going to take a “first aid” approach to Spark debugging: We’ll review some signs and symptoms of problems in your Spark jobs, including signs that you might observe (e.g., slow tasks) as well as symptoms from Spark itself (e.g., OutOfMemoryError). There are many issues that may affect Spark jobs, so it’s impossible to cover everything. But we will discuss some of the more common Spark issues you may encounter. In addition to the signs and symptoms, we’ll also look at some potential treatments for these issues. 前面的部分定义了一些核心的“生命体征” ——也就是说，我们可以监视以检查Spark应用程序运行状况的事情。对于本章的其余部分，我们将对Spark调试采取“急救”方法： 我们将审查Spark作业中的一些问题迹象和症状，包括您可能观察到的迹象（例如，缓慢的任务）以及来自Spark本身的症状（例如，OutOfMemoryError）。有许多问题可能会影响Spark作业，因此无法涵盖所有内容。但我们将讨论您可能遇到的一些更常见的Spark问题。除了症状和体征外，我们还将研究这些问题的一些潜在治疗方法。 Most of the recommendations about fixing issues refer to the configuration tools discussed in Chapter 16. 有关修复问题的大多数建议都参考了第16章中讨论的配置工具。 Spark Jobs Not Starting Spark工作没有开始This issue can arise frequently, especially when you’re just getting started with a fresh deployment or environment. 这个问题可能经常出现，特别是当您刚刚开始全新部署或环境。 Signs and symptoms 迹象和症状 Spark jobs don’t start. Spark工作无法启动。 The Spark UI doesn’t show any nodes on the cluster except the driver. 除驱动程序（driver）外，Spark UI不显示集群上的任何节点。 The Spark UI seems to be reporting incorrect information. Spark UI 似乎报告了错误的信息。 Potential treatments 可能的疗法This mostly occurs when your cluster or your application’s resource demands are not configured properly. Spark, in a distributed setting, does make some assumptions about networks, file systems, and other resources. During the process of setting up the cluster, you likely configured something incorrectly, and now the node that runs the driver cannot talk to the executors. This might be because you didn’t specify what IP and port is open or didn’t open the correct one. This is most likely a cluster level, machine, or configuration issue. Another option is that your application requested more resources per executor than your cluster manager currently has free, in which case the driver will be waiting forever for executors to be launched. 当您的集群或应用程序的资源需求未正确配置时，通常会发生这种情况。 Spark 在分布式设置中确实对网络，文件系统和其他资源做出了一些假设。在设置集群的过程中，您可能错误地配置了某些内容，现在运行驱动程序（driver）的节点无法与执行程序（executor）通信。这可能是因为您未指定打开的IP和端口或未打开正确的IP和端口。这很可能是集群级别，计算机或配置问题。另一个选择是，您的应用程序为每个执行程序（executor）请求的资源比您的集群管理器当前有空的资源要多，在这种情况下，驱动程序（driver）将永远等待执行程序（executor）启动。 Ensure that machines can communicate with one another on the ports that you expect. Ideally, you should open up all ports between the worker nodes unless you have more stringent security constraints. 确保计算机可以在您期望的端口上相互通信。理想情况下，您应该打开工作节点之间的所有端口，除非您有更严格的安全约束。 Ensure that your Spark resource configurations are correct and that your cluster manager is properly set up for Spark. Try running a simple application first to see if that works. One common issue may be that you requested more memory per executor than the cluster manager has free to allocate, so check how much it is reporting free (in its UI) and your sparksubmit memory configuration. 确保Spark资源配置正确并且已正确设置集群管理器以用于 Spark。尝试先运行一个简单的应用程序，看看是否有效。一个常见问题可能是您为每个执行程序（executor）请求的内存多于集群管理器可以自由分配的内存，因此请检查它是报告空闲的多少（在其 UI 中）和 sparksubmit 内存配置。 Errors Before Execution 执行前的错误This can happen when you’re developing a new application and have previously run code on this cluster, but now some new code won’t work. 当您开发新应用程序并且之前在此集群上运行代码时，可能会发生这种情况，但现在某些新代码将无法运行。 Signs and symptoms 迹象和症状 Commands don’t run at all and output large error messages. 命令根本不运行并输出大的错误消息。 You check the Spark UI and no jobs, stages, or tasks seem to run. 您检查Spark UI并且似乎没有任何作业，阶段或任务运行。 Potential treatments 潜在的治疗方法After checking and confirming that the Spark UI environment tab shows the correct information for your application, it’s worth double-checking your code. Many times, there might be a simple typo or incorrect column name that is preventing the Spark job from compiling into its underlying Spark plan (when using the DataFrame API). 在检查并确认 Spark UI 环境选项卡显示应用程序的正确信息后，值得仔细检查您的代码。很多时候，可能会出现一个简单的拼写错误或不正确的列名，导致Spark作业无法编译到其基础 Spark 计划中（使用 DataFrame API 时）。 You should take a look at the error returned by Spark to confirm that there isn’t an issue in your code, such as providing the wrong input file path or field name. Double-check to verify that the cluster has the network connectivity that you expect between your driver, your workers, and the storage system you are using. 您应该查看Spark返回的错误，以确认代码中没有问题，例如提供错误的输入文件路径或字段名称。仔细检查以验证集群是否具有您期望的驱动程序（driver），工作人员和正在使用的存储系统之间的网络连接。 There might be issues with libraries or classpaths that are causing the wrong version of a library to be loaded for accessing storage. Try simplifying your application until you get a smaller version that reproduces the issue (e.g., just reading one dataset). 库或类路径可能存在导致加载库的错误版本以访问存储的问题。尝试简化您的应用程序，直到您获得重现问题的较小版本（例如，只读取一个数据集）。 Errors During Execution 执行期间的错误 This kind of issue occurs when you already are working on a cluster or parts of your Spark 当您已经在集群或部分Spark上工作时，会出现此类问题。 Application run before you encounter an error. This can be a part of a scheduled job that runs at some interval or a part of some interactive exploration that seems to fail after some time. 在遇到错误之前运行应用程序。这可以是某个时间间隔运行的已经安排作业的一部分，也可能是某些时间后似乎失败的某些交互式探索的一部分。 Signs and symptoms 迹象和症状One Spark job runs successfully on the entire cluster but the next one fails. 一个 Spark 作业在整个集群上成功运行，但下一个失败。 A step in a multistep query fails. 多步查询中的步骤失败。 A scheduled job that ran yesterday is failing today. 昨天运行的预定工作今天失败了。 Difficult to parse error message. 难以解析错误消息。 Potential treatments 可能的疗法 Check to see if your data exists or is in the format that you expect. This can change over time or some upstream change may have had unintended consequences on your application. If an error quickly pops up when you run a query (i.e., before tasks are launched), it is most likely an analysis error while planning the query. This means that you likely misspelled a column name referenced in the query or that a column, view, or table you referenced does not exist. 检查您的数据是否存在或是否符合您的预期格式。这可能会随着时间的推移而改变，或者某些上游更改可能会对您的应用程序产生意外后果。如果在运行查询时（即，在启动任务之前）快速弹出错误，则在计划查询时很可能是分析错误。这意味着您可能拼错了查询中引用的列名称，或者您引用的列，视图或表不存在&gt;。 Read through the stack trace to try to find clues about what components are involved (e.g., what operator and stage it was running in). Try to isolate the issue by progressively double-checking input data and ensuring the data conforms to your expectations. Also try removing logic until you can isolate the problem in a smaller version of your application. 读取堆栈跟踪以尝试查找涉及哪些组件的线索（例如，运行的算子和阶段）。尝试通过逐步检查输入数据并确保数据符合您的期望来隔离问题。还可以尝试删除逻辑，直到您可以在较小版本的应用程序中隔离问题。 If a job runs tasks for some time and then fails, it could be due to a problem with the input data itself, wherein the schema might be specified incorrectly or a particular row does not conform to the expected schema. For instance, sometimes your schema might specify that the data contains no nulls but your data does actually contain nulls, which can cause certain transformations to fail. 如果作业运行任务一段时间然后失败，则可能是由于输入数据本身存在问题，其中可能未正确指定模式或特定行不符合预期模式。例如，有时您的模式可能指定数据不包含空值，但您的数据确实包含空值，这可能导致某些转换失败。 It’s also possible that your own code for processing the data is crashing, in which case Spark will show you the exception thrown by your code. In this case, you will see a task marked as “failed” on the Spark UI, and you can also view the logs on that machine to understand what it was doing when it failed. Try adding more logs inside your code to figure out which data record was being processed. 您自己的处理数据的代码也可能崩溃，在这种情况下，Spark会向您显示代码抛出的异常。在这种情况下，您将在Spark UI上看到标记为“失败”的任务，您还可以查看该计算机上的日志以了解失败时正在执行的操作。尝试在代码中添加更多日志，以确定正在处理哪些数据记录。 Slow Tasks or Stragglers 缓慢的任务或StragglersThis issue is quite common when optimizing applications, and can occur either due to work not being evenly distributed across your machines (“skew”), or due to one of your machines being slower than the others (e.g., due to a hardware problem). 在优化应用程序时，此问题非常常见，并且可能由于工作不均匀分布在您的计算机上（“倾斜”），或者由于您的某台计算机比其他计算机慢（例如，由于硬件问题）而发生。 Signs and symptoms 迹象和症状Any of the following are appropriate symptoms of the issue : 以下任何一种都是该问题的适当症状： Spark stages seem to execute until there are only a handful of tasks left. Those tasks then take a long time. Spark阶段似乎执行，直到只剩下少数任务。那些任务需要很长时间。 These slow tasks show up in the Spark UI and occur consistently on the same dataset(s). 这些缓慢的任务显示在 Spark UI 中，并在相同的数据集上一致地发生。 These occur in stages, one after the other. 这些是分阶段发生的，一个接一个。 Scaling up the number of machines given to the Spark Application doesn’t really help—some tasks still take much longer than others. 扩大提供给 Spark 应用程序的机器数量并没有多大帮助——某些任务仍然需要比其他任务更长的时间。 In the Spark metrics, certain executors are reading and writing much more data than others. 在Spark指标中，某些执行程序（executor）正在读取和写入比其他数据更多的数据。 Potential treatments 可能的治疗方法 Slow tasks are often called “stragglers.” There are many reasons they may occur, but most often the source of this issue is that your data is partitioned unevenly into DataFrame or RDD partitions. When this happens, some executors might need to work on much larger amounts of work than others. One particularly common case is that you use a group-by-key operation and one of the keys just has more data than others. In this case, when you look at the Spark UI, you might see that the shuffle data for some nodes is much larger than for others. 缓慢的任务通常被称为“落后者”。它们可能出现的原因很多，但大多数情况下，这个问题的根源是您的数据被不均匀地划分为 DataFrame 或 RDD 分区。当发生这种情况时，一些执行程序（executor）可能需要处理比其他工作量大得多的工作。一个特别常见的情况是您使用逐个键操作，其中一个键只是比其他键更多的数据。在这种情况下，当您查看 Spark UI 时，您可能会看到某些节点的 shuffle 数据比其他节点大得多。 Try increasing the number of partitions to have less data per partition. 尝试增加分区数，以使每个分区的数据更少。 Try repartitioning by another combination of columns. For example, stragglers can come up when you partition by a skewed ID column, or a column where many values are null. In the latter case, it might make sense to first filter out the null values. Try increasing the memory allocated to your executors if possible. 尝试通过另一个列组合重新分区。例如，当您通过倾斜的 ID 列或许多值为 null 的列进行分区时，straggler 可能会出现。在后一种情况下，首先过滤掉空值可能是有意义的。如果可能，尝试增加分配给执行程序（executor）的内存。 Monitor the executor that is having trouble and see if it is the same machine across jobs; you might also have an unhealthy executor or machine in your cluster—for example, one whose disk is nearly full. If this issue is associated with a join or an aggregation, see “Slow Joins” or “Slow Aggregations”. 监视有问题的执行程序（executor），看看它是否是跨不同作业的同一台机器；您的集群中可能还有一个不健康的执行程序（executor）或计算机——例如，磁盘几乎已满的计算机。如果此问题与连接或聚合相关联，请参阅“慢速连接”或“慢速聚合”。 Check whether your user-defined functions (UDFs) are wasteful in their object allocation or business logic. Try to convert them to DataFrame code if possible. Ensure that your UDFs or User-Defined Aggregate Functions (UDAFs) are running on a small enough batch of data. Oftentimes an aggregation can pull a lot of data into memory for a common key, leading to that executor having to do a lot more work than others. 检查用户定义的函数（UDF）在对象分配或业务逻辑中是否浪费。如果可能，尝试将它们转换为 DataFrame 代码。确保您的 UDF 或用户定义的聚合函数（UDAF）在足够小的数据批量上运行。通常，聚合可以将大量数据拉入内存以用于普通的键，从而导致执行程序（executor）必须比其他人执行更多的工作。 Turning on speculation, which we discuss in “Slow Reads and Writes”, will have Spark run a second copy of tasks that are extremely slow. This can be helpful if the issue is due to a faulty node because the task will get to run on a faster one. Speculation does come at a cost, however, because it consumes additional resources. In addition, for some storage systems that use eventual consistency, you could end up with duplicate output data if your writes are not idempotent . (We discussed speculation configurations in Chapter 17.) 打开我们在“慢速读取和写入”中讨论的推测（speculation），将使 Spark 运行极其缓慢的第二个任务副本。如果问题是由于故障节点引起的，这可能会有所帮助，因为任务将以更快的速度运行。然而，推测（speculation）确实需要付出代价，因为它消耗了额外的资源。此外，对于某些使用最终一致性的存储系统，如果写入不是幂等的，则最终可能会出现重复的输出数据。 （我们在第17章讨论了推测(speculation)配置） Another common issue can arise when you’re working with Datasets. Because Datasets perform a lot of object instantiation to convert records to Java objects for UDFs, they can cause a lot of garbage collection. If you’re using Datasets, look at the garbage collection metrics in the Spark UI to see if they’re consistent with the slow tasks. 当您使用 Datasets 时，可能会出现另一个常见问题。由于 Datasets 执行大量对象实例化以将记录转换为UDF 的 Java 对象，因此它们可能导致大量垃圾回收。如果您正在使用 Datasets，请查看Spark UI中的垃圾收集指标，以查看它们是否与缓慢的任务一致。 Stragglers can be one of the most difficult issues to debug, simply because there are so many possible causes. However, in all likelihood, the cause will be some kind of data skew, so definitely begin by checking the Spark UI for imbalanced amountsimbalanced amounts of data across tasks. Stragglers可能是最难调试的问题之一，因为有很多可能的原因。但是，很有可能，原因将是某种数据偏差，因此必须首先检查Spark UI以查找跨任务的不平衡数据量。 Slow Aggregations 慢的聚合If you have a slow aggregation, start by reviewing the issues in the “Slow Tasks” section before proceeding. Having tried those, you might continue to see the same problem. 如果您的聚合速度较慢，请先继续查看“慢速任务”部分中的问题，然后再继续。尝试过这些后，您可能会继续看到同样的问题。 Signs and symptoms 迹象和症状 Slow tasks during a groupBy call. 在 groupBy 调用期间缓慢执行任务。 Jobs after the aggregation are slow, as well. 聚合后的工作也很慢。 Potential treatments 可能的疗法 Unfortunately, this issue can’t always be solved. Sometimes, the data in your job just has some skewed keys, and the operation you want to run on them needs to be slow. Increasing the number of partitions, prior to an aggregation, might help by reducing the number of different keys processed in each task. 不幸的是，这个问题并不总能解决。有时，作业中的数据只有一些倾斜的键，您想要在它们上运行的操作需要很慢。在聚合之前增加分区数可能有助于减少每个任务中处理的不同键的数量。 Increasing executor memory can help alleviate this issue, as well. If a single key has lots of data, this will allow its executor to spill to disk less often and finish faster, although it may still be much slower than executors processing other keys. 增加执行程序（executor）内存也有助于缓解此问题。如果单个键有大量数据，这将允许其执行程序（executor）更少地溢出到磁盘并更快地完成，尽管它可能仍然比处理其他键的执行程序（executor）慢得多。 If you find that tasks after the aggregation are also slow, this means that your dataset might have remained unbalanced after the aggregation. Try inserting a repartition call to partition it randomly. 如果您发现聚合后的任务也很慢，这意味着聚合后您的数据集可能仍然不平衡。尝试插入重新分区调用以随机分区。 Ensuring that all filters and SELECT statements that can be are above the aggregation can help to ensure that you’re working only on the data that you need to be working on and nothing else. Spark’s query optimizer will automatically do this for the structured APIs. 确保可以在聚合之上的所有过滤器和 SELECT 语句可以帮助确保您仅处理您需要处理的数据而不是其他任何内容。 Spark的查询优化器将自动为结构化API执行此操作。 Ensure null values are represented correctly (using Spark’s concept of null) and not as some default value like “ “ or “EMPTY”. Spark often optimizes for skipping nulls early in the job when possible, but it can’t do so for your own placeholder values. 确保正确表示空值（使用Spark的null概念）而不是像“”或“EMPTY”那样的默认值。 Spark通常会尽可能优化在作业的早期跳过空值，但是对于您自己的占位符值，它不能这样做。 Some aggregation functions are also just inherently slower than others. For instance, collect_list and collect_set are very slow aggregation functions because they must return all the matching objects to the driver, and should be avoided in performance-critical code. 某些聚合函数本身也比其他函数慢。例如，collect_list和collect_set是非常慢的聚合函数，因为它们必须将所有匹配的对象返回给驱动程序（driver），并且应该在性能关键代码中避免使用。 Slow Joins 慢加入Joins and aggregations are both shuffles, so they share some of the same general symptoms as well as treatments. 连接和聚合都是随机洗牌(shuffle)，因此它们共享一些相同的症状和应对方法。 Signs and symptoms 迹象和症状 A join stage seems to be taking a long time. This can be one task or many tasks. l连接阶段似乎需要很长时间。这可以是一个任务或许多任务。 Stages before and after the join seem to be operating normally. 连接之前和之后的阶段似乎正常运行。 Potential treatments 可能的疗法 Many joins can be optimized (manually or automatically) to other types of joins. We covered how to select different join types in Chapter 8. 许多连接可以优化（手动或自动）到其他类型的连接。我们在第8章介绍了如何选择不同的连接类型。 Experimenting with different join orderings can really help speed up jobs, especially if some of those joins filter out a large amount of data; do those first. 尝试不同的连接顺序可以真正帮助加快工作，特别是如果其中一些连接过滤掉大量数据; 先做那些。 Partitioning a dataset prior to joining can be very helpful for reducing data movement across the cluster, especially if the same dataset will be used in multiple join operations. It’s worth experimenting with different prejoin partitioning. Keep in mind, again, that this isn’t “free” and does come at the cost of a shuffle. 在连接之前对数据集进行分区对于减少集群中的数据移动非常有用，尤其是在多个连接操作中将使用相同的数据集时。值得尝试不同的预连接（prejoin）分区。请记住，这不是“免费”，而是以洗牌(shuffle)为代价。 Slow joins can also be caused by data skew. There’s not always a lot you can do here, but sizing up the Spark application and/or increasing the size of executors can help, as described in earlier sections. 数据倾斜也可能导致慢连接。你可以在这里做很多事情，但是调整 Spark 应用程序和/或增加执行程序（executor）的数量可以提供帮助，如前面部分所述。 Ensuring that all filters and select statements that can be are above the join can help to ensure that you’re working only on the data that you need for the join. 确保可以在连接之上的所有筛选器和选择语句可以帮助确保您仅处理连接所需的数据。 Ensure that null values are handled correctly (that you’re using null) and not some default value like “ “ or “EMPTY”, as with aggregations. 确保正确处理空值（您使用的是null），而不是像聚合一样处理某些默认值，如“”或“EMPTY”。 Sometimes Spark can’t properly plan for a broadcast join if it doesn’t know any statistics about the input DataFrame or table. If you know that one of the tables that you are joining is small, you can try to force a broadcast (as discussed in Chapter 8), or use Spark’s statistics collection commands to let it analyze the table. 如果 Spark 不知道有关输入DataFrame或表的任何统计信息，Spark有时无法正确规划广播连接（broadcast join）。如果您知道要加入的其中一个表很小，则可以尝试强制广播（如第8章中所述），或使用Spark的统计信息收集命令让它分析表。 Slow Reads and Writes 慢的读和写Slow I/O can be difficult to diagnose, especially with networked file systems. 慢速 I/O 可能难以诊断，尤其是对于网络文件系统。 Signs and symptoms 迹象和症状Slow reading of data from a distributed file system or external system. Slow writes from network file systems or Blob storage. 从分布式文件系统或外部系统缓慢读取数据。从网络文件系统或 Blob 存储缓慢写入。 Potential treatments 潜在的治疗Turning on speculation (set spark.speculation to true) can help with slow reads and writes. This will launch additional tasks with the same operation in an attempt to see whether it’s just some transient issue in the first task. Speculation is a powerful tool and works well with consistent file systems. However, it can cause duplicate data writes with some eventually consistent cloud services, such as Amazon S3, so check whether it is supported by the storage system connector you are using. 打开推测（将 spark.speculation 设置为 true）可以帮助减慢读取和写入。这将使用相同的操作启动其他任务，以尝试查看它是否只是第一个任务中的一些短暂问题。推测(speculation)是一种功能强大的工具，适用于一致的文件系统。但是，它可能导致重复数据写入与一些最终一致的云服务（如Amazon S3），因此请检查您使用的存储系统连接器是否支持它。 Ensuring sufficient network connectivity can be important—your Spark cluster may simply not have enough total network bandwidth to get to your storage system. 确保足够的网络连接非常重要——您的 Spark 集群可能根本没有足够的总网络带宽来访问您的存储系统。 For distributed file systems such as HDFS running on the same nodes as Spark, make sure Spark sees the same hostnames for nodes as the file system. This will enable Spark to do locality-aware scheduling, which you will be able to see in the “locality” column in the Spark UI. We’ll talk about locality a bit more in the next chapter. 对于与Spark在相同节点上运行的分布式文件系统（如HDFS），请确保Spark看到与文件系统相同的节点主机名。这将使Spark能够进行关注局部性的调度，您可以在Spark UI的“locality”列中看到该调度。我们将在下一章中讨论一下局部性。 Driver OutOfMemoryError or Driver Unresponsive 驱动程序OutOfMemoryError或驱动程序无响应This is usually a pretty serious issue because it will crash your Spark Application. It often happens due to collecting too much data back to the driver, making it run out of memory. 这通常是一个相当严重的问题，因为它会使您的Spark应用程序崩溃。它经常发生，因为收集了太多的数据回到驱动程序，使其耗尽内存。 Signs and symptoms 迹象和症状 Spark Application is unresponsive or crashed. OutOfMemoryErrors or garbage collection messages in the driver logs. Spark应用程序无响应或崩溃。驱动程序日志中的OutOfMemoryErrors或垃圾回收消息。 Commands take a very long time to run or don’t run at all.命令需要很长时间才能运行或根本不运行。 Interactivity is very low or non-existent.交互性很低或根本不存在。 Memory usage is high for the driver JVM.驱动程序JVM的内存使用率很高。 Potential treatments 可能的治疗方法There are a variety of potential reasons for this happening, and diagnosis is not always straightforward. 这种情况有多种可能的原因，诊断并不总是直截了当的。 Your code might have tried to collect an overly large dataset to the driver node using operations such as collect. 您的代码可能尝试使用诸如collect之类的操作将过大的数据集收集到驱动程序节点。 You might be using a broadcast join where the data to be broadcast is too big. Use Spark’s maximum broadcast join configuration to better control the size it will broadcast. 您可能正在使用广播连接，其中要广播的数据太大。使用Spark的最大广播连接配置可以更好地控制它将广播的大小。 A long-running application generated a large number of objects on the driver and is unable to release them. Java’s jmap tool can be useful to see what objects are filling most of the memory of your driver JVM by printing a histogram of the heap. However, take note that jmap will pause that JVM while running. Increase the driver’s memory allocation if possible to let it work with more data. 长时间运行的应用程序在驱动程序上生成了大量对象，无法释放它们。 Java 的 jmap 工具可以通过打印堆的直方图来查看哪些对象填充了驱动程序 JVM 的大部分内存。但请注意，jmap 会在运行时暂停该JVM。如果可能的话，增加驱动程序的内存分配，让它可以处理更多数据。 Issues with JVMs running out of memory can happen if you are using another language binding, such as Python, due to data conversion between the two requiring too much memory in the JVM. Try to see whether your issue is specific to your chosen language and bring back less data to the driver node, or write it to a file instead of bringing it back as in-memory objects. 如果您使用其他语言绑定（如Python），JVM内存不足会出现问题，因为两者之间的数据转换需要JVM中的内存过多。尝试查看您的问题是否特定于您选择的语言，并将较少的数据带回驱动程序节点，或将其写入文件而不是将其作为内存中对象重新引入。 If you are sharing a SparkContext with other users (e.g., through the SQL JDBC server and some notebook environments), ensure that people aren’t trying to do something that might be causing large amounts of memory allocation in the driver (like working overly large arrays in their code or collecting large datasets). 如果您与其他用户共享 SparkContext（例如，通过SQL JDBC服务器和某些 notebook 环境），请确保人们不会尝试执行可能导致驱动程序中大量内存分配的操作（例如，过度工作代码中的大型数组或收集大型数据集）。 Executor OutOfMemoryError or Executor Unresponsive Executor OutOfMemoryError或Executor无响应Spark applications can sometimes recover from this automatically, depending on the true underlyingissue. Spark应用程序有时可以自动从中恢复，具体取决于真正的底层问题。 Signs and symptoms 迹象和症状 OutOfMemoryErrors or garbage collection messages in the executor logs. You can find these in the Spark UI. 执行程序(executor)日志中的OutOfMemoryErrors或垃圾回收消息。您可以在Spark UI中找到它们。 Executors that crash or become unresponsive. 崩溃或无响应的执行程序（executor）。 Slow tasks on certain nodes that never seem to recover. 某些节点上的缓慢任务似乎永远无法恢复。 Potential treatments 潜在的疗法 Try increasing the memory available to executors and the number of executors. 尝试增加执行程序(executor)可用的内存和执行程序(executor)的数量。 Try increasing PySpark worker size via the relevant Python configurations. 尝试通过相关的Python配置增加PySpark工作者大小。 Look for garbage collection error messages in the executor logs. Some of the tasks that are running, especially if you’re using UDFs, can be creating lots of objects that need to be garbage collected. Repartition your data to increase parallelism, reduce the amount of records per task, and ensure that all executors are getting the same amount of work. 在执行程序(executor)日志中查找垃圾收集错误消息。正在运行的某些任务（尤其是在使用UDF时）可能会创建大量需要进行垃圾回收的对象。重新分区数据以增加并行度，减少每个任务的记录数量，并确保所有执行程序(executor)获得相同的工作量。 Ensure that null values are handled correctly (that you’re using null) and not some default value like “ “ or “EMPTY”, as we discussed earlier. 确保正确处理空值（您正在使用null）而不是像我们之前讨论的那样的默认值，如“”或“EMPTY”。 This is more likely to happen with RDDs or with Datasets because of object instantiations. 由于对象实例化，这更有可能发生在 RDD 或 Datasets 中。 Try using fewer UDFs and more of Spark’s structured operations when possible. 尽可能尝试使用更少的UDF和更多Spark的结构化操作。 Use Java monitoring tools such as jmap to get a histogram of heap memory usage on your executors, and see which classes are taking up the most space. 使用 jmap 等Java监视工具获取执行程序(executor)堆内存使用情况的直方图，并查看哪些类占用的空间最多。 If executors are being placed on nodes that also have other workloads running on them, such as a key-value store, try to isolate your Spark jobs from other jobs. 如果将执行程序(executor)放置在也运行其他工作负载的节点上（例如键值存储），请尝试将Spark作业与其他作业隔离开来。 Unexpected Nulls in Results 结果中出现意外空白Signs and symptoms 迹象和症状 Unexpected null values after transformations. 转换后出现意外的空值。 Scheduled production jobs that used to work no longer work, or no longer produce the right results. 过去工作安排的生产作业不再起作用，或者不再产生正确的结果。 Potential treatments 潜在的治疗 It’s possible that your data format has changed without adjusting your business logic. This means that code that worked before is no longer valid. 您的数据格式可能已更改，而无需调整业务逻辑。这意味着以前工作的代码不再有效。 Use an accumulator to try to count records or certain types, as well as parsing or processing errors where you skip a record. This can be helpful because you might think that you’re parsing data of a certain format, but some of the data doesn’t. Most often, users will place the accumulator in a UDF when they are parsing their raw data into a more controlled format and perform the counts there. This allows you to count valid and invalid records and then operate accordingly after the fact. 使用累加器尝试计算记录或某些类型，以及解析或处理跳过记录的错误。这可能很有用，因为您可能认为您正在解析某种格式的数据，但有些数据却没有。大多数情况下，用户在将原始数据解析为更受控制的格式并在那里执行计数时，会将累加器放在 UDF 中。这允许您计算有效和无效的记录，然后在事后进行相应的操作。 Ensure that your transformations actually result in valid query plans. Spark SQL sometimes does implicit type coercions that can cause confusing results. For instance, the SQL expression SELECT 5“23” results in 115 because the string “25” converts to an the value 25 as an integer, but the expression SELECT 5 “ “ results in null because casting the empty string to an integer gives null. Make sure that your intermediate datasets have the schema you expect them to (try using printSchema on them), and look for any CAST operations in the final query plan. 确保您的转换实际上产生有效的查询计划。 Spark SQL有时会执行隐式类型强制，这可能会导致混乱的结果。例如，SQL表达式SELECT 5 “23”导致115，因为字符串“25”将整数转换为值25，但表达式SELECT 5 “”导致null，因为将空字符串转换为整数给出null。确保您的中间数据集具有您期望的模式（尝试对它们使用printSchema），并在最终查询计划中查找任何CAST操作。 ​ No Space Left on Disk Errors 磁盘错误没有剩余空间Signs and symptoms 迹象和症状 You see “no space left on disk” errors and your jobs fail. 您看到“磁盘上没有剩余空间”错误，您的作业失败。 Potential treatments 潜在的治疗 The easiest way to alleviate this, of course, is to add more disk space. You can do this by sizing up the nodes that you’re working on or attaching external storage in a cloud environment. 当然，减轻这种情况的最简单方法是添加更多磁盘空间。您可以通过调整正在处理的节点或在云环境中连接外部存储来实现此目的。 If you have a cluster with limited storage space, some nodes may run out first due to skew. 如果您的集群存储空间有限，则某些节点可能会由于数据倾斜而首先耗尽。 Repartitioning the data as described earlier may help here. 如前所述重新分区数据可能对此有所帮助。 There are also a number of storage configurations with which you can experiment. Some of these determine how long logs should be kept on the machine before being removed. For more information, see the Spark executor logs rolling configurations in Chapter 16. 您还可以使用许多存储配置进行试验。其中一些决定了在移除之前应该在机器上保留多长时间的日志。有关更多信息，请参阅第16章中的Spark执行程序(executor)日志滚动配置。 Try manually removing some old log files or old shuffle files from the machine(s) in question. This can help alleviate some of the issue although obviously it’s not a permanent fix. 尝试从相关机器手动删除一些旧的日志文件或旧的随机洗牌文件。这可以帮助减轻一些问题，虽然显然它不是永久性的修复。 Serialization Errors 序列化错误Signs and symptoms 迹象和症状 You see serialization errors and your jobs fail.您看到序列化错误，您的作业失败。 Potential treatments潜在的治疗方法 This is very uncommon when working with the Structured APIs, but you might be trying to perform some custom logic on executors with UDFs or RDDs and either the task that you’re trying to serialize to these executors or the data you are trying to share cannot be serialized. This often happens when you’re working with either some code or data that cannot be serialized into a UDF or function, or if you’re working with strange data types that cannot be serialized. If you are using (or intend to be using Kryo serialization), verify that you’re actually registering your classes so that they are indeed serialized. 这在使用结构化API时非常罕见，但您可能尝试使用UDF或RDD在执行程序(executor)上执行某些自定义逻辑，以及您尝试序列化到这些执行程序(executor)的任务或您尝试共享的数据无法序列化。当您使用某些无法序列化为UDF或函数的代码或数据时，或者您正在使用无法序列化的奇怪数据类型时，通常会发生这种情况。如果您正在使用（或打算使用Kryo序列化），请验证您实际上是在注册类，以便它们确实是序列化的。 Try not to refer to any fields of the enclosing object in your UDFs when creating UDFs inside a Java or Scala class. This can cause Spark to try to serialize the whole enclosing object, which may not be possible. Instead, copy the relevant fields to local variables in the same scope as closure and use those. 在Java或Scala类中创建UDF时，尽量不要引用UDF中封闭对象的任何字段。这可能导致Spark尝试序列化整个封闭对象，这可能是不可能的。相反，将相关字段复制到与闭包相同的范围内的局部变量并使用它们。 Conclusion 结论This chapter covered some of the main tools that you can use to monitor and debug your Spark jobs and applications, as well as the most common issues we see and their resolutions. As with debugging any complex software, we recommend taking a principled, step-by-step approach to debug issues. Add logging statements to figure out where your job is crashing and what type of data arrives at each stage, try to isolate the problem to the smallest piece of code possible, and work up from there. For data skew issues, which are unique to parallel computing, use Spark’s UI to get a quick overview of how much work each task is doing. In Chapter 19, we discuss performance tuning in particular and various tools you can use for that. 本章介绍了一些可用于监视和调试Spark作业和应用程序的主要工具，以及我们看到的最常见问题及其解决方案。与调试任何复杂软件一样，我们建议采用有原则的逐步方法来调试问题。添加日志记录语句以确定作业崩溃的位置以及每个阶段到达的数据类型，尝试将问题隔离到可能的最小代码段，并从那里开始工作。对于并行计算所特有的数据偏差问题，请使用Spark的UI快速了解每项任务的工作量。在第19章中，我们特别讨论了性能调优以及可以使用的各种工具。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 23 Structured Streaming in Production]]></title>
    <url>%2F2019%2F08%2F10%2FChapter23_StructuredStreamingInProduction(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 23 Structured Streaming in Production 生产环境中的结构化流The previous chapters of this part of the book have covered Structured Streaming from a user’s perspective. Naturally this is the core of your application. This chapter covers some of the operational tools needed to run Structured Streaming robustly in production after you’ve developed an application. 本书这一部分的前几章从用户的角度介绍了结构化流。当然，这是应用程序的核心。本章介绍在开发应用程序之后，在生产环境中可靠地运行结构化流所需的一些操作工具。 Structured Streaming was marked as production-ready in Apache Spark 2.2.0, meaning that this release has all the features required for production use and stabilizes the API. Many organizations are already using the system in production because, frankly, it’s not much different from running other production Spark applications. Indeed, through features such as transactional sources/sinks and exactly-once processing, the Structured Streaming designers sought to make it as easy to operate as possible. This chapter will walk you through some of the key operational tasks specific to Structured Streaming. This should supplement everything we saw and learned about Spark operations in Part II. 结构化流在 Apache Spark 2.2.0中被标记为生产就绪，这意味着该版本具有生产使用所需的所有功能，并稳定了API。许多组织已经在生产中使用该系统，因为坦率地说，它与运行其他生产Spark应用程序没有太大区别。事实上，通过事务性源/接收器和一次性处理等功能，结构化流设计人员力求使其尽可能易于操作。本章将引导您完成特定于结构化流的一些关键操作任务。这应该补充我们在第二部分中看到和学到的关于 Spark 操作的所有知识。 Fault Tolerance and Checkpointing 容错和检查点The most important operational concern for a streaming application is failure recovery. Faults are inevitable: you’re going to lose a machine in the cluster, a schema will change by accident without a proper migration, or you may even intentionally restart the cluster or application. In any of these cases, Structured Streaming allows you to recover an application by just restarting it. To do this, you must configure the application to use checkpointing and write-ahead logs, both of which are handled automatically by the engine. Specifically, you must configure a query to write to a checkpoint location on a reliable file system (e.g., HDFS, S3, or any compatible filesystem). Structured Streaming will then periodically save all relevant progress information (for instance, the range of offsets processed in a given trigger) as well as the current intermediate state values to the checkpoint location. In a failure scenario, you simply need to restart your application, making sure to point to the same checkpoint location, and it will automatically recover its state and start processing data where it left off. You do not have to manually manage this state on behalf of the application—Structured Streaming does it for you. 流应用程序最重要的操作问题是故障恢复。错误是不可避免的：您将失去集群中的一台机器，一个模式将在没有适当迁移的情况下意外更改，或者您甚至可能有意重新启动集群或应用程序。在这些情况下，结构化流允许您通过重新启动应用程序来恢复应用程序。为此，必须将应用程序配置为使用检查点和提前写入日志，这两个日志都由引擎自动处理。具体来说，您必须配置一个查询，以写入可靠文件系统（例如，HDFS、S3或任何兼容的文件系统）上的检查点位置。结构化流将定期将所有相关的进度信息（例如，在给定触发器中处理的偏移范围）以及当前中间状态值保存到检查点位置。在失败的情况下，只需重新启动应用程序，确保指向相同的检查点位置，它将自动恢复其状态，并在停止的地方开始处理数据。您不必代表应用程序手动管理此状态，结构化流为您做到了这一点。 To use checkpointing, specify your checkpoint location before starting your application through the checkpointLocation option on writeStream. You can do this as follows: 要使用检查点，请在通过 writeStream 上的检查点位置选项启动应用程序之前指定检查点位置。您可以这样做： 123456789101112131415// in Scalaval static = spark.read.json("/data/activity-data")val streaming = spark.readStream.schema(static.schema).option("maxFilesPerTrigger", 10).json("/data/activity-data").groupBy("gt").count()val query = streaming.writeStream.outputMode("complete").option("checkpointLocation", "/some/location/").queryName("test_stream").format("memory").start() 12345678910111213141516# in Pythonstatic = spark.read.json("/data/activity-data")streaming = spark\.readStream\.schema(static.schema)\.option("maxFilesPerTrigger", 10)\.json("/data/activity-data")\.groupBy("gt")\.count()query = streaming\.writeStream\.outputMode("complete")\.option("checkpointLocation", "/some/python/location/")\.queryName("test_python_stream")\.format("memory")\.start() If you lose your checkpoint directory or the information inside of it, your application will not be able to recover from failures and you will have to restart your stream from scratch. 如果丢失了检查点目录或其中的信息，应用程序将无法从失败中恢复，您必须从头开始重新启动流。 Updating Your Application 更新应用程序 Checkpointing is probably the most important thing to enable in order to run your applications in production. This is because the checkpoint will store all of the information about what your stream has processed thus far and what the intermediate state it may be storing is. However, checkpointing does come with a small catch—you’re going to have to reason about your old checkpoint data when you update your streaming application. When you update your application, you’re going to have to ensure that your update is not a breaking change. Let’s cover these in detail when we review the two types of updates: either an update to your application code or running a new Spark version. 为了在生产环境中运行应用程序，检查点可能是最重要的。这是因为检查点将存储到目前为止您的流处理的内容以及它可能存储的中间状态的所有信息。但是，检查点的出现只是一个小问题，当您更新流应用程序时，您必须考虑旧的检查点数据。当你更新你的应用程序时，你必须确保你的更新不是一个突破性的改变。当我们回顾这两种类型的更新时，让我们详细介绍一下这些：应用程序代码的更新或者运行一个新的 Spark 版本。 Updating Your Streaming Application Code 更新流应用程序代码Structured Streaming is designed to allow certain types of changes to the application code between application restarts. Most importantly, you are allowed to change user-defined functions (UDFs) as long as they have the same type signature. This feature can be very useful for bug fixes. For example, imagine that your application starts receiving a new type of data, and one of the data parsing functions in your current logic crashes. With Structured Streaming, you can recompile the application with a new version of that function and pick up at the same point in the stream where it crashed earlier. While small adjustments like adding a new column or changing a UDF are not breaking changes and do not require a new checkpoint directory, there are larger changes that do require an entirely new checkpoint directory. For example, if you update your streaming application to add a new aggregation key or fundamentally change the query itself, Spark cannot construct the required state for the new query from an old checkpoint directory. In these cases, Structured Streaming will throw an exception saying it cannot begin from a checkpoint directory, and you must start from scratch with a new (empty) directory as your checkpoint location. 结构化流的设计允许在应用程序重新启动之间对应用程序代码进行某些类型的更改。最重要的是，您可以更改用户定义函数（UDF），只要它们具有相同的类型签名。这个特性对于错误修复非常有用。例如，假设应用程序开始接收新类型的数据，并且当前逻辑崩溃时的一个数据解析函数。使用结构化流，您可以使用该函数的新版本重新编译应用程序，并在流中之前崩溃的同一点上继续进行。虽然诸如添加新列或更改UDF之类的小调整不是突破性的改变，也不需要新的检查点目录，但仍有较大的更改需要全新的检查点目录。例如，如果更新流应用程序以添加新的聚合键或从根本上更改查询本身，Spark将无法从旧的检查点目录构造新查询所需的状态。在这些情况下，结构化流将抛出一个异常，说明它不能从检查点目录开始，并且必须从头开始，使用一个新的（空）目录作为检查点位置。 Updating Your Spark Version 更新Spark版本Structured Streaming applications should be able to restart from an old checkpoint directory across patch version updates to Spark (e.g., moving from Spark 2.2.0 to 2.2.1 to 2.2.2). The checkpoint format is designed to be forward-compatible, so the only way it may be broken is due to critical bug fixes. If a Spark release cannot recover from old checkpoints, this will be clearly documented in its release notes. The Structured Streaming developers also aim to keep the format compatible across minor version updates (e.g., Spark 2.2.x to 2.3.x), but you should check the release notes to see whether this is supported for each upgrade. In either case, if you cannot start from a checkpoint, you will need to start your application again using a new checkpoint directory. 结构化流应用程序应该能够从旧的检查点目录跨补丁版本更新重新启动到 Spark（例如，从 Spark 2.2.0迁移到2.2.1到2.2.2）。检查点格式设计为向前兼容，因此唯一可能被破坏的方法是修复关键的错误。如果Spark发行版不能从旧的检查点恢复，那么它的发行说明中会清楚地记录这一点。结构化流式开发人员还致力于保持格式在次要版本更新（例如spark 2.2.x到2.3.x）之间的兼容性，但是您应该检查发行说明，以查看是否支持每次升级。在这两种情况下，如果无法从检查点启动，则需要使用新的检查点目录重新启动应用程序。 Sizing and Rescaling Your Application 调整应用程序的大小和重新缩放In general, the size of your cluster should be able to comfortably handle bursts above your data rate. The key metrics you should be monitoring in your application and cluster are discussed as follows. In general, if you see that your input rate is much higher than your processing rate (elaborated upon momentarily), it’s time to scale up your cluster or application. Depending on your resource manager and deployment, you may just be able to dynamically add executors to your application. When it comes time, you can scale-down your application in the same way—remove executors (potentially through your cloud provider) or restart your application with lower resource counts. These changes will likely incur some processing delay (as data is recomputed or partitions are shuffled around when executors are removed). In the end, it’s a business decision as to whether it’s worthwhile to create a system with more sophisticated resource management capabilities. 一般来说，集群的大小应该能够轻松地处理高于数据速率的突发事件。您应该在应用程序和集群中监控的关键指标讨论如下。一般来说，如果您看到您的输入速率远远高于您的处理速率（马上详细描述），那么是时候扩展集群或应用程序了。根据您的资源管理器和部署，您可能只能动态地向应用程序添加执行器。当遇到这种情况时，您可以用同样的方法缩小应用程序的规模，删除执行者（可能通过云提供商）或以较低的资源计数重新启动应用程序。这些更改可能会导致一些处理延迟（当执行器被删除时，数据会重新计算或分区会四处移动）。最后，对于是否值得创建一个具有更复杂资源管理功能的系统，这是一个业务决策。 While making underlying infrastructure changes to the cluster or application are sometimes necessary, other times a change may only require a restart of the application or stream with a new configuration. For instance, changing spark.sql.shuffle.partitions is not supported while a stream is currently running (it won’t actually change the number of shuffle partitions). This requires restarting the actual stream, not necessarily the entire application. Heavier weight changes, like changing arbitrary Spark application configurations, will likely require an application restart. 虽然有时需要对集群或应用程序进行基础结构更改，但在其他情况下，更改可能只需要用新配置重新启动应用程序或流。例如，当流当前正在运行时，不支持更改 spark.sql.shuffle.partitions（它实际上不会更改shuffle分区的数目）。这需要重新启动实际流，而不一定是整个应用程序。更重的重量变化，如改变任意的 Spark 应用程序配置，可能需要重新启动应用程序。 Metrics and Monitoring 量化指标和监控Metrics and monitoring in streaming applications is largely the same as for general Spark applications using the tools described in Chapter 18. However, Structured Streaming does add several more specifics in order to help you better understand the state of your application. There are two key APIs you can leverage to query the status of a streaming query and see its recent execution progress. With these two APIs, you can get a sense of whether or not your stream is behaving as expected. 流应用程序中的量化指标和监控与使用第18章中描述的工具的一般 Spark 应用程序基本相同。但是，结构化流确实添加了更多的细节，以帮助您更好地了解应用程序的状态。您可以利用两个关键API来查询流式查询的状态并查看其最近的执行进度。通过这两个API，您可以了解流是否按预期运行。 Query StatusThe query status is the most basic monitoring API, so it’s a good starting point. It aims to answer the question, “What processing is my stream performing right now?” This information is reported in the status field of the query object returned by startStream. For example, you might have a simple counts stream that provides counts of IOT devices defined by the following query (here we’re just using the same query from the previous chapter without the initialization code) : 查询状态是最基本的监控API，所以它是一个很好的起点。它的目的是回答这个问题，“我的流现在正在执行什么处理？”“此信息在 startStream 返回的查询对象的 status 字段中报告。例如，您可能有一个简单的计数流，它提供由以下查询定义的物联网设备计数（这里我们只使用上一章中的相同查询，而不使用初始化代码）： 1query.status To get the status of a given query, simply running the command query.status will return the current status of the stream. This gives us details about what is happening at that point in time in the stream. Here’s a sample of what you’ll get back when querying this status: 要获取给定查询的状态，只需运行命令 query.status 即可返回流的当前状态。这为我们提供了有关流中那个时间点发生的事情的详细信息。以下是查询此状态时将返回的示例： 12345&#123; &quot;message&quot; : &quot;Getting offsets from ...&quot;, &quot;isDataAvailable&quot; : true, &quot;isTriggerActive&quot; : true&#125; The above snippet describes getting the offsets from a Structured Streaming data source (hence the message describing getting offsets). There are a variety of messages to describe the stream’s status. 上面的代码段描述了从结构化流数据源获取偏移量（因此描述获取偏移量的消息）。有各种各样的消息来描述流的状态。 NOTE We have shown the status command inline here the way you would call it in a Spark shell. However, for a standalone application, you may not have a shell attached to run arbitrary code inside your process. In that case, you can expose its status by implementing a monitoring server, such as a small HTTP server that listens on a port and returns query.status when it gets a request. Alternatively, you can use the richer StreamingQueryListener API described later to listen to more events. 我们已经在这里显示了 status 命令，您可以在 Spark shell中调用它。但是，对于独立的应用程序，可能没有附加 shell 到进程内运行任意代码。在这种情况下，您可以通过实现监控服务器来公开其状态，例如在端口上侦听并在收到请求时返回 query.status 的小型 HTTP 服务器。或者，您可以使用后面描述的更丰富的 streamingQueryListener API来监听更多的事件。 Recent ProgressWhile the query’s current status is useful to see, equally important is an ability to view the query’s progress. The progress API allows us to answer questions like “At what rate am I processing tuples?” or “How fast are tuples arriving from the source?” By running query.recentProgress, you’ll get access to more time-based information like the processing rate and batch durations. The streaming query progress also includes information about the input sources and output sinks behind your stream. 虽然查询的当前状态很有用，但查看查询进度的能力同样重要。progress API 允许我们回答“我以什么速率处理元组（tuples）？”或者“元组（tuples）从源文件到达的速度有多快？”“通过运行 query.recentProgress，您可以访问更多基于时间的信息，如处理速率和批处理持续时间。流查询进度还包括有关流后面的输入源和输出接收器的信息。 1query.recentProgress Here’s the result of the Scala version after we ran the code from before; the Python one will be similar: 下面是 Scala 版本在运行之前的代码之后的结果；Python 版本将类似： 1234567891011121314151617181920212223242526272829303132Array(&#123; &quot;id&quot; : &quot;d9b5eac5-2b27-4655-8dd3-4be626b1b59b&quot;, &quot;runId&quot; : &quot;f8da8bc7-5d0a-4554-880d-d21fe43b983d&quot;, &quot;name&quot; : &quot;test_stream&quot;, &quot;timestamp&quot; : &quot;2017-08-06T21:11:21.141Z&quot;, &quot;numInputRows&quot; : 780119, &quot;processedRowsPerSecond&quot; : 19779.89350912779, &quot;durationMs&quot; : &#123; &quot;addBatch&quot; : 38179, &quot;getBatch&quot; : 235, &quot;getOffset&quot; : 518, &quot;queryPlanning&quot; : 138, &quot;triggerExecution&quot; : 39440, &quot;walCommit&quot; : 312 &#125;, &quot;stateOperators&quot; : [ &#123; &quot;numRowsTotal&quot; : 7, &quot;numRowsUpdated&quot; : 7 &#125; ], &quot;sources&quot; : [ &#123; &quot;description&quot; : &quot;FileStreamSource[/some/stream/source/]&quot;, &quot;startOffset&quot; : null, &quot;endOffset&quot; : &#123; &quot;logOffset&quot; : 0 &#125;, &quot;numInputRows&quot; : 780119, &quot;processedRowsPerSecond&quot; : 19779.89350912779 &#125; ], &quot;sink&quot; : &#123; &quot;description&quot; : &quot;MemorySink&quot; &#125;&#125;) As you can see from the output just shown, this includes a number of details about the state of the stream. It is important to note that this is a snapshot in time (according to when we asked for the query progress). In order to consistently get output about the state of the stream, you’ll need to query this API for the updated state repeatedly. The majority of the fields in the previous output should be selfexplanatory. However, let’s review some of the more consequential fields in detail. 正如您从刚刚显示的输出中看到的那样，这包括一些关于流状态的详细信息。需要注意的是，这是一个及时的快照（根据我们何时请求查询进度）。为了一致地获得有关流状态的输出，您需要反复查询此API以获取更新状态。上一个输出中的大多数字段都应该是一目了然的。但是，让我们详细回顾一些更重要的字段。 Input rate and processing rate 输入速率和处理速率The input rate specifies how much data is flowing into Structured Streaming from our input source. The processing rate is how quickly the application is able to analyze that data. In the ideal case, the input and processing rates should vary together. Another case might be when the input rate is much greater than the processing rate. When this happens, the stream is falling behind and you will need to scale the cluster up to handle the larger load. 输入速率指定从输入源流入结构化流的数据量。处理速度是应用程序分析数据的速度。在理想情况下，输入和处理速率应该同时变化。另一种情况可能是输入速率远远大于处理速率。当这种情况发生时，流将落在后面，您需要向上扩展集群以处理更大的负载。 Batch duration 批处理持续时间Nearly all streaming systems utilize batching to operate at any reasonable throughput (some have an option of high latency in exchange for lower throughput). Structured Streaming achieves both. As it operates on the data, you will likely see batch duration oscillate as Structured Streaming processes varying numbers of events over time. Naturally, this metric will have little to no relevance when the continuous processing engine is made an execution option. 几乎所有的流系统都利用批处理以任何合理的吞吐量运行（有些系统可以选择高延迟，以换取较低的吞吐量）。结构化流实现了这两个目标。当它对数据进行操作时，您可能会看到批处理持续时间随着结构化流处理时间的变化而波动。当然，当连续处理引擎成为一个执行选项时，这个量化指标几乎没有相关性。 TIP 提示 Generally it’s a best practice to visualize the changes in batch duration and input and processing rates. It’s much more helpful than simply reporting changes over time. 一般来说，将批处理持续时间、输入和处理速率的变化可视化是最佳实践。它比简单地报告随时间变化更有用。 Spark UI Spark用户界面The Spark web UI, covered in detail in Chapter 18, also shows tasks, jobs, and data processing metrics for Structured Streaming applications. On the Spark UI, each streaming application will appear as a sequence of short jobs, one for each trigger. However, you can use the same UI to see metrics, query plans, task durations, and logs from your application. One departure of note from the DStream API is that the Streaming Tab is not used by Structured Streaming. 第18章详细介绍了Spark Web 用户界面，它还显示了结构化流应用程序的任务、作业和数据处理指标。在Spark用户界面上，每个流式应用程序将显示为一系列短作业，每个触发器一个。但是，您可以使用同一个UI查看来自应用程序的量化指标、查询计划、任务工期和日志。与 DStream API 不同的一点是，结构化流不使用流选项卡。 Alerting 警告Understanding and looking at the metrics for your Structured Streaming queries is an important first step. However, this involves constantly watching a dashboard or the metrics in order to discover potential issues. You’re going to need robust automatic alerting to notify you when your jobs are failing or not keeping up with the input data rate without monitoring them manually. There are several ways to integrate existing alerting tools with Spark, generally building on the recent progress API we covered before. For example, you may directly feed the metrics to a monitoring system such as the open source Coda Hale Metrics library or Prometheus, or you may simply log them and use a log aggregation system like Splunk. In addition to monitoring and alerting on queries, you’re also going to want to monitor and alert on the state of the cluster and the overall application (if you’re running multiple queries together). 了解和查看结构化流式查询的指标是重要的第一步。但是，这需要不断观察仪表盘或指标，以发现潜在的问题。当你的工作失败或者没有手动监控就不能跟上输入数据速率时，你需要强大的自动警报来通知你。有几种方法可以将现有的警报工具与Spark集成在一起，通常基于我们之前介绍的新近发展的API。例如，您可以直接将量化指标输入监控系统，如开源 Coda Hale Metrics 库或 Prometheus ，也可以简单地将其记录并使用日志聚合系统，如Splunk。除了对查询进行监控和警报之外，您还需要对集群和整个应用程序的状态进行监控和发出警报（如果您一起运行多个查询）。 Advanced Monitoring with the Streaming Listener 使用流式侦听器进行高级监控We already touched on some of the high-level monitoring tools in Structured Streaming. With a bit of glue logic, you can use the status and queryProgress APIs to output monitoring events into your organization’s monitoring platform of choice (e.g., a log aggregation system or Prometheus dashboard). Beyond these approaches, there is also a lower-level but more powerful way to observe an application’s execution: the StreamingQueryListener class. 我们已经讨论了结构化流中的一些高级监控工具。使用一些粘合逻辑，您可以使用状态和 queryProgress API将监控事件输出到组织的监控平台（例如，日志聚合系统或 Prometheus 仪表板）。除了这些方法之外，还有一种更低阶但更强大的方法来观察应用程序的执行：StreamingQueryListener 类。 The StreamingQueryListener class will allow you to receive asynchronous updates from the streaming query in order to automatically output this information to other systems and implement robust monitoring and alerting mechanisms. You start by developing your own object to extend StreamingQueryListener, then attach it to a running SparkSession. Once you attach your custom listener with sparkSession.streams.addListener(), your class will receive notifications when a query is started or stopped, or progress is made on an active query. Here’s a simple example of a listener from the Structured Streaming documentation: StreamingQueryListener 类将允许您从流查询接收异步更新，以便自动将此信息输出到其他系统，并实现可靠的监控和警报机制。首先开发自己的对象来扩展 StreamingQueryListener，然后将其附加到正在运行的SparkSession。使用 sparkSession.streams.addListener（）附加自定义侦听器后，当查询启动或停止，或在活动查询上取得进展时，类将收到通知。以下是结构化流文档中侦听器的简单示例： 123456789101112131415val spark: SparkSession = ... spark.streams.addListener(new StreamingQueryListener() &#123; override def onQueryStarted(queryStarted: QueryStartedEvent): Unit = &#123; println("Query started: " + queryStarted.id) &#125; override def onQueryTerminated(queryTerminated: QueryTerminatedEvent): Unit = &#123; println("Query terminated: " + queryTerminated.id) &#125; override def onQueryProgress(queryProgress: QueryProgressEvent): Unit = &#123; println("Query made progress: " + queryProgress.progress) &#125;&#125;) Streaming listeners allow you to process each progress update or status change using custom code and pass it to external systems. For example, the following code for a StreamingQueryListener that will forward all query progress information to Kafka. You’ll have to parse this JSON string once you read data from Kafka in order to access the actual metrics: 流式侦听器（streaming listeners）允许您使用自定义代码处理每个进度更新或状态更改，并将其传递给外部系统。例如，下面的代码用于将所有查询进度信息转发到 Kafka 的 StreamingQueryListener。从Kafka读取数据后，必须解析这个JSON字符串，才能访问实际的量化指标： 12345678910111213141516171819class KafkaMetrics(servers: String) extends StreamingQueryListener &#123; val kafkaProperties = new Properties() kafkaProperties.put("bootstrap.servers", servers) kafkaProperties.put( "key.serializer", "kafkashaded.org.apache.kafka.common.serialization.StringSerializer") kafkaProperties.put( "value.serializer", "kafkashaded.org.apache.kafka.common.serialization.StringSerializer") val producer = new KafkaProducer[String, String](kafkaProperties) import org.apache.spark.sql.streaming.StreamingQueryListener import org.apache.kafka.clients.producer.KafkaProduceroverride def onQueryProgress(event : StreamingQueryListener.QueryProgressEvent): Unit = &#123; producer.send(new ProducerRecord("streaming-metrics", event.progress.json)) &#125; override def onQueryStarted(event: StreamingQueryListener.QueryStartedEvent) : Unit = &#123;&#125; override def onQueryTerminated(event: StreamingQueryListener.QueryTerminatedEvent) : Unit = &#123;&#125;&#125; Using the StreamingQueryListener interface, you can even monitor Structured Streaming applications on one cluster by running a Structured Streaming application on that same (or another) cluster. You could also manage multiple streams in this way. 使用streamingquerylistener接口，您甚至可以通过在同一个（或另一个）集群上运行结构化流应用程序来监控一个集群上的结构化流应用程序。您还可以用这种方式管理多个流。 Conclusion 结论In this chapter, we covered the main tools needed to run Structured Streaming in production: checkpoints for fault tolerance and various monitoring APIs that let you observe how your application is running. Lucky for you, if you’re running Spark in production already, many of the concepts and tools are similar, so you should be able to reuse a lot of your existing knowledge. Be sure to check Part IV to see some other helpful tools for monitoring Spark Applications. 在本章中，我们介绍了在生产环境中运行结构化流所需的主要工具：容错检查点和各种监控API，这些API允许您观察应用程序的运行情况。幸运的是，如果您已经在生产中运行了Spark，那么许多概念和工具都是类似的，因此您应该能够重用大量现有的知识。一定要检查第四部分，看看其他一些有助于监测 Spark 应用的工具。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 17 Deploying Spark]]></title>
    <url>%2F2019%2F08%2F07%2FChapter17_Deploying-Spark(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 17 Deploying SparkThis chapter explores the infrastructure you need in place for you and your team to be able to run Spark Applications: 本章将探讨您和您的团队能够运行Spark应用程序所需的基础架构： Cluster deployment choices 集群部署选择 Spark’s different cluster managers Spark的不同集群管理器 Deployment considerations and configuring deployments 部署考虑事项和配置部署 For the most, part Spark should work similarly with all the supported cluster managers ; however, customizing the setup means understanding the intricacies of each of the cluster management systems. The hard part is deciding on the cluster manager (or choosing a managed service). Although we would be happy to include all the minute details about how you can configure different cluster with different cluster managers, it’s simply impossible for this book to provide hyper-specific details for every situation in every single environment. The goal of this chapter, therefore, is not to discuss each of the cluster managers in full detail, but rather to look at their fundamental differences and to provide a reference for a lot of the material already available on the Spark website. Unfortunately, there is no easy answer to “which is the easiest cluster manager to run” because it varies so much by use case, experience, and resources. The Spark documentation site offers a lot of detail about deploying Spark with actionable examples. We do our best to discuss the most relevant points. 对于大多数人来说，Spark应该与所有受支持的集群管理器类似地工作；但是，自定义设置意味着要了解每个集群管理系统的复杂性。困难的部分是决定集群管理器（或选择托管服务）。虽然我们很乐意提供有关如何使用不同集群管理器配置不同集群的所有细节，但本书根本不可能为每个环境中的每种情况提供超特定的详细信息。因此，本章的目标不是详细讨论每个集群管理器，而是要查看它们的基本差异，并为Spark网站上已有的许多资料提供参考。不幸的是，“哪个是最容易运行的集群管理器”没有简单的答案，因为它因用例，经验和资源而变化很大。 Spark文档站点提供了有关使用可操作示例部署 Spark 的大量详细信息。我们会尽力讨论最相关的观点。As of this writing, Spark has three officially supported cluster managers : 在开始撰写本文时，Spark有三个官方支持的集群管理器： Standalone mode 独立模式 Hadoop YARN Apache Mesos These cluster managers maintain a set of machines onto which you can deploy Spark Applications. Naturally, each of these cluster managers has an opinionated view toward management, and so there are trade-offs and semantics that you will need to keep in mind. However, they all run Spark applications the same way (as covered in Chapter 16). Let’s begin with the first point: where to deploy your cluster. 这些集群管理器维护一组可以部署Spark应用程序的计算机。当然，这些集群管理者中的每一个都对管理有一种自以为是的观点，因此需要记住权衡和语义。但是，它们都以相同的方式运行 Spark 应用程序（如第16章所述）。让我们从第一点开始：部署集群的位置。 Where to Deploy Your Cluster to Run Spark Applications 在何处部署集群以运行Spark应用程序There are two high-level options for where to deploy Spark clusters: deploy in an on-premises cluster or in the public cloud. This choice is consequential and is therefore worth discussing. 在哪里部署Spark集群有两个高级选项：在内部部署集群或公共云中部署。这种选择是重要的，因此值得讨论。 On-Premises Cluster Deployments 内部部署集群部署Deploying Spark to an on-premises cluster is sometimes a reasonable option, especially for organizations that already manage their own datacenters. As with everything else, there are trade-offs to this approach. An on-premises cluster gives you full control over the hardware used, meaning you can optimize performance for your specific workload. However, it also introduces some challenges, especially when it comes to data analytics workloads like Spark. First, with on-premises deployment, your cluster is fixed in size, whereas the resource demands of data analytics workloads are often elastic. If you make your cluster too small, it will be hard to launch the occasional very large analytics query or training job for a new machine learning model, whereas if you make it large, you will have resources sitting idle. Second, for on-premises clusters, you need to select and operate your own storage system, such as a Hadoop file system or scalable key-value store. This includes setting up georeplication and disaster recovery if required. 将 Spark 部署到内部部署集群有时是一种合理的选择，特别是对于已经管理自己的数据中心的组织。与其他一切一样，这种方法存在权衡取舍。内部部署集群使您可以完全控制所使用的硬件，这意味着您可以针对特定工作负载优化性能。但是，它也带来了一些挑战，特别是在Spark等数据分析工作负载方面。首先，通过内部部署，您的集群的大小是固定的，而数据分析工作负载的资源需求通常是弹性的。如果您的集群太小，则很难为新的机器学习模型启动偶尔的非常大的分析查询或训练工作，而如果您将其扩大，则会使资源闲置。其次，对于内部部署集群，您需要选择并运行自己的存储系统，例如 Hadoop 文件系统或可伸缩键值存储。这包括在需要时设置地理复制和灾难恢复。If you are going to deploy on-premises, the best way to combat the resource utilization problem is to use a cluster manager that allows you to run many Spark applications and dynamically reassign resources between them, or even allows non-Spark applications on the same cluster. All of Spark’s supported cluster managers allow multiple concurrent applications, but YARN and Mesos have better support for dynamic sharing and also additionally support non-Spark workloads. Handling on-premisesresource sharing is likely going to be the biggest difference your users see day to day with Spark on-premises versus in the cloud: in public clouds, it’s easy to give each application its own cluster of exactly the required size for just the duration of that job. 如果要部署内部部署，解决资源利用率问题的最佳方法是使用集群管理器，它允许您运行许多 Spark 应用程序并在它们之间动态重新分配资源，甚至允许在相同集群上使用非 Spark 应用程序。所有 Spark 支持的集群管理器都允许多个并发应用程序，但 YARN 和 Mesos 可以更好地支持动态共享，还可以支持非Spark工作负载。处理资源共享可能是您的用户每天使用Spark内部部署与云中看到的最大差异：在公共云中，很容易为每个应用程序提供自己的集群，其中包含完全所需的大小那份工作。For storage, you have several different options, but covering all the trade-offs and operational details in depth would probably require its own book. The most common storage systems used for Spark are distributed file systems such as Hadoop’s HDFS and key-value stores such as Apache Cassandra. Streaming message bus systems such as Apache Kafka are also often used for ingesting data. All these systems have varying degrees of support for management, backup, and georeplication, sometimes built into the system and sometimes only through third-party commercial tools. Before choosing a storage option, we recommend evaluating the performance of its Spark connector and evaluating the available management tools. 对于存储，您有几种不同的选择，但是深入讨论所有权衡和操作细节可能需要它自己的书。用于 Spark 的最常见存储系统是分布式文件系统，例如 Hadoop 的 HDFS 和键值存储，例如 Apache Cassandra。诸如 Apache Kafka之类的流式消息总线系统也经常用于摄取数据。所有这些系统都对管理，备份和地理复制有不同程度的支持，有时内置于系统中，有时仅通过第三方商业工具。在选择存储选项之前，我们建议您评估其Spark连接器的性能并评估可用的管理工具。 Spark in the Cloud 云端SparkWhile early big data systems were designed for on-premises deployment, the cloud is now an increasingly common platform for deploying Spark. The public cloud has several advantages when it comes to big data workloads. First, resources can be launched and shut down elastically, so you can run that occasional “monster” job that takes hundreds of machines for a few hours without having to pay for them all the time. Even for normal operation, you can choose a different type of machine and cluster size for each application to optimize its cost performance—for example, launch machines with Graphics Processing Units (GPUs) just for your deep learning jobs. Second, public clouds include low-cost, georeplicated storage that makes it easier to manage large amounts of data. 虽然早期的大数据系统是为内部部署而设计的，但云现在是部署Spark的日益普遍的平台。在涉及大数据工作负载时，公共云有几个优点。首先，资源可以弹性地启动和关闭，因此您可以运行偶尔的“怪物”工作，这需要数百台机器几个小时，而无需一直为它们付费。即使是正常操作，您也可以为每个应用程序选择不同类型的计算机和群集大小，以优化其性价比——例如，仅为您的深度学习作业启动具有图形处理单元（GPU）的计算机。其次，公共云包括低成本，地理复制的存储，可以更轻松地管理大量数据。Many companies looking to migrate to the cloud imagine they’ll run their applications in the same way that they run their on-premises clusters. All the major cloud providers (Amazon Web Services [AWS], Microsoft Azure, Google Cloud Platform [GCP], and IBM Bluemix) include managed Hadoop clusters for their customers, which provide HDFS for storage as well as Apache Spark. This is actually not a great way to run Spark in the cloud, however, because by using a fixed-size cluster and file system, you are not going to be able to take advantage of elasticity. Instead, it is generally a better idea to use global storage systems that are decoupled from a specific cluster, such as Amazon S3, Azure Blob Storage, or Google Cloud Storage and spin up machines dynamically for each Spark workload. With decoupled compute and storage, you will be able to pay for computing resources only when needed, scale them up dynamically, and mix different hardware types. Basically, keep in mind that running Spark in the cloud need not mean migrating an on-premises installation to virtual machines: you can run Spark natively against cloud storage to take full advantage of the cloud’s elasticity, cost-saving benefit, and management tools without having to manage an on-premise computing stack within your cloud environment. 许多希望迁移到云的公司想象他们将以运行其内部部署集群的方式运行其应用程序。所有主要云提供商（Amazon Web Services [AWS]，Microsoft Azure，Google Cloud Platform [GCP]和IBM Bluemix）都为其客户提供托管Hadoop集群，这些集群为存储和Apache Spark提供HDFS。然而，这实际上不是在云中运行Spark的好方法，因为通过使用固定大小的集群和文件系统，您将无法利用弹性。而是，通常最好使用与特定集群（例如Amazon S3，Azure Blob存储或Google云存储）分离的全局存储系统，并为每个Spark工作负载动态启动计算机。通过分离计算和存储，您将能够仅在需要时为计算资源付费，动态扩展计算资源，并混合使用不同的硬件类型。基本上，请记住，在云中运行Spark并不意味着将本地安装迁移到虚拟机：您可以针对云存储本地运行Spark以充分利用云的弹性，成本节约优势和管理工具，而无需必须在云环境中管理内部部署计算堆栈。Several companies provide “cloud-native” Spark-based services, and all installations of Apache Spark can of course connect to cloud storage. Databricks, the company started by the Spark team from UC Berkeley, is one example of a service provider built specifically for Spark in the cloud. Databricks provides a simple way to run Spark workloads without the heavy baggage of a Hadoop installation. The company provides a number of features for running Spark more efficiently in the cloud, such as auto-scaling, auto-termination of clusters, and optimized connectors to cloud storage, as well as a collaborative environment for working on notebooks and standalone jobs. The company also provides a free Community Edition for learning Spark where you can run notebooks on a small cluster and share them live with others. A fun fact is that this entire book was written using the free Community Edition of Databricks, because we found the integrated Spark notebooks, live collaboration, and cluster management the easiest way to produce and test this content. 有几家公司提供“基于云原生”的基于 Spark 的服务，Apache Spark的所有安装当然都可以连接到云存储。由加州大学伯克利分校的 Spark 团队发起的公司 Databricks 是专门为云中的 Spark 构建的服务提供商的一个例子。 Databricks 提供了一种运行Spark工作负载的简单方法，而无需承担 Hadoop 安装的沉重负担。该公司提供了许多功能，可以在云中更有效地运行Spark，例如自动扩展，集群自动终止，云存储的优化连接器，以及用于处理 notebook 和独立作业的协作环境。该公司还提供免费的社区版学习 Spark，您可以在小型集群上运行 notebook 并与他人分享。一个有趣的事实是，整本书是使用免费的 Databricks 社区版编写的，因为我们发现集成的 Spark notebook，实时协作和集群管理是生成和测试此内容的最简单方法。If you run Spark in the cloud, much of the content in this chapter might not be relevant because you can often create a separate, short-lived Spark cluster for each job you execute. In that case, the standalone cluster manager is likely the easiest to use. However, you may still want to read this content if you’d like to share a longer-lived cluster among many applications, or to install Spark on virtual machines yourself. 如果您在云中运行Spark，本章中的大部分内容可能都不相关，因为您通常可以为您执行的每个作业创建一个单独的，短期的Spark集群。在这种情况下，独立集群管理器可能是最容易使用的。但是，如果您希望在许多应用程序之间共享一个较长期的集群，或者您自己在虚拟机上安装Spark，则可能仍希望阅读此内容。 Cluster Managers 集群管理器Unless you are using a high-level managed service, you will have to decide on the cluster manager to use for Spark. Spark supports three aforementioned cluster managers: standalone clusters, Hadoop YARN, and Mesos. Let’s review each of these. 除非您使用的是高级托管服务，否则您必须决定要用于Spark的集群管理器。Spark 支持三个上述集群管理器：独立集群（standalone clusters），Hadoop YARN 和 Mesos。让我们回顾一下这些。 Standalone Mode 独立模式Spark’s standalone cluster manager is a lightweight platform built specifically for Apache Spark workloads. Using it, you can run multiple Spark Applications on the same cluster. It also provides simple interfaces for doing so but can scale to large Spark workloads. The main disadvantage of the standalone mode is that it’s more limited than the other cluster managers—in particular, your cluster can only run Spark. It’s probably the best starting point if you just want to quickly get Spark running on a cluster, however, and you do not have experience using YARN or Mesos. Spark 的独立集群管理器是专为 Apache Spark 工作负载构建的轻量级平台。使用它，您可以在同一个集群上运行多个 Spark 应用程序。它还提供了简单的界面，但可以扩展到大型 Spark 工作负载。独立模式的主要缺点是它比其他集群管理器更有限——特别是，您的集群只能运行 Spark。如果您只想快速让 Spark 在群集上运行，那么这可能是最好的起点，但是您没有使用 YARN 或 Mesos 的经验。 Starting a standalone cluster 启动独立集群Starting a standalone cluster requires provisioning the machines for doing so. That means starting them up, ensuring that they can talk to one another over the network, and getting the version of Spark you would like to run on those sets of machines. After that, there are two ways to start the cluster: by hand or using built-in launch scripts. 启动独立群集需要配置计算机。这意味着启动它们，确保它们可以通过网络相互通信，并获得您希望在这些机器上运行的 Spark 版本。之后，有两种方法可以启动集群：手动或使用内置启动脚本。 Let’s first launch a cluster by hand. The first step is to start the master process on the machine that we want that to run on, using the following command : 让我们首先手动启动一个集群。第一步是使用以下命令在我们希望运行的机器上启动主进程： 1$SPARK_HOME/sbin/start-master.sh When we run this command, the cluster manager master process will start up on that machine. Once started, the master prints out a spark://HOST:PORT URI. You use this when you start each of the worker nodes of the cluster, and you can use it as the master argument to your SparkSession on application initialization. You can also find this URI on the master’s web UI, which is http://masterip-address:8080 by default. With that URI, start the worker nodes by logging in to each machine and running the following script using the URI you just received from the master node. The master machine must be available on the network of the worker nodes you are using, and the port must be open on the master node, as well: 运行此命令时，集群管理器主进程将在该计算机上启动。一旦启动，master 就打印出一个 spark://HOST:PORT URI。在启动集群的每个工作节点时使用它，并且可以在应用程序初始化时将其用作 SparkSession 的主参数。您还可以在 master 的 Web UI 上找到此 URI，默认情况下为 http://masterip-address:8080 。使用该 URI，通过登录到每台计算机并使用刚从主节点收到的 URI 运行以下脚本来启动工作节点。主机必须在您正在使用的工作节点的网络上可用，并且端口必须在主节点上打开，以及 ： 1$SPARK_HOME/sbin/start-slave.sh &lt;master-spark-URI&gt; As soon as you’ve run that on another machine, you have a Spark cluster running! This process is naturally a bit manual; thankfully there are scripts that can help to automate this process. 只要你在另一台机器上运行它，就会运行一个Spark集群！这个过程自然需要一点手动; 谢天谢地，有些脚本可以帮助自动化这个过程。 Cluster launch scripts 集群启动脚本You can configure cluster launch scripts that can automate the launch of standalone clusters. To do this, create a file called conf/slaves in your Spark directory that will contain the hostnames of all the machines on which you intend to start Spark workers, one per line. If this file does not exist, everything will launch locally. When you go to actually start the cluster, the master machine will access each of the worker machines via Secure Shell (SSH). By default, SSH is run in parallel and requires that you configure password-less (using a private key) access. If you do not have a password-less setup, you can set the environment variable SPARK_SSH_FOREGROUND and serially provide a password for each worker. 您可以配置可以自动启动独立集群的集群启动脚本。为此，在Spark目录中创建一个名为 conf/slaves 的文件，该文件将包含要在其上启动 Spark 工作的所有计算机的主机名，每行一个。如果此文件不存在，则所有内容都将在本地启动。当您真正启动集群时，主计算机将通过 Secure Shell(SSH) 访问每个工作计算机。默认情况下，SSH 是并行运行的，需要您配置无密码（使用私钥）访问。如果您没有无密码设置，则可以设置环境变量SPARK_SSH_FOREGROUND 并为每个工作人员连续提供密码。 After you set up this file, you can launch or stop your cluster by using the following shell scripts, based on Hadoop’s deploy scripts, and available in $SPARK_HOME/sbin: 设置此文件后，可以使用以下基于 Hadoop 部署脚本的 shell 脚本启动或停止集群，并在 $SPARK_HOME/sbin 中提供： $SPARK_HOME/sbin/start-master.sh Starts a master instance on the machine on which the script is executed. 在执行脚本的计算机上启动主实例。 $SPARK_HOME/sbin/start-slaves.sh Starts a slave instance on each machine specified in the conf/slaves file. 在conf / slaves文件中指定的每台计算机上启动从属实例。 $SPARK_HOME/sbin/start-slave.sh Starts a slave instance on the machine on which the script is executed. 在执行脚本的计算机上启动从属实例。 $SPARK_HOME/sbin/start-all.sh Starts both a master and a number of slaves as described earlier. 如前所述，启动主服务器和多个从服务器。 $SPARK_HOME/sbin/stop-master.sh Stops the master that was started via the bin/start-master.sh script. 停止通过bin / start-master.sh脚本启动的主服务器。 $SPARK_HOME/sbin/stop-slaves.sh Stops all slave instances on the machines specified in the conf/slaves file. 停止conf / slaves文件中指定的计算机上的所有从属实例。 $SPARK_HOME/sbin/stop-all.sh Stops both the master and the slaves as described earlier. 如前所述，停止主站和从站。 Standalone cluster configurations 独立集群配置Standalone clusters have a number of configurations that you can use to tune your application. These control everything from what happens to old files on each worker for terminated applications to the worker’s core and memory resources. These are controlled via environment variables or via application properties. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Standalone Environment Variables in the Spark documentation. 独立集群具有许多可用于调整应用程序的配置。它们控制从终止应用程序的每个工作程序上的旧文件发生到工作程序的核心和内存资源的所有内容。这些是通过环境变量或应用程序属性控制的。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中有关独立环境变量的相关表。 Submitting applications 提交申请After you create the cluster, you can submit applications to it using the spark://URI of the master. You can do this either on the master node itself or another machine using spark-submit. There are some specific command-line arguments for standalone mode, which we covered in “Launching Applications”. 创建集群后，可以使用主服务器的 spark://URI 向其提交应用程序。您可以在主节点本身或使用 spark-submit 的其他计算机上执行此操作。独立模式有一些特定的命令行参数，我们在“启动应用程序”中介绍了这些参数。 Spark on YARN 运行在YARN上SparkHadoop YARN is a framework for job scheduling and cluster resource management. Even though Spark is often (mis)classified as a part of the “Hadoop Ecosystem,” in reality, Spark has little to do with Hadoop. Spark does natively support the Hadoop YARN cluster manager but it requires nothing from Hadoop itself. Hadoop YARN 是一个用于作业调度和集群资源管理的框架。尽管 Spark 经常（错误地）归类为 “Hadoop生态系统” 的一部分，但事实上，Spark与Hadoop几乎没有关系。 Spark 本身支持 Hadoop YARN 集群管理器，但它不需要Hadoop本身。 You can run your Spark jobs on Hadoop YARN by specifying the master as YARN in the spark-submit command-line arguments. Just like with standalone mode, there are a number of knobs that you are able to tune according to what you would like the cluster to do. The number of knobs is naturally larger than that of Spark’s standalone mode because Hadoop YARN is a generic scheduler for a large number of different execution frameworks. 您可以通过在 spark-submit 命令行参数中将 master 指定为 YARN 来在 Hadoop YARN 上运行 Spark 作业。就像独立模式一样，有许多旋钮可以根据您希望集群执行的操作进行调整。旋钮的数量自然大于 Spark 的独立模式，因为 Hadoop YARN 是大量不同执行框架的通用调度程序。 Setting up a YARN cluster is beyond the scope of this book, but there are some great books on the topic as well as managed services that can simplify this experience. 设置 YARN 集群超出了本书的范围，但是有一些关于该主题的优秀书籍以及可以简化此体验的托管服务。 Submitting applications 提交申请When submitting applications to YARN, the core difference from other deployments is that --master will become yarn as opposed the master node IP, as it is in standalone mode. Instead, Spark will find the YARN configuration files using the environment variable HADOOP_CONF_DIR or YARN_CONF_DIR. Once you have set those environment variables to your Hadoop installation’s configuration directory, you can just run spark-submit like we saw in Chapter 16. 在向 YARN 提交申请时，与其他部署的核心区别在于 --master 将成为 YARN 而不是主节点 IP，正如它处于独立模式的那样。相反，Spark 将使用环境变量 HADOOP_CONF_DIR 或 YARN_CONF_DIR 找到 YARN 配置文件。一旦将这些环境变量设置到 Hadoop 安装的配置目录中，就可以像我们在第16章中看到的那样运行 spark-submit。 NOTE 注意There are two deployment modes that you can use to launch Spark on YARN. As discussed in previous chapters, cluster mode has the spark driver as a process managed by the YARN cluster, and the client can exit after creating the application. In client mode, the driver will run in the client process and therefore YARN will be responsible only for granting executor resources to the application, not maintaining the master node. Also of note is that in cluster mode, Spark doesn’t necessarily run on the same machine on which you’re executing. Therefore libraries and external jars must be distributed manually or through the --jars command-line argument. 您可以使用两种部署模式在 YARN 上启动 Spark。如前几章所述，集群模式将 Spark 驱动程序作为 YARN 集群管理的进程，客户端可以在创建应用程序后退出。在客户端模式下，驱动程序将在客户端进程中运行，因此 YARN 仅负责向应用程序授予执行器（executor）资源，而不是维护主节点。另外值得注意的是，在集群模式下，Spark不一定在您正在执行的同一台机器上运行。因此，必须手动或通过 --jars 命令行参数分发库和外部jar。 There are a few YARN-specific properties that you can set by using spark-submit. These allow you to control priority queues and things like keytabs for security. We covered these in “Launching Applications” in Chapter 16. 您可以使用 spark-submit 设置一些特定于 YARN 的属性。这些允许您控制优先级队列和诸如 keytabs 之类的东西以确保安全性。我们在第16章的“启动应用程序”中介绍了这些内容。 Configuring Spark on YARN Applications 在YARN应用程序上配置SparkDeploying Spark as YARN applications requires you to understand the variety of different configurations and their implications for your Spark applications. This section covers some best practices for basic configurations and includes references to some of the important configuration for running your Spark applications. 将Spark部署为YARN应用程序需要您了解各种不同的配置及其对 Spark 应用程序的影响。本节介绍了一些基本配置的最佳实践，并包括对运行Spark应用程序的一些重要配置的引用。 Hadoop configurations Hadoop配置If you plan to read and write from HDFS using Spark, you need to include two Hadoop configuration files on Spark’s classpath: hdfs-site.xml, which provides default behaviors for the HDFS client; and core-site.xml, which sets the default file system name. The location of these configuration files varies across Hadoop versions, but a common location is inside of /etc/hadoop/conf. Some tools create these configurations on the fly, as well, so it’s important to understand how your managed service might be deploying these, as well. 如果您计划使用 Spark 从 HDFS 读取和写入，则需要在 Spark 的类路径中包含两个Hadoop配置文件：hdfs-site.xml，它为 HDFS 客户端提供默认行为; 和 core-site.xml，它设置默认文件系统名称。这些配置文件的位置因 Hadoop 版本而异，但常见位置在 /etc/hadoop/conf 中。有些工具也可以动态创建这些配置，因此了解托管服务如何部署这些配置也很重要。 To make these files visible to Spark, set HADOOP_CONF_DIR in $SPARK_HOME/spark-env.sh to a location containing the configuration files or as an environment variable when you go to spark–submit your application. 要使这些文件对 Spark 可见，请将 $SPARK_HOME/spark-env.sh 中的 HADOOP_CONF_DIR 设置为包含配置文件的位置，或者当您转到 spark-submit 应用程序时将其设置为环境变量。 Application properties for YARN YARN的应用程序属性There are a number of Hadoop-related configurations and things that come up that largely don’t have much to do with Spark, just running or securing YARN in a way that influences how Spark runs. Due to space limitations, we cannot include the configuration set here. Refer to the relevant table on YARN Configurations in the Spark documentation. 有许多与 Hadoop 相关的配置和出现的东西很大程度上与 Spark 没什么关系，只是以影响 Spark 运行或保护 YARN 的方式。 由于空间限制，我们不能在此处包含配置集。 请参阅 Spark文档 中有关 YARN配置的相关表。 Spark on Mesos 在Mesos运行的SparkApache Mesos is another clustering system that Spark can run on. A fun fact about Mesos is that the project was also started by many of the original authors of Spark, including one of the authors of this book. In the Mesos project’s own words : Apache Mesos 是 Spark 可以运行的另一个集群系统。关于 Mesos 的一个有趣的事实是该项目也是由Spark的许多原作者创建的，包括本书的作者之一。在Mesos项目中用自己的话来说 ： Apache Mesos abstracts CPU, memory, storage, and other compute resources away from machines (physical or virtual), enabling fault-tolerant and elastic distributed systems to easily be built and run effectively. Apache Mesos将CPU，内存，存储和其他计算资源从机器（物理或虚拟）中抽象出来，使容错和弹性分布式系统能够轻松构建并有效运行。For the most part, Mesos intends to be a datacenter scale-cluster manager that manages not just short-lived applications like Spark, but long-running applications like web applications or other resource interfaces. Mesos is the heaviest-weight cluster manager, simply because you might choose this cluster manager only if your organization already has a large-scale deployment of Mesos, but it makes for a good cluster manager nonetheless. 在大多数情况下，Mesos打算成为一个数据中心规模集群管理器，它不仅管理像Spark这样的短期应用程序，而且管理长期运行的应用程序，如Web应用程序或其他资源接口。 Mesos是最重的集群管理器，仅仅因为您可能只在您的组织已经大规模部署Mesos时才选择此集群管理器，但它仍然是一个优秀的集群管理器。Mesos is a large piece of infrastructure, and unfortunately there’s simply too much information for us to cover how to deploy and maintain Mesos clusters. There are many great books on the subject for that, including Dipa Dubhashi and Akhil Das’s Mastering Mesos (O’Reilly, 2016). The goal here is to bring up some of the considerations that you’ll need to think about when running Spark Applications on Mesos. Mesos 是一个很大的基础架构，不幸的是，我们有太多的信息来介绍如何部署和维护 Mesos 集群。有很多关于这个主题的好书，包括 Dipa Dubhashi 和 Akhil Das 的 《Mastering Mesos》（O’Reilly，2016）。这里的目标是提出在 Mesos 上运行 Spark 应用程序时需要考虑的一些注意事项。 For instance, one common thing you will hear about Spark on Mesos is fine-grained versus coarse-grained mode. Historically Mesos supported a variety of different modes (fine-grained and coarse-grained), but at this point, it supports only coarse-grained scheduling (fine-grained has been deprecated). Coarse-grained mode means that each Spark executor runs as a single Mesos task. Spark executors are sized according to the following application properties : 例如，你会听到关于 Spark on Mesos 的一个常见的事情是细粒度和粗粒度模式。从历史上看，Mesos支持各种不同的模式（细粒度和粗粒度），但此时，它仅支持粗粒度调度（细粒度已被弃用）。粗粒度模式意味着每个Spark执行程序作为单个Mesos任务运行。 Spark执行程序根据以下应用程序属性调整大小： 123spark.executor.memoryspark.executor.coresspark.cores.max/spark.executor.cores Submitting applicationsSubmitting applications to a Mesos cluster is similar to doing so for Spark’s other cluster managers. For the most part you should favor cluster mode when using Mesos. Client mode requires some extra configuration on your part, especially with regard to distributing resources around the cluster. For instance, in client mode, the driver needs extra configuration information in spark-env.sh to work with Mesos. 将应用程序提交到 Mesos 集群与 Spark 的其他集群管理器类似。 在大多数情况下，您应该在使用 Mesos 时使用集群模式。 客户端模式需要您进行一些额外配置，尤其是在集群上分配资源方面。 例如，在客户端模式下，驱动程序需要 spark-env.sh 中的额外配置信息才能使用Mesos。In spark-env.sh set some environment variables : 在spark-env.sh中设置一些环境变量： 1234export MESOS_NATIVE_JAVA_LIBRARY=&lt;path to libmesos.so&gt;This path is typically &lt;prefix&gt;/lib/libmesos.so where the prefix is /usr/local by default. On Mac OSX, the library is called libmesos.dylib instead of libmesos.so:export SPARK_EXECUTOR_URI=&lt;URL of spark-2.2.0.tar.gz uploaded above&gt; Finally, set the Spark Application property spark.executor.uri to . Now, when starting a Spark application against the cluster, pass a mesos://URL as the master when creating a SparkContex , and set that property as a parameter in your SparkConf variable or the initialization of a SparkSession : 最后，将 Spark Application 属性 spark.executor.uri 设置为 。 现在，在针对集群启动 Spark 应用程序时，在创建 SparkContext 时将 mesos://URL 作为 master 传递，并将该属性设置为 SparkConf 变量中的参数或 SparkSession 的初始化： 1234567// in Scalaimport org.apache.spark.sql.SparkSessionval spark = SparkSession.builder.master("mesos://HOST:5050").appName("my app").config("spark.executor.uri", "&lt;path to spark-2.2.0.tar.gz uploaded above&gt;").getOrCreate() Submitting cluster mode applications is fairly straightforward and follows the same spark-submit structure you read about before. We covered these in “Launching Applications”. 提交集群模式应用程序非常简单，并遵循您之前阅读的相同的 spark-submit 结构。 我们在“启动应用程序”中介绍了这些内容。 Configuring Mesos 配置MesosJust like any other cluster manager, there are a number of ways that we can configure our Spark Applications when they’re running on Mesos. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Mesos Configurations in the Spark documentation. 就像任何其他集群管理器一样，我们可以通过多种方式在 Spark 应用程序运行时配置它们。 由于篇幅限制，我们无法在此处包含整个配置集。 请参阅 Spark文档 中有关 Mesos配置的相关表 。 Secure Deployment Configurations 安全部署配置Spark also provides some low-level ability to make your applications run more securely, especially in untrusted environments. Note that the majority of this setup will happen outside of Spark. These configurations are primarily network-based to help Spark run in a more secure manner. This means authentication, network encryption, and setting TLS and SSL configurations. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Security Configurations in the Spark documentation. Spark 还提供了一些低阶功能，使您的应用程序运行更安全，尤其是在不受信任的环境中。请注意，此设置的大部分将在 Spark 之外发生。这些配置主要基于网络，以帮助 Spark 以更安全的方式运行。这意味着身份验证，网络加密以及设置 TLS 和 SSL 配置。由于篇幅限制，我们无法在此处包含整个配置集。请参阅 Spark文档 中有关 安全配置的相关表 。 Cluster Networking Configurations 集群网络配置Just as shuffles are important, there can be some things worth tuning on the network. This can also be helpful when performing custom deployment configurations for your Spark clusters when you need to use proxies in between certain nodes. If you’re looking to increase Spark’s performance, these should not be the first configurations you go to tune, but may come up in custom deployment scenarios. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Networking Configurations in the Spark documentation. 正如洗牌（shuffle）很重要，网络上可能会有一些值得调整的东西。当您需要在某些节点之间使用代理时，在为Spark 集群执行自定义部署配置时，这也很有用。如果您希望提高Spark的性能，这些不应该是您要调整的第一个配置，但可能会出现在自定义部署方案中。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档 中的网络配置相关表。 Application Scheduling 应用程序调度Spark has several facilities for scheduling resources between computations. First, recall that, as described earlier in the book, each Spark Application runs an independent set of executor processes. Cluster managers provide the facilities for scheduling across Spark applications. Second, within each Spark application, multiple jobs (i.e., Spark actions) may be running concurrently if they were submitted by different threads. This is common if your application is serving requests over the network. Spark includes a fair scheduler to schedule resources within each application. We introduced this topic in the previous chapter. Spark 有几种用于在计算之间调度资源的工具。首先，回想一下，如本书前面所述，每个 Spark 应用程序都运行一组独立的执行器（executor）进程。集群管理器提供跨Spark应用程序进行调度的工具。其次，在每个Spark应用程序中，如果它们是由不同的线程提交的，则多个作业（即 Spark actions ）可以同时运行。如果您的应用程序通过网络提供请求，这种情况很常见。 Spark包含一个公平的调度程序（fair scheduler）来安排每个应用程序中的资源。我们在前一章介绍了这个主题。 If multiple users need to share your cluster and run different Spark Applications, there are different options to manage allocation, depending on the cluster manager. The simplest option, available on all cluster managers, is static partitioning of resources. With this approach, each application is given a maximum amount of resources that it can use, and holds onto those resources for the entire duration. In spark-submit there are a number of properties that you can set to control the resource allocation of a particular application. Refer to Chapter 16 for more information. In addition, dynamic allocation (described next) can be turned on to let applications scale up and down dynamically based on their current number of pending tasks. If, instead, you want users to be able to share memory and executor resources in a fine-grained manner, you can launch a single Spark Application and use thread scheduling within it to serve multiple requests in parallel. 如果多个用户需要共享您的集群并运行不同的Spark应用程序，则可以使用不同的选项来管理分配，具体取决于集群管理器。所有集群管理器上都可以使用的最简单的选项是资源的静态分区（static partitioning of resources）。通过这种方法，每个应用程序都可以使用最多的资源，并在整个持续时间内保留这些资源。在spark-submit中，您可以设置许多属性来控制特定应用程序的资源分配。有关更多信息，请参阅第16章。此外，可以打开动态分配（下面描述），让应用程序根据当前挂起的任务数量动态扩展和缩小。相反，如果您希望用户能够以细粒度的方式共享内存和执行程序资源，则可以启动单个Spark应用程序并在其中使用线程调度来并行处理多个请求。 Dynamic allocation 动态分配If you would like to run multiple Spark Applications on the same cluster, Spark provides a mechanism to dynamically adjust the resources your application occupies based on the workload. This means that your application can give resources back to the cluster if they are no longer used, and request them again later when there is demand. This feature is particularly useful if multiple applications share resources in your Spark cluster. 如果您希望在同一个集群上运行多个Spark应用程序，Spark会提供一种机制，根据工作负载动态调整应用程序占用的资源。这意味着如果不再使用，您的应用程序可以将资源提供给集群，并在需要时稍后再次请求它们。如果多个应用程序共享Spark集群中的资源，则此功能特别有用。This feature is disabled by default and available on all coarse-grained cluster managers; that is, standalone mode, YARN mode, and Mesos coarse-grained mode. There are two requirements for using this feature. First, your application must set spark.dynamicAllocation.enabled to true. Second, you must set up an external shuffle service on each worker node in the same cluster and set spark.shuffle.service.enabled to true in your application. The purpose of the external shuffle service is to allow executors to be removed without deleting shuffle files written by them. This is set up differently for each cluster manager and is described in the job scheduling configuration. Due to space limitations, we cannot include the configuration set for dynamic allocation. Refer to the relevant table on Dynamic Allocation Configurations. 默认情况下禁用此功能，并且所有粗粒度集群管理器均可使用此功能; 也就是独立模式，YARN模式和Mesos粗粒度模式。使用此功能有两个要求。首先，您的应用程序必须将 spark.dynamicAllocation.enabled 设置为true。其次，必须在同一群集中的每个工作节点上设置外部洗牌（shuffle） 服务，并在应用程序中将 spark.shuffle.service.enabled 设置为 true。外部 shuffle 服务的目的是允许删除执行程序而不删除它们写入的shuffle文件。对于每个集群管理器，此设置不同，并在作业调度配置中进行了描述。由于空间限制，我们不能包含动态分配的配置集。请参阅动态分配配置的相关表。 Miscellaneous Considerations 各种各样的考虑因素There several other topics to consider when deploying Spark applications that may affect your choice of cluster manager and its setup. These are just things that you should think about when comparing different deployment options. 在部署可能影响您选择的集群管理器及其设置的 Spark 应用程序时，还需要考虑其他几个主题。在比较不同的部署选项时，您应该考虑这些事项。 One of the more important considerations is the number and type of applications you intend to be running. For instance, YARN is great for HDFS-based applications but is not commonly used for much else. Additionally, it’s not well designed to support the cloud, because it expects information to be available on HDFS. Also, compute and storage is largely coupled together, meaning that scaling your cluster involves scaling both storage and compute instead of just one or the other. Mesos does improve on this a bit conceptually, and it supports a wide range of application types, but it still requires pre-provisioning machines and, in some sense, requires buy-in at a much larger scale. For instance, it doesn’t really make sense to have a Mesos cluster for only running Spark Applications. Spark standalone mode is the lightest-weight cluster manager and is relatively simple to understand and take advantage of, but then you’re going to be building more application management infrastructure that you could get much more easily by using YARN or Mesos. 其中一个更重要的考虑因素是您打算运行的应用程序的数量和类型。例如，YARN 非常适合基于 HDFS 的应用程序，但并不常用于其他许多应用程序。此外，它还没有很好地支持云，因为它希望在 HDFS 上提供信息。此外，计算和存储在很大程度上是耦合在一起的，这意味着扩展集群涉及扩展存储和计算，而不仅仅是一个或另一个。 Mesos 在概念上确实有所改进，它支持多种应用类型，但它仍然需要预配置机器，从某种意义上说，需要更大规模的支持。例如，只运行 Spark 应用程序的 Mesos 集群没有真正发挥它的价值。 Spark 独立模式是最轻量级的集群管理器，并且相对易于理解和利用，但随后您将构建更多应用程序管理基础架构，使用 YARN 或 Mesos 可以更轻松地获得这些基础架构。Another challenge is managing different Spark versions. Your hands are largely tied if you want to try to run a variety of different applications running different Spark versions, and unless you use a well-managed service, you’re going to need to spend a fair amount of time either managing different setup scripts for different Spark services or removing the ability for your users to use a variety of different Spark applications. 另一个挑战是管理不同的 Spark 版本。如果你想尝试运行运行不同 Spark 版本的各种不同应用程序，你的手很大程度上是捆绑在一起的，除非你使用管理良好的服务，否则你需要花费相当多的时间来管理不同的设置脚本用于不同的 Spark 服务或删除用户使用各种不同 Spark 应用程序的能力。Regardless of the cluster manager that you choose, you’re going to want to consider how you’re going to set up logging, store logs for future reference, and allow end users to debug their applications. These are more “out of the box” for YARN or Mesos and might need some tweaking if you’re using standalone. 无论您选择哪个集群管理器，您都会想要考虑如何设置日志记录，存储日志以供将来参考，以及允许最终用户调试其应用程序。对于 YARN 或 Mesos 来说，这些更“开箱即用”，如果您使用独立的话，可能需要进行一些调整。One thing you might want to consider—or that might influence your decision making—is maintaining a metastore in order to maintain metadata about your stored datasets, such as a table catalog. We saw how this comes up in Spark SQL when we are creating and maintaining tables. Maintaining an Apache Hive metastore, a topic beyond the scope of this book, might be something that’s worth doing to facilitate more productive, cross-application referencing to the same datasets. Depending on your workload, it might be worth considering using Spark’s external shuffle service. Typically Spark stores shuffle blocks (shuffle output) on a local disk on that particular node. An external shuffle service allows for storing those shuffle blocks so that they are available to all executors, meaning that you can arbitrarily kill executors and still have their shuffle outputs available to other applications. 您可能想要考虑的一件事——或者可能影响您决策的一件事——是维护一个 Metastore，以维护有关您存储的数据集的元数据，例如表目录。我们在创建和维护表时看到了如何在 Spark SQL 中出现这种情况。维护 Apache Hive Metastore 是一个超出本书范围的主题，可能值得做些什么来促进对同一数据集的更高效，跨应用程序的引用。根据您的工作量，可能值得考虑使用 Spark 的外部 shuffle 服务。通常，Spark 将 shuffle 块（ shuffle 输出）存储在该特定节点上的本地磁盘上。外部 shuffle 服务允许存储这些 shuffle 块，以便它们可供所有执行程序使用，这意味着您可以任意杀死执行程序，并且仍然可以将其 shuffle 输出提供给其他应用程序。Finally, you’re going to need to configure at least some basic monitoring solution and help users debug their Spark jobs running on their clusters. This is going to vary across cluster management options and we touch on some of the things that you might want to set up in Chapter 18. 最后，您将需要至少配置一些基本监视解决方案，并帮助用户调试在其集群上运行的 Spark 作业。这在集群管理选项中会有所不同，我们会讨论您可能希望在第18章中设置的一些内容。 Conclusion 结论This chapter looked at the world of configuration options that you have when choosing how to deploy Spark. Although most of the information is irrelevant to the majority of users, it is worth mentioning if you’re performing more advanced use cases. It might seem fallacious, but there are other configurations that we have omitted that control even lower-level behavior. You can find these in the Spark documentation or in the Spark source code. Chapter 18 talks about some of the options that we have when monitoring Spark Applications. 本章介绍了在选择部署 Spark 时所具有的配置选项的世界。虽然大多数信息与大多数用户无关，但如果您正在执行更高级的用户案例，则值得一提。它可能看起来很荒谬，但是我们已经省略了其他配置来控制甚至更低阶的行为。您可以在 Spark 文档或 Spark 源代码中找到它们。第18章讨论了监视Spark应用程序时的一些选项。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 15 How Spark Runs on a Cluster]]></title>
    <url>%2F2019%2F08%2F05%2FChapter15_HowSparkRuns-on-a-Cluster(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 15 How Spark Runs on a Cluster Spark如何在集群上的运行Thus far in the book, we focused on Spark’s properties as a programming interface. We have discussed how the structured APIs take a logical operation, break it up into a logical plan, and convert that to a physical plan that actually consists of Resilient Distributed Dataset (RDD) operations that execute across the cluster of machines. This chapter focuses on what happens when Spark goes about executing that code. We discuss this in an implementation-agnostic way—this depends on neither the cluster manager that you’re using nor the code that you’re running. At the end of the day, all Spark code runs the same way. 到目前为止，在书中，我们将重点放在Spark作为编程接口的属性上。我们已经讨论了结构化API如何执行逻辑操作，将其分解为逻辑计划，并将其转换为实际由跨机器集群执行的弹性分布式数据集（RDD）操作组成的物理计划。本章主要讨论 Spark 执行该代码时会发生什么。我们以一种不知实现的方式讨论这个问题，这既不依赖于您正在使用的集群管理器，也不依赖于您正在运行的代码。一天结束时，所有 Spark 代码都以相同的方式运行。 This chapter covers several key topics:本章包括几个关键主题： - The architecture and components of a Spark Application Spark应用程序的体系结构和组件- The life cycle of a Spark Application inside and outside of Spark Spark 内外 Spark 应用的生命周期- Important low-level execution properties, such as pipelining 重要的低级执行属性，如管道- What it takes to run a Spark Application, as a segue into Chapter 16. 运行Spark应用程序需要什么，作为转到第16章的桥接。Let’s begin with the architecture.让我们从架构开始## The Architecture of a Spark Application Spark应用程序的架构In Chapter 2, we discussed some of the high-level components of a Spark Application. Let’s review those again:在第2章中，我们讨论了 Spark 应用程序的一些高级组件。让我们再次回顾一下：The Spark driver Spark驱动器The driver is the process “in the driver seat” of your Spark Application. It is the controller of the execution of a Spark Application and maintains all of the state of the Spark cluster (the state and tasks of the executors). It must interface with the cluster manager in order to actually get physical resources and launch executors. At the end of the day, this is just a process on a physical machine that is responsible for maintaining the state of the application running on the cluster.驱动器是 Spark 应用程序处在“驾驶员席位”的进程。它是Spark应用程序执行的控制器，维护Spark集群的所有状态（执行器的状态和任务）。它必须与群集管理器接口，以便实际获得物理资源和启动执行器。最后，这只是一个物理机器上的进程，负责维护集群上运行的应用程序的状态。The Spark executors Spark执行器Spark executors are the processes that perform the tasks assigned by the Spark driver. Executors have one core responsibility: take the tasks assigned by the driver, run them, and report back their state (success or failure) and results. Each Spark Application has its own separate executor processes.Spark 执行器是执行 Spark 驱动程序分配的任务的进程。执行者有一个核心责任：承担驱动程序分配的任务，运行它们，并报告它们的状态（成功或失败）和结果。每个Spark应用程序都有自己的独立执行器进程。The cluster manager 集群管理员The Spark Driver and Executors do not exist in a void, and this is where the cluster manager comes in. The cluster manager is responsible for maintaining a cluster of machines that will run your Spark Application(s). Somewhat confusingly, a cluster manager will have its own “driver” (sometimes called master) and “worker” abstractions. The core difference is that these are tied to physical machines rather than processes (as they are in Spark). Figure 15-1 shows a basic cluster setup. The machine on the left of the illustration is the Cluster Manager Driver Node. The circles represent daemon processes running on and managing each of the individual worker nodes. There is no Spark Application running as of yet—these are just the processes from the cluster manager.Spark驱动程序和执行器不存在于一个空间，这就是集群管理器所处的位置。集群管理器负责维护运行Spark应用程序的机器集群。有些令人困惑的是，集群管理器将有自己的“驱动程序（driver）”（有时称为master）和“工作者（worker）”的抽象结构。核心区别在于，它们与物理机器而不是进程（如 Spark 中的进程）联系在一起。 图15-1显示了一个基本的集群设置。图左侧的机器是群集管理器驱动程序节点。圆圈表示运行在每个工作节点上并管理每个工作节点的守护进程。到目前为止还没有运行spark应用程序，这些只是来自集群管理器的进程。 When it comes time to actually run a Spark Application, we request resources from the cluster manager to run it. Depending on how our application is configured, this can include a place to run the Spark driver or might be just resources for the executors for our Spark Application. Over the course of Spark Application execution, the cluster manager will be responsible for managing the underlying machines that our application is running on.在实际运行Spark应用程序时，我们从集群管理器请求资源来运行它。根据应用程序的配置方式，这可能包括一个运行Spark驱动程序的位置，或者可能只是Spark应用程序的执行者的资源。在Spark应用程序执行过程中，集群管理员将负责管理应用程序运行的底层机器。Spark currently supports three cluster managers: a simple built-in standalone cluster manager, Apache Mesos, and Hadoop YARN. However, this list will continue to grow, so be sure to check the documentation for your favorite cluster manager. Now that we’ve covered the basic components of an application, let’s walk through one of the first choices you will need to make when running your applications: choosing the execution mode.Spark目前支持三个集群管理器：一个简单的内置独立集群管理器、Apache Mesos 和 Hadoop Yarn。但是，这个列表将继续增长，因此一定要检查您最喜欢的集群管理器的文档。既然我们已经介绍了应用程序的基本组件，那么让我们来看看在运行应用程序时需要做的第一个选择：选择执行模式。 ### Execution Modes 执行模式An execution mode gives you the power to determine where the aforementioned resources are physically located when you go to run your application. You have three modes to choose from:执行模式使您能够在运行应用程序时确定上述资源的物理位置。您有三种模式可供选择： - Cluster mode- Client mode- Local modeWe will walk through each of these in detail using Figure 15-1 as a template. In the following section, rectangles with solid borders represent Spark driver process whereas those with dotted borders represent the executor processes.我们将使用图15-1作为模板详细介绍每种方法。在下面的部分中，带实心边框的矩形表示 Spark 驱动程序进程，而带虚线边框的矩形表示执行程序进程。#### Cluster mode 集群模式Cluster mode is probably the most common way of running Spark Applications. In cluster mode, a user submits a pre-compiled JAR, Python script, or R script to a cluster manager. The cluster manager then launches the driver process on a worker node inside the cluster, in addition to the executor processes. This means that the cluster manager is responsible for maintaining all Spark Application–related processes. Figure 15-2 shows that the cluster manager placed our driver on a worker node and the executors on other worker nodes.集群模式可能是运行Spark应用程序的最常见方式。在集群模式下，用户向集群管理器提交预编译的JAR、Python脚本或R脚本。然后，除了执行器进程之外，集群管理员在集群内的工作节点上启动驱动程序进程。这意味着集群管理员负责维护所有与Spark应用程序相关的流程。图15-2显示集群管理器将我们的驱动程序放在一个工作节点上，而执行器放在其他工作节点上。#### Client mode 客户端模式Client mode is nearly the same as cluster mode except that the Spark driver remains on the client machine that submitted the application. This means that the client machine is responsible for maintaining the Spark driver process, and the cluster manager maintains the executor processses. In Figure 15-3, we are running the Spark Application from a machine that is not colocated on the cluster. These machines are commonly referred to as gateway machines or edge nodes. In Figure 15-3, you can see that the driver is running on a machine outside of the cluster but that the workers are located on machines in the cluster.客户端模式与集群模式几乎相同，只是Spark驱动程序保留在提交应用程序的客户端上。这意味着客户端负责维护Spark 驱动程序进程，集群管理员维护执行器进程。在图15-3中，我们运行的Spark应用程序来自一台未在集群上并置的机器。这些机器通常被称为网关机器或边缘节点。在图15-3中，您可以看到驱动程序（driver）在集群外部的一台机器上运行，但工作人员（worker）位于集群中的机器上。 #### Local mode 当地模式Local mode is a significant departure from the previous two modes: it runs the entire Spark Application on a single machine. It achieves parallelism through threads on that single machine. This is a common way to learn Spark, to test your applications, or experiment iteratively with local development. However, we do not recommend using local mode for running production applications.本地模式与前两种模式有很大的不同：它在一台机器上运行整个Spark应用程序。它通过单个机器上的线程实现并行性。这是学习Spark、测试应用程序或使用本地开发进行迭代实验的常用方法。但是，我们不建议在运行生产应用程序时使用本地模式。## The Life Cycle of a Spark Application (Outside Spark) Spark 应用的生命周期（Spark外部）This chapter has thus far covered the vocabulary necessary for discussing Spark Applications. It’s now time to talk about the overall life cycle of Spark Applications from “outside” the actual Spark code. We will do this with an illustrated example of an application run with spark-submit (introduced in Chapter 3). We assume that a cluster is already running with four nodes, a driver (not a Spark driver but cluster manager driver) and three worker nodes. The actual cluster manager does not matter at this point: this section uses the vocabulary from the previous section to walk through a step-by-step Spark Application life cycle from initialization to program exit.本章迄今为止涵盖了讨论 Spark 应用程序所需的词汇。现在是时候从实际的 Spark 代码“外部”来讨论 Spark 应用程序的整个生命周期了。我们将通过一个使用 spark-submit 运行的应用程序的示例（在第3章中介绍）来实现这一点。我们假设一个集群已经运行了四个节点、一个驱动程序（不是 Spark 驱动程序，而是集群管理器驱动程序）和三个工作节点。此时，实际的集群管理器并不重要：本节使用上一节中的词汇表逐步遍历从初始化到程序退出的 Spark 应用程序生命周期。—NOTE 注意This section also makes use of illustrations and follows the same notation that we introduced previously. Additionally, we now introduce lines that represent network communication. Darker arrows represent communication by Spark or Spark related processes, whereas dashed lines represent more general communication (like cluster management communication).本节还使用了插图，并遵循我们前面介绍的相同的符号。此外，我们现在引入表示网络通信的线。较暗的箭头表示通过 Spark 或 Spark 相关进程进行的通信，而虚线表示更一般的通信（如集群管理通信）。—### Client Request 客户端请求The first step is for you to submit an actual application. This will be a pre-compiled JAR or library. At this point, you are executing code on your local machine and you’re going to make a request to the cluster manager driver node (Figure 15-4). Here, we are explicitly asking for resources for the Spark driver process only. We assume that the cluster manager accepts this offer and places the driver onto a node in the cluster. The client process that submitted the original job exits and the application is off and running on the cluster.第一步是提交实际的申请。这将是一个预编译的 JAR 或库。此时，您正在本地计算机上执行代码，并将向集群管理员驱动程序节点发出请求（图15-4）。这里，我们明确地要求只为 Spark 驱动程序进程提供资源。我们假设集群管理员接受这个提议，并将驱动程序放在集群中的一个节点上。提交原始作业的客户端进程退出，应用程序在集群上关闭并运行。To do this, you’ll run something like the following command in your terminal:为此，您将在终端中运行如下命令：12345678./bin/spark-submit \--class &lt;main-class&gt; \--master &lt;master-url&gt; \--deploy-mode cluster \--conf &lt;key&gt;=&lt;value&gt; \... # other options&lt;application-jar&gt; \[application-arguments]### Launch 启动（应用程序）Now that the driver process has been placed on the cluster, it begins running user code (Figure 15-5). This code must include a SparkSession that initializes a Spark cluster (e.g., driver + executors). The SparkSession will subsequently communicate with the cluster manager (the darker line), asking it to launch Spark executor processes across the cluster (the lighter lines). The number of executors and their relevant configurations are set by the user via the command-line arguments in the original spark-submit call.现在驱动程序进程已经放置在集群上，它开始运行用户代码（图15-5）。此代码必须包含初始化 Spark 集群的SparkSession（例如，驱动程序+执行器）。SparkSession 随后将与集群管理器（较暗的线）通信，要求它在集群中启动 Spark executor进程（较亮的线）。执行器（executor）的数量及其相关配置由用户通过原始 spark-submit 调用中的命令行参数设置。The cluster manager responds by launching the executor processes (assuming all goes well) and sends the relevant information about their locations to the driver process. After everything is hooked upcorrectly, we have a “Spark Cluster” as you likely think of it today.集群管理器通过启动执行器进程（假设一切正常）进行响应，并将有关其位置的相关信息发送到驱动程序进程。在所有的东西都连接正确之后，我们就有了一个“Spark 集群”，就像你今天想象的那样。### Execution 执行Now that we have a “Spark Cluster,” Spark goes about its merry way executing code, as shown in Figure 15-6. The driver and the workers communicate among themselves, executing code and moving data around. The driver schedules tasks onto each worker, and each worker responds with the status of those tasks and success or failure. (We cover these details shortly.)既然我们有了一个“Spark 集群”，Spark就会以一种愉快的方式执行代码，如图15-6所示。驱动程序和工作人员（workers ）之间进行通信，执行代码并移动数据。驱动程序将任务调度到每个工作人员（workers ）身上，每个工作人员对这些任务的状态以及成功或失败作出响应。（我们将很快介绍这些细节。）### Completion 完成After a Spark Application completes, the driver processs exits with either success or failure (Figure 15-7). The cluster manager then shuts down the executors in that Spark cluster for the driver. At this point, you can see the success or failure of the Spark Application by asking the cluster manager for this information.Spark应用程序完成后，驱动程序进程以成功或失败退出（图15-7）。然后，集群管理员为驱动程序关闭该 Spark 集群中的执行器。此时，通过向集群管理器询问这些信息，您可以看到 Spark 应用程序的成功或失败。## The Life Cycle of a Spark Application (Inside Spark) Spark应用程序的生命周期（Spark内部）We just examined the life cycle of a Spark Application outside of user code (basically the infrastructure that supports Spark), but it’s arguably more important to talk about what happens within Spark when you run an application. This is “user-code” (the actual code that you write that defines your Spark Application). Each application is made up of one or more Spark jobs. Spark jobs within an application are executed serially (unless you use threading to launch multiple actions in parallel).我们刚刚研究了 Spark 应用程序在用户代码之外的生命周期（基本上是支持 Spark 的基础设施），但是讨论运行应用程序时 Spark 内发生的事情可能更重要。这是“用户代码”（定义 Spark 应用程序的实际代码）。每个应用程序由一个或多个 Spark 作业组成。应用程序中的 Spark 作业是串行执行的（除非使用线程并行启动多个操作）。 ### The SparkSessionThe first step of any Spark Application is creating a SparkSession. In many interactive modes, this is done for you, but in an application, you must do it manually. Some of your legacy code might use the new SparkContext pattern. This should be avoided in favor of the builder method on the SparkSession, which more robustly instantiates the Spark and SQL Contexts and ensures that there is no context conflict, given that there might be multiple libraries trying to create a session in the same Spark Appication:任何Spark应用程序的第一步都是创建 SparkSession。在许多交互模式中，这是为您完成的，但在应用程序中，您必须手动完成。一些遗留代码可能使用新的 SparkContext 模式。应该避免这样做，因为 SparkSession 上的builder方法更能有力地实例化 Spark 和 SQL 上下文，并确保没有上下文冲突，因为可能有多个库试图在同一Spark应用程序中创建会话： 1234// Creating a SparkSession in Scalaimport org.apache.spark.sql.SparkSessionval spark = SparkSession.builder().appName("Databricks Spark Example").config("spark.sql.warehouse.dir", "/user/hive/warehouse").getOrCreate()12345# Creating a SparkSession in Pythonfrom pyspark.sql import SparkSessionspark = SparkSession.builder.master("local").appName("Word Count")\.config("spark.some.config.option", "some-value")\.getOrCreate()After you have a SparkSession, you should be able to run your Spark code. From the SparkSession, you can access all of low-level and legacy contexts and configurations accordingly, as well. Note that the SparkSession class was only added in Spark 2.X. Older code you might find would instead directly create a SparkContext and a SQLContext for the structured APIs.在进行 SparkSession 之后，您应该能够运行spark代码。通过 SparkSession，您还可以相应地访问所有低阶和遗留上下文和配置。请注意，SparkSession 类只添加在 Spark 2.x 中。您可能会发现，较旧的代码将直接为结构化API创建 SparkContext 和 sqlContext。#### The SparkContextA SparkContext object within the SparkSession represents the connection to the Spark cluster. This class is how you communicate with some of Spark’s lower-level APIs, such as RDDs. It is commonly stored as the variable sc in older examples and documentation. Through a SparkContext, you can create RDDs, accumulators, and broadcast variables, and you can run code on the cluster. For the most part, you should not need to explicitly initialize a SparkContext; you should just be able to access it through the SparkSession. If you do want to, you should create it in the most general way, through the getOrCreate method:SparkSession 中的 SparkContext 对象表示与 Spark 群集的连接。这个类是如何与Spark的一些低阶API（如RDD）通信的。在旧的示例和文档中，它通常存储为变量 sc 。通过 SparkContext，您可以创建RDD、累加器（accumulators）和广播（broadcast）变量，并且可以在集群上运行代码。在大多数情况下，您不需要显式初始化 SparkContext；您只需要能够通过 SparkSession 访问它。如果您愿意，您应该通过 getOrCreate 方法以最一般的方式创建它： 123// in Scalaimport org.apache.spark.SparkContextval sc = SparkContext.getOrCreate()THE SPARKSESSION, SQLCONTEXT, AND HIVECONTEXT In previous versions of Spark, the SQLContext and HiveContext provided the ability to work with DataFrames and Spark SQL and were commonly stored as the variable sqlContext in examples, documentation, and legacy code. As a historical point, Spark 1.X had effectively two contexts. The SparkContext and the SQLContext. These two each performed different things. The former focused on more fine-grained control of Spark’s central abstractions, whereas the latter focused on the higher-level tools like Spark SQL. In Spark 2.X, the communtiy combined the two APIs into the centralized SparkSession that we have today. However, both of these APIs still exist and you can access them via the SparkSession. It is important to note that you should never need to use the SQLContext and rarely need to use the SparkContext. 在Spark的早期版本中，SQLContext 和 HiveContext提供了使用 DataFrame 和 Spark SQL的能力，并且通常作为变量SQLContext存储在示例、文档和旧代码中。作为一个历史点，spark 1.x实际上有两个上下文。SparkContext和SQLContext。这两个人的表现各不相同。前者侧重于对Spark的中心抽象进行更细粒度的控制，而后者则侧重于更高级的工具，如Spark SQL。在spark 2.x中，社区将这两个API合并到了我们今天使用的集中式 SparkSession 中。但是，这两个API仍然存在，您可以通过SparkSession访问它们。需要注意的是，您不应该需要使用 SQLContext，而且很少需要使用 SparkContext。After you initialize your SparkSession, it’s time to execute some code. As we know from previous chapters, all Spark code compiles down to RDDs. Therefore, in the next section, we will take some logical instructions (a DataFrame job) and walk through, step by step, what happens over time.初始化 SparkSession 之后，该执行一些代码了。正如我们从前面的章节所知道的，所有 Spark 代码都编译成RDD。因此，在下一节中，我们将接受一些逻辑指令（一个 DataFrame 作业）并逐步了解随着时间的推移会发生什么。 Logical Instructions 逻辑指令As you saw in the beginning of the book, Spark code essentially consists of transformations and actions. How you build these is up to you—whether it’s through SQL, low-level RDD manipulation, or machine learning algorithms. Understanding how we take declarative instructions like DataFrames and convert them into physical execution plans is an important step to understanding how Spark runs on a cluster. In this section, be sure to run this in a fresh environment (a new Spark shell) to follow along with the job, stage, and task numbers. 正如您在书的开头所看到的，Spark代码基本上由转换（transformation ）和动作（action）组成。无论是通过SQL、低阶的RDD操作还是机器学习算法，如何构建这些都取决于您。了解我们如何使用声明性指令（如DataFrame）并将其转换为物理执行计划是了解Spark如何在集群上运行的重要步骤。在本节中，请确保在新的环境（新的 Spark shell）中运行此程序，以跟踪作业（job）、阶段（stage）和任务（task）编号。 Logical instructions to physical execution 物理执行的逻辑指令We mentioned this in Part II, but it’s worth reiterating so that you can better understand how Spark takes your code and actually runs the commands on the cluster. We will walk through some more code, line by line, explain what’s happening behind the scenes so that you can walk away with a better understanding of your Spark Applications. In later chapters, when we discuss monitoring, we will perform a more detailed tracking of a Spark job through the Spark UI. In this current example, we’ll take a simpler approach. We are going to do a three-step job: using a simple DataFrame, we’ll repartition it, perform a value-by-value manipulation, and then aggregate some values and collect the final result. 我们在第二部分中提到了这一点，但是值得重申，这样您就可以更好地理解 Spark 是如何使用代码并在集群上实际运行命令的。我们将一行一行地介绍更多的代码，解释幕后发生的事情，以便您能够更好地了解 Spark 应用程序。在后面的章节中，当我们讨论监控时，我们将通过 Spark UI 对 Spark 作业执行更详细的跟踪。在当前的示例中，我们将采用更简单的方法。我们要做一个三步的工作：使用一个简单的数据框架，我们将对它重新分区，执行一个值一个值的操作，然后聚合一些值并收集最终的结果。 NOTE 注意 This code was written and runs with Spark 2.2 in Python (you’ll get the same result in Scala, so we’ve omitted it). The number of jobs is unlikely to change drastically but there might be improvements to Spark’s underlying optimizations that change physical execution strategies. 这段代码是用 Python 中的 Spark 2.2 编写和运行的（您将在 Scala 中得到相同的结果，所以我们省略了它）。工作数量不太可能大幅度改变，但 Spark 的底层优化可能会有所改进，从而改变物理执行策略。 123456789# in Pythondf1 = spark.range(2, 10000000, 2)df2 = spark.range(2, 10000000, 4)step1 = df1.repartition(5)step12 = df2.repartition(6)step2 = step1.selectExpr("id * 5 as id")step3 = step2.join(step12, ["id"])step4 = step3.selectExpr("sum(id)")step4.collect() # 2500000000000 When you run this code, we can see that your action triggers one complete Spark job. Let’s take a look at the explain plan to ground our understanding of the physical execution plan. We can access this information on the SQL tab (after we actually run a query) in the Spark UI, as well: 当您运行此代码时，我们可以看到您的操作触发了一个完整的Spark作业。让我们看一下解释计划，以加深我们对实际执行计划的理解。我们可以在Spark UI中的SQL选项卡（在实际运行查询之后）上访问这些信息，以及： 1step4.explain() 123456789101112131415== Physical Plan ==*HashAggregate(keys=[], functions=[sum(id#15L)])+- Exchange SinglePartition +- *HashAggregate(keys=[], functions=[partial_sum(id#15L)]) +- *Project [id#15L] +- *SortMergeJoin [id#15L], [id#10L], Inner :- *Sort [id#15L ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(id#15L, 200) : +- *Project [(id#7L * 5) AS id#15L] : +- Exchange RoundRobinPartitioning(5) : +- *Range (2, 10000000, step=2, splits=8) +- *Sort [id#10L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(id#10L, 200) +- Exchange RoundRobinPartitioning(6) +- *Range (2, 10000000, step=4, splits=8) What you have when you call collect (or any action) is the execution of a Spark job that individually consist of stages and tasks. Go to localhost:4040 if you are running this on your local machine to see the Spark UI. We will follow along on the “jobs” tab eventually jumping to stages and tasks as we proceed to further levels of detail. 当您调用 collect（或任何操作）时，您所拥有的是 Spark作业的执行，它分别由阶段（stage）和任务（task）组成。如果您在本地机器上运行这个程序，请转到 localhost:4040 查看Spark用户界面。我们将继续关注“jobs”选项卡，最终跳到阶段（stage）和任务（task），继续深入到更详细的层次。 A Spark Job 一个Spark作业In general, there should be one Spark job for one action. Actions always return results. Each job breaks down into a series of stages, the number of which depends on how many shuffle operations need to take place. 通常，一个动作（action）应该有一个Spark作业。操作始终返回结果。每项工作分为一系列阶段（stage），其数量取决于需要进行多少次洗牌（shuffle）操作。 This job breaks down into the following stages and tasks:这项工作分为以下几个阶段（stage）和任务（task）： Stage 1 with 8 Tasks第1阶段，8个任务 Stage 2 with 8 Tasks第2阶段，8个任务 Stage 3 with 6 Tasks第3阶段有6个任务 Stage 4 with 5 Tasks第4阶段有5个任务 Stage 5 with 200 Tasks第5阶段，200个任务 Stage 6 with 1 Task第6阶段，1个任务 I hope you’re at least somewhat confused about how we got to these numbers so that we can take the time to better understand what is going on! 我希望你至少对我们如何得到这些数字感到困惑，以便我们可以花时间更好地了解正在发生的事情！ Stages 阶段Stages in Spark represent groups of tasks that can be executed together to compute the same operation on multiple machines. In general, Spark will try to pack as much work as possible (i.e., as many transformations as possible inside your job) into the same stage, but the engine starts new stages after operations called shuffles. A shuffle represents a physical repartitioning of the data—for example, sorting a DataFrame, or grouping data that was loaded from a file by key (which requires sending records with the same key to the same node). This type of repartitioning requires coordinating across executors to move data around. Spark starts a new stage after each shuffle, and keeps track of what order the stages must run in to compute the final result. Spark中的阶段（stage）表示可以一起执行以在多台计算机上计算相同操作的任务（task）组。一般来说，Spark会尝试将尽可能多的工作（即工作中尽可能多的转换）打包到同一个阶段（stage），但引擎会在称为洗牌（shuffle）的操作后启动新的阶段（stage）。 shuffle 表示数据的物理重新分区——例如，对 DataFrame 进行排序，或者根据键（key）分组从文件加载的数据（这需要将具有相同键的记录发送到同一节点）。这种类型的重新分区需要跨执行器（executor）进行协调以移动数据。 Spark在每次shuffle之后开始一个新阶段（stage），并跟踪阶段（stage）必须运行的顺序以计算最终结果。 In the job we looked at earlier, the first two stages correspond to the range that you perform in order to create your DataFrames. By default when you create a DataFrame with range, it has eight partitions. The next step is the repartitioning. This changes the number of partitions by shuffling the data. These DataFrames are shuffled into six partitions and five partitions, corresponding to the number of tasks in stages 3 and 4. 在我们之前查看的工作中，前两个阶段（stage）对应于您为创建DataFrame而执行的范围（range）。默认情况下，当您使用范围（range）创建DataFrame时，它有八个分区。下一步是重新分区（repartitioning）。这会通过对数据洗牌（shuffle）来更改分区数。这些DataFrame被洗牌到六个分区和五个分区，对应于阶段3和4中的任务数。 Stages 3 and 4 perform on each of those DataFrames and the end of the stage represents the join (a shuffle). Suddenly, we have 200 tasks. This is because of a Spark SQL configuration. The spark.sql.shuffle.partitions default value is 200, which means that when there is a shuffle performed during execution, it outputs 200 shuffle partitions by default. You can change this value, and the number of output partitions will change. 阶段（stage）3和4对每个DataFrame执行，阶段（stage）结束表示连接（join）。突然间，我们有200个任务（task）。这是因为Spark SQL配置。 spark.sql.shuffle.partitions 默认值为200，这意味着当执行期间执行了洗牌（shuffle）时，它默认输出200个洗牌（shuffle）分区。您可以更改此值，并且输出分区的数量将更改。—TIP 提示We cover the number of partitions in a bit more detail in Chapter 19 because it’s such an important parameter. This value should be set according to the number of cores in your cluster to ensure efficient execution. Here’s how to set it:我们在第19章中更详细地介绍了分区的数量，因为它是一个非常重要的参数。这个值应该根据集群中核心的数量来设置，以确保高效执行。设置方法如下： 1spark.conf.set("spark.sql.shuffle.partitions", 50) A good rule of thumb is that the number of partitions should be larger than the number of executors on your cluster, potentially by multiple factors depending on the workload. If you are running code on your local machine, it would behoove you to set this value lower because your local machine is unlikely to be able to execute that number of tasks in parallel. This is more of a default for a cluster in which there might be many more executor cores to use. Regardless of the number of partitions, that entire stage is computed in parallel. The final result aggregates those partitions individually, brings them all to a single partition before finally sending the final result to the driver. We’ll see this configuration several times over the course of this part of the book. 一个好的经验法则是分区数应该大于集群上执行器（executor）的数量，可能由多个因素决定，具体取决于工作负载。如果您在本地计算机上运行代码，那么您可以将此值设置得更低，因为本地计算机不太可能并行执行该数量的任务。对于可能需要使用更多执行程序核心的集群，这更像是一个默认设置。无论分区数量如何，整个阶段都是并行计算的。最终结果单独聚合这些分区，在最后将最终结果发送给驱动程序之前将它们全部带到一个分区。在本书的这一部分过程中，我们会多次看到这种配置。 Tasks 任务Stages in Spark consist of tasks. Each task corresponds to a combination of blocks of data and a set of transformations that will run on a single executor. If there is one big partition in our dataset, we will have one task. If there are 1,000 little partitions, we will have 1,000 tasks that can be executed in parallel. A task is just a unit of computation applied to a unit of data (the partition). Partitioning your data into a greater number of partitions means that more can be executed in parallel. This is not a panacea, but it is a simple place to begin with optimization. Spark中的阶段（stage）由任务（task）组成。每个任务（task）对应于将在单个执行器（executor）上运行的数据块和一组转换的组合。如果我们的数据集中有一个大分区，我们将有一个任务。如果有1000个小分区，我们将有1,000个可以并行执行的任务。任务只是应用于数据单元（分区）的计算单位。将数据划分为更多数量的分区意味着可以并行执行更多数据。这不是灵丹妙药，但它是一个简单的开始优化的入手之处。 Execution Details 执行细节Tasks and stages in Spark have some important properties that are worth reviewing before we close out this chapter. First, Spark automatically pipelines stages and tasks that can be done together, such as a map operation followed by another map operation. Second, for all shuffle operations, Spark writes the data to stable storage (e.g., disk), and can reuse it across multiple jobs. We’ll discuss these concepts in turn because they will come up when you start inspecting applications through the Spark UI. 在我们结束本章之前，Spark中的任务和阶段具有一些值得检查的重要属性。首先，Spark自动管理可以一起完成的阶段和任务，例如映射（map）操作，然后是另一个映射（map）操作。其次，对于所有洗牌（shuffle）操作，Spark将数据写入稳定存储（例如，磁盘），并且可以在多个作业中重复使用它。我们将依次讨论这些概念，因为当您开始通过Spark UI检查应用程序时，它们会出现。 Pipelining 管道化An important part of what makes Spark an “in-memory computation tool” is that unlike the tools that came before it (e.g., MapReduce), Spark performs as many steps as it can at one point in time before writing data to memory or disk. One of the key optimizations that Spark performs is pipelining, which occurs at and below the RDD level. With pipelining, any sequence of operations that feed data directly into each other, without needing to move it across nodes, is collapsed into a single stage of tasks that do all the operations together. For example, if you write an RDD-based program that does a map, then a filter, then another map, these will result in a single stage of tasks that immediately read each input record, pass it through the first map, pass it through the filter, and pass it through the last map function if needed. This pipelined version of the computation is much faster than writing the intermediate results to memory or disk after each step. The same kind of pipelining happens for a DataFrame or SQL computation that does a select, filter, and select. 使Spark成为“内存计算工具”的一个重要部分是，与之前的工具（例如，MapReduce）不同，Spark在将数据写入内存或磁盘之前的一个时间点执行尽可能多的步骤。 Spark执行的一个关键优化是流水线操作，它发生在RDD级别和低于RDD级别。通过流水线操作，将数据直接相互馈送而无需跨节点移动的任何操作序列都会折叠为一起完成所有操作的任务。例如，如果你编写一个基于RDD的程序来执行一个映射（map），然后是一个过滤器（filter），然后是另一个映射（map），这些将导致一个阶段的任务立即读取每个输入记录，将其传递通过第一个映射，传递给它过滤器，如果需要，将其传递给最后一个映射（map）函数。这个流水线版的计算比在每个步骤之后将中间结果写入内存或磁盘要快得多。对于执行select，filter和select的DataFrame或SQL计算，会发生相同类型的流水线操作。 From a practical point of view, pipelining will be transparent to you as you write an application—the Spark runtime will automatically do it—but you will see it if you ever inspect your application through the Spark UI or through its log files, where you will see that multiple RDD or DataFrame operations were pipelined into a single stage. 从实际的角度来看，在编写应用程序时，流水线操作对您来说是透明的——Spark运行时会自动执行——但如果您通过Spark UI或其日志文件检查应用程序，您将看到它将看到多个RDD或DataFrame操作被流水线化为单个阶段。 Shuffle Persistence 洗牌的持久化The second property you’ll sometimes see is shuffle persistence. When Spark needs to run an operation that has to move data across nodes, such as a reduce-by-key operation (where input data for each key needs to first be brought together from many nodes), the engine can’t perform pipelining anymore, and instead it performs a cross-network shuffle. Spark always executes shuffles by first having the “source” tasks (those sending data) write shuffle files to their local disks during their execution stage. Then, the stage that does the grouping and reduction launches and runs tasks that fetch their corresponding records from each shuffle file and performs that computation (e.g., fetches and processes the data for a specific range of keys). Saving the shuffle files to disk lets Spark run this stage later in time than the source stage (e.g., if there are not enough executors to run both at the same time), and also lets the engine re-launch reduce tasks on failure without rerunning all the input tasks. 你有时会看到的第二个属性是随机持久性。当 Spark 需要运行必须跨节点移动数据的操作时，例如 reduce-by-key 操作（每个键的输入数据需要首先从许多节点聚集在一起），引擎不能再执行流水线操作了，而是它执行交叉网络随机洗牌（shuffle）。 在执行阶段，Spark总是首先通过让“源”任务（那些发送数据的任务）将洗牌（shuffle）文件写入本地磁盘来执行洗牌（shuffle）操作。然后，执行分组和减少启动项，并运行从每个洗牌文件获取其相应记录的任务并执行该计算（例如，获取和处理特定范围的键的数据）。将洗牌（shuffle）文件保存到磁盘允许Spark比源阶段更晚地运行此阶段（例如，如果没有足够的执行器（executor）同时运行两者），并且还允许引擎重新启动以在故障时且不用重新运行所有输入任务的情况下减少任务。 One side effect you’ll see for shuffle persistence is that running a new job over data that’s already been shuffled does not rerun the “source” side of the shuffle. Because the shuffle files were already written to disk earlier, Spark knows that it can use them to run the later stages of the job, and it need not redo the earlier ones. In the Spark UI and logs, you will see the pre-shuffle stages marked as “skipped”. This automatic optimization can save time in a workload that runs multiple jobs over the same data, but of course, for even better performance you can perform your own caching with the DataFrame or RDD cache method, which lets you control exactly which data is saved and where. You’ll quickly grow accustomed to this behavior after you run some Spark actions on aggregated data and inspect them in the UI.您将看到的随机持久性的一个副作用是，对已经被洗牌的数据运行新作业不会重新运行“源”端的洗牌操作。因为洗牌（shuffle）文件早先已经写入磁盘，所以Spark知道它可以使用它们来运行作业的后期阶段，并且它不需要重做早期的那些（任务）。在Spark UI和日志中，您将看到标记为“已跳过”的预洗牌阶段。这种自动优化可以节省在同一数据上运行多个作业的工作负载的时间，但当然，为了获得更好的性能，您可以使用DataFrame或RDD缓存方法执行自己的缓存，这样您就可以精确控制保存的数据和哪里。在对聚合数据运行一些Spark操作并在UI中检查它们之后，您将很快习惯于此行为。 Conclusion 结论In this chapter, we discussed what happens to Spark Applications when we go to execute them on a cluster. This means how the cluster will actually go about running that code as well as what happens within Spark Applications during the process. At this point, you should feel quite comfortable understanding what happens within and outside of a Spark Application. This will give you a starting point for debugging your applications. Chapter 16 will discuss writing Spark Applications and the things you should consider when doing so. 在本章中，我们讨论了当我们在集群上执行它们时Spark应用程序会发生什么。 这意味着集群将如何实际运行该代码以及在此过程中Spark应用程序中发生的事情。 此时，您应该非常自如地了解Spark应用程序内部和外部发生的情况。 这将为您调试应用程序提供一个起点。 第16章将讨论编写Spark应用程序以及执行此操作时应考虑的事项。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 16 Developing Spark Applications]]></title>
    <url>%2F2019%2F08%2F05%2FChapter16_DevelopingSparkApplications(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 16 Developing Spark ApplicationsTesting Spark Applications 测试Spark应用程序You now know what it takes to write and run a Spark Application, so let’s move on to a less exciting but still very important topic: testing. Testing Spark Applications relies on a couple of key principles and tactics that you should keep in mind as you’re writing your applications. 您现在知道编写和运行Spark应用程序需要什么，所以让我们继续讨论一个不太令人兴奋但仍然非常重要的主题：测试。测试Spark应用程序依赖于在编写应用程序时应该记住的几个关键原则和策略。 Strategic Principles 战略原则Testing your data pipelines and Spark Applications is just as important as actually writing them. This is because you want to ensure that they are resilient to future change, in data, logic, and output. In this section, we’ll first discuss what you might want to test in a typical Spark Application, then discuss how to organize your code for easy testing. 测试数据管道和 Spark 应用程序与实际编写它们一样重要。这是因为您希望确保它们能够适应未来的变化，包括数据，逻辑和输出。在本节中，我们将首先讨论您可能希望在典型的 Spark 应用程序中测试的内容，然后讨论如何组织代码以便于轻松测试。 Input data resilience 输入数据弹性Being resilient to different kinds of input data is something that is quite fundamental to how you write your data pipelines. The data will change because the business needs will change. Therefore your Spark Applications and pipelines should be resilient to at least some degree of change in the input data or otherwise ensure that these failures are handled in a graceful and resilient way. For the most part this means being smart about writing your tests to handle those edge cases of different inputs and making sure that the pager only goes off when it’s something that is truly important. 对不同类型的输入数据具有弹性对于编写数据管道非常重要。数据将发生变化，因为业务需求将发生变化。因此，您的Spark应用程序和管道应该能够适应输入数据的至少某种程度的变化，或者确保以优雅和弹性的方式处理这些故障。在大多数情况下，这意味着要聪明地编写测试来处理不同输入的边缘情况（极端情况），并确保报警器仅在真正重要的事情发生时才会发出警报。 Business logic resilience and evolution 业务逻辑弹性和演变The business logic in your pipelines will likely change as well as the input data. Even more importantly, you want to be sure that what you’re deducing from the raw data is what you actually think that you’re deducing. This means that you’ll need to do robust logical testing with realistic data to ensure that you’re actually getting what you want out of it. One thing to be wary of here is trying to write a bunch of “Spark Unit Tests” that just test Spark’s functionality. You don’t want to be doing that; instead, you want to be testing your business logic and ensuring that the complex business pipeline that you set up is actually doing what you think it should be doing. 管道中的业务逻辑可能会随输入数据而变化。更重要的是，您希望确保从原始数据中推断出的内容是您实际认为正在推导的内容。这意味着您需要使用真实数据进行强大的逻辑测试，以确保您实际上从中获得了您想要的内容。需要注意的一件事是尝试编写一组只测试Spark功能的“Spark Unit Tests”。你不想这样做;相反，您希望测试业务逻辑并确保您设置的复杂业务管道实际上正在执行您认为应该执行的操作。 Resilience in output and atomicity 输出的弹性和原子性Assuming that you’re prepared for departures in the structure of input data and that your business logic is well tested, you now want to ensure that your output structure is what you expect. This means you will need to gracefully handle output schema resolution. It’s not often that data is simply dumped in some location, never to be read again—most of your Spark pipelines are probably feeding other Spark pipelines. For this reason you’re going to want to make certain that your downstream consumers understand the “state” of the data—this could mean how frequently it’s updated as well as whether the data is “complete” (e.g., there is no late data) or that there won’t be any last-minute corrections to the data. All of the aforementioned issues are principles that you should be thinking about as you build your data pipelines (actually, regardless of whether you’re using Spark). This strategic thinking is important for laying down the foundation for the system that you would like to build. 假设您已准备好在输入数据结构中离开，并且您的业务逻辑已经过充分测试，那么您现在需要确保您的输出结构符合您的预期。这意味着您需要优雅地处理输出模式解析。通常不会将数据简单地转储到某个位置，永远不会再次读取——大多数Spark管道可能正在为其他 Spark 管道提供服务。因此，您需要确保下游消费者了解数据的“状态”——这可能意味着更新频率以及数据是否“完整”（例如，没有后期数据）或者不会对数据进行任何最后修正。所有上述问题都是您在构建数据管道时应该考虑的原则（实际上，无论您是否使用Spark）。这种战略思维对于为您希望构建的系统奠定基础非常重要。 Tactical Takeaways 战术外卖Although strategic thinking is important, let’s talk a bit more in detail about some of the tactics that you can actually use to make your application easy to test. The highest value approach is to verify that your business logic is correct by employing proper unit testing and to ensure that you’re resilient to changing input data or have structured it so that schema evolution will not become unwielding in the future. The decision for how to do this largely falls on you as the developer because it will vary according to your business domain and domain expertise. 虽然战略思维很重要，但让我们更详细地谈谈您可以实际使用的一些策略，以使您的应用程序易于测试。最有价值的方法是通过采用适当的单元测试来验证您的业务逻辑是否正确，并确保您能够适应不断变化的输入数据，或者对其进行结构化，以便模式演变在未来不会变得无法使用。关于如何做到这一点的决定很大程度上取决于您作为开发人员，因为它将根据您的业务领域和领域专业知识而有所不同。 Managing SparkSessions 管理SparkSessionsTesting your Spark code using a unit test framework like JUnit or ScalaTest is relatively easy because of Spark’s local mode—just create a local mode SparkSession as part of your test harness to run it. However, to make this work well, you should try to perform dependency injection as much as possible when managing SparkSessions in your code. That is, initialize the SparkSession only once and pass it around to relevant functions and classes at runtime in a way that makes it easy to substitute during testing. This makes it much easier to test each individual function with a dummy SparkSession in unit tests. 使用单元测试框架（如 JUnit 或 ScalaTest ）测试Spark代码相对容易，因为Spark的本地模式——只需创建一个本地模式 SparkSession 作为测试工具的一部分来运行它。但是，为了使这项工作更好，您应该在代码中管理SparkSessions 时尽可能多地执行依赖项注入。也就是说，只将 SparkSession 初始化一次并在运行时将其传递给相关的函数和类，以便在测试期间轻松替换。这使得在单元测试中使用仿真的SparkSession测试每个单独的函数变得更加容易。 Which Spark API to Use? 使用哪种Spark API？Spark offers several choices of APIs, ranging from SQL to DataFrames and Datasets, and each of these can have different impacts for maintainability and testability of your application. To be perfectly honest, the right API depends on your team and its needs: some teams and projects will need the less strict SQL and DataFrame APIs for speed of development, while others will want to use type-safe Datasets or RDDs. Spark提供了多种API选择，从 SQL 到 DataFrames 和 Datasets，每种API都会对应用程序的可维护性和可测试性产生不同的影响。说实话，正确的API取决于您的团队及其需求：一些团队和项目需要不太严格的SQL和DataFrame API来提高开发速度，而其他团队和项目则需要使用类型安全的 DataSet 或 RDD 。 In general, we recommend documenting and testing the input and output types of each function regardless of which API you use. The type-safe API automatically enforces a minimal contract for your function that makes it easy for other code to build on it. If your team prefers to use DataFrames or SQL, then spend some time to document and test what each function returns and what types of inputs it accepts to avoid surprises later, as in any dynamically typed programming language. While the lower-level RDD API is also statically typed, we recommend going into it only if you need low-level features such as partitioning that are not present in Datasets, which should not be very common; the Dataset API allows more performance optimizations and is likely to provide even more of them in the future. 通常，我们建议记录和测试每个函数的输入和输出类型，无论您使用哪个API。类型安全的API会自动为您的函数强制执行最小的约定（可以理解为：可使用的条款少，灵活度低），以便在其上构建其他代码。如果您的团队更喜欢使用DataFrames或SQL，那么花一些时间来记录和测试每个函数返回的内容以及它接受哪些类型的输入以避免以后出现意外，就像在任何动态类型编程语言中一样。虽然较低阶的RDD API也是静态类型的，但我们建议只有在需要低阶功能（例如数据集中不存在的分区）时才进入它，这应该不常见; DataSet API允许更多性能优化，并且可能在将来提供更多性能优化。 A similar set of considerations applies to which programming language to use for your application: there certainly is no right answer for every team, but depending on your needs, each language will provide different benefits. We generally recommend using statically typed languages like Scala and Java for larger applications or those where you want to be able to drop into low-level code to fully control performance, but Python and R may be significantly better in other cases—for example, if you need to use some of their other libraries. Spark code should easily be testable in the standard unit testing frameworks in every language. 一组类似的注意事项适用于您的应用程序使用哪种编程语言：每个团队肯定没有正确的答案，但根据您的需求，每种语言都会提供不同的好处。我们通常建议对大型应用程序使用静态类型语言（如Scala和Java），或者希望能够放入低阶代码以完全控制性能的语言，但在其他情况下，Python和R可能会明显更好——例如，如果你需要使用他们的一些其他库。 Spark代码应该可以在每种语言的标准单元测试框架中轻松测试。 Connecting to Unit Testing Frameworks 连接到单元测试框架To unit test your code, we recommend using the standard frameworks in your langage (e.g., JUnit or ScalaTest ), and setting up your test harnesses to create and clean up a SparkSession for each test. Different frameworks offer different mechanisms to do this, such as “before” and “after” methods. We have included some sample unit testing code in the application templates for this chapter. 要对代码进行单元测试，我们建议您使用标准框架（例如，JUnit 或 ScalaTest），并设置测试工具来为每个测试创建和清理 SparkSession。不同的框架提供了不同的机制来实现这一点，例如“之前”和“之后”方法。我们在本章的应用程序模板中包含了一些示例单元测试代码。 Connecting to Data Sources 连接到数据源As much as possible, you should make sure your testing code does not connect to production data sources, so that developers can easily run it in isolation if these data sources change. One easy way to make this happen is to have all your business logic functions take DataFrames or Datasets as input instead of directly connecting to various sources; after all, subsequent code will work the same way no matter what the data source was. If you are using the structured APIs in Spark, another way to make this happen is named tables: you can simply register some dummy datasets (e.g., loaded from small text file or from in-memory objects) as various table names and go from there. 您应尽可能确保测试代码不会连接到生产数据源，以便开发人员可以在这些数据源发生更改时轻松地单独运行它。实现这一目标的一种简单方法是让所有业务逻辑功能将DataFrames或Datasets作为输入，而不是直接连接到各种源; 毕竟，无论数据源是什么，后续代码都将以相同的方式工作。如果您在Spark中使用结构化API，另一种实现此目的的方法是命名表：您只需将一些仿真数据集（例如，从小文本文件或内存中对象加载）注册为各种表名，然后从那里开始。 The Development Process 开发过程The development process with Spark Applications is similar to development workflows that you have probably already used. First, you might maintain a scratch space, such as an interactive notebook or some equivalent thereof, and then as you build key components and algorithms, you move them to a more permanent location like a library or package. The notebook experience is one that we often recommend (and are using to write this book) because of its simplicity in experimentation. There are also some tools, such as Databricks, that allow you to run notebooks as production applications as well. Spark应用程序的开发过程类似于您可能已经使用过的开发工作流程。首先，您可以维护一个临时空间，例如交互式笔记本或其等效物，然后在构建关键组件和算法时，将它们移动到更永久的位置，如库或包。notebook（与jupter notebook类似）的体验是我们经常推荐的（并且正在用来编写本书），因为它的实验非常简单。还有一些工具，如 Databricks，允许您将笔记本作为生产应用程序运行。 When running on your local machine, the spark-shell and its various language-specific implementations are probably the best way to develop applications. For the most part, the shell is for interactive applications, whereas spark-submit is for production applications on your Spark cluster. You can use the shell to interactively run Spark, just as we showed you at the beginning of this book. This is the mode with which you will run PySpark, Spark SQL, and SparkR. In the bin folder, when you download Spark, you will find the various ways of starting these shells. Simply run sparkshell(for Scala), spark-sql, pyspark, and sparkR. After you’ve finished your application and created a package or script to run, spark-submit will become your best friend to submit this job to a cluster. 在本地计算机上运行时，spark-shell 及其各种特定于语言的实现可能是开发应用程序的最佳方式。在大多数情况下，shell用于交互式应用程序，而 spark-submit 用于Spark集群上的生产应用程序。您可以使用shell以交互方式运行Spark，就像我们在本书开头部分向您展示的那样。这是运行 PySpark，Spark SQL 和 SparkR 的模式。在bin文件夹中，当您下载 Spark 时，您将找到启动这些 shell 的各种方法。只需运行 sparkshell（适用于Scala），spark-sql，pyspark 和 sparkR。在您完成应用程序并创建要运行的包或脚本后，spark-submit 将成为您将此作业提交到群集的最佳朋友。 Launching Applications 启动应用程序The most common way for running Spark Applications is through spark-submit. Previously in this chapter, we showed you how to run spark-submit; you simply specify your options, the application JAR or script, and the relevant arguments: 运行Spark应用程序的最常用方法是通过spark-submit。 在本章的前面，我们向您展示了如何运行spark-submit; 您只需指定选项，应用程序 JAR 或脚本以及相关参数： 12345678./bin/spark-submit \--class &lt;main-class&gt; \--master &lt;master-url&gt; \--deploy-mode &lt;deploy-mode&gt; \--conf &lt;key&gt;=&lt;value&gt; \... # other options&lt;application-jar-or-script&gt; \[application-arguments] You can always specify whether to run in client or cluster mode when you submit a Spark job with spark-submit. However, you should almost always favor running in cluster mode (or in client mode on the cluster itself) to reduce latency between the executors and the driver.当您使用 spark-submit 提交 Spark 作业时，您始终可以指定是以客户端还是群集模式运行。 但是，您几乎总是倾向于在群集模式下运行（或在集群本身的客户端模式下）以减少执行程序和驱动程序之间的延迟。When submitting applciations, pass a .py file in the place of a .jar, and add Python .zip, .egg, or .py to the search path with –py-files.提交 applciations 时，在 .jar 的位置传递 .py 文件，并使用 –py-files 将 Python .zip，.egg 或 .py 添加到搜索路径。For reference, Table 16-1 lists all of the available spark-submit options, including those that are particular to some cluster managers. To enumerate all these options yourself, run spark-submit with –help.作为参考，表16-1列出了所有可用的 spark-submit 选项，包括某些集群管理器特有的选项。 要自己枚举所有这些选项，请使用 –help 运行 spark-submit。Table 16-1. Spark submit help text| Parameter | Description || —————————- | ———————————————————— || –masterMASTER_URL | spark://host:port, mesos://host:port, yarn, or localSpark 连接的资源管理器 || –deploymodeDEPLOY_MODE | Whether to launch the driver program locally (“client”) or on one of the worker machines inside the cluster (“cluster”) (Default: client)是在本地（“client”）还是在集群内（“cluster”）的某个工作机器上启动驱动程序（默认值：client） || –classCLASS_NAME | Your application’s main class (for Java / Scala apps).您的应用程序的主类（适用于Java / Scala应用程序）。 || –name NAME | A name of your application.你的应用程序主类。 || –jars JARS | Comma-separated list of local JARs to include on the driver and executor classpaths.以逗号分隔的本地 JAR 列表，包含在驱动程序和执行程序类路径中。 || –packages | search the local Maven repo, then Maven Central and any additional remote repositories given by –repositories. The format for the coordinates should be groupId:artifactId:version.搜索本地 Maven 仓库，然后搜索 Maven Central 以及 --repositories 给出的任何其他远程仓库。 坐标的格式应为 groupId:artifactId:version。 || –exclude-packages | Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in –packages to avoid dependency conflicts.逗号分隔的 groupId:artifactId 列表，在解析 --packages 中提供的依赖项时排除，以避免依赖性冲突。 || –repositories | Comma-separated list of additional remote repositories to search for the Maven coordinates given with –packages.以逗号分隔的其他远程仓库列表，用于搜索 --packages 给出的Maven坐标 || –py-filesPY_FILES | Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps.以逗号分隔的 .zip，.egg 或 .py 文件列表，放在 Python 应用程序的 PYTHONPATH 上。 || –filesFILES | Comma-separated list of files to be placed in the working directory of each executor.以逗号分隔的文件列表，放在每个执行程序的工作目录中。 || –confPROP=VALUE | Arbitrary Spark configuration property.任意Spark配置属性 || –propertiesfile FILE | Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf.从中加载额外属性的文件的路径。 如果未指定，则会查找 conf/spark-defaults.conf。 || –driver-memory MEM | Memory for driver (e.g., 1000M, 2G) (Default: 1024M).驱动器的内存（默认：1024M）。 || –driver-java-options | Extra Java options to pass to the driver.传递给驱动器的额外 Java 选项。 || –driver-library-path | Extra library path entries to pass to the driver.要传递给驱动程序的额外库路径条目。 || –driver-class-path | Extra class path entries to pass to the driver. Note that JARs added with –jars are automatically included in the classpath.要传递给驱动程序的额外类路径条目。请注意，添加了 --jars 的 JAR 会自动包含在类路径中。 || –executor-memory MEM | Memory per executor (e.g., 1000M, 2G) (Default: 1G).每个执行器的内存（例如，1000M，2G）（默认值：1G）。 || –proxy-user NAME | User to impersonate when submitting the application. This argument does not work with –principal / keytab.用户在提交申请时进行冒充。此参数不适用于 --principal/keytab。 || –help, -h | Show this help message and exit.显示此帮助消息并退出。 || –verbose, -v | Print additional debug output.打印其他调试输出。 || –version | Print the version of current Spark.打印当前Spark的版本。 |There are some deployment-specific configurations as well (see Table 16-2).还有一些特定于部署的配置（参见表16-2）。| Cluster Managers | Modes | Conf | Description || —————- | ——- | ————————— | ———————————————————— || Standalone | Cluster | –driver-cores NUM | Cores for driver (Default: 1)驱动核心（默认值：1） || Standalone/Mesos | Cluster | –supervise | If given, restarts the driver on failure.如果给定，则在失败时重新启动驱动程序。 || Standalone/Mesos | Cluster | –killSUBMISSION_ID | If given, kills the driver specified.如果给定，则杀死指定的驱动程序。 || Standalone/Mesos | Cluster | –statusSUBMISSION_ID | If given, requests the status of the driver specified.如果给定，请求指定的驱动程序的状态。 || Standalone/Mesos | Either | –total-executor-cores NUM | Total cores for all executors.所有执行器（executor）的核心总数。 || Standalone/YARN | Either | –total-executor-cores NUM1 | Number of cores per executor. (Default: 1 in YARN mode or all available cores on the worker in standalone mode)每个执行器（executor）的核心数。 （默认值：YARN模式下为1或独立模式下工作线程上的所有可用核心） || YARN | Either | –driver-cores NUM | Number of cores used by the driver, only in cluster mode (Default: 1).驱动程序使用的核心数，仅在集群模式下（默认值：1）。 || YARN | Either | queue QUEUE_NAME | The YARN queue to submit to (Default: “default”).要提交的YARN队列（默认值：“默认”）。 || YARN | Either | –num-executors NUM | Number of executors to launch (Default: 2). If dynamic allocation is enabled, the initial number of executors will be at least NUM.要启动的执行程序数（默认值：2）。如果启用了动态分配，则执行程序的初始数量将至少为NUM。 || YARN | Either | –archivesARCHIVES | Comma-separated list of archives to be extracted into the working directory of each executor.以逗号分隔的档案列表，提取到每个执行程序的工作目录中。 || YARN | Either | –principal PRINCIPAL | Principal to be used to log in to KDC, while running on secure HDFS.Principal 用于在安全 HDFS 上运行时登录 KDC。 || YARN | Either | –keytabKEYTAB | The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the Application Master via the Secure Distributed Cache, for renewing the login tickets and the delegation tokens periodically.包含上面指定的主体的keytab的文件的完整路径。此密钥表将通过安全分布式缓存复制到运行Application Master的节点，以定期更新登录票证和委派令牌。 |最新的 Spark 应用程序提交帮助文档### Application Launch ExamplesWe already covered some local-mode application examples previously in this chapter, but it’s worth looking at how we use some of the aforementioned options, as well. Spark also includes several examples and demonstration applications in the examples directory that is included when you download Spark. If you’re stuck on how to use certain parameters, simply try them first on your local machine and use the SparkPi class as the main class:我们已经介绍了本章前面的一些本地模式应用程序示例，但是值得一看的是我们如何使用上述一些选项。 Spark还包含下载Spark时包含的示例目录中的几个示例和演示应用程序。 如果你坚持使用某些参数，只需先在本地机器上尝试它们，然后使用SparkPi类作为主类：1234567./bin/spark-submit \--class org.apache.spark.examples.SparkPi \--master spark://207.184.161.138:7077 \--executor-memory 20G \--total-executor-cores 100 \replace/with/path/to/examples.jar \1000The following snippet does the same for Python. You run it from the Spark directory and this will allow you to submit a Python application (all in one script) to the standalone cluster manager. You can also set the same executor limits as in the preceding example:以下代码段对Python也是如此。 您可以从Spark目录运行它，这将允许您将Python应用程序（所有在一个脚本中）提交给独立的集群管理器。 您还可以设置与前面示例中相同的执行器（executor）限制：1234./bin/spark-submit \--master spark://207.184.161.138:7077 \examples/src/main/python/pi.py \1000You can change this to run in local mode as well by setting the master to local or local[] to run on all the cores on your machine. You will also need to change the /path/to/examples.jar to the relevant Scala and Spark versions you are running.您可以将此更改为在本地模式下运行，方法是将主服务器设置为 local 或 local[] 以在计算机上的所有核心上运行。 您还需要将 /path/to/examples.jar 更改为您正在运行的相关Scala和Spark版本。## Configuring Applications 配置应用程序Spark includes a number of different configurations, some of which we covered in Chapter 15. There are many different configurations, depending on what you’re hoping to achieve. This section covers those very details. For the most part, this information is included for reference and is probably worth skimming only, unless you’re looking for something in particular. The majority of configurations fall into the following categories:Spark包含许多不同的配置，其中一些我们在第15章中介绍过。根据您希望实现的目标，有许多不同的配置。本节介绍了这些细节。在大多数情况下，这些信息仅供参考，可能仅值得略读，除非您特别寻找某些内容。大多数配置分为以下几类：- Application properties 应用属性- Runtime environment 运行环境- Shuffle behavior 洗牌行为- Spark UI- Compression and serialization 解压缩- Memory management 内存管理- Execution behavior 执行行为- Networking 网络- Scheduling 调度- Dynamic allocation 动态分配- Security 安全- Encryption 加密- Spark SQL- Spark streaming Spark流- SparkRSpark provides three locations to configure the system:Spark提供三个位置来配置系统：- Spark properties control most application parameters and can be set by using a SparkConf object Spark 属性控制大多数应用程序参数，可以使用 SparkConf 对象进行设置- Java system properties Java系统属性- Hardcoded configuration files 硬编码配置文件There are several templates that you can use, which you can find in the /conf directory available in the root of the Spark home folder. You can set these properties as hardcoded variables in your applications or by specifying them at runtime. You can use environment variables to set per-machine settings, such as the IP address, through the conf/spark-env.sh script on each node. Lastly, you can configure logging through log4j.properties.您可以使用几个模板，您可以在Spark主文件夹的根目录中的 /conf 目录中找到这些模板。您可以在应用程序中将这些属性设置为硬编码变量，也可以在运行时指定它们。您可以使用环境变量通过每个节点上的 conf/spark-env.sh 脚本设置每台计算机设置，例如IP地址。最后，您可以通过 log4j.properties 配置日志记录。### The SparkConfThe SparkConf manages all of our application configurations. You create one via the import statement, as shown in the example that follows. After you create it, the SparkConf is immutable for that specific Spark Application:SparkConf 管理我们的所有应用程序配置。您可以通过 import 语句创建一个，如下面的示例所示。创建它之后，SparkConf 对于特定的Spark应用程序是不可变的：1234// in Scalaimport org.apache.spark.SparkConfval conf = new SparkConf().setMaster("local[2]").setAppName("DefinitiveGuide").set("some.conf", "to.some.value")1234# in Pythonfrom pyspark import SparkConfconf = SparkConf().setMaster("local[2]").setAppName("DefinitiveGuide")\.set("some.conf", "to.some.value")You use the SparkConf to configure individual Spark Applications with Spark properties. These Spark properties control how the Spark Application runs and how the cluster is configured. The example that follows configures the local cluster to have two threads and specifies the application name that shows up in the Spark UI.您可以使用 SparkConf 使用 Spark 属性配置各个 Spark 应用程序。这些 Spark 属性控制 Spark 应用程序的运行方式以及集群的配置方式。以下示例将本地集群配置为具有两个线程，并指定在 Spark UI 中显示的应用程序名称。You can configure these at runtime, as you saw previously in this chapter through command-line arguments. This is helpful when starting a Spark Shell that will automatically include a basic Spark Application for you; for instance:您可以在运行时配置它们，如本章前面通过命令行参数所见。这在启动Spark Shell时非常有用，它将自动包含一个基本的Spark应用程序；例如：1./bin/spark-submit --name "DefinitiveGuide" --master local[4] ...Of note is that when setting time duration-based properties, you should use the following format:值得注意的是，在设置基于持续时间的属性时，您应该使用以下格式：- 25ms (milliseconds 毫秒)- 5s (seconds 秒)- 10m or 10min (minutes 分钟)- 3h (hours 小时)- 5d (days 天)- 1y (years 年)### Application Properties 应用属性 Application properties are those that you set either from spark-submit or when you create your Spark Application. They define basic application metadata as well as some execution characteristics. Table 16-3 presents a list of current application properties. 应用程序属性是您通过 spark-submit 或创建 Spark 应用程序时设置的属性。它们定义了基本的应用程序元数据以及一些执行特性。表 16-3 列出了当前的应用程序属性。 Table 16-3. Application properties Property name 属性名 Default默认值 Meaning 意思 spark.app.name (none) The name of your application. This will appear in the UI and in log data.您的应用程序的名称。 这将显示在UI和日志数据中。 spark.driver.cores 1 Number of cores to use for the driver process, only in cluster mode.仅在集群模式下用于驱动程序进程的核心数。 spark.driver.maxResultSize 1g Limit of total size of serialized results of all partitions for each Spark action (e.g., collect). Should be at least 1M, or 0 for unlimited. Jobs will be aborted if the total size exceeds this limit. Having a high limit can cause OutOfMemoryErrors in the driver (depends on spark.driver.memory and memory overhead of objects in JVM). Setting a proper limit can protect the driver from OutOfMemoryErrors.每个Spark操作的所有分区的序列化结果的总大小限制（例如，collect）。 应至少为1M，或0为无限制。 如果总大小超过此限制，则将中止作业。 具有上限可能会导致驱动程序中的OutOfMemoryErrors（取决于spark.driver.memory和JVM中对象的内存开销）。 设置适当的限制可以保护驱动程序免受OutOfMemoryErrors的影响。 spark.driver.memory 1g Amount of memory to use for the driver process, where SparkContext is initialized. (e.g. 1g, 2g). Note: in client mode, this must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, set this through the –driver-memory command-line option or in your default properties file.用于初始化 SparkContext 的驱动程序进程的内存量。 （例如1g，2g）。 注意：在客户端模式下，不能直接在应用程序中通过 SparkConf 设置，因为驱动程序JVM已在此时启动。 而是通过–driver-memory命令行选项或在默认属性文件中设置它。 spark.executor.memory 1g Amount of memory to use per executor process (e.g., 2g, 8g).每个执行程序进程使用的内存量（例如，2g，8g）。 spark.extraListeners (none) A comma-separated list of classes that implement SparkListener; when initializing SparkContext, instances of these classes will be created and registered with Spark’s listener bus. If a class has a single-argument constructor that accepts a SparkConf, that constructor will be called; otherwise, a zero-argument constructor will be called. If no valid constructor can be found, the SparkContext creation will fail with an exception.以逗号分隔的实现SparkListener的类列表; 在初始化SparkContext时，将创建这些类的实例并使用Spark的侦听器总线进行注册。 如果一个类有一个接受SparkConf的单参数构造函数，那么将调用该构造函数; 否则，将调用零参数构造函数。 如果找不到有效的构造函数，SparkContext创建将失败并出现异常。 spark.logConf (false) Logs the effective SparkConf as INFO when a SparkContext is started.启动 SparkContext 时，将有效的 SparkConf 记录为INFO。 spark.master (none) The cluster manager to connect to. See the list of allowed master URLs.要连接的集群管理器。 请参阅允许的主URL列表。 spark.submit.deployMode (none) The deploy mode of the Spark driver program, either “client” or “cluster,” which means to launch driver program locally (“client”) or remotely (“cluster”) on one of the nodes inside the cluster.Spark驱动程序的部署模式，“客户端”或“集群”，这意味着在集群内的一个节点上本地（“客户端”）或远程（“集群”）启动驱动程序。 spark.log.callerContext (none) Application information that will be written into Yarn RM log/HDFS audit log when running on Yarn/HDFS. Its length depends on the Hadoop configuration hadoop.caller.context.max.size. It should be concise, and typically can have up to 50 characters.在 Yarn/HDFS 上运行时将写入Yarn RM log / HDFS 审核日志的应用程序信息。 它的长度取决于Hadoop配置hadoop.caller.context.max.size。 它应该简洁，通常最多可包含50个字符。 spark.driver.supervise (false) If true, restarts the driver automatically if it fails with a non-zero exit status. Only has effect in Spark standalone mode or Mesos cluster deploy mode.如果为true，则在失败且退出状态为非零时自动重新启动驱动程序。 仅在Spark独立模式或Mesos集群部署模式下有效。 You can ensure that you’ve correctly set these values by checking the application’s web UI on port 4040 of the driver on the “Environment” tab. Only values explicitly specified through sparkdefaults.conf, SparkConf, or the command line will appear. For all other configuration properties, you can assume the default value is used. 您可以通过在“环境”选项卡上检查驱动程序的端口4040上的应用程序的Web UI来确保您正确设置了这些值。仅显示那些通过 sparkdefaults.conf，SparkConf或命令行显式指定的值。对于所有其他配置属性，您可以假设使用默认值。 Runtime Properties 运行时属性Although less common, there are times when you might also need to configure the runtime environment of your application. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on the Runtime Environment in the Spark documentation. These properties allow you to configure extra classpaths and python paths for both drivers and executors, Python worker configurations, as well as miscellaneous logging properties. 虽然不太常见，但有时您可能还需要配置应用程序的运行时环境。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中的 Runtime Environment 上的相关表。这些属性允许您对驱动程序（driver）和执行器（excutor），Python工作程序进行配置以及各种日志记录属性配置额外的类路径和python路径。 Execution Properties 执行属性These configurations are some of the most relevant for you to configure because they give you finer-grained control on actual execution. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Execution Behavior in the Spark documentation. The most common configurations to change are spark.executor.cores (to control the number of available cores) and spark.files.maxPartitionBytes (maximum partition size when reading files). 这些配置与您配置最相关，因为它们可以为您提供对实际执行的精细控制。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中的执行行为相关表。要更改的最常见配置是 spark.executor.cores（用于控制可用内核的数量）和 spark.files.maxPartitionBytes（读取文件时的最大分区大小）。 Configuring Memory Management 配置内存管理There are times when you might need to manually manage the memory options to try and optimize your applications. Many of these are not particularly relevant for end users because they involve a lot of legacy concepts or fine-grained controls that were obviated in Spark 2.X because of automatic memory management. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Memory Management in the Spark documentation. 有时您可能需要手动管理内存选项以尝试和优化您的应用程序。 其中许多与最终用户并不特别相关，因为它们涉及很多遗留概念或由于自动内存管理而在Spark 2.X中避免的细粒度控制。 由于篇幅限制，我们无法在此处包含整个配置集。 请参阅Spark文档中的内存管理相关表。 Configuring Shuffle Behavior 配置洗牌行为We’ve emphasized how shuffles can be a bottleneck in Spark jobs because of their high communication overhead. Therefore there are a number of low-level configurations for controlling shuffle behavior. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Shuffle Behavior in the Spark documentation. 我们已经强调了洗牌如何成为Spark工作的瓶颈，因为它们的通信开销很高。因此，存在许多用于控制洗牌行为的低阶配置。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中有关洗牌行为的相关表。 Environmental Variables 环境变量You can configure certain Spark settings through environment variables, which are read from the conf/spark-env.sh script in the directory where Spark is installed (or conf/spark-env.cmd on Windows). In Standalone and Mesos modes, this file can give machine-specific information such as hostnames. It is also sourced when running local Spark Applications or submission scripts. Note that conf/spark-env.sh does not exist by default when Spark is installed. However, you can copy conf/spark-env.sh.template to create it. Be sure to make the copy executable. 您可以通过环境变量配置某些Spark设置，这些环境变量是从安装Spark的目录中的 conf/spark-env.sh 脚本（或Windows上的 conf/spark-env.cmd）中读取的。 在Standalone和Mesos模式下，此文件可以提供特定于机器的信息，例如主机名。 它还在运行本地Spark应用程序或提交脚本时获取。 请注意，安装Spark时默认情况下不存在conf/spark-env.sh。 但是，您可以复制 conf/spark-env.sh.template 来创建它。 务必使副本可执行。 The following variables can be set in spark-env.sh: 可以在spark-env.sh中设置以下变量： JAVA_HOME ​ Location where Java is installed (if it’s not on your default PATH).​ 安装Java的位置（如果它不在您的默认PATH上）。 PYSPARK_PYTHON ​ Python binary executable to use for PySpark in both driver and workers (default is python2.7 if available; otherwise, python). Property spark.pyspark.python takes precedence if it is set.​ 在驱动程序和工作程序中用于 PySpark 的 Python 二进制可执行文件（如果可用，默认为 python2.7; 否则为python）。 如果设置了属性 spark.pyspark.python，则优先级。 PYSPARK_DRIVER_PYTHON ​ Python binary executable to use for PySpark in driver only (default is PYSPARK_PYTHON). Property spark.pyspark.driver.python takes precedence if it is set.​ Python二进制可执行文件仅用于驱动程序中的PySpark（默认为PYSPARK_PYTHON）。 如果设置了属性spark.pyspark.driver.python，则优先级。 SPARKR_DRIVER_R ​ R binary executable to use for SparkR shell (default is R). Property spark.r.shell.command takes precedence if it is set.​ 用于SparkR shell的R二进制可执行文件（默认为R）。 如果设置了属性spark.r.shell.command优先。 SPARK_LOCAL_IP ​ IP address of the machine to which to bind.​ 要绑定的计算机的IP地址。 SPARK_PUBLIC_DNS ​ Hostname your Spark program will advertise to other machines.​ 您的Spark程序的主机名将通告给其他计算机。 In addition to the variables ust listed, there are also options for setting up the Spark standalone cluster scripts, such as number of cores to use on each machine and maximum memory. Because spark-env.sh is a shell script, you can set some of these programmatically; for example, you might compute SPARK_LOCAL_IP by looking up the IP of a specific network interface. 除了列出的变量之外，还有用于设置Spark独立集群脚本的选项，例如每台计算机上使用的核心数和最大内存。因为spark-env.sh是一个shell脚本，你可以通过编程方式设置其中一些;例如，您可以通过查找特定网络接口的IP来计算SPARK_LOCAL_IP。 NOTE 注意When running Spark on YARN in cluster mode, you need to set environment variables by using the spark.yarn.appMasterEnv.[EnvironmentVariableName] property in your conf/spark-defaults.conf file. Environment variables that are set in spark-env.sh will not be reflected in the YARN Application Master process in cluster mode. See the YARN-related Spark Properties for more information. 在集群模式下在 YARN 上运行Spark时，需要使用 conf/spark-defaults.conf 文件中的spark.yarn.appMasterEnv.[EnvironmentVariableName] 属性设置环境变量。在 spark-env.sh 中设置的环境变量不会在集群模式下反映在 YARN Application Master 进程中。有关更多信息，请参阅与 YARN 相关的 Spark属性。 Job Scheduling Within an Application 应用程序内的作业调度Within a given Spark Application, multiple parallel jobs can run simultaneously if they were submitted from separate threads. By job, in this section, we mean a Spark action and any tasks that need to run to evaluate that action. Spark’s scheduler is fully thread-safe and supports this use case to enable applications that serve multiple requests (e.g., queries for multiple users). By default, Spark’s scheduler runs jobs in FIFO fashion. If the jobs at the head of the queue don’t need to use the entire cluster, later jobs can begin to run right away, but if the jobs at the head of the queue are large, later jobs might be delayed significantly. 在给定的 Spark 应用程序中，如果从不同的的线程提交多个并行作业，则它们可以同时运行。按照作业，在本节中，我们指的是 Spark 操作以及需要运行以评估该操作的任何任务。 Spark 的调度程序是完全线程安全的，并支持此用户案例以支持提供多个请求的应用程序（例如，为多个用户进行查询）。默认情况下，Spark 的调度程序以FIFO方式运行作业。如果队列头部的作业不需要使用整个集群，则以后的作业可以立即开始运行，但如果队列头部的作业很大，则后续作业可能会显着延迟。 It is also possible to configure fair sharing between jobs. Under fair sharing, Spark assigns tasks between jobs in a round-robin fashion so that all jobs get a roughly equal share of cluster resources. This means that short jobs submitted while a long job is running can begin receiving resources right away and still achieve good response times without waiting for the long job to finish. This mode is best for multiuser settings. 也可以在作业之间配置公平共享。在公平共享下，Spark以循环方式在作业之间分配任务，以便所有作业获得大致相等的集群资源份额。这意味着当长期运行的工作正在执行时所提交的短期工作可以立即开始接收资源，并且仍然可以实现良好的响应时间，而无需等待长时间的工作完成。此模式最适合多用户设置。 To enable the fair scheduler, set the spark.scheduler.mode property to FAIR when configuring a SparkContext. 要启用公平调度器，请在配置 SparkContext 时将 spark.scheduler.mode 属性设置为 FAIR。 123val conf = new SparkConf().setMaster(...).setAppName(...)conf.set("spark.scheduler.mode", "FAIR")val sc = new SparkContext(conf) The fair scheduler also supports grouping jobs into pools, and setting different scheduling options, or weights, for each pool. This can be useful to create a high-priority pool for more important jobs or to group the jobs of each user together and give users equal shares regardless of how many concurrent jobs they have instead of giving jobs equal shares. This approach is modeled after the Hadoop Fair Scheduler. 公平调度器还支持将作业分组到池中，并为每个池设置不同的调度选项或权重。这对于为更重要的作业创建高优先级池或将每个用户的作业组合在一起并为用户提供相同的份额非常有用，无论他们有多少并发作业而不是给予作业相等的份额。此方法模拟Hadoop Fair Scheduler。 Without any intervention, newly submitted jobs go into a default pool, but jobs pools can be set by adding the spark.scheduler.pool local property to the SparkContext in the thread that’s submitting them. This is done as follows (assuming sc is your SparkContext )： 在没有任何干预的情况下，新提交的作业将进入默认池，但可以通过将 spark.scheduler.pool 本地属性添加到提交它们的线程中的 SparkContext 来设置作业池。这是完成如下（假设sc是你的 SparkContext )： 1sc.setLocalProperty("spark.scheduler.pool", "pool1") After setting this local property, all jobs submitted within this thread will use this pool name. The setting is per-thread to make it easy to have a thread run multiple jobs on behalf of the same user. If you’d like to clear the pool that a thread is associated with, set it to null. 设置此本地属性后，此线程中提交的所有作业都将使用此池名称。该设置是每个线程，以便让线程代表同一个用户运行多个作业变得容易。如果要清除与线程关联的池，请将其设置为null。 Conclusion 结论This chapter covered a lot about Spark Applications; we learned how to write, test, run, and configure them in all of Spark’s languages. In Chapter 17, we talk about deploying and the cluster management options you have when it comes to running Spark Applications. 本章介绍了 Spark 应用程序；我们学习了如何使用Spark的所有语言编写，测试，运行和配置它们。在第17章中，我们将讨论在运行 Spark 应用程序时的部署和集群管理选项。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 20 Stream Processing Fundamentals]]></title>
    <url>%2F2019%2F06%2F02%2FChapter20_StreamProcessingFundamentals(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 20 Stream Processing Fundamentals 流处理基础Stream processing is a key requirement in many big data applications. As soon as an application computes something of value—say, a report about customer activity, or a new machine learning model —an organization will want to compute this result continuously in a production setting. As a result, organizations of all sizes are starting to incorporate stream processing, often even in the first version of a new application. 流处理是许多大数据应用程序的关键要求。一旦一个应用程序计算出一些有价值的东西，比如关于客户活动的报告，或者一个新的机器学习模型，一个组织就会想要在生产环境中连续计算这个结果。因此，各种规模的组织都开始合并流处理，甚至在新应用程序的第一个版本中也是如此。 Luckily, Apache Spark has a long history of high-level support for streaming. In 2012, the project incorporated Spark Streaming and its DStreams API, one of the first APIs to enable stream processing using high-level functional operators like map and reduce. Hundreds of organizations now use DStreams in production for large real-time applications, often processing terabytes of data per hour. Much like the Resilient Distributed Dataset (RDD) API, however, the DStreams API is based on relatively low-level operations on Java/Python objects that limit opportunities for higher-level optimization. Thus, in 2016, the Spark project added Structured Streaming, a new streaming API built directly on DataFrames that supports both rich optimizations and significantly simpler integration with other DataFrame and Dataset code. The Structured Streaming API was marked as stable in Apache Spark 2.2, and has also seen swift adoption throughout the Spark community. 幸运的是，Apache Spark 有很长的高级流支持历史。2012年，该项目整合了 Spark Streaming 及其 DStreams API，这是第一批使用诸如 Map 和 Reduce 之类的高级功能操作符实现流处理的 API 之一。数百个组织现在在生产中使用数据流来处理大型实时应用程序，通常每小时处理数兆字节的数据。与弹性分布式数据集（RDD）API非常相似，但是，DStreams API 是基于 Java/Python 对象上相对较低级别的操作，这些对象限制了更高级别优化的机会。因此，在2016年，Spark项目添加了结构化流式处理（Structured Streaming），这是一种直接在 DataFrames 上构建的新流式API，它既支持丰富的优化，也支持与其他 DataFrame 和 Dataset 代码的显著简化集成。结构化流式API在ApacheSpark2.2中被标记为稳定的，并且在整个Spark社区中也得到了迅速的采用。 In this book, we will focus only on the Structured Streaming API, which integrates directly with the DataFrame and Dataset APIs we discussed earlier in the book and is the framework of choice for writing new streaming applications. If you are interested in DStreams, many other books cover that API, including several dedicated books on Spark Streaming only, such as Learning Spark Streaming by Francois Garillot and Gerard Maas (O’Reilly, 2017). Much as with RDDs versus DataFrames, however, Structured Streaming offers a superset of the majority of the functionality of DStreams, and will often perform better due to code generation and the Catalyst optimizer. Before we discuss the streaming APIs in Spark, let’s more formally define streaming and batch processing. This chapter will discuss some of the core concepts in this area that we will need throughout this part of the book. It won’t be a dissertation on this topic, but will cover enough of the concepts to let you make sense of systems in this space. 在本书中，我们将只关注结构化流式API（Structured Streaming API），它直接与本书前面讨论的 DataFrame 和 Dataset API 集成，是编写新流式应用程序的首选框架。如果您对 DStream 感兴趣，那么许多其他书籍都涉及该 API，其中包括一些专门的关于 Spark Streaming 的书籍，例如 Francois Garillot和Gerard Maas的 Learning Spark Streaming（O’Reilly，2017）。然而，与RDD与 DataFrame 相比，结构化流提供了数据流大部分功能的超集，并且由于代码生成和Catalyst优化器，通常性能会更好。 在讨论Spark中的流式API之前，让我们更正式地定义流式处理和批处理。本章将讨论本书这一部分中我们需要的这一领域的一些核心概念。这不是一篇关于这个主题的论文，但将涵盖足够多的概念，使您能够理解这个空间中的系统。 What Is Stream Processing 什么是流处理? Stream processing is the act of continuously incorporating new data to compute a result. In stream processing, the input data is unbounded and has no predetermined beginning or end. It simply forms a series of events that arrive at the stream processing system (e.g., credit card transactions, clicks on a website, or sensor readings from Internet of Things [IoT] devices). User applications can then compute various queries over this stream of events (e.g., tracking a running count of each type of event or aggregating them into hourly windows). The application will output multiple versions of the result as it runs, or perhaps keep it up to date in an external “sink” system such as a key-value store. 流处理是不断合并新数据以计算结果的行为。在流处理中，输入数据是无边界的，没有预先确定的开始或结束。它只是形成一系列到达流处理系统的事件（例如，信用卡交易、网站点击或物联网设备的传感器读数）。然后，用户应用程序可以计算对该事件流的各种查询（例如，跟踪每种类型事件的运行计数，或将其聚合到每小时一次的窗口中）。应用程序将在运行时输出结果的多个版本，或者在外部的“接收器”系统（如键值存储）中使其事件保持最新。 Naturally, we can compare streaming to batch processing, in which the computation runs on a fixedinput dataset. Oftentimes, this might be a large-scale dataset in a data warehouse that contains all the historical events from an application (e.g., all website visits or sensor readings for the past month). Batch processing also takes a query to compute, similar to stream processing, but only computes the result once. 当然，我们可以将流式处理与批处理进行比较，在批处理中，计算运行在固定的输入数据集上。通常，这可能是数据仓库中的大型数据集，其中包含应用程序的所有历史事件（例如，过去一个月的所有网站访问或传感器读数）。批处理也需要一个查询来计算，类似于流处理，但只计算一次结果。 Although streaming and batch processing sound different, in practice, they often need to work together. For example, streaming applications often need to join input data against a dataset written periodically by a batch job, and the output of streaming jobs is often files or tables that are queried in batch jobs. Moreover, any business logic in your applications needs to work consistently across streaming and batch execution: for example, if you have a custom code to compute a user’s billing amount, it would be harmful to get a different result when running it in a streaming versus batch fashion! To handle these needs, Structured Streaming was designed from the beginning to interoperate easily with the rest of Spark, including batch applications. Indeed, the Structured Streaming developers coined the term continuous applications to capture end-to-end applications that consist of streaming, batch, and interactive jobs all working on the same data to deliver an end product. Structured Streaming is focused on making it simple to build such applications in an end-to-end fashion instead of only handling stream-level per-record processing. 虽然流式处理和批处理听起来不同，但在实践中，它们通常需要一起工作。例如，流式处理应用程序通常需要将输入数据与批处理作业定期写入的数据集连接起来，而流式处理作业的输出通常是批处理作业中查询的文件或表。此外，应用程序中的任何业务逻辑都需要在流式处理和批处理执行之间始终如一地工作：例如，如果您有一个自定义代码来计算用户的账单金额，那么以流式处理与批处理方式运行时获得不同的结果将是有害的！为了处理这些需求，从一开始就设计了结构化流（Structured Streaming），以便与Spark的其余部分（包括批处理应用程序）轻松地进行互操作。实际上，结构化流式开发人员创造了术语“连续应用程序”，以捕获由流式、批处理和交互式作业组成的端到端应用程序，这些作业都处理相同的数据以交付最终产品。结构化流的重点是使端到端的方式构建此类应用程序变得简单，而不是只应对流级别的每个记录处理。 Stream Processing Use Cases 流处理使用案例We defined stream processing as the incremental processing of unbounded datasets, but that’s a strange way to motivate a use case. Before we get into advantages and disadvantages of streaming, let’s explain why you might want to use streaming. We’ll describe six common use cases with varying requirements from the underlying stream processing system. 我们将流处理定义为对无边界数据集的增量处理，但这是一种激活使用案例的奇怪方式。在我们讨论流的优点和缺点之前，让我们解释一下为什么您可能想要使用流。我们将描述来自底层流处理系统的具有不同需求的六个常见用户案例。 Notifications and alerting 通知和警报Probably the most obvious streaming use case involves notifications and alerting. Given some series of events, a notification or alert should be triggered if some sort of event or series of events occurs. This doesn’t necessarily imply autonomous or preprogrammed decision making; alerting can also be used to notify a human counterpart of some action that needs to be taken. An example might be driving an alert to an employee at a fulfillment center that they need to get a certain item from a location in the warehouse and ship it to a customer. In either case, the notification needs to happen quickly. 可能最明显的流式用户案例涉及通知和警报。对于某些事件系列，如果发生某种事件或事件系列，则应触发通知或警报。这并不一定意味着自主或预先编程的决策；警报也可以用来通知人类对应方需要采取的某些行动。例如，向履行中心的员工发出警报，提醒他们需要从仓库中的某个位置获取某个项目并将其发送给客户。在这两种情况下，通知都需要快速发生。 Real-time reporting 实时报告Many organizations use streaming systems to run real-time dashboards that any employee can look at. For example, this book’s authors leverage Structured Streaming every day to run real-time reporting dashboards throughout Databricks (where both authors of this book work). We use these dashboards to monitor total platform usage, system load, uptime, and even usage of new features as they are rolled out, among other applications. 许多组织使用流媒体系统来运行任何员工都可以查看的实时仪表盘。例如，本书的作者利用结构化流媒体每天在整个 Databricks（本书的两位作者都在这里工作）中运行实时报告仪表盘。我们使用这些仪表盘来监控平台的总使用量、系统负载、正常运行时间，甚至在推出新功能时对它们的使用情况，以及其他应用程序。 Incremental ETL 增量的ETLOne of the most common streaming applications is to reduce the latency companies must endure while retreiving information into a data warehouse—in short, “my batch job, but streaming.” Spark batch jobs are often used for Extract, Transform, and Load (ETL) workloads that turn raw data into a structured format like Parquet to enable efficient queries. Using Structured Streaming, these jobs can incorporate new data within seconds, enabling users to query it faster downstream. In this use case, it is critical that data is processed exactly once and in a fault-tolerant manner: we don’t want to lose any input data before it makes it to the warehouse, and we don’t want to load the same data twice. Moreover, the streaming system needs to make updates to the data warehouse transactionally so as not to confuse the queries running on it with partially written data. 最常见的流式应用程序之一是减少公司在将信息检索到数据仓库（简而言之，“我的批处理作业，但流式处理”）时必须忍受的延迟。Spark批处理作业通常用于提取、转换和加载（ETL）的工作，将原始数据转换为类似 Parquet的结构化格式，以实现高效查询。使用结构化流，这些作业可以在几秒钟内合并新数据，使用户能够更快地向下游查询数据。在这个用户案例中，以一种容错的方式处理数据且有且仅且处理一次是非常关键的：我们不希望在数据到达仓库之前丢失任何输入数据，也不希望加载相同的数据两次。此外，流媒体系统需要对数据仓库进行事务性更新，以免将运行在数据仓库上的查询与部分写入的数据混淆。 Update data to serve in real time 更新数据以提供实时服务Streaming systems are frequently used to compute data that gets served interactively by another application. For example, a web analytics product such as Google Analytics might continuously track the number of visits to each page, and use a streaming system to keep these counts up to date. When users interact with the product’s UI, this web application queries the latest counts. Supporting this use case requires that the streaming system can perform incremental updates to a key–value store (or other serving system) as a sync, and often also that these updates are transactional, as in the ETL case, to avoid corrupting the data in the application. 流系统通常用于计算由其他应用程序交互提供服务的数据。例如，像Google Analytics这样的Web Analytics产品可能会持续跟踪每个页面的访问次数，并使用流式系统保持这些计数的最新。当用户与产品的UI交互时，此Web应用程序查询最新计数。支持此用户案例要求流系统可以同步对键值存储（或其他服务系统）执行增量更新，并且通常这些更新是事务性的，如ETL情况，以避免损坏应用程序中的数据。 Real-time decision making 实时决策Real-time decision making on a streaming system involves analyzing new inputs and responding to them automatically using business logic. An example use case would be a bank that wants to automatically verify whether a new transaction on a customer’s credit card represents fraud based on their recent history, and deny the transaction if the charge is determined fradulent. This decision needs to be made in real-time while processing each transaction, so developers could implement this business logic in a streaming system and run it against the stream of transactions. This type of application will likely need to maintain a significant amount of state about each user to track their current spending patterns, and automatically compare this state against each new transaction. 流媒体系统的实时决策涉及到分析新的输入并使用业务逻辑自动响应它们。例如，一家银行希望根据客户最近的历史自动验证其信用卡上的新交易是否代表欺诈行为，并在确定费用过期时拒绝该交易。这个决策需要在处理每个事务时实时做出，因此开发人员可以在流系统中实现这个业务逻辑，并针对事务流运行它。这种类型的应用程序可能需要维护每个用户的大量状态，以跟踪他们当前的支出模式，并自动将这种状态与每个新事务进行比较。 Online machine learning 在线机器学习A close derivative of the real-time decision-making use case is online machine learning. In this scenario, you might want to train a model on a combination of streaming and historical data from multiple users. An example might be more sophisticated than the aforementioned credit card transaction use case: rather than reacting with hardcoded rules based on one customer’s behavior, the company may want to continuously update a model from all customers’ behavior and test each transaction against it. This is the most challenging use case of the bunch for stream processing systems because it requires aggregation across multiple customers, joins against static datasets, integration with machine learning libraries, and low-latency response times. 实时决策用户案例的一个密切派生是在线机器学习。在这个场景中，您可能希望对来自多个用户的流数据和历史数据组合的模型进行训练。一个例子可能比前面提到的信用卡交易用户案例更复杂：公司可能希望从所有客户的行为中不断更新一个模型，并根据它测试每个交易，而不是根据一个客户的行为对硬编码规则做出反应。对于流处理系统来说，这是最具挑战性的用户案例，因为它需要跨多个客户进行聚合、针对静态数据集进行连接、与机器学习库集成以及低延迟响应时间。 Advantages of Stream ProcessingNow that we’ve seen some use cases for streaming, let’s crystallize some of the advantages of stream processing. For the most part, batch is much simpler to understand, troubleshoot, and write applications in for the majority of use cases. Additionally, the ability to process data in batch allows for vastly higher data processing throughput than many streaming systems. However, stream processing is essential in two cases. First, stream processing enables lower latency: when your application needs to respond quickly (on a timescale of minutes, seconds, or milliseconds), you will need a streaming system that can keep state in memory to get acceptable performance. Many of the decision making and alerting use cases we described fall into this camp. Second, stream processing can also be more efficient in updating a result than repeated batch jobs, because it automatically incrementalizes the computation. For example, if we want to compute web traffic statistics over the past 24 hours, a naively implemented batch job might scan all the data each time it runs, always processing 24 hours’ worth of data. In contrast, a streaming system can remember state from the previous computation and only count the new data. If you tell the streaming system to update your report every hour, for example, it would only need to process 1 hour’s worth of data each time (the new data since the last report). In a batch system, you would have to implement this kind of incremental computation by hand to get the same performance, resulting in a lot of extra work that the streaming system will automatically give you out of the box. 既然我们已经看到了流的一些用例，那么让我们具体说明流处理的一些优势。在大多数情况下，对于大多数用例，批处理更容易理解、排除故障和编写应用程序。此外，批量处理数据的能力使得数据处理吞吐量大大高于许多流系统。然而，流处理在两种情况下是必要的。首先，流处理可以降低延迟：当应用程序需要快速响应（以分钟、秒或毫秒为时间刻度）时，您将需要一个流系统，它可以在内存中保持状态以获得可接受的性能。我们描述的许多决策和警报用例都属于这个阵营。第二，流处理在更新结果方面也比重复的批处理作业更有效，因为它会自动增加计算量。例如，如果我们想计算过去24小时内的Web流量统计数据，一个简单实现的批处理作业可能会在每次运行时扫描所有数据，总是处理24小时的数据。相反，流系统可以记住以前计算的状态，并且只计算新数据。例如，如果您告诉流系统每小时更新一次报告，则每次只需要处理1小时的数据（自上次报告以来的新数据）。在批处理系统中，您必须手工实现这种增量计算，以获得相同的性能，从而导致流系统自动提供的大量额外工作。 Challenges of Stream ProcessingWe discussed motivations and advantages of stream processing, but as you likely know, there’s never a free lunch. Let’s discuss some of the challenges of operating on streams. To ground this example, let’s imagine that our application receives input messages from a sensor (e.g., inside a car) that report its value at different times. We then want to search within this stream for certain values, or certain patterns of values. One specific challenge is that the input records might arrive to our application out-of-order: due to delays and retransmissions, for example, we might receive the following sequence of updates in order, where the time field shows the time when the value was actually measured: 我们讨论了流处理的动机和优势，但正如您可能知道的，从来没有免费的午餐。让我们讨论在流上操作的一些挑战。为了使这个例子更加简单，让我们假设我们的应用程序从一个传感器（例如，在车内）接收输入消息，该传感器在不同的时间报告其值。然后我们希望在这个流中搜索特定的值或值的特定模式。一个具体的挑战是，输入记录可能会无序到达我们的应用程序：例如，由于延迟和重新传输，我们可能会按顺序接收以下更新序列，其中时间字段显示实际测量值的时间： 12345&#123;value: 1, time: "2017-04-07T00:00:00"&#125;&#123;value: 2, time: "2017-04-07T01:00:00"&#125;&#123;value: 5, time: "2017-04-07T02:00:00"&#125;&#123;value: 10, time: "2017-04-07T01:30:00"&#125;&#123;value: 7, time: "2017-04-07T03:00:00"&#125; In any data processing system, we can construct logic to perform some action based on receiving the single value of “5.” In a streaming system, we can also respond to this individual event quickly. However, things become more complicated if you want only to trigger some action based on a specific sequence of values received, say, 2 then 10 then 5. In the case of batch processing, this is not particularly difficult because we can simply sort all the events we have by time field to see that 10 did come between 2 and 5. However, this is harder for stream processing systems. The reason is that the streaming system is going to receive each event individually, and will need to track some state across events to remember the 2 and 5 events and realize that the 10 event was between them. The need to remember such state over the stream creates more challenges. For instance, what if you have a massive data volume (e.g., millions of sensor streams) and the state itself is massive? What if a machine in the sytem fails, losing some state? What if the load is imbalanced and one machine is slow? And how can your application signal downstream consumers when analysis for some event is “done” (e.g., the pattern 2-10-5 did not occur)? Should it wait a fixed amount of time or remember some state indefinitely? All of these challenges and others—such as making the input and the output of the system transactional—can come up when you want to deploy a streaming application. 在任何数据处理系统中，我们都可以在接收到单个值“5”的基础上构造逻辑来执行某些操作。在流系统中，我们还可以快速响应这个单独的事件。但是，如果您只想根据接收到的特定值序列（例如2，然后10，然后5）触发一些操作，那么事情会变得更加复杂。在批处理的情况下，这并不特别困难，因为我们可以简单地按时间字段对所有事件进行排序，以查看10是否在2到5之间。然而，对于流处理系统来说，这是很困难的。**原因是流系统将单独接收每个事件，并且需要跨事件跟踪一些状态，以记住值为2和5的事件，并认识到值为10的事件介于两者之间。需要记住这样的状态会带来更多的挑战。例如，如果你有一个巨大的数据量（例如，数百万个传感器流），而状态本身也是巨大的呢？如果系统中的一台机器发生故障，失去某种状态，会怎么样？如果负载不平衡，一台机器运行缓慢怎么办？当某些事件的分析“完成”时（例如，模式2-10-5没有发生），您的应用程序如何向下游消费者发出信号？它应该等待固定的时间还是无限期地记住某个状态？当您想要部署流式应用程序时，所有这些挑战和其他挑战（如使系统事务性的输入和输出）都会出现。 To summarize, the challenges we described in the previous paragraph and a couple of others, are as follows: 综上所述，我们在上一段和其他几段中描述的挑战如下： Processing out-of-order data based on application timestamps (also called event time) 基于应用程序时间戳（也称为事件时间）处理无序数据 Maintaining large amounts of state 维持大量的状态 Supporting high-data throughput 支持大数据吞吐量 Processing each event exactly once despite machine failures 尽管机器出现故障，但每个事件处理一次 Handling load imbalance and straggler 处理负载不平衡和散乱 Responding to events at low latency 响应低延迟事件 Joining with external data in other storage systems 与其他存储系统中的外部数据连接 Determining how to update output sinks as new events arrive 确定新事件到达时如何更新输出接收器 Writing data transactionally to output systems 以事务方式将数据写入输出系统 Updating your application’s business logic at runtime 在运行时更新应用程序的业务逻辑 Each of these topics are an active area of research and development in large-scale streaming systems. To understand how different streaming systems have tackled these challenges, we describe a few of the most common design concepts you will see across them. 这些课题中的每一个都是大规模流系统研究和开发的活跃领域。为了了解不同的流系统如何应对这些挑战，我们将介绍一些您将看到的最常见的设计概念。 Stream Processing Design Points 流处理设计点To support the stream processing challenges we described, including high throughput, low latency, and out-of-order data, there are multiple ways to design a streaming system. We describe the most common design options here, before describing Spark’s choices in the next section. 为了支持我们描述的流处理挑战，包括高吞吐量、低延迟和无序数据，设计流系统有多种方法。在下一节描述Spark的选择之前，我们先在这里描述最常见的设计选项。 Record-at-a-Time Versus Declarative APIs 记录一次与声明性APIThe simplest way to design a streaming API would be to just pass each event to the application and let it react using custom code. This is the approach that many early streaming systems, such as Apache Storm, implemented, and it has an important place when applications need full control over the processing of data. Streaming that provide this kind of record-at-a-time API just give the user a collection of “plumbing” to connect together into an application. However, the downside of these systems is that most of the complicating factors we described earlier, such as maintaining state, are solely governed by the application. For example, with a record-at-a-time API, you are responsible for tracking state over longer time periods, dropping it after some time to clear up space, and responding differently to duplicate events after a failure. Programming these systems correctly can be quite challenging. At its core, low-level APIs require deep expertise to be develop and maintain. 设计流式API的最简单方法是将每个事件传递给应用程序，并让它使用自定义代码进行响应。这是许多早期流媒体系统（如 Apache Storm）实现的方法，在应用程序需要完全控制数据处理时，它具有重要的地位。流式处理提供了这种一次记录的API，它只给用户一个“管道”集合，将它们连接到一个应用程序中。但是，这些系统的缺点是，我们前面描述的大多数复杂因素（如维护状态）都是由应用程序单独控制的。例如，对于记录一次的API，您负责在较长的时间段内跟踪状态，在一段时间后将其丢弃以清除空间，并对失败后的重复事件做出不同的响应。对这些系统进行正确的编程是非常有挑战性的。核心层面来说，低阶API需要深厚的专业知识去执行开发和维护。 As a result, many newer streaming systems provide declarative APIs, where your application specifies what to compute but not how to compute it in response to each new event and how to recover from failure. Spark’s original DStreams API, for example, offered functional API based on operations like map, reduce and filter on streams. Internally, the DStream API automatically tracked how much data each operator had processed, saved any relevant state reliably, and recovered the computation from failure when needed. Systems such as Google Dataflow and Apache Kafka Streams provide similar, functional APIs. Spark’s Structured Streaming actually takes this concept even further, switching from functional operations to relational (SQL-like) ones that enable even richer automatic optimization of the execution without programming effort. 因此，许多较新的流系统都提供声明性API（declarative APIs），其中应用程序指定要计算什么，而不是如何响应每个新事件以及如何从故障中恢复。例如，Spark 最初的 DStreams API 提供了基于 map、reduce 和 filter 等操作的函数式 API。在内部，DStream API自动跟踪每个操作员处理了多少数据，可靠地保存了任何相关状态，并在需要时从失败中恢复计算。 Google Dataflow 和 Apache Kafka 流等系统提供类似的功能性API。Spark的结构化流实际上更进一步地采用了这一概念，从功能操作转换为关系操作（类似于SQL），从而在不需要编程的情况下实现更丰富的自动执行优化。 Event Time Versus Processing Time 事件时间与处理时间For the systems with declarative APIs, a second concern is whether the system natively supports event time. Event time is the idea of processing data based on timestamps inserted into each record at the source, as opposed to the time when the record is received at the streaming application (which is called processing time). In particular, when using event time, records may arrive to the system out of order (e.g., if they traveled back on different network paths), and different sources may also be out of sync with each other (some records may arrive later than other records for the same event time). If your application collects data from remote sources that may be delayed, such as mobile phones or IoT devices, event-time processing is crucial: without it, you will miss important patterns when some data is late. In contrast, if your application only processes local events (e.g., ones generated in the same datacenter), you may not need sophisticated event-time processing. 对于具有声明性API的系统，第二个问题是系统本身是否支持事件时间。事件时间是基于在源位置插入到每个记录中的时间戳来处理数据的概念，而不是在流应用程序接收记录的时间（称为处理时间）。特别是，在使用事件时间时，记录可能会无序到达系统（例如，如果它们返回到不同的网络路径上），并且不同的源也可能不同步（某些记录可能比同一事件时间的其他记录晚到达）。如果您的应用程序从可能延迟的远程源（如移动电话或物联网设备）收集数据，事件时间处理至关重要：如果没有它，当某些数据延迟时，您将错过重要的模式。相反，如果应用程序只处理本地事件（例如，在同一个数据中心生成的事件），则可能不需要复杂的事件时间处理。 When using event-time, several issues become common concerns across applications, including tracking state in a manner that allows the system to incorporate late events, and determining when it is safe to output a result for a given time window in event time (i.e., when the system is likely to have received all the input up to that point). Because of this, many declarative systems, including Structured Streaming, have “native” support for event time integrated into all their APIs, so that these concerns can be handled automatically across your whole program. 当使用事件时间时，几个问题成为应用程序中常见的问题，包括以允许系统合并延迟事件的方式跟踪状态，以及在给定的事件时间窗口内，确定安全输出结果的时间（即，系统可能具有接收到该点之前的所有输入）。因此，许多声明性系统（包括结构化流）都支持将事件时间集成到其所有API中，以便在整个程序中自动处理这些问题。 Continuous Versus Micro-Batch Execution 连续与微批量执行The final design decision you will often see come up is about continuous versus micro-batch execution. In continuous processing-based systems, each node in the system is continually listening to messages from other nodes and outputting new updates to its child nodes. For example, suppose that your application implements a map-reduce computation over several input streams. In a continuous processing system, each of the nodes implementing map would read records one by one from an input source, compute its function on them, and send them to the appropriate reducer. The reducer would then update its state whenever it gets a new record. The key idea is that this happens on each individual record, as illustrated in Figure 20-1. 您经常会看到的最终设计决策是关于连续与微批量执行的。在基于连续处理的系统中，系统中的每个节点都在不断地监听来自其他节点的消息，并将新的更新输出到其子节点。例如，假设您的应用程序在多个输入流上实现了一个map reduce计算。在连续处理系统中，实现映射的每个节点将从输入源中逐个读取记录，计算其功能，并将其发送到相应的reducer。然后，每当有新的记录时，reducer就会更新其状态。关键的想法是这发生在每个单独的记录上，如图20-1所示。 Continuous processing has the advantage of offering the lowest possible latency when the total input rate is relatively low, because each node responds immediately to a new message. However, continuous processing systems generally have lower maximum throughput, because they incur a significant amount of overhead per-record (e.g., calling the operating system to send a packet to a downstream node). In addition, continous systems generally have a fixed topology of operators that cannot be moved at runtime without stopping the whole system, which can introduce load balancing issues. 当总输入速率相对较低时，连续处理的优点是提供尽可能低的延迟，因为每个节点都会立即响应新消息。但是，连续处理系统通常具有较低的最大吞吐量，因为它们在每条记录上都会产生大量开销（例如，调用操作系统向下游节点发送数据包）。此外，连续系统通常具有固定的算子（ operator: [mathematics] a symbol or function which represents an operation in mathematics）拓扑结构，如果不停止整个系统，就无法在运行时移动这些算子，这会引入负载平衡问题。 In contrast, micro-batch systems wait to accumulate small batches of input data (say, 500 ms’ worth), then process each batch in parallel using a distributed collection of tasks, similar to the execution of a batch job in Spark. Micro-batch systems can often achieve high throughput per node because they leverage the same optimizations as batch systems (e.g., vectorized processing), and do not incur any extra per-record overhead, as illustrated in Figure 20-2. 相反，微批处理系统等待积累小批量的输入数据（比如500毫秒的值），然后使用分布式任务集合并行处理每一批，类似于Spark中批处理作业的执行。微批处理系统通常可以实现每个节点的高吞吐量，因为它们利用与批处理系统相同的优化（例如，矢量化处理），并且不会产生任何额外的每记录开销，如图20-2所示。 Thus, they need fewer nodes to process the same rate of data. Micro-batch systems can also use dynamic load balancing techniques to handle changing workloads (e.g., increasing or decreasing the number of tasks). The downside, however, is a higher base latency due to waiting to accumulate a micro-batch. In practice, the streaming applications that are large-scale enough to need to distribute their computation tend to prioritize throughput, so Spark has traditionally implemented micro-batch processing. In Structured Streaming, however, there is an active development effort to also support a continuous processing mode beneath the same API. 因此，它们需要更少的节点来处理相同的数据速率。微批处理系统还可以使用动态负载平衡技术来处理不断变化的工作负载（例如，增加或减少任务数量）。但是，缺点是，由于等待积累一个微批处理，所以基本延迟更高。在实践中，流应用程序的规模足够大，需要分配它们的计算（资源），倾向于优先考虑吞吐量，因此Spark传统上已经实现了微批处理。然而，在结构化流中，也有一项积极的开发工作来支持同一API下的连续处理模式。 When choosing between these two execution modes, the main factors you should keep in mind are your desired latency and total cost of operation (TCO). Micro-batch systems can comfortably deliver latencies from 100 ms to a second, depending on the application. Within this regime, they will generally require fewer nodes to achieve the same throughput, and hence lower operational cost (including lower maintenance cost due to less frequent node failures). For much lower latencies, you should consider a continuous processing system, or using a micro-batch system in conjunction with a fast serving layer to provide low-latency queries (e.g., outputting data into MySQL or Apache Cassandra, where it can be served to clients in milliseconds). 在这两种执行模式之间进行选择时，您应该记住的主要因素是所需的延迟和总操作成本（TCO: total cost of operation）。根据应用程序的不同，微批处理系统可以轻松地将延迟从100毫秒传递到一秒钟。在这种情况下，它们通常需要更少的节点来实现相同的吞吐量，从而降低运营成本（包括由于节点故障频率较低而导致的维护成本）。对于较低的延迟，您应该考虑使用连续处理系统，或者将微批处理系统与快速服务层结合使用，以提供低延迟查询（例如，将数据输出到 MySQL 或 Apache Cassandra 中，在那里它可以以毫秒为单位提供给客户机）。 In general, Structured Streaming is meant to be an easier-to use and higher-performance evolution of Spark Streaming’s DStream API, so we will focus solely on this new API in this book. Many of the concepts, such as building a computation out of a graph of transformations, also apply to DStreams, but we leave the exposition of that to other books. Spark’s Streaming APIs Spark的流式APIWe covered some high-level design approaches to stream processing, but thus far we have not discussed Spark’s APIs in detail. Spark includes two streaming APIs, as we discussed at the beginning of this chapter. The earlier DStream API in Spark Streaming is purely micro-batch oriented. It has a declarative (functional-based) API but no support for event time. The newer Structured Streaming API adds higher-level optimizations, event time, and support for continuous processing. 我们介绍了流处理的一些高级设计方法，但到目前为止，我们还没有详细讨论Spark的API。Spark包含两个流式API，正如我们在本章开头所讨论的。Spark Streaming 中早期的 DStream API 纯粹是面向微批处理的。它有一个声明性（基于函数的）API，但不支持事件时间。更新的结构化流式API增加了更高级的优化、事件时间和对连续处理的支持。 The DStream API 数据流APISpark’s original DStream API has been used broadly for stream processing since its first release in 2012. For example, DStreams was the most widely used processing engine in Datanami’s 2016 survey. Many companies use and operate Spark Streaming at scale in production today due to its highlevel API interface and simple exactly-once semantics. Interactions with RDD code, such as joins with static data, are also natively supported in Spark Streaming. Operating Spark Streaming isn’t much more difficult than operating a normal Spark cluster. However, the DStreams API has several limitations. First, it is based purely on Java/Python objects and functions, as opposed to the richer concept of structured tables in DataFrames and Datasets. This limits the engine’s opportunity to perform optimizations. Second, the API is purely based on processing time—to handle event-time operations, applications need to implement them on their own. Finally, DStreams can only operate in a micro-batch fashion, and exposes the duration of micro-batches in some parts of its API, making it difficult to support alternative execution modes. Spark 的原始 DStream API 自2012年首次发布以来，已广泛用于流处理。例如，DStreams是Datanami 2016年调查中使用最广泛的处理引擎。由于其高级API接口和简单的一次性语义，许多公司现在在生产中大规模使用和操作Spark流。与 RDD 代码的交互（如与静态数据的连接）也在Spark流中得到原生支持。操作 Spark 流并不比操作普通的 Spark 集群更困难。但是，DStreams API有几个限制。首先，它纯粹基于Java/Python 对象和函数，而不是 DataFrames 和 Datasets 中结构化表的更丰富的概念。这限制了引擎执行优化的机会。第二，API 纯粹是基于处理时间来处理事件时间操作，应用程序需要自己实现它们。最后，DStream 只能以微批量方式操作，并在其API的某些部分公开微批量的持续时间，这使得支持替代执行模式变得困难。 Structured Streaming 结构化流Structured Streaming is a higher-level streaming API built from the ground up on Spark’s Structured APIs. It is available in all the environments where structured processing runs, including Scala, Java, Python, R, and SQL. Like DStreams, it is a declarative API based on high-level operations, but by building on the structured data model introduced in the previous part of the book, Structured Streaming can perform more types of optimizations automatically. However, unlike DStreams, Structured Streaming has native support for event time data (all of its the windowing operators automatically support it). As of Apache Spark 2.2, the system only runs in a micro-batch model, but the Spark team at Databricks has announced an effort called Continuous Processing to add a continuous execution mode. This should become an option for users in Spark 2.3. 结构化流是在Spark的结构化API基础上构建的更高阶的流式API。它可以在结构化处理运行的所有环境中使用，包括Scala、Java、Python、R和SQL。与数据流一样，它是一个基于高阶操作的声明性API，但是通过构建本书上一部分介绍的结构化数据模型，结构化流可以自动执行更多类型的优化。但是，与数据流不同，结构化流具有对事件时间数据的原生支持（它的所有窗口化算子都自动支持它）。从 Apache Spark 2.2开始，系统只运行在一个微批量模型中，但 Databricks 的 Spark团队已经宣布了一项称为“连续处理”的工作，以添加连续执行模式。这应该成为 Spark 2.3 中用户的一个选项。 More fundamentally, beyond simplifying stream processing, Structured Streaming is also designed to make it easy to build end-to-end continuous applications using Apache Spark that combine streaming, batch, and interactive queries. For example, Structured Streaming does not use a separate API from DataFrames: you simply write a normal DataFrame (or SQL) computation and launch it on a stream. Structured Streaming will automatically update the result of this computation in an incremental fashion as data arrives. This is a major help when writing end-to-end data applications: developers do not need to maintain a separate streaming version of their batch code, possibly for a different execution system, and risk having these two versions of the code fall out of sync. As another example, Structured Streaming can output data to standard sinks usable by Spark SQL, such as Parquet tables, making it easy to query your stream state from another Spark applications. In future versions of Apache Spark, we expect more and more components of the project to integrate with Structured Streaming, including online learning algorithms in MLlib. 更重要的是，除了简化流处理之外，结构化流还设计为使用结合流、批处理和交互式查询的Apache Spark轻松构建端到端连续应用程序。例如，结构化流不使用不同于 DataFrames 的API：您只需编写一个普通的 DataFrame（或SQL）计算并在流上启动它。当数据到达时，结构化流将以增量方式自动更新计算结果。在编写端到端数据应用程序时，主要的帮助是：可能是针对不同的执行系统，开发人员不需要维护其批处理代码的不同的流式版本，并且拥有两个版本的代码将有不同步的风险。作为另一个例子，结构化流可以将数据输出到 Spark Sql 可用的标准接收器，例如 Parquet 表，这样就可以很容易地从另一个Spark应用程序查询流状态。在 Apache Spark的未来版本中，我们期望项目中越来越多的组件与结构化流集成，包括MLLIB中的在线学习算法。 Conclusion 总结This chapter covered the basic concepts and ideas that you’re going to need to understand stream processing. The design approaches introduced in this chapter should clarify how you can evaluate streaming systems for a given application. You should also feel comfortable understanding what trade-offs the authors of DStreams and Structured Streaming have made, and why the direct support for DataFrame programs is a big help when using Structured Streaming: there is no need to duplicate your application logic. In the upcoming chapters, we’ll dive right into Structured Streaming to understand how to use it. 本章介绍了理解流处理所需的基本概念和想法。本章介绍的设计方法应该阐明如何评估给定应用程序的流系统。您还应该理解数据流和结构化流的作者所做的权衡，以及为什么在使用结构化流时直接支持 DataFrame 程序是一个很大的帮助：不需要复制应用程序逻辑。在接下来的章节中，我们将直接深入到结构化流，了解如何使用它。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop YARN Federation]]></title>
    <url>%2F2019%2F05%2F14%2FHadoopYARNFederation%2F</url>
    <content type="text"><![CDATA[Preface写这个阅读笔记之前，Hadoop YARN 的稳定发行版是：2.9.2 。 YARN Federation 思想源自：HDFS Federation，官方文档介绍，HDFS Federation 解决的是 NameNode 的横向扩展，HDFS HA 解决的是 NameNode 的单点问题。 PurposeYARN is known to scale to thousands of nodes. The scalability of YARN is determined by the Resource Manager, and is proportional to number of nodes, active applications, active containers, and frequency of heartbeat (of both nodes and applications). Lowering heartbeat can provide scalability increase, but is detrimental to utilization (see old Hadoop 1.x experience). This document described a federation-based approach to scale a single YARN cluster to tens of thousands of nodes, by federating multiple YARN sub-clusters. The proposed approach is to divide a large (10-100k nodes) cluster into smaller units called sub-clusters, each with its own YARN RM and compute nodes. The federation system will stitch these sub-clusters together and make them appear as one large YARN cluster to the applications. The applications running in this federated environment will see a single massive YARN cluster and will be able to schedule tasks on any node of the federated cluster. Under the hood, the federation system will negotiate with sub-clusters resource managers and provide resources to the application. The goal is to allow an individual job to “span” sub-clusters seamlessly. YARN 可以伸缩到数千个节点。 YARN 的可伸缩性由资源管理器决定，与节点数量、活动的应用程序、活动的容器和心跳频率（节点和应用程序）成正比。降低心跳可以提高可伸缩性，但不利于利用率（请参阅旧的Hadoop1.x体验）。本文描述了一种基于联邦（federation）的方法，通过将多个 YARN 子集群结成（federate）联邦，将单个 YARN 集群扩展到数万个节点。该方法将一个大的（10-100K节点）集群划分为更小的子集群单元，每个子集群都有自己的 YARN RM 和计算节点。联邦系统（federation system）将这些子集群结合（stitch）在一起，使它们成为应用程序中的一个大型 YARN 集群。在这个联合环境中运行的应用程序将看到一个单个巨大的 YARN 集群，并且能够在联邦集群的任何节点上调度任务。在幕后，联邦系统将与子集群资源管理器协商，并向应用程序提供资源。目标是允许单个作业无缝地“跨越”子集群。 This design is structurally scalable, as we bound the number of nodes each RM is responsible for, and appropriate policies, will try to ensure that the majority of applications will reside within a single sub-cluster, thus the number of applications each RM will see is also bounded. This means we could almost linearly scale, by simply adding sub-clusters (as very little coordination is needed across them). This architecture can provide very tight enforcement of scheduling invariants within each sub-cluster (simply inherits from YARN), while continuous rebalancing across subcluster will enforce (less strictly) that these properties are also respected at a global level (e.g., if a sub-cluster loses a large number of nodes, we could re-map queues to other sub-clusters to ensure users running on the impaired sub-cluster are not unfairly affected). 这种设计在结构上是可伸缩的，因为我们限制了了每个 RM 负责的节点的数量，并且适当的策略将尝试确保大多数应用程序将驻留在单个子集群中，因此每个 RM 将看到的应用程序的数量也是有界的。这意味着我们可以通过简单地添加子集群（因为在它们之间几乎不需要协调）来线性扩展。这种体系结构可以在每个子集群中提供对调度不变量进行非常严格执行（简单地继承自 YARN），而跨子集群的连续重新平衡将强制（不太严格）在全局级别上也遵守这些属性（例如，如果子集群丢失了大量的节点，我们可以将队列重新映射到其他子集群，以确保在受损子集群上运行的用户不会受到不公平的影响）。 Federation is designed as a “layer” atop of existing YARN codebase, with limited changes in the core YARN mechanisms. 联邦被设计为现有 YARN 代码库的顶“层”，核心 YARN 机制的变化有限。 Assumptions: We assume reasonably good connectivity across sub-clusters (e.g., we are not looking to federate across DC yet, though future investigations of this are not excluded). 我们假设子集群之间具有相当好的连通性（例如，我们还不希望在整个DC之间建立联邦，尽管未来对此的调查并未排除在外）。 We rely on HDFS federation (or equivalently scalable DFS solutions) to take care of scalability of the store side. 我们依赖 HDFS 联邦（或同等可扩展的 HDFS 解决方案）来处理存储端的可扩展性。 ArchitectureOSS YARN has been known to scale up to about few thousand nodes. The proposed architecture leverages the notion of federating a number of such smaller YARN clusters, referred to as sub-clusters, into a larger federated YARN cluster comprising of tens of thousands of nodes. The applications running in this federated environment see a unified large YARN cluster and will be able to schedule tasks on any nodes in the cluster. Under the hood, the federation system will negotiate with sub-clusters RMs and provide resources to the application. The logical architecture in Figure 1 shows the main components that comprise the federated cluster, which are described below. 据了解，OSS YARN 可以扩展到大约几千个节点。所提出的架构利用了一些较小的 YARN 集群（称为子集群）联合成由数万个节点组成的较大的联邦 YARN 集群的概念。在这个联邦环境中运行的应用程序可以看到一个统一的大型 YARN 集群，并且能够在集群中的任何节点上调度任务。在这种情况下，联邦系统将与子集群的 RMs 进行协商，并为应用程序提供资源。图1中的逻辑架构显示了组成联邦集群的主要组件，如下所述。 YARN Sub-clusterA sub-cluster is a YARN cluster with up to few thousands nodes. The exact size of the sub-cluster will be determined considering ease of deployment/maintenance, alignment with network or availability zones and general best practices. 子集群是一个具有数千个节点的 YARN 集群。考虑到易于部署或维护、与网络或区域可用性以及通用最佳实践，将确定子集群的准确大小。 The sub-cluster YARN RM will run with work-preserving high-availability turned-on, i.e., we should be able to tolerate YARN RM, NM failures with minimal disruption. If the entire sub-cluster is compromised, external mechanisms will ensure that jobs are resubmitted in a separate sub-cluster (this could eventually be included in the federation design). 子集群 YARN RM 将在保持高可用性的情况下运行，即，我们应该能够承受 YARN RM、NM故障，且受损最小。如果整个子集群受到破坏，外部机制将确保在不同的子集群中重新提交作业（这可能最终包括在联邦集群设计中）。 Sub-cluster is also the scalability unit in a federated environment. We can scale out the federated environment by adding one or more sub-clusters. 子集群也是联邦环境中的可伸缩性单元。我们可以通过添加一个或多个子集群来扩展联邦环境。 Note: by design each sub-cluster is a fully functional YARN RM, and its contribution to the federation can be set to be only a fraction of its overall capacity, i.e. a sub-cluster can have a “partial” commitment to the federation, while retaining the ability to give out part of its capacity in a completely local way. 注：根据设计，每个子集群都是一个功能齐全的 YARN RM，其对联邦的贡献可以设置为其总容量的一小部分，即子集群可以对联邦“部分”承诺，同时保留了部分容量完全给本地运行的的能力。 RouterYARN applications are submitted to one of the Routers, which in turn applies a routing policy (obtained from the Policy Store), queries the State Store for the sub-cluster URL and redirects the application submission request to the appropriate sub-cluster RM. We call the sub-cluster where the job is started the “home sub-cluster”, and we call “secondary sub-clusters” all other sub-cluster a job is spanning on. The Router exposes the ApplicationClientProtocol to the outside world, transparently hiding the presence of multiple RMs. To achieve this the Router also persists the mapping between the application and its home sub-cluster into the State Store. This allows Routers to be soft-state while supporting user requests cheaply, as any Router can recover this application to home sub-cluster mapping and direct requests to the right RM without broadcasting them. For performance caching and session stickiness might be advisable. The state of the federation (including applications and nodes) is exposed through the Web UI. YARN 应用程序被提交到其中一个路由器（Router），该路由器依次应用路由策略（从 Policy Store 中获得），查询 Policy Store 得到子集群 URL，并将应用程序提交的请求重定向到相应的子集群 RM。我们将启动作业的子集群称为“home sub-cluster”，并将作业所跨越的所有其他子集群称为“secondary sub-cluster”。路由器向外界公开ApplicationClientProtocol，透明（transparently ）地隐藏多个 RMs 的存在。为了实现这一点，路由器（Router）还将应用程序与其 home sub-cluster 之间的映射一直保存到 State Store 中。这允许路由器处于软状态（soft-state），同时以较低的成本支持用户请求，因为任何路由器都可以将此应用程序恢复到 home sub-cluster 映射，并将请求直接发送到正确的 RM，而无需广播它们。对于性能缓存（performance caching）和会话粘性（session stickiness）可能是明智的。联邦状态（包括应用程序和节点）通过Web UI公开。 AMRMProxyThe AMRMProxy is a key component to allow the application to scale and run across sub-clusters. The AMRMProxy runs on all the NM machines and acts as a proxy to the YARN RM for the AMs by implementing the ApplicationMasterProtocol. Applications will not be allowed to communicate with the sub-cluster RMs directly. They are forced by the system to connect only to the AMRMProxy endpoint, which would provide transparent access to multiple YARN RMs ( by dynamically routing/splitting/merging the communications ). At any one time, a job can span across one home sub-cluster and multiple secondary sub-clusters, but the policies operating in the AMRMProxy try to limit the footprint of each job to minimize overhead on the scheduling infrastructure (more in section on scalability/load). The interceptor chain architecture of the ARMMProxy is showing in figure. AMRMProxy 是允许应用程序在子集群之间扩展和运行的关键组件。AMRMProxy 运行在所有 NM 机器上，通过实现 ApplicationMasterProtocol 作为 AMs 的 YARN RM的代理。不允许应用程序直接与子集群 RMs 通信。它们被系统强制只连接到 AMRMProxy 端点，这将提供对多个 YARN RMs 的透明（transparently）访问（通过动态路由/拆分/合并通信）。在任何时候，一个作业都可以跨越一个 home sub-cluster 和多个 secondary sub-clusters，但是 AMRMProxy 中运行的策略试图限制每个作业的占用空间，以最小化在负责调度的基础结构（scheduling infrastructure）上的开销（更多内容请参见可伸缩性/负载部分）。AMRMProxy 的拦截器链结构如图所示。 Role of AMRMProxy Protect the sub-cluster YARN RMs from misbehaving AMs. The AMRMProxy can prevent DDOS attacks by throttling/killing AMs that are asking too many resources. 保护 sub-cluster YARN RMs 不受不良 AMs 的影响。AMRMProxy 可以通过限制/杀死请求过多资源的 AMs 来防止DDO攻击。 Mask the multiple YARN RMs in the cluster, and can transparently allow the AM to span across sub-clusters. All container allocations are done by the YARN RM framework that consists of the AMRMProxy fronting the home and other sub-cluster RMs. 遮掩集群中的多个 YARN RMs ，并透明地（transparently）允许 AM 跨越子群。所有的容器分配都是通过YARN RM 框架完成的，该框架由 home sub-cluster RM 和 other sub-cluster RMs 的 AMRMProxy 组成。 Intercepts all the requests, thus it can enforce application quotas, which would not be enforceable by sub-cluster RM (as each only see a fraction of the AM requests). 截取所有请求，因此它可以强制应用程序配额，而子集群RM将无法强制应用程序配额（因为每个请求只看到AM请求的一部分）。 The AMRMProxy can enforce load-balancing / overflow policies. AMRMProxy 可以强制执行负载平衡/溢出策略。 Global Policy GeneratorGlobal Policy Generator overlooks the entire federation and ensures that the system is configured and tuned properly all the time. A key design point is that the cluster availability does not depends on an always-on GPG. The GPG operates continuously but out-of-band from all cluster operations, and provide us with a unique vantage point, that allows to enforce global invariants, affect load balancing, trigger draining of sub-clusters that will undergo maintenance, etc. More precisely the GPG will update user capacity allocation-to-subcluster mappings, and more rarely change the policies that run in Routers, AMRMProxy (and possible RMs). 全局策略生成器（Global Policy Generator: GPG）整体把控（overlook）整个联邦系统，并确保系统始终正确配置和调优。一个关键的设计点是集群的可用性并不依赖于一个始终在线的 GPG。GPG连续运行，但在所有集群的操作都是out-of-band，并为我们提供了一个独特的优势点，允许强制执行全局不变量、影响负载平衡、触发以下动作：排除要进行维护的子集群等。更准确地说，GPG将更新：用户与分配给子集群的容量之间的映射关系，并且很少更改在路由器、AMRMProxy （以及可能的 RMs）中运行的策略。 In case the GPG is not-available, cluster operations will continue as of the last time the GPG published policies, and while a long-term unavailability might mean some of the desirable properties of balance, optimal cluster utilization and global invariants might drift away, compute and access to data will not be compromised. 如果 GPG 不可用，则集群操作将按照在 GPG 上次发布的策略继续进行，而长期不可用可能意味着平衡、最佳集群利用率和全局不变量的某些理想属性可能会偏移，计算以及访问数据不会被妥协。 NOTE: In the current implementation the GPG is a manual tuning process, simply exposed via a CLI (YARN-3657).注意：在当前的实现中，GPG 是一个手动调优过程，只需通过 CLI（yarn-3657）公开即可。 This part of the federation system is part of future work in YARN-5597.联邦系统的这一部分是 YARN-5597 未来工作的一部分。 Federation State-StoreThe Federation State defines the additional state that needs to be maintained to loosely couple multiple individual sub-clusters into a single large federated cluster. This includes the following information: 联邦状态（ Federation State）定义了需要维护的额外状态，以便将多个子集群松散地耦合到单个大型联邦集群中。这包括以下信息： 1. Sub-cluster MembershipThe member YARN RMs continuously heartbeat to the state store to keep alive and publish their current capability/load information. This information is used by the Global Policy Generator (GPG) to make proper policy decisions. Also this information can be used by routers to select the best home sub-cluster. This mechanism allows us to dynamically grow/shrink the “cluster fleet” by adding or removing sub-clusters. This also allows for easy maintenance of each sub-cluster. This is new functionality that needs to be added to the YARN RM but the mechanisms are well understood as it’s similar to individual YARN RM HA. 成员 YARN RMs 发出连续心跳到 State Store来保持激活状态并发布其当前能力值（capability）/负载信息（load information）。全局策略生成器（GPG）使用此信息做出正确的策略决策。路由器也可以利用这些信息来选择最佳的 home sub-cluster。这个机制允许我们通过添加或删除子集群来动态地增长（grow）/收缩（shrink）“集群的机群（cluster fleet）”。这还允许轻松维护每个子集群。这是需要添加到 YARN RM 中的新功能，但机制已被很好地理解，因为它类似于单个 YARN RM HA。 2. Application’s Home Sub-clusterThe sub-cluster on which the Application Master (AM) runs is called the Application’s “home sub-cluster”. The AM is not limited to resources from the home sub-cluster but can also request resources from other sub-clusters, referred to as secondary sub-clusters. The federated environment will be configured and tuned periodically such that when an AM is placed on a sub-cluster, it should be able to find most of the resources on the home sub-cluster. Only in certain cases it should need to ask for resources from other sub-clusters. 运行 Application Master（AM）的子集群称为应用程序的 “home sub-cluster” 。AM不限于来自 home sub-cluster 的资源，还可以从其他子集群（被称为 secondary sub-clusters）请求资源。联邦环境将定期进行配置和调优，这样当 AM 放置在一个子集群上时，它应该能够找到 home sub-cluster 上的大部分资源。只有在某些情况下，它才需要从其他子集群请求资源。 Federation Policy StoreThe federation Policy Store is a logically separate store (while it might be backed by the same physical component), which contains information about how applications and resource requests are routed to different sub-clusters. The current implementation provides several policies, ranging from random/hashing/round robin/priority to more sophisticated ones which account for sub-cluster load, and request locality needs. 联邦 Policy Store 是一个逻辑上独立的存储（虽然它可能由同一物理组件支持），其中包含如何将应用程序和资源请求路由到不同子群集的信息。当前的实现提供了几个策略，从随机/哈希/循环/优先级到更复杂的策略，这些策略负责子集群负载和请求位置需求。 Running Applications across Sub-ClustersWhen an application is submitted, the system will determine the most appropriate sub-cluster to run the application, which we call as the application’s home sub-cluster. All the communications from the AM to the RM will be proxied via the AMRMProxy running locally on the AM machine. AMRMProxy exposes the same ApplicationMasterService protocol endpoint as the YARN RM. The AM can request containers using the locality information exposed by the storage layer. In ideal case, the application will be placed on a sub-cluster where all the resources and data required by the application will be available, but if it does need containers on nodes in other sub-clusters, AMRMProxy will negotiate with the RMs of those sub-clusters transparently and provide the resources to the application, thereby enabling the application to view the entire federated environment as one massive YARN cluster. AMRMProxy, Global Policy Generator (GPG) and Router work together to make this happen seamlessly. 提交应用程序时，系统将确定运行应用程序的最合适的子集群，我们称之为应用程序的 home sub-cluster。从 AM 到 RM 的所有通信都将通过在 AM 机器上运行的 AMRMProxy 进行代理。 AMRMProxy 开放了相同的 ApplicationMasterService 协议端点作为与 YARN RM。AM可以使用存储层公开的位置信息请求容器。理想情况下，应用程序将被放置在子集群上，应用程序所需的所有资源和数据都将可用，但如果它确实需要其他子集群中节点上的容器， AMRMProxy 将透明地与这些子集群的 RMs 协商，并提供资源。源到应用程序，从而使应用程序能够将整个联邦环境视为一个巨大的 YARN 集群。 AMRMProxy 、全局策略生成器（gpg）和路由器协同工作，实现无缝连接。 The figure shows a sequence diagram for the following job execution flow: 该图显示了以下作业执行流程的序列图： The Router receives an application submission request that is complaint to the YARN Application Client Protocol. 路由器接收到一个应用程序提交请求，这是对 YARN 应用客户端协议的投诉。 The router interrogates a routing table / policy to choose the “ “home RM” ” for the job (the policy configuration is received from the state-store on heartbeat). 路由器询问路由表/策略以选择作业的“ “home RM” ”（有心跳的时候，策略配置从状态存储 (state-store) 接收）。 The router queries the membership state to determine the endpoint of the “home RM” . 路由器查询成员身份状态以确定 “home RM” 的端点。 The router then redirects the application submission request to the “home RM” . 然后，路由器将应用程序提交请求重定向到 “home RM” 。 The router updates the application state with the home sub-cluster identifier. 路由器使用 home sub-cluster 标识更新应用程序状态。 Once the application is submitted to the “home RM” , the stock YARN flow is triggered, i.e. the application is added to the scheduler queue and its AM started in the home sub-cluster, on the first NodeManager that has available resources. 一旦应用程序提交到 “home RM” ，库存的 YARN 流就会被触发，即将应用程序添加到调度器队列中，并在home sub-cluster 中具有可用资源的第一个节点管理器（NM） 来启动它的 AM。 a. During this process, the AM environment is modified by indicating that the address of the AMRMProxy as the YARN RM to talk to. a. 在此过程中，通过指出 AMRMProxy 的地址作为要通信的 YARN RM 来修改 AM 环境（变量）。 b. The security tokens are also modified by the NM when launching the AM, so that the AM can only talk with the AMRMProxy. Any future communication from AM to the YARN RM is mediated by the AMRMProxy. b.安全令牌（security tokens）在启动AM时也会被NM修改，因此 AM 只能与 AMRMproxy 通信。从 AM 到 YARN RM 的任何未来通信都由 Amrmproxy 做媒。 The AM will then request containers using the locality information exposed by HDFS. 然后，AM 将使用 HDFS 公开的位置信息请求容器。 Based on a policy the AMRMProxy can impersonate the AM on other sub-clusters, by submitting an Unmanaged AM, and by forwarding the AM heartbeats to relevant sub-clusters. 根据策略， AMRMProxy 可以通过提交还未被管理的AM（Unmanaged AM）以及将 AM 心跳转发到相关子群集来扮演其他子群集上的 AM。 a. Federation supports multiple application attempts with AMRMProxy HA. AM containers will have different attempt id in home sub-cluster, but the same Unmanaged AM in secondaries will be used across attempts. a. 联邦支持使用 AMRMProxy HA 进行多个应用程序尝试。AM 容器 在 home sub-cluster 中具有不同的尝试ID，但在不同的尝试之间将使用secondaries中相同的 UAM (Unmanaged AM)。 b. When AMRMProxy HA is enabled, UAM token will be stored in Yarn Registry. In the registerApplicationMaster call of each application attempt, AMRMProxy will go fetch existing UAM tokens from registry (if any) and re-attached to the existing UAMs. b. 启用 AMRMProxy HA 后，UAM(Unmanaged AM) 令牌将存储在 YARN 注册表中。在每次应用程序尝试的registerApplicationMaster 调用中， AMRMProxy 将从注册表（如果有）中获取现有的UAM令牌，并重新连接到现有的UAM。 The AMRMProxy will use both locality information and a pluggable policy configured in the state-store to decide whether to forward the resource requests received by the AM to the Home RM or to one (or more) Secondary RMs. In Figure 1, we show the case in which the AMRMProxy decides to forward the request to the secondary RM. AMRMProxy 将同时使用位置信息和状态存储（state-store）中配置的可插拔策略来决定是将AM接收到的资源请求转发到 Home RM 还是一个（或多个）Secondary RMs。在图1中，我们展示了 AMRMProxy 决定将请求转发到 secondary RM 的情况。 The secondary RM will provide the AMRMProxy with valid container tokens to start a new container on some node in its sub-cluster. This mechanism ensures that each sub-cluster uses its own security tokens and avoids the need for a cluster wide shared secret to create tokens. The AMRMProxy forwards the allocation response back to the AM. secondary RM 将向 AMRMProxy 提供有效的容器令牌，以便在其子集群中的某个节点上启动新的容器。此机制确保每个子集群使用自己的安全令牌（security tokens），并避免需要集群范围的共享机密来创建令牌。 AMRMProxy 将分配响应转发回 AM。 The AM starts the container on the target NodeManager (on sub-cluster 2) using the standard YARN protocols.AM使用标准 YARN 协议在目标节点管理器（在子集群2）上启动容器。 ConfigurationTo configure the YARN to use the Federation, set the following property in the conf/yarn-site.xml: 要将 YARN 配置为使用 Federation，请在 conf/yarn-site.xml 中设置以下属性： EVERYWHERE:These are common configurations that should appear in the conf/yarn-site.xml at each machine in the federation. 这些是常见的配置，应该出现在联邦中每台机器的 conf/yarn-site.xml 中。 Property Example Description yarn.federation.enabled true Whether federation is enabled or not是否启用联邦机制 yarn.resourcemanager.cluster-id &lt;unique-subcluster-id&gt; The unique subcluster identifier for this RM (same as the one used for HA).为这个 RM 使用子集群的标识符（HA模式下，互为备份的 RM 也是同样的配置） State-Store:Currently, we support ZooKeeper and SQL based implementations of the state-store.目前，我们支持状态存储的ZooKeeper和基于SQL的实现。 Note: The State-Store implementation must always be overwritten with one of the below.注意：状态存储实现必须始终被下面的某个覆盖。 ZooKeeper: one must set the ZooKeeper settings for Hadoop:动物园管理员：必须为Hadoop设置 ZooKeeper： Property Example Description yarn.federation.state-store.class org.apache.hadoop.yarn.&lt;br /&gt;server.federation.store.&lt;br /&gt;impl.ZookeeperFederationStateStore The type of state-store to use. hadoop.zk.address host:port The address for the ZooKeeper ensemble. SQL: one must setup the following parameters: SQL: 必须设置以下参数： Property Example Description yarn.federation.state-store.class org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore The type of state-store to use. yarn.federation.state-store.sql.url jdbc:mysql://&lt;host&gt;:&lt;port&gt;/FederationStateStore For SQLFederationStateStore the name of the DB where the state is stored. yarn.federation.state-store.sql.jdbc-class com.mysql.jdbc.jdbc2.optional.MysqlDataSource For SQLFederationStateStore the jdbc class to use. yarn.federation.state-store.sql.username &lt;dbuser&gt; For SQLFederationStateStore the username for the DB connection. yarn.federation.state-store.sql.password &lt;dbpass&gt; For SQLFederationStateStore the password for the DB connection. We provide scripts for MySQL and Microsoft SQL Server. 我们为MySQL和Microsoft SQL Server提供脚本。 For MySQL, one must download the latest jar version 5.x from MVN Repository and add it to the CLASSPATH. Then the DB schema is created by executing the following SQL scripts in the database: 对于MySQL，必须从MVN存储库下载最新的JAR5.x版本，并将其添加到类路径。然后，通过在数据库中执行以下SQL脚本来创建 DB schema： sbin/FederationStateStore/MySQL/FederationStateStoreDatabase.sql sbin/FederationStateStore/MySQL/FederationStateStoreUser.sql sbin/FederationStateStore/MySQL/FederationStateStoreTables.sql sbin/FederationStateStore/MySQL/FederationStateStoreStoredProcs.sql In the same directory we provide scripts to drop the Stored Procedures, the Tables, the User and the Database. 在同一个目录中，我们提供了删除存储过程、表、用户和数据库的脚本。 Note: the defines a default user/password for the DB that you are highly encouraged to set this to a proper strong password. 注意：FederationStateStoreUser.sql 定义了数据库的默认用户/密码，强烈建议您将其设置为正确的强密码。 For SQL-Server, the process is similar, but the jdbc driver is already included. SQL-Server scripts are located in sbin/FederationStateStore/SQLServer/. 对于SQL Server，过程类似，但已经包含了JDBC驱动程序。SQL Server脚本位于sbin/federationstatestore/sql server/中。 Optional: Property Example Description yarn.federation.failover.enabled true Whether should retry considering RM failover within each subcluster.考虑到每个子群集中的RM故障转移，是否应重试。 yarn.federation.blacklist-subclusters &lt;subcluster-id&gt; A list of black-listed sub-clusters, useful to disable a sub-cluster黑名单子群集的列表，用于关闭子集群 yarn.federation.policy-manager org.apache.hadoop.yarn.&lt;br /&gt;server.federation.policies.&lt;br /&gt;manager.&lt;br /&gt;WeightedLocalityPolicyManager The choice of policy manager determines how Applications and ResourceRequests are routed through the system.策略管理器的选择决定了应用程序和资源请求在系统中的路由方式。 yarn.federation.policy-manager-params &lt;binary&gt; The payload that configures the policy. In our example a set of weights for router and amrmproxy policies. This is typically generated by serializing a policymanager that has been configured programmatically, or by populating the state-store with the .json serialized form of it.配置策略的有效负载。在我们的示例中，路由器和 AMRMProxy 策略的一组权重。这通常是通过序列化已通过编程方式配置的 PolicyManager 或使用.json序列化形式填充状态存储来生成的。 yarn.federation.subcluster-resolver.class org.apache.hadoop.&lt;br /&gt;yarn.server.federation.&lt;br /&gt;resolver.&lt;br /&gt;DefaultSubClusterResolverImpl The class used to resolve which subcluster a node belongs to, and which subcluster(s) a rack belongs to.用于解析节点所属的子群集和机架所属的子群集的类。 yarn.federation.machine-list node1,subcluster1,rack1\n&lt;br /&gt; node2 , subcluster2, RACK1\n&lt;br /&gt; node3,subcluster3, rack2\n&lt;br /&gt; node4, subcluster3, rack2\n a list of Nodes, Sub-clusters, Rack, used by the DefaultSubClusterResolverImpl默认子集群使用的节点、子群集、机架列表。 ON RMs:These are extra configurations that should appear in the conf/yarn-site.xml at each ResourceManager.这些是额外的配置，应该出现在每个资源管理器的conf/yarn-site.xml中。 Property Example Description yarn.resourcemanager.epoch &lt;unique-epoch&gt; The seed value for the epoch. This is used to guarantee uniqueness of container-IDs generate by different RMs. It must therefore be unique among sub-clusters and well-spaced to allow for failures which increment epoch. Increments of 1000 allow for a large number of sub-clusters and practically ensure near-zero chance of collisions (a clash will only happen if a container is still alive for 1000 restarts of one RM, while the next RM never restarted, and an app requests more containers).epoch的种子值。这是为了保证不同 RMs 生成的容器ID的唯一性。因此，它在子集群中必须是唯一的，并且具有良好的间隔以允许增加epoch出现的失败。增量1000允许大量的子集群，并且实际上可以确保几乎没有发生冲突的机会（只有当容器在1000次重新启动一个 RM 时仍处于活动状态，而下一个 RM 从未重新启动，并且应用程序请求更多容器时，才会发生冲突）。 Optional: Property Example Description yarn.federation.state-store.heartbeat-interval-secs 60 The rate at which RMs report their membership to the federation to the central state-store.RMs 向中央状态存储报告其联盟成员身份的时间间隔。 ON ROUTER:These are extra configurations that should appear in the conf/yarn-site.xml at each Router.这些是额外的配置，应该出现在每个路由器的 conf/yarn-site.xml 中。 Property Example Description yarn.router.bind-host 0.0.0.0 Host IP to bind the router to. The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in yarn.router.*.address respectively. This is most useful for making Router listen to all interfaces by setting to 0.0.0.0.路由器绑定的主机IP。服务器将绑定到的实际地址。如果设置了此可选地址，则 RPC 和 webapp 服务器将分别绑定到此地址和 yarn.router.*.address 中指定的端口。这对于通过设置为0.0.0.0将路由器列表设置为所有接口最有用。 yarn.router.clientrm.&lt;br /&gt;interceptor-class.pipeline org.apache.hadoop.yarn.&lt;br /&gt;server.router.clientrm.&lt;br /&gt;FederationClientInterceptor A comma-seperated list of interceptor classes to be run at the router when interfacing with the client. The last step of this pipeline must be the Federation Client Interceptor.与客户端接口交互时要在路由器上运行的截断类的逗号分隔列表。此管道的最后一步必须是Federation Client拦截器。 Optional: Property Example Description yarn.router.hostname 0.0.0.0 Router host name. yarn.router.clientrm.address 0.0.0.0:8050 路由主机名 yarn.router.webapp.address 0.0.0.0:8089 Webapp address at the router.webapp在路由器的地址 yarn.router.admin.address 0.0.0.0:8052 Admin address at the router.路由器的管理地址 yarn.router.webapp.https.address 0.0.0.0:8091 Secure webapp address at the router.在路由器上安全的webapp地址 yarn.router.submit.retry 3 The number of retries in the router before we give up.在放弃之前，在路由器上的尝试次数。 yarn.federation.statestore.max-connections 10 This is the maximum number of parallel connections each Router makes to the state-store.每个路由器连接到state-store的最大并发数 yarn.federation.cache-ttl.secs 60 The Router caches informations, and this is the time to leave before the cache is invalidated.路由器缓存信息，这是缓存的有效时间。 yarn.router.webapp.&lt;br /&gt;interceptor-class.pipeline org.apache.hadoop.yarn.&lt;br /&gt;server.router.webapp.&lt;br /&gt;FederationInterceptorREST A comma-seperated list of interceptor classes to be run at the router when interfacing with the client via REST interface. The last step of this pipeline must be the Federation Interceptor REST.当通过 REST 接口与客户机交互时，要在路由器上运行的拦截器类的逗号分隔列表。此管道的最后一步必须是 Federation Interceptor REST 。 ON NMs:These are extra configurations that should appear in the conf/yarn-site.xml at each NodeManager.这些额外的配置应该出现在每个节点管理器的 conf/yarn-site.xml 中。 Property Example Description yarn.nodemanager.&lt;br /&gt;amrmproxy.enabled true Whether or not the AMRMProxy is enabled.是否启用 AMRMProxy yarn.nodemanager.amrmproxy.&lt;br /&gt;interceptor-class.pipeline org.apache.hadoop.yarn.&lt;br /&gt;server.nodemanager.amrmproxy.&lt;br /&gt;FederationInterceptor A comma-separated list of interceptors to be run at the amrmproxy. For federation the last step in the pipeline should be the FederationInterceptor.要在 AMRMProxy 上运行的拦截器的逗号分隔列表。对于联邦，管道中的最后一步应该是联合拦截器。 yarn.client.&lt;br /&gt;failover-proxy-provider org.apache.hadoop.yarn.&lt;br /&gt;server.federation.failover&lt;br /&gt;.FederationRMFailoverProxyProvider The class used to connect to the RMs by looking up the membership information in federation state-store. This must be set if federation is enabled, even if RM HA is not enabled.通过在联邦 state-store 中查找成员身份信息来连接到 RMs的类。如果启用了联邦，即使未启用RM HA，也必须设置此选项。 Optional: Property Example Description yarn.nodemanager.amrmproxy.ha.enable true Whether or not the AMRMProxy HA is enabled for multiple application attempt suppport.是否为多个应用程序尝试支持启用 AMRMProxy HA。 yarn.federation.statestore.max-connections 1 The maximum number of parallel connections from each AMRMProxy to the state-store. This value is typically lower than the router one, since we have many AMRMProxy that could burn-through many DB connections quickly.从每个 AMRMProxy 到状态存储的最大并行连接数。这个值通常低于Router在这个属性的值，因为我们有许多 AMRMProxy 可以快速通过许多 DB 连接。 yarn.federation.cache-ttl.secs 300 The time to leave for the AMRMProxy cache. Typically larger than at the router, as the number of AMRMProxy is large, and we want to limit the load to the centralized state-store.离开 Amrmproxy 缓存的时间。通常比 Router在这个属性上设置的值要大，因为AMRMProxy 的数量很大，我们希望将负载限制到集中式状态存储。 Running a Sample JobIn order to submit jobs to a Federation cluster one must create a seperate set of configs for the client from which jobs will be submitted. In these, the conf/yarn-site.xml should have the following additional configurations: 为了将作业提交到联邦集群，必须为将从中提交作业的客户端创建一组单独的配置。在这些配置中，conf/yarn-site.xml 应该具有以下附加配置： Property Example Description yarn.resourcemanager.address &lt;router_host&gt;:8050 Redirects jobs launched at the client to the router’s client RM port.将在客户端启动的作业重定向到路由器的客户端 RM 端口。 yarn.resourcemanger.scheduler.address localhost:8049 Redirects jobs to the federation AMRMProxy port.将作业重定向到联邦的AMRMProxy 端口 Any YARN jobs for the cluster can be submitted from the client configurations described above. In order to launch a job through federation, first start up all the clusters involved in the federation as described here. Next, start up the router on the router machine with the following command: 集群的任何 YARN job 都可以从上面描述的客户机配置提交。为了通过联邦启动一个作业，首先启动联邦中涉及的所有集群，如本文所述。接下来，使用以下命令在路由器计算机上启动路由器： 1$HADOOP_HOME/bin/yarn --daemon start router Now with $HADOOP_CONF_DIR pointing to the client configurations folder that is described above, run your job the usual way. The configurations in the client configurations folder described above will direct the job to the router’s client RM port where the router should be listening after being started. Here is an example run of a Pi job on a federation cluster from the client: 现在，当 $HADOOP_CONF_DIR 指向上面描述的客户端配置文件夹时，以通常的方式运行您的作业。上面描述的客户端配置文件夹中的配置将把作业引导到路由器的客户端 RM 端口，路由器在启动后应该在该端口上进行侦听。下面是从客户端在联邦集群上运行 Pi 作业的示例： 1$HADOOP_HOME/bin/yarn jar hadoop-mapreduce-examples-3.0.0.jar pi 16 1000 This job is submitted to the router which as described above, uses a generated policy from the GPG to pick a home RM for the job to which it is submitted. 此作业将提交给路由器，如上文所述，路由器使用GPG生成的策略为其提交到的作业选择一个 Home RM。 The output from this particular example job should be something like: 这个特定示例作业的输出应该类似于： 1234567891011122017-07-13 16:29:25,055 INFO mapreduce.Job: Job job_1499988226739_0001 running in uber mode : false2017-07-13 16:29:25,056 INFO mapreduce.Job: map 0% reduce 0%2017-07-13 16:29:33,131 INFO mapreduce.Job: map 38% reduce 0%2017-07-13 16:29:39,176 INFO mapreduce.Job: map 75% reduce 0%2017-07-13 16:29:45,217 INFO mapreduce.Job: map 94% reduce 0%2017-07-13 16:29:46,228 INFO mapreduce.Job: map 100% reduce 100%2017-07-13 16:29:46,235 INFO mapreduce.Job: Job job_1499988226739_0001 completed successfully...Job Finished in 30.586 secondsEstimated value of Pi is 3.14250000...... The state of the job can also be tracked on the Router Web UI at routerhost:8089. Note that no change in the code or recompilation of the input jar was required to use federation. Also, the output of this job is the exact same as it would be when run without federation. Also, in order to get the full benefit of federation, use a large enough number of mappers such that more than one cluster is required. That number happens to be 16 in the case of the above example. 作业的状态也可以在路由器 Web UI 上的 routerhost:8089上跟踪。注意，使用联邦不需要更改代码或重新编译输入的jar包。此外，此作业的输出与在没有联合的情况下运行时的输出完全相同。此外，为了充分利用联合，请使用足够多的映射器，以便多个集群的需求。在上面的例子中，这个数字恰好是16。 A list of References 官方关于 YARN Federation issue: https://issues.apache.org/jira/browse/YARN-2915 HDFS基于路由的Federation方案 YARN Federation的架构设计]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Hadoop YARN</tag>
        <tag>Distributed System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transformer论文简记]]></title>
    <url>%2F2019%2F03%2F10%2FTransformer_learning-resources%2F</url>
    <content type="text"><![CDATA[资源Transformer来自论文: All Attention Is You Need 别人的总结资源： 谷歌官方AI博客: Transformer: A Novel Neural Network Architecture for Language Understanding Attention机制详解（二）——Self-Attention与Transformer 谷歌软件工程师 一个是Jay Alammar可视化地介绍Transformer的博客文章 The Illustrated Transformer，非常容易理解整个机制，建议先从这篇看起，这是中文翻译版本； 放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较 中科院软件所 · 自然语言处理 /搜索 10年工作经验的博士（阿里，微博）； Calvo的博客：Dissecting BERT Part 1: The Encoder，尽管说是解析Bert，但是因为Bert的Encoder就是Transformer，所以其实它是在解析Transformer，里面举的例子很好； 再然后可以进阶一下，参考哈佛大学NLP研究组写的“The Annotated Transformer. ”，代码原理双管齐下，讲得也很清楚。 《Attention is All You Need》浅读（简介+代码） 这个总结的角度也很棒。 总结这里总结的思路：自顶向下方法 model architecture一图胜千言，6层编码器和解码器，论文中没有说为什么是6这个特定的数字 Encoder Decoder如果我们想做堆叠了2个Encoder和2个Decoder的Transformer，那么它可视化就会如下图所示： 翻译输出的时候，前一个时间步的输出，要作为下一个时间步的解码器端的输入，下图展示第2~6步： 下面是一个单层：Nx 表示 N1, … , N6 层 partsMulti-head Attention其实就是多个Self-Attention结构的结合，每个head学习到在不同表示空间中的特征，所谓“多头”（Multi-Head），就是做h次同样的事情（参数不共享），然后把结果拼接。 Self-Attention实际上是scaled dot-product attention 缩放的点积注意力： Addresidual connection: skip connection 跳跃了解 Normlayer norm 归一化层 Positional encodinggoogle的这个位置编码很魔幻，是两个周期函数：sine cosine数学系出生的博主的解释：《Attention is All You Need》浅读（简介+代码），相比之下Bert的位置编码直观的多。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[了解Hadoop YARN架构]]></title>
    <url>%2F2019%2F02%2F01%2FAchitecture_of_Apache_Hadoop_YARN%2F</url>
    <content type="text"><![CDATA[前言本文是对 官网：Architecture of Apache Hadoop YARN 的翻译，记录本篇的时候，版本为 2.9.2。本文简单过一下，深入了解可以看看列出的参考资料。 正文The fundamental idea of YARN is to split up the functionalities of resource management and job scheduling/monitoring into separate daemons. The idea is to have a global ResourceManager (RM) and per-application ApplicationMaster (AM). An application is either a single job or a DAG of jobs. YARN 的基本思想是将资源管理和作业调度/监视功能拆分为单独的守护进程。其想法是拥有一个全局资源管理器（ResourceManager: RM）和每个应用程序的应用程序主控器（ApplicationMaster：AM）。应用程序可以是单个作业，也可以是一个DAG作业。 The ResourceManager and the NodeManager form the data-computation framework. The ResourceManager is the ultimate authority that arbitrates resources among all the applications in the system. The NodeManager is the per-machine framework agent who is responsible for containers, monitoring their resource usage (cpu, memory, disk, network) and reporting the same to the ResourceManager/Scheduler. ResourceManager 和 NodeManager 构成了数据计算框架。ResourceManager 是在系统中所有应用程序之间仲裁资源的最高权威（机构）。NodeManager 是每台计算机框架的代理，它负责容器、监视其资源使用情况（CPU、内存、磁盘、网络），并向资源管理器（ResourceManager）/调度程序（Scheduler）报告这些情况。 The per-application ApplicationMaster is, in effect, a framework specific library and is tasked with negotiating resources from the ResourceManager and working with the NodeManager(s) to execute and monitor the tasks. 实际上，每个应用程序的 ApplicationMaster 是一个特定于框架的库，它的任务是与ResourceManager协商资源，并与节点管理器一起执行和监视任务。 The ResourceManager has two main components: Scheduler and ApplicationsManager. ResourceManager有两个主要组件：调度器（Scheduler ）和应用程序管理器（ApplicationsManager）。 The Scheduler is responsible for allocating resources to the various running applications subject to familiar constraints of capacities, queues etc. The Scheduler is pure scheduler in the sense that it performs no monitoring or tracking of status for the application. Also, it offers no guarantees about restarting failed tasks either due to application failure or hardware failures. The Scheduler performs its scheduling function based on the resource requirements of the applications; it does so based on the abstract notion of a resource Container which incorporates elements such as memory, cpu, disk, network etc. 调度器（Scheduler）负责根据熟悉的容量、队列等限制将资源分配给各种正在运行的应用程序。调度器是纯粹的调度程序，在某种意义上，它不执行对应用程序状态的监视或跟踪。此外，它不能保证由于应用程序故障或硬件故障而重新启动失败的任务。调度器根据应用程序的资源需求来执行其调度功能；它是基于资源容器的抽象概念来执行的，该资源容器包含内存、CPU、磁盘、网络等元素。 The Scheduler has a pluggable policy which is responsible for partitioning the cluster resources among the various queues, applications etc. The current schedulers such as the CapacityScheduler and the FairScheduler would be some examples of plug-ins. 调度器有一个可插拔的策略，负责在不同的队列、应用程序等之间划分集群资源。当前的调度器（如CapacityScheduler 和 FairScheduler）是插件的一些示例。 The ApplicationsManager is responsible for accepting job-submissions, negotiating the first container for executing the application specific ApplicationMaster and provides the service for restarting the ApplicationMaster container on failure. The per-application ApplicationMaster has the responsibility of negotiating appropriate resource containers from the Scheduler, tracking their status and monitoring for progress. ApplicationsManager负责接受作业提交、协商用于执行某一个具体应用程序的ApplicationMaster的第一个容器，并提供在失败时重新启动ApplicationMaster容器的服务。每个应用程序ApplicationMaster负责与Scheduler协商合适的资源容器，跟踪其状态并监视进度。 MapReduce in hadoop-2.x maintains API compatibility with previous stable release (hadoop-1.x). This means that all MapReduce jobs should still run unchanged on top of YARN with just a recompile. hadoop-2.x中的 MapReduce 与以前的稳定版本（hadoop-1.x）保持API兼容性。这意味着，所有 MapReduce 作业都应该在 YARN 上保持不变，只需重新编译即可。 YARN supports the notion of resource reservation via the ReservationSystem, a component that allows users to specify a profile of resources over-time and temporal constraints (e.g., deadlines), and reserve resources to ensure the predictable execution of important jobs.The ReservationSystem tracks resources over-time, performs admission control for reservations, and dynamically instruct the underlying scheduler to ensure that the reservation is fullfilled. YARN 支持通过 ReservationSystem 保存资源的概念，ReservationSystem 是一个组件，允许用户指定一个临时和时间限制的资源配置文件（例如截止日期），并保存资源以确保重要作业的可预测执行。ReservationSystem 跟踪随着时间的推移，资源将对预订执行许可控制，并动态指示基础调度器以确保预订得到执行。 In order to scale YARN beyond few thousands nodes, YARN supports the notion of Federation via the YARN Federation feature. Federation allows to transparently wire together multiple yarn (sub-)clusters, and make them appear as a single massive cluster. This can be used to achieve larger scale, and/or to allow multiple independent clusters to be used together for very large jobs, or for tenants who have capacity across all of them. 为了将YARN扩展到数千个节点之外，YARN 通过 YARN Federation 特性支持 Federation 的概念。联邦（Federation）允许透明地将多个YARN（子）集群连接在一起，并使它们看起来像一个单独的大集群。这可以用于实现更大的规模，和/或允许将多个独立的集群一起用于非常大的工作，或用于具有所有能力使用所有集群的租户（tennant: 用户。租户）。 YARN Federation 详细参考的另一篇读记：Hadoop YARN Federation 参考资料 Hadoop In Action, 2nd Edition Hadoop: The Definitive Guide, 4th Edition Apache Hadoop YARN-Moving beyond MapReduce and Batch Processing with Apache Hadoop 2 YARN Architecture 笔记小结 - 葛尧的文章 - 知乎 YARN Architecture 笔记二 - 葛尧的文章 - 知乎 Hadoop技术内幕：深入解析YARN架构设计与实现原理]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Hadoop YARN</tag>
        <tag>Distributed System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle首战Titanic 0.82275-Top3% & 0.83732-Top2%]]></title>
    <url>%2F2019%2F01%2F06%2FTitanic_with_name_sex_age_and_ticket_features-0.82275-0.83732%2F</url>
    <content type="text"><![CDATA[本文用数据分析探索规律，效果好于一堆的随机森林和xgboost，超过参加这个比赛的很多ensemble模型，至少排在前156/10021（Top 2%），最终只选择 name，sex，age，Ticket 4个特征，构建出新的特征，然后进行规则判断，即多个嵌套的if-else，再一次感受到了特征工程的强大。省了数据缺失弥补，其他繁琐的数据预处理，数据清洗，后续的调参和集成模型。需要注意的是：需要自己定制交叉验证函数。 具体方案细节，查看我的jupyter notebook： Titanic_with_name_sex_age_and_ticket_features-0.82275.ipynb Titanic_with_name_sex_age_and_ticket_features-0.83732.ipynb]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[极大化似然估计与贝叶斯估计区别]]></title>
    <url>%2F2019%2F01%2F01%2FMaximum-Likelihood-Estimation_VS_Bayes-Estimation%2F</url>
    <content type="text"><![CDATA[即使学了很久，很多人都没弄清楚极大化似然估计与贝叶斯估计区别，本文将简要概述一下区别，如果要详细搞清楚，建议食用MIT概率论教材《Introduction to probability》的第8, 9两章，本文只是简单总结和《统计学习方法》第4章——朴素贝叶斯方法中的例子。 正文下图摘录自教材《Introduction to probability》的第8章 参考 http://www.cnblogs.com/little-YTMM/p/5399532.html]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>probability</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark编程基础笔记]]></title>
    <url>%2F2018%2F12%2F28%2Ffoundations_of_spark_programming%2F</url>
    <content type="text"><![CDATA[花了4天时间快速过了一遍这个课程，这个课程好处都是有代码实例，毕竟再复杂的工程都是一个个模块堆积起来的。笔记 pdf百度链接链接：提取码：d72o，自己做的书签。 第2章 scala语言基础 第3章 spark的设计与运行原理 第4章 Spark环境搭建和使用方法 第5章 RDD编程 第6章 Spark SQL 第7章 Spark Streaming 第8章 Spark MLlib]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从特征工程到XGBoost参数调优]]></title>
    <url>%2F2018%2F12%2F18%2Fget_started_feature-engineering%2F</url>
    <content type="text"><![CDATA[前言本文陈述脉络：理论结合kaggle上一个具体的比赛。 正文数据科学的一般流程 指南 特征工程 评价指标 XGBoost参数调优 XGBoost并行处理 特征工程结合以下案例分析： Two Sigma Connect: Rental Listing Inquiries 任务：根据公寓的listing 内容，预测纽约市某公寓租赁listing的受欢迎程度标签： interest_level，该listing被咨询的次数 选择这个案例是因为小而精，虽然只有14维特征，但是基本上都涉及各种类型特征。 有三个取值：: ‘high’, ‘medium’, ‘low’，是一个多类分类任务 Listing内容有： 浴室和卧室的数目bathrooms， bedrooms 地理位置（ longitude 、 latitude ） 地址： display_address、 street_address building_id、 listing_id、 manager_id Created：创建日期 Description：更多描述信息 features: 公寓的一些特征描述 photos: a list of photo links 价格：price 数据分析方法对数据进行探索性的分析的工具包：pandas、 matplotlib／seaborn 读取训练数据，取少量样本进行观测，并查看数据规模和数据类型 标签、特征意义、特征类型等 分析每列特征的分布 直方图 包括标签列（对分类问题，可看出类别样本是否均衡） 检测奇异点（outliers） 分析每两列特征之间的相关性 – 特征与特征之间信息是否冗余 – 特征与标签是否线性相关 histogram 直方图 直方图：每个取值在数据集中出现的次数，可视为概率函 数（PDF）的估计（seaborn可视化工具比较简单） 123import seaborn as sns%matplotlib inline（ seaborn 是基于matplotlib 的）sns.distplot(train.price.values, bins=50, kde=True) 核密度估计 Kernel Density Estimation, KDE 对直方图的加窗平滑 在分类任务中，我们关心不同类别的特征分布 violinplot 提供不同类别条件下特征更多的分部信息 核密度估计（KDE） 三个4分位数（quartile）：1/4, 1/2, 3/4 1.5倍四分数间距（nterquartile range, IQR） IQR ：第三四分位数和第一分位数的区别（即Q1~Q3的差距），表示变量的分散情况，播放差更稳健的统计量 12order = ['low', 'medium', 'high']sns.violinplot(x='interest_level', y='price', data=train, order = order) outliers 奇异点奇异点：或称离群点，指远离大多数样本的样本点。通常认为这些点是噪声，对模型有坏影响 可以通过直方图或散点图发现奇异点 直方图的尾巴 散点图上孤立的点 12345plt.figure(figsize=(8,6))plt.scatter(range(train_df.shape[0]), train_df.price.values, color = color[6])plt.xlabel('the number of train data', fontsize=12)plt.ylabel('price', fontsize=12)plt.show() 可以通过只保留某些分位数内的点去掉奇异点 如0.5%-99.5%，或&gt;99% 12ulimit = np.percentile(train.price.values, 99)train['price'].loc[train['price']&gt;ulimit] = ulimit correlation 相关性 相关性可以通过计算相关系数或打印散点图来发现 相关系数：两个随机变量x,y之间的线性相关程度，不线性相关并不代表不相关，可能高阶相关，如 $y=x^2$ $r = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=0}^{n}}(x_i-\bar{x})^2\sum_{i=0}^{n}(y_i-\bar{y})^2}, -1\le r \le 1$ 通常 $|r| &gt; 0.5$ ，认为两者相关性比较强 $r\cases{=0, &amp;\text{完全线性不相关}\\ &gt;0, &amp;\text{正相关}\\ &lt;0, &amp;\text{负相关}}$ 相关性只能是数值型特征之间相关性 我们希望特征与标签强相关，分类直方图可以从某种程度上看出特征与标签的相关性：不同类别的直方图差异大 1234order = ['low', 'medium', 'high']sns.stripplot(train_df.interest_level, train_df.price.values, jitter=True, order=order)plt.title("Price VS Interest Level")plt.show() 特征与特征之间强相关的话意味着信息冗余 可以两个特征可以只保留一个特征 或采用主成分分析（PCA）等降维 123456789101112contFeaturelist = []contFeaturelist.append('bathrooms')contFeaturelist.append('bedrooms')contFeaturelist.append('price')correlationMatrix = train_df[contFeaturelist].corr().abs()plt.subplots()sns.heatmap(correlationMatrix, annot=True)#Mask unimportant featuressns.heatmap(correlationMatrix, mask=correlationMatrix &lt; 1, cbar = False)plt.show() 数据类型XGBoost 模型内部将所有的问题都建模成一个回归预测问题，输入特征只能是数值型。如果给定的数据是不同的类型，必须先将数据变成数值型。 类别型特征（ categorical features） LabelEncoder： 对不连续的数字或者文本进行编号 可用在对字符串型的标签编码（测试结果需进行反变换） 编号默认有序数关系 存储量小 如不希望有序数关系： OneHotEncoder：将类别型整数输入从 1 维 K 维的稀疏编码（K 种类别） 对XGBoost，OneHotEncoder不是必须，因为XGBoost对特征进行排序从而进行分裂建树；如果用OneHotEncoder得到稀疏编码，XGBoost建树过程中对稀疏特征处理速度块 输入必须是数值型数据（对字符串输入，先调用LabelEncoder变成数字，再用OneHotEncoder ） 存储要求高 低基数（low-cardinality ）类别型特征： OneHotEncoder 1维到K维， K为该特征不同的取值数目 通常在K &lt;10的情况下采用 高基数（high-cardinality）类别型特征：通常有成百上千个不同的取值，可先降维，如：邮政编码、街道名称… 聚类（Clustering）： 1 维 到 K维，K为聚类的类别数 主成分分析（principle component analysis, PCA）：但对大矩阵操作费资源 均值编码：在贝叶斯的架构下，利用标签变量，有监督地确定最适合特定特征的编码方式。均值编码详细参考： Mean Encoding: A Preprocessing Scheme for High-Cardinality Categorical Features 平均数编码：针对高基数定性特征（类别特征）的数据预处理/特征工程 日期型特征 日期特征：年月日 时间特征：小时分秒 时间段：早中晚 星期，工作日／周末 123456789train_test['Date'] = pd.to_datetime(train_test['created'])train_test['Year'] = train_test['Date'].dt.yeartrain_test['Month'] = train_test['Date'].dt.monthtrain_test['Day'] = train_test['Date'].dt.daytrain_test['Wday'] = train_test['Date'].dt.dayofweektrain_test['Yday'] = train_test['Date'].dt.dayofyeartrain_test['hour'] = train_test['Date'].dt.hour train_test = train_test.drop(['Date', 'created'], axis=1) 文本型特征 可用词云（wordcloud）可视化 文本词频统计函数，自动统计词的个数，以字典形式内部存储，在显示的时候词频大的词的字体更大 123456789# wordcloud for street addressplt.figure()wordcloud = WordCloud(background_color='white', width=600, height=300, max_font_size=50, max_words=40)wordcloud.generate(text_street)wordcloud.recolor(random_state=0)plt.imshow(wordcloud)plt.title("Wordcloud for street address", fontsize=30)plt.axis("off")plt.show() TF-IDF 通俗易懂原理参考：廖雪峰老师的TF-IDF，概率解释参考：CoolShell 陈皓的 TF-IDF 实战参考官网和使用sklearn提取文本的tfidf特征 下面是个例子123456789101112from sklearn.feature_extraction.text import TfidfVectorizerX_train = ['This is the first document.', 'This is the second document.']X_test = ['This is the third document.']vectorizer = TfidfVectorizer()# 用X_train数据来fitvectorizer.fit(X_train)# 得到tfidf的矩阵tfidf_train = vectorizer.transform(X_train)tfidf_test = vectorizer.transform(X_test)tfidf_train.toarray() 数据预处理from sklearn.preprocessing import … 数据标准化 数据归一化 数据二值化 数据缺失 XGBoost对数据预处理要求少，以上操作都不是必须 特征工程小结 如果知道数据的物理意义（领域专家），可能可以设计更多特征 如Higgs Boson任务中有几维特征是物理学家设计的，还有些有高能物理 研究经验的竞赛者设计了其他一些特征 如房屋租赁任务中，利用常识可设计出一些特征，例子：租金/卧室数目=单价 如果不是领域专家，一些通用的规则： 字符串型特征：Label编码 时间特征：年月日、时间段（早中晚）… 数值型特征：加减乘除，多项式，log, exp 低基数类别特征：one-hot编码 高基数类别特征：先降维，再one-hot编码；均值编码 非结构化特征 文本 语音 图像／视频 fMRI … 利用领域知识设计特征 如曾经流行的图像目标检测特征HOG… 利用深度学习从数据中学习特征表示 采用end-to-end方式一起学习特征和分类／回归／排序 学习好特征可以送入XGBoost学习器 信息泄漏训练数据特征不应该包含标签的信息– 如Rent Listing Inquries任务中图片压缩文件里文件夹的创造时间：加入这个特征后，模型普遍能提高0.01的public LB分数 特征工程案例实践这是我的 jupyter notebook: Rent Listing Inquries 评价指标回归问题的评价指标损失函数可以作为评价指标，以下约定俗成： $\hat{y_i}$ 是预测值，$y$ 是标签值 L1: mean absolute error (MAE) $MAE = \frac{1}{N}\sum_{i=0}^{N}|\hat{y_i}-y_i|$ L2: Root Mean Squared Error(RMSE) $RMSE = \sqrt{\frac{1}{N}\sum_{i=0}^{N}|\hat{y_i}-y_i|}$ Root Mean Sqared Logarithmic Error (RMSLE) $RMLSE = \sqrt{\frac{1}{N}\sum_{i=0}^{N} \big( \log(\hat{y_i}+1) - \log(y_i+1) \big) ^2}$ https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError 当不想在给预测值与真值差距施加很大惩罚时，采用RMSLE 分类任务的评价指标同样地，损失函数可以作为评价指标 logistic/负log似然损失 $\text{logloss}= -{1\over N}\sum_{i=0}^{N}\sum_{j=0}^{M}y_{ij}\log{P_{ij}}$，M是类别数，$y_{ij}$ 为二值函数，当 i 个样本是第 j 类时 $y_{ij}=1$ ，否则取 $0$ ；$p_{ij}$ 为模型预测的第 i 个样本为第 j 类的概率。当 $M=2$ 时， $\text{logloss} = -{1\over N}\sum_{i=0}^{N}\big(y_i\log p_i + (1-y_i)\log(1-p_i)\big)$ ，$y_i$ 为第 i 个样本类别，$p_i$ 为模型预测的第 i 个样本为第 1 类的概率。 0-1损失对应的Mean Consequential Error (MCME) $\text{MCE}=-\frac{1}{N}\sum\limits_{\hat{y_i}\ne y_i}1$ 两类分类任务中更多评价指标 ROC／AUC PR曲线 MAP@n 0-1损失：假设两种错误的代价相等 ​ False Positive （FP） &amp; False Negative（FN） 有些任务中可能某一类错误的代价更大 如蘑菇分类中将毒蘑菇误分为可食用代价更大 因此单独列出每种错误的比例：混淆矩阵 混淆矩阵（confusion matrix） 真正的正值（true positives） 假的正值（false positives） 真正的负值（true negatives） 假的负值（false negatives ） SciKit-Learn实现了多类分类任务的混淆矩阵 sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None) y_true： N个样本的标签真值 y_pred： N个样本的预测标签值 labels：C个类别在矩阵的索引顺序，缺省为y_true或y_pred类别出现的顺序 sample_weight： N个样本的权重 Receiver Operating Characteristic (ROC)上面我们讨论给定阈值 $τ$ 的TPR和FPR 如果不是只考虑一个阈值，而是在一些列阈值上运行检测器，并画出TPR和FPR为阈值 $τ$ 的隐式函数，得到ROC曲线在此处键入公式。 PR曲线Precision and Recall (PR曲线)：用于稀有事件检测，如目标检测、信息检索 负样本非常多，因此 FPR = FP /N_ 很小，比较 TPR 和 FPR 不是很有用的信息 （ROC曲线中只有左边很小一部分有意义） $\rightarrow$ 只讨论正的样本 Precision（精度，查准率，正确 率）：以信息检索为例，对于一个查询，返回了一系列的文档，正确率指的是返回结果中相关文档占的比例 $\text{precision}= TP /\hat{N}_+$ 预测结果真正为正的比例 Recall（召回率，查全率）：返回结果中相关文档占所有相关文档的比例 $\text{Recall}=TP/N_+$ 被正确预测到正样本的比例 Precision and Recall (PR曲线) 阈值变化时的P 和R ，只考虑了返回结果中相关文档的个数，没有考虑文档之间的序。 对一个搜索引擎或推荐系统而言，返回的结果必然是有序的，而且越相关的文档排的越靠前越好，于是有了 AP 的概念。 AP: Average Precision，对不同召回率点上的正确率进行平均。 Average PrecisionPrecision只考虑了返回结果中相关文档的个数，没有考虑文档之间的序。 对一个搜索引擎或推荐系统而言，返回的结果必然是有序的，而且越相关的文档排的越靠前越好，于是有了AP的概念。 AP: Average Precision，对不同召回率点上的正确率进行平均 $AP = \int_{0}^{1}p(k)dr = \sum_{k=0}^{n}p(k)\Delta r(k)$ 即 PR 曲线下的面积（Recall: AUC 为 ROC 下的面积） 其中 k 为返回文档中序位，n 为返回文档的数目，$p(k)$ 为列表中k截至点的precision，$\Delta r(k)$ 表示从 $k-1$ 到 $k$ 召回率的变化 上述离散求和表示等价于 $AP=\sum_{k=0}^{n}p(k)rel(k)/\text{相关文档的数目}$ ，其中 $rel(k)$ 为示性函数，即第 $k$ 个位置为相关文档则取1，否则取0。 计算每个位置上的 precision，如果该位置的文档是不相关的则该位置 precision=0，然后对所有的位置的precision 再做 average 。 MAP: Mean Average Precision $MAP = (\sum_{q=0}^{Q}AP(q)/(Q))$ ，其中 $Q$ 为查询的数目，$n$ 为文档数目。 MAP@K （MAPK） 在现代web信息检索中，recall其实已经没有意义，因为相关文档有成千上万个，很少有人会关心所有文档 Precision@K：在第K个位置上的Precision 对于搜索引擎，考虑到大部分作者只关注前一、两页的结果，所以Precision @10， Precision @20对大规模搜索引擎非常有效 MAP@K：多个查询Precision@K的平均 F1 分数/调和平均 亦被称为F1 score, balanced F-score or F-measure Precision 和 Recall 加权平均： $F1=\frac{2(\text{Precision Recall)}}{(\text{Precision + Recall)}}$ 最好为1，最差为0 多类：每类的F1平均值 Scikit-Learn: Scoring 用交叉验证（cross_val_score和GridSearchCV）评价模型性能时，用scoring参数定义评价指标。 评价指标是越高越好，因此用一些损失函数当评价指标时，需要再加负号，如neg_log_loss，neg_mean_squared_error 详见sklearn文档：http://scikit-learn.org/stable/modules/model_evaluation.html Scoring Function Comment Classification ‘accuracy’ metrics.accuracy_score ‘balanced_accuracy’ metrics.balanced_accuracy_score for binary targets ‘average_precision’ metrics.average_precision_score ‘brier_score_loss’ metrics.brier_score_loss ‘f1’ metrics.f1_score for binary targets ‘f1_micro’ metrics.f1_score micro-averaged ‘f1_macro’ metrics.f1_score macro-averaged ‘f1_weighted’ metrics.f1_score weighted average ‘f1_samples’ metrics.f1_score by multilabel sample ‘neg_log_loss’ metrics.log_loss requires predict_proba support ‘precision’ etc. metrics.precision_score suffixes apply as with ‘f1’ ‘recall’ etc. metrics.recall_score suffixes apply as with ‘f1’ ‘roc_auc’ metrics.roc_auc_score Clustering ‘adjusted_mutual_info_score’ metrics.adjusted_mutual_info_score ‘adjusted_rand_score’ metrics.adjusted_rand_score ‘completeness_score’ metrics.completeness_score ‘fowlkes_mallows_score’ metrics.fowlkes_mallows_score ‘homogeneity_score’ metrics.homogeneity_score ‘mutual_info_score’ metrics.mutual_info_score ‘normalized_mutual_info_score’ metrics.normalized_mutual_info_score ‘v_measure_score’ metrics.v_measure_score Regression ‘explained_variance’ metrics.explained_variance_score ‘neg_mean_absolute_error’ metrics.mean_absolute_error ‘neg_mean_squared_error’ metrics.mean_squared_error ‘neg_mean_squared_log_error’ metrics.mean_squared_log_error ‘neg_median_absolute_error’ metrics.median_absolute_error ‘r2’ metrics.r2_score SciKit-Learn：sklearn.metrics metrics模块还提供为其他目的而实现的预测误差评估函数 分类任务的评估函数如表所示，其他任务评估函数请见：http://scikitlearn.org/stable/modules/classes.html#module-sklearn.metrics Classification metricsSee the Classification metrics section of the user guide for further details. metrics.accuracy_score(y_true, y_pred[, …]) Accuracy classification score. metrics.auc(x, y[, reorder]) Compute Area Under the Curve (AUC) using the trapezoidal rule metrics.average_precision_score(y_true, y_score) Compute average precision (AP) from prediction scores metrics.balanced_accuracy_score(y_true, y_pred) Compute the balanced accuracy metrics.brier_score_loss(y_true, y_prob[, …]) Compute the Brier score. metrics.classification_report(y_true, y_pred) Build a text report showing the main classification metrics metrics.cohen_kappa_score(y1, y2[, labels, …]) Cohen’s kappa: a statistic that measures inter-annotator agreement. metrics.confusion_matrix(y_true, y_pred[, …]) Compute confusion matrix to evaluate the accuracy of a classification metrics.f1_score(y_true, y_pred[, labels, …]) Compute the F1 score, also known as balanced F-score or F-measure metrics.fbeta_score(y_true, y_pred, beta[, …]) Compute the F-beta score metrics.hamming_loss(y_true, y_pred[, …]) Compute the average Hamming loss. metrics.hinge_loss(y_true, pred_decision[, …]) Average hinge loss (non-regularized) metrics.jaccard_similarity_score(y_true, y_pred) Jaccard similarity coefficient score metrics.log_loss(y_true, y_pred[, eps, …]) Log loss, aka logistic loss or cross-entropy loss. metrics.matthews_corrcoef(y_true, y_pred[, …]) Compute the Matthews correlation coefficient (MCC) metrics.precision_recall_curve(y_true, …) Compute precision-recall pairs for different probability thresholds metrics.precision_recall_fscore_support(…) Compute precision, recall, F-measure and support for each class metrics.precision_score(y_true, y_pred[, …]) Compute the precision metrics.recall_score(y_true, y_pred[, …]) Compute the recall metrics.roc_auc_score(y_true, y_score[, …]) Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores. metrics.roc_curve(y_true, y_score[, …]) Compute Receiver operating characteristic (ROC) metrics.zero_one_loss(y_true, y_pred[, …]) Zero-one classification loss. Regression metricsSee the Regression metrics section of the user guide for further details. metrics.explained_variance_score(y_true, y_pred) Explained variance regression score function metrics.mean_absolute_error(y_true, y_pred) Mean absolute error regression loss metrics.mean_squared_error(y_true, y_pred[, …]) Mean squared error regression loss metrics.mean_squared_log_error(y_true, y_pred) Mean squared logarithmic error regression loss metrics.median_absolute_error(y_true, y_pred) Median absolute error regression loss metrics.r2_score(y_true, y_pred[, …]) R^2 (coefficient of determination) regression score function. Multilabel ranking metricsSee the Multilabel ranking metrics section of the user guide for further details. metrics.coverage_error(y_true, y_score[, …]) Coverage error measure metrics.label_ranking_average_precision_score(…) Compute ranking-based average precision metrics.label_ranking_loss(y_true, y_score) Compute Ranking loss measure XGBoost 原理部分参考： XGBoost第一课 XGBoost支持的目标函数objective参数，这个参数在 XGBoost 里面属于任务参数（Learning Task Parameters） [default=reg:linear] reg:linear: linear regression reg:logistic: logistic regression binary:logistic: logistic regression for binary classification, output probability binary:logitraw: logistic regression for binary classification, output score before logistic transformation binary:hinge: hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities. count:poisson –poisson regression for count data, output mean of poisson distribution 计数问题的poisson回归，输出结果为poisson分布。 max_delta_step is set to 0.7 by default in poisson regression (used to safeguard optimization) survival:cox: Cox regression for right censored survival time data (negative values are considered right censored). Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function h(t) = h0(t) * HR). multi:softmax: set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes) 让XGBoost采用softmax目标函数处理多分类问题 multi:softprob: same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata * nclass matrix. The result contains predicted probability of each data point belonging to each class. 和softmax一样，但是输出的是ndata * nclass的向量，可以将该向量 reshape成ndata行nclass列的矩阵。没行数据表示样本所属于每个类别的概率。 rank:pairwise: Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized rank:ndcg: Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized rank:map: Use LambdaMART to perform list-wise ranking where Mean Average Precision (MAP) is maximized reg:gamma: gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed. reg:tweedie: Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed. XGBoost自定义目标函数 在GBDT训练过程，当每步训练得到一棵树，要调用目标函数得到其梯度作为下一棵树拟合的目标 XGBoost在调用obj函数时会传入两个参数：preds和dtrain preds为当前模型完成训练时，所有训练数据的预测值 dtrain为训练集，可以通过dtrain.get_label()获取训练样本的label 同时XGBoost规定目标函数需返回当前preds基于训练label的一阶和二阶梯度 例子 参考官网：https://github.com/dmlc/xgboost/blob/master/demo/guide-python/custom_objective.py 12345678#user define objective function, given prediction, return gradient and second order gradient#this is log likelihood lossdef logregobj(preds, dtrain): #自定义损失函数 labels = dtrain.get_label() preds = 1.0 / (1.0 + np.exp(-preds)) grad = preds - labels #梯度 hess = preds * (1.0-preds) #2阶导数 return grad, hess 调用的时候：123# training with customized objective, we can also do step by step training# simply look at xgboost.py's implementation of trainbst = xgb.train(param, dtrain, num_round, watchlist, obj=logregobj, feval=evalerror) XGBoost支持的评价函数eval_metric参数，这个参数在 XGBoost 里面属于任务参数（Learning Task Parameters） [default according to objective] Evaluation metrics for validation data, a default metric will be assigned according to objective (rmse for regression, and error for classification, mean average precision for ranking) User can add multiple evaluation metrics. Python users: remember to pass the metrics in as list of parameters pairs instead of map, so that latter eval_metric won’t override previous one The choices are listed below: rmse: root mean square error mae: mean absolute error logloss: negative log-likelihood error: Binary classification error rate. It is calculated as #(wrong cases)/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances. error@t: a different than 0.5 binary classification threshold value could be specified by providing a numerical value through ‘t’. merror: Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases). mlogloss: Multiclass logloss. auc: Area under the curve aucpr: Area under the PR curve ndcg: Normalized Discounted Cumulative Gain map: Mean Average Precision ndcg@n, map@n: ‘n’ can be assigned as an integer to cut off the top positions in the lists for evaluation. ndcg-, map-, ndcg@n-, map@n-: In XGBoost, NDCG and MAP will evaluate the score of a list without any positive samples as 1. By adding “-” in the evaluation metric XGBoost will evaluate these score as 0 to be consistent under some conditions. poisson-nloglik: negative log-likelihood for Poisson regression gamma-nloglik: negative log-likelihood for gamma regression cox-nloglik: negative partial log-likelihood for Cox proportional hazards regression gamma-deviance: residual deviance for gamma regression tweedie-nloglik: negative log-likelihood for Tweedie regression (at a specified value of the tweedie_variance_power parameter) XGBoost自定义评价函数例子参考官网：https://github.com/dmlc/xgboost/blob/master/demo/guide-python/custom_objective.py 123456789101112131415# user defined evaluation function, return a pair metric_name, result# NOTE: when you do customized loss function, the default prediction value is margin# this may make builtin evaluation metric not function properly# for example, we are doing logistic loss, the prediction is score before logistic transformation# the builtin evaluation error assumes input is after logistic transformation# Take this in mind when you use the customization, and maybe you need write customized evaluation functiondef evalerror(preds, dtrain): labels = dtrain.get_label() # return a pair metric_name, result. The metric name must not contain a colon (:) or a space # since preds are margin(before logistic transformation, cutoff at 0) return 'my-error', float(sum(labels != (preds &gt; 0.0))) / len(labels)# training with customized objective, we can also do step by step training# simply look at xgboost.py's implementation of trainbst = xgb.train(param, dtrain, num_round, watchlist, obj=logregobj, feval=evalerror) 调用的时候： 123# training with customized objective, we can also do step by step training# simply look at xgboost.py's implementation of trainbst = xgb.train(param, dtrain, num_round, watchlist, obj=logregobj, feval=evalerror) XGBoost参数调优XGBoost参数列表 参数 说明 max_depth 树的最大深度。树越深通常模型越复杂，更容易过拟合。 learning_rate 学习率或收缩因子。学习率和迭代次数／弱分类器数目n_estimators相关。 缺省：0.1 n_estimators 弱分类器数目. 缺省:100 slient 参数值为1时，静默模式开启，不输出任何信息 objective 待优化的目标函数，常用值有： binary:logistic 二分类的逻辑回归，返回预测的概率 multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。 multi:softprob 和 multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。支持用户自定义目标函数 nthread 用来进行多线程控制。 如果你希望使用CPU全部的核，那就不用缺省值-1，算法会自动检测它。 booster 选择每次迭代的模型，有两种选择： gbtree：基于树的模型，为缺省值。 gbliner：线性模型 gamma 节点分裂所需的最小损失函数下降值 min_child_weight 叶子结点需要的最小样本权重（hessian）和 max_delta_step 允许的树的最大权重 subsample 构造每棵树的所用样本比例（样本采样比例），同GBM colsample_bytree 构造每棵树的所用特征比例 colsample_bylevel 树在每层每个分裂的所用特征比例 reg_alpha L1/L0正则的惩罚系数 reg_lambda L2正则的惩罚系数 scale_pos_weight 正负样本的平衡 base_score 每个样本的初始估计，全局偏差 random_state 随机种子 seed 随机种子 missing 当数据缺失时的填补值。缺省为np.nan kwargs XGBoost Booster的Keyword参数 参数类别 通用参数：这部分参数通常我们不需要调整，默认值就好 学习目标参数：与任务有关，定下来后通常也不需要调整 booster参数：弱学习器相关参数，需要仔细调整，会影响模型性能 通用参数 booster：弱学习器类型 可选gbtree（树模型）或gbliner（线性模型） 或 dart （参考我的另一篇博文： XGBoost第一课） 默认为gbtree（树模型为非线性模型，能处理更复杂的任务） silent：是否开启静默模式 1：静默模式开启，不输出任何信息 默认值为0：输出一些中间信息，以助于我们了解模型的状态 nthread：线程数 默认值为-1，表示使用系统所有CPU核 学习目标参数 objective: 损失函数 支持分类／回归／排 eval_metric：评价函数 seed：随机数的种子 默认为0 设置seed可复现随机数据的结果，也可以用于调整参数 booster参数弱学习器的参数，尽管有两种booster可供选择，这里只介绍gbtree learning_rate : 收缩步长 vs. n_estimators：树的数目 较小的学习率通常意味着更多弱分学习器 通常建议学习率较小（ $\eta &lt; 0.1$ ）弱学习器数目n_estimators大 $f_m(x_i)=f_{m-1}(x_i)+\eta\beta_m WeakLearner_m(x_i) $ 可以设置较小的学习率，然后用交叉验证确定n_estimators 行（subsample）列（colsample_bytree、colsample_bylevel）下采样比例 默认值均为1，即不进行下采样，使用所有数据 随机下采样通常比用全部数据的确定性过程效果更好，速度更快 建议值：0.3 - 0.8 树的最大深度： max_depth max_depth越大，模型越复杂，会学到更具体更局部的样本 需要使用交叉验证进行调优，默认值为6，建议3-10 min_child_weight ：孩子节点中最小的样本权重和 如果一个叶子节点的样本权重和小于min_child_weight则分裂过程 结束 Kaggle竞赛优胜者的建议 Tong He（XGBoost R语言版本开发者）： 三个最重要的参数为：树的数目、树的深度和学习率。建议参数调整策略为： 采用默认参数配置试试 如果系统过拟合了，降低学习率 如果系统欠拟合，加大学习率 油管上作者视频：Kaggle Winning Solution Xgboost Algorithm - Learn from Its Author, Tong He Owen Zhang （常使用XGBoost）建议： n_estimators和learning_rate：固定n_estimators为100（数目不大，因为树的深度较大，每棵树比较复杂），然后调整learning_rate 树的深度max_depth：从6开始，然后逐步加大 $\text{min_child_weight}={1\over\sqrt{\text{rare_events}}}$ ，其中 rare_events 为稀有事件的数目 列采样 ${\text{colsample_bytree}\over \text{colsample_bylevel}}$ ：在 $[0.3,0.5]$ 之间进行网格搜索 行采样subsample：固定为1 gamma: 固定为0.0 油管上大神的视频：Learn Kaggle techniques from Kaggle #1, Owen Zhang 参数调优的一般方法 选择较高的学习率(learning rate)，并选择对应于此学习率的理想的树数量 学习率以工具包默认值为0.1。 XGBoost直接引用函数“cv”可以在每一次迭代中使用交叉验证，并返回理想的树数量（因为交叉验证很慢，所以可以import两种XGBoost：直接引用xgboost（用“cv”函数调整树的数目）和XGBClassifier —xgboost的sklearn包（用GridSearchCV调整其他参数 ）。 对于给定的学习率和树数量，进行树参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree, colsample_bylevel) xgboost的正则化参数(lambda, alpha)的调优 降低学习率，确定理想参数 XGBoost参数调优案例分析 竞赛官网：Otto Group Product Classification Challenge 是关于电商商品分类的案例，其中 Target：共9个商品类别 93个特征：整数型特征 详细请看我的jupyter notebook: kaggle Titanic 案例 详细请看我的jupyter notebook: XGBoost并行处理XGBoost工程实现 XGBoost用C++实现，显示地采用OpenMP API做并行处理 建单棵树时并行（Random Forest在建不同树时并行，但Boosting增加树是一个串行操作） XGBoost的scikit-learn接口中的参数 nthread 可指定线程数 -1 表示使用系统所有的核资源 model = XGBClassifier(nthread=-1) 在准备建树数据时高效（近似建树、稀疏、 Cache、数据分块） 交叉验证也支持并行（由scikit-learn 提供支持） scikit-learn 支持的k折交叉验证也支持多线程 cross_val_score() 函数中的参数：n_ jobs = -1 表示使用系统所有的CPU核 results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring= ’neg_log_loss’ , n_jobs=-1, verbose=1) 并行处理的三种配置 交叉验证并行，XGBoost建树不并行 交叉验证不并行，XGBoost建树并行 交叉验证并行，XGBoost建树并行 Otto数据集上的10折交叉验证实验结果： Single Thread XGBoost, Parallel Thread CV: 359.854589 Parallel Thread XGBoost, Single Thread CV: 330.498101 Parallel Thread XGBoost and CV: 313.382301，并行 XGBoost 比并行交叉验证好，两者都并行更好 例子查看 12345678910# evaluate the effect of the number of threads results = [] num_threads = [1, 2, 3, 4] for n in num_threads: start = time.time() model = XGBClassifier(nthread=n) model.fit(X_train, y_train) elapsed = time.time() - start print(n, elapsed) results.append(elapsed) XGBoost总结 XGBoost是一个用于监督学习的非参数模型 目标函数（损失函数、正则项） 参数（树的每个分支分裂特征及阈值） 优化：梯度下降 参数优化 决定模型复杂度的重要参数：learning_rate, n_estimators, max_depth, min_child_weight, gamma, reg_alpha, reg_lamba 随机采样参数也影响模型的推广性： subsample, colsample_bytree, colsample_bylevel 其他未涉及的部分 分布式XGBoost AWS YARN Cluster … GPU加速 并行计算与内存优化的细节 主要关注XGBoost的对外接口 XGBoost资源 XGBoost官方文档：https://xgboost.readthedocs.io/en/latest/ Python API：http://xgboost.readthedocs.io/en/latest/python/python_api.html Github： https://github.com/dmlc/xgboost 很多有用的资源：https://github.com/dmlc/xgboost/blob/master/demo/README.md GPU加速：https://github.com/dmlc/xgboost/blob/master/plugin/updater_gpu/README.md XGBoost原理：XGBoost: A Scalable Tree Boosting System https://arxiv.org/abs/1603.02754 其他资源 XGBoost参数调优： https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuningxgboost-with-codes-python/ 中文版：http://blog.csdn.net/u010657489/article/details/51952785 Owen Zhang, Winning Data Science Competitions https://www.slideshare.net/OwenZhang2/tips-for-data-sciencecompetitions?from_action=save XGBoost User Group： https://groups.google.com/forum/#!forum/xgboost-user/]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning， feature engineering</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost原理和底层实现剖析]]></title>
    <url>%2F2018%2F10%2F02%2Fget-started-XGBoost%2F</url>
    <content type="text"><![CDATA[前言在深度学习火起来之前，集成学习 （ensemble learning 包括 boosting: GBDT, XGBoost）是 kaggle 等比赛中的利器，所以集成学习是机器学习必备的知识点，如果提升树或者GBDT不熟悉，最好先看一下我的另一文： 《统计学习方法》第8章 提升方法之AdaBoost\BoostingTree\GBDT ，陈天奇 的 XGBoost (eXtreme Gradient Boosting) 和 微软的 lightGBM 是 GBDT 算法模型的实现，非常巧妙，是比赛的屠龙之器，算法不仅仅是数学，还涉及系统设计和工程优化。以下引用陈天奇 XGBoost论文 的一段话： Among the 29 challenge winning solutions 3 published at Kaggle’s blog during 2015, 17 solutions used XGBoost. Among these solutions, eight solely used XGBoost to train the model, while most others combined XGBoost with neural nets in ensembles. For comparison, the second most popular method, deep neural nets, was used in 11 solutions. The success of the system was also witnessed in KDDCup 2015, where XGBoost was used by every winning team in the top-10. Moreover, the winning teams reported that ensemble methods outperform a well-configured XGBoost by only a small amount [1]. 正文分成以下几个部分 快速了解：来自陈天奇的ppt XGBoost的设计精髓：来自陈天奇的关于XGBoost的论文 参数详解：结合原理+XGBoost官网API的翻译 正文XGBoost快速了解这部分内容基本上是对陈天奇幻灯片：官网幻灯片 outlook 幻灯片大纲• 监督学习的主要概念的回顾• 回归树和集成模型 (What are we Learning)• 梯度提升 (How do we Learn)• 总结 Review of key concepts of supervised learning 监督学习的关键概念的回顾概念 符号 含义 $R^d$ 特征维度为d的数据集 $x_i∈R^d$ 第i个样本 $w_j$ 第j个特征的权重 $\hat{y}_i$ $x_i$ 的预测值 $y_i$ 第i个训练集的对应的标签 $\Theta$ 特征权重的集合 模型 基本上相关的所有模型都是在下面这个线性式子上发展起来的$$\hat y_i = \sum_{j = 0}^{d} w_j x_{ij}$$上式中 $x_0=1$，就是引入了一个偏差量，或者说加入了一个常数项。由该式子可以得到一些模型： 线性模型，最后的得分就是 $\hat{y}_i$ 。 logistic模型，最后的得分是sigmoid函数 $\frac{1}{1+e^{−\hat{y}_i}}$ 。然后设置阀值，转为正负实例。 其余的大部分也是基于 $\hat{y}_i$ 做了一些运算得到最后的分数 参数 参数就是 $\Theta=\{w_j|j=1,…,d\}$ ，这也正是我们所需要通过训练得出的。 训练时的目标函数 训练时通用的目标函数如下：$$Obj(\Theta)=L(\Theta)+Ω(\Theta)$$在上式中 $L(\Theta)$ 代表的是训练误差，表示该模型对于训练集的匹配程度。$Ω(\Theta)$ 代表的是正则项，表明的是模型的复杂度。 训练误差可以用 $L = \sum_{i = 1}^n l(y_i, \hat y_i)$ 来表示，一般有方差和logistic误差。 方差: $l(y_i,\hat y_i) = (y_i - \hat y_i)^2$ logstic误差: $l(y_i, \hat y_i) = y_i ln(1 + e^{- \hat y_i}) + (1 - y_i)ln(1 + e^{\hat y_i})$ 正则项按照Andrew NG的话来说，就是避免过拟合的。为什么能起到这个作用呢？正是因为它反应的是模型复杂度。模型复杂度，也就是我们的假设的复杂度，按照奥卡姆剃刀的原则，假设越简单越好。所以我们需要这一项来控制。 L2 范数: $Ω(w)=λ||w||_2$ L1 范数(lasso): $Ω(w)=λ||w||_1$ 常见的优化函数有有岭回归，logstic回归和Lasso，具体的式子如下​： 岭回归，这是最常见的一种，由线性模型，方差和L2范数构成。具体式子为 $\sum\limits^n_{i=1}(y_i−w^Tx_i)^2+λ||w||_2$ logstic回归，这也是常见的一种，主要是用于二分类问题，比如爱还是不爱之类的。由线性模型，logistic 误差和L2范数构成。具体式子为 $\sum\limits^n_{i=1} [y_iln(1+e^{−w^Tx_i})+(1−y_i)ln(1+e^{w^Tx_i})]+λ||w||_2$ lasso比较少见，它是由线性模型，方差和L1范数构成的。具体式子为 $\sum\limits_{i = 1}^n (y_i - w^T x_i)^2 + \lambda \vert \vert w \vert \vert _1$ 我们的目标的就是让 $Obj(\Theta)$ 最小。那么由上述分析可见，这时必须让 $L(\Theta$ ) 和 $Ω(\Theta)$ 都比较小。而我们训练模型的时候，要在 bias 和 variance 中间找平衡点。bias 由 $L(\Theta)$ 控制，variance 由 $Ω(\Theta)$ 控制。欠拟合，那么 $L(\Theta)$ 和 $Ω(\Theta)$ 都会比较大，过拟合的话 $Ω(\Theta)$ 会比较大，因为模型的扩展性不强，或者说稳定性不好。 回归树和集成模型 (What are we Learning)Regression Tree (CART) 回归树，也叫做分类与回归树，我认为就是一个叶子节点具有权重的二叉决策树。它具有以下两点特征 决策规则与决策树的一样。 每个叶子节点上都包含了一个权重，也有人叫做分数。 下图就是一个回归树的示例： 回归树的集成模型 回归 小男孩落在第一棵树的最左叶子和第二棵树的最左叶子，所以它的得分就是这两片叶子的权重之和，其余也同理。 树有以下四个优点： 使用范围广，像GBM，随机森林等。(PS: 据陈天奇大神的统计，至少有超过半数的竞赛优胜者的解决方案都是用回归树的变种) 对于输入范围不敏感，所以并不需要对输入归一化 能学习特征之间更高级别的相互关系 很容易对其扩展 模型和参数 假设我们有 $K$ 棵树，那么$$\hat y_i = \sum_{k = 1}^K f_k(x_i),\ \ f_k \in \cal F$$上式中 $\cal F$ 表示的是回归森林中的所有函数空间。$f_k(x_i)$ 表示的就是第 $i$ 个样本在第 $k$ 棵树中落在的叶子的权重。那么现在我们需要求的参数就是每棵树的结构和每片叶子的权重，或者简单的来说就是求 $f_k$ 。那么为了和上一节所说的通用结构统一，可以设$$\Theta = \lbrace f_1,f_2,f_3, \cdots ,f_k \rbrace$$ 在单一变量上学习一棵树 定义一个目标对象，优化它。 例如： 考虑这样一个问题：在输入只有时间（t）的回归树 我想预测在时间是t的时候，我是否喜欢浪漫风格的音乐？ 可见分段函数的分割点就是回归树的非叶子节点，分段函数每一段的高度就是回归树叶子的权重。那么就可以直观地看到欠拟合和过拟合曲线所对应的回归树的结构。根据我们上一节的讨论，$Ω(f)$ 表示模型复杂度，那么在这里就对应着分段函数的琐碎程度。$L(f)$ 表示的就是函数曲线和训练集的匹配程度。 学习阶跃函数 第二幅图：太多的分割点，$\Omega(f)$ 即模型复杂度很高；第三幅图：错误的分割点，$L(f)$ 即损失函数很高。第四幅图：在模型复杂度和损失函数之间取得很好的平衡。 综上所述 模型：假设我们有k棵树，那么模型的表达式 $\hat{y}_i = \sum\limits_{k=1}^{K}f_k(x_i), f_k\in \cal{F}$ 目标函数：$Obj =\underbrace{\sum_{i=1}^{n}l(y_i, \hat{y_i})}_{训练误差} +\underbrace{\sum_{k=1}^{K}\Omega(f_k)}_{树的复杂度}$ 定义树的复杂度几种方式 树的节点数或深度 树叶子节点的L2范式 …（后面会介绍有更多的细节） 目标函数 vs 启发式当你讨论决策树，它通常是启发式的 按信息增益 对树剪枝 最大深度 对叶子节点进行平滑 大多数启发式可以很好地映射到目标函数 信息增益 -&gt; 训练误差 剪枝 -&gt; 按照树节点的数目定义的正则化项 最大深度 -&gt; 限制函数空间 对叶子值进行平滑操作 -&gt; 叶子权重的L2正则化项 回归树不仅仅用于回归 回归树的集成模型定义了你如何创建预测的分数，它能够用于 分类，回归，排序 … … 回归树的功能取决于你怎么定义目标函数 目前为止我们已经学习过 使用方差损失（Square Loss） $l(y_i, \hat{y_i})=(y_i-\hat{y}_i)$ ，这样就产生了普通的梯度提升机（common gradient boosted machine） 使用逻辑损失（Logistic loss）$l(y, \hat{y}_i)=y_i\ln(1+e^{-\hat{y}_i}) + (1-y_i)\ln(1+e^{\hat{y}_i})$ ，这样就产生了逻辑梯度提升（LogitBoost）。 梯度提升Gradient Boosting (How do we Learn) 那怎么学习？ 目标对象：$\sum_{i=1}^{n}l(y_i,\hat{y_i}) + \sum_k\Omega(f_k), f_k \in \cal{F}$ 我们不能用像SGD（随机梯度下降）这样的方法去找到 f，因为他们是树而不是仅仅是数值向量。 解决方案：加法训练 Additive Training（提升方法boosting） 从常量方法开始，每一次（轮）添加一个新的方法 这个算法的思想很简单，一棵树一棵树地往上加，一直到 $K$ 棵树停止。过程可以用下式表达：$$\begin{align}\hat y_i^{(0)} &amp;= 0 \\\hat y_i^{(1)} &amp;= f_1(x_i) = \hat y_i^{(0)} + f_1(x_i) \\\hat y_i^{(2)} &amp;= f_1(x_i) + f_2(x_i) = \hat y_i^{(1)} + f_2(x_i) \\&amp; \cdots \\\hat y_i^{(t)} &amp;= \sum_{k = 1}^t f_k(x_i) = \hat y_i^{(t - 1)} + f_t(x_i)\end{align}$$ 加法训练 我们如何决定什么样的 $f$ 加到模型中？ 优化目标 在 $t$ 轮的预测是：$\hat y_i^{(t)} = \hat y_i^{(t - 1)} + f_t(x_i) $ 加号右边这一项就是我们在 t 轮需要决定的东西 $$ \begin{align} Obj^{(t)} &amp;= \sum_{i = 1}^n l(y_i, \hat y_i^{(t)}) + \sum_{i = 1}^t \Omega (f_i) \\ &amp;= \sum_{i = 1}^n l(y_i, \hat y_i^{(t - 1)} + f_t(x_i)) + \Omega (f_t) + constant \end{align} $$ 考虑平方误差 $$ \begin{align} Obj^{(t)} &amp;= \sum_{i=1}^{n} \left \{y_i-(\hat{y}^{(t-1)}_i)+f_t(x_i)\right \}^2 +\Omega(f_t)+const \\ &amp;= \sum_{i=1}^{n} \left \{2(\hat{y}^{(t-1)}_i-y_i)+f_t(x_i)^2\right \} +\Omega(f_t)+const \\ \end{align} $$ $(\hat{y}^{(t-1)}_i-y_i)$ 称为残差。 损失函数的泰勒展开可由泰勒公式得到下式$$f(x + \Delta x) \approx f(x) +f^{\prime}(x) \Delta x + \frac 1 2 f^{\prime \prime}(x) \Delta x^2$$那么现在可以把 $y^{(t)}_i$看成上式中的 $f(x+Δx)$ ，$y^{(t−1)}_i$ 就是 $f(x)$ ，$f_t(x_i)$ 为 $Δx$ 。然后设 $g_i$ 代表 $f′(x)$ ，也就是 $g_i = {\partial}_{\hat y^{(t - 1)}} \ l(y_i, \hat y^{(t - 1)})$ 用 $h_i$ 代表 $f′′(x)$， 于是 $h_i = {\partial}_{\hat y^{(t - 1)}}^2 \ l(y_i, \hat y^{(t - 1)})$ 于是现在目标函数就为下式:$$\begin{align}Obj^{(t)} &amp;\approx \sum_{i = 1}^n [l(y_i, \hat y_i^{(t - 1)}) + g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t) + constant \\&amp;= \sum_{i = 1}^n [g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t) + [\sum_{i = 1}^n l(y_i, \hat y_i^{(t - 1)}) + constant]\end{align}$$可以用平方误差的例子进行泰勒展开看看结果是否一致，很明显，上式中后面那项 $[\sum_{i = 1}^n l(y_i, \hat y_i^{(t - 1)}) + constant]$ 对于该目标函数我们求最优值点的时候并无影响，所以，现在有了新的优化目标$$Obj^{(t)} \approx \sum_{i = 1}^n [g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t)$$ 这么苦逼图啥？ 改进树的定义 Refine the definition of tree上一节讨论了 $f_t(x)$ 的物理意义，现在我们对其进行数学公式化。设 $w∈R^T$ ， $w$ 为树叶的权重序列，$q:R^d \rightarrow \lbrace 1,2, \cdots ,T \rbrace$ ，$q$ 为树的结构。那么 $q(x)$ 表示的就是样本 $x$ 所落在树叶的位置。可以用下图形象地表示 现在对训练误差部分的定义已经完成。那么对模型的复杂度应该怎么定义呢？ 定义树的复杂度 Define Complexity of a Tree树的深度？最小叶子权重？叶子个数？叶子权重的平滑程度？等等有许多选项都可以描述该模型的复杂度。为了方便，现在用叶子的个数和叶子权重的平滑程度来描述模型的复杂度。可以得到下式：$$\Omega(f_t) = \gamma T + \frac 1 2 \lambda \sum_{j = 1}^T w_j^2$$说明：上式中前一项用叶子的个数乘以一个收缩系数，后一项用L2范数来表示叶子权重的平滑程度。 下图就是计算复杂度的一个示例： 修改目标函数 Revisit the Objectives最后再增加一个定义，用 $I_j$ 来表示第 $j$ 个叶子里的样本集合。也就是上图中，第 $j$ 个圈，就用 $I_j$ 来表示。$$I_j = \lbrace i|q(x_i) = j \rbrace$$好了，最后把优化函数重新按照每个叶子组合,并舍弃常数项：$$\begin{align}Obj^{(t)} &amp;\approx \sum_{i = 1}^n [g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t) \\ &amp;= \sum_{i = 1}^n [ g_i w_{q(x_i)} + \frac 1 2 h_i w_{q(x)}^2] + \gamma T + \frac 1 2 \lambda \sum_{j = 1}^T w_j^2 \\ &amp;= \sum_{j = 1}^T [(\sum_{i \in I_j } g_i)w_j + \frac 1 2 (\sum_{i \in I_j}h_i + \lambda)w_j^2] + \gamma T\end{align}$$ 这是 $T$ 个独立的二次函数的和。 结构分 The Structure Score初中时所学的二次函数的最小值可以推广到矩阵函数里$$\mathop{\min_x}\{Gx+ \frac 1 2 Hx^2\} = - \frac 1 2 \frac {G^2} H, \quad H \gt 0 \\\mathop{\arg\min_x}\{Gx+\frac{1}{2}Hx^2\} = -\frac{G}{H}，H \ge 0$$设 $G_j = \sum_{i \in I_j } g_i,\ H_j = \sum_{i \in I_j}h_i$ ，那么$$\begin{align}Obj^{(t)} &amp;= \sum_{j = 1}^T [(\sum_{i \in I_j } g_i)w_j + \frac 1 2 (\sum_{i \in I_j}h_i + \lambda)w_j^2] + \gamma T \\ &amp;= \sum_{j = 1}^T [G_j w_j + \frac 1 2 (H_j + \lambda)w_j^2] + \gamma T\end{align}$$因此，若假设我们的树的结构已经固定，就是 $q(x)$ 已经固定，那么$$\begin{align}W_j^* &amp;= - \frac {G_j}{H_j + \lambda} \\Obj &amp;= - \frac 1 2 \sum_{j = 1}^T \frac {G_j^2}{H_j + \lambda} + \gamma T\end{align}$$例子 用于单棵树的搜索算法 Searching Algorithm for Single Tree现在只要知道树的结构，就能得到一个该结构下的最好分数。可是树的结构应该怎么确定呢？ 枚举可能的树结构 q 使用分数公式来计算 q 的结构分： $Obj = -\frac{1}{2} \sum\limits_{j=1}^{T}\frac{G_j^2}{H_j+\lambda} + \gamma T$ 找到最好的树结构，然后使用优化的叶子权重： $w^*_j=-\frac{G_j}{H_j+\lambda}$ 但是这可能有无限多个可能的树结构 树的贪婪学习 Greedy Learning of the Tree 从深度为 0 的树开始 对树的每个叶子节点，试着添加一个分裂点。添加这个分裂点后目标函数（即损失函数）的值变化 $$ \begin{align} Obj_{split} &amp;= - \frac{1}{2}[\underbrace{\frac{G_L^2}{H_L+\lambda}}_{左孩子节点分数} + \underbrace {\frac{G^2_R}{H_R+\lambda}}_{右孩子节点分数}] + \gamma T_{split} \\ Obj_{unsplit} &amp;= - \frac{1}{2}\underbrace{\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}}_{分裂前的分数} + \gamma T_{unsplit} \\ Gain &amp;= Obj_{unsplit} - Obj_{split} \\ &amp;= \frac 1 2 [\frac {G_L^2}{H_L + \lambda} + \frac {G_R^2}{H_R + \lambda} - \frac {(G_L + G_R)^2}{H_L + H_R + \lambda}] - \gamma(T_{split} - T_{unsplit}) \end{align} $$ 剩下的问题：我们如何找到最好的分裂点？ 最好分裂点的查找 Efficient Finding of the Best Split 当分裂规则是 $x_j&lt;a$ 时，树的增益是 ? 假设 $x_j$ 是年龄 我们所需要就是上图的两边 $g$ 和 $h$ 的和，然后计算 $$ Gain = \frac{G_L^2}{H_L+\lambda} + \frac{G_L^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda} - \gamma $$ 在一个特征上，从左至右对已经排序的实例进行线性扫描能够决定哪个是最好的分裂点。 分裂点查找算法 An Algorithm for Split Finding 对于每个节点，枚举所有的特征 对于每个特征，根据特征值对实例（样本）进行排序 在这个特征上，使用线性扫描决定哪个是最好的分裂点 在所有特征上采用最好分裂点的方案 深度为 $K$ 的生长树的时间复杂度 $O(K\ d\ n\log n)$ ：每一层需要 $O(n\ \log n)$ 时间去排序，且需要在 $d$ 个特征上排序，我们需要在 $K$ 层进行这些排序。（补充：$O(n)$ 时间计算当前特征的最佳分裂点，即最后实际上 $O(d\ K\ (n\log n +n)$） 这些可以进一步优化（例如：使用近似算法和缓存已经排序的特征） 能够拓展到非常大的数据集 类变量（categorical variables） 有一些树处理分开处理类变量和连续值的变量 xgboost可以简单地使用之前推导的分数公式去计算基于类变量的分裂分数 实际上，没有必要分开处理类变量 我们可以使用独热编码（one-hot encoding）将类变量编码成数值向量。分配一个维度为类数量的向量。 $$ z_j=\cases{1,\quad &amp;\text{if $x$ is in category $j$}\\ 0,\quad &amp;otherwise} $$ 如果有很多类变量，这个数值向量将是稀疏的，xgboost学习算法被设计成偏爱处理稀疏数据。 补充：对某个节点的分割时，是需要按某特征的值排序，那么对于无序的类别变量，就需要进行one-hot化。否则，举个例子：假设某特征有1，2，3三种变量，进行比较时，就会只比较左子树为1, 2或者右子树为2, 3，或者不分割，哪个更好，但是左子树为 1,3 的分割的这种情况就会忘记考虑。因为 $Gain$ 于特征的值范围是无关的，它采用的是已经生成的树的结构与权重来计算的。所以不需要对特征进行归一化处理。 剪枝和正则化 Pruning and Regularization 回忆一下增益公式： $Gain=\underbrace{\frac{G^2_L}{H_L+\lambda} + \frac{G^2_R}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda}}_{训练损失的减少量} - \underbrace{\gamma}_{正则项}$ 当训练损失减少量小于正则项的时候，分裂后的增益就变成负的。 在树的简化度（simplicity）和预测性能（predictiveness）的权衡（trade-off） 提早终止（Pre-stopping） 如果最好的分裂产生的增益计算出来是负的，那么停止分裂。 但是（当前的）一个分裂可能对未来的分裂有益。 后剪枝 （Post-Prunning） 生长一棵树到最大深度，再递归地剪枝所有具有负增益的叶子分裂节点。 回顾提升树算法 Recap: Boosted Tree Algorithm 每一轮添加一棵树 每一轮开始的时候，计算 $g_i=\partial_{\hat{y}_i^{(t-1)}}l(y_i,\hat{y}^{(t-1)}), h_i=\partial_{\hat{y}^{(t-1)}}l(y_i, \hat{y}^{(t-1)})$ 使用统计学知识（统计所有分裂点信息：一节梯度和二阶梯度），用贪婪的方式生长一棵树 $f_t(x)$ ： $$ Obj = -\frac{1}{2}\sum\limits_{j=1}^{T}\frac{G_j^2}{H_j+\lambda} + \gamma T $$ 添加 $f_t(x)$ 到模型 $\hat{y}_i^{(t)}=\hat{y}_i^{(t-1)} + f_t(x_i)$ 通常，我们这么做令 $\hat{y}_i^{(t)}=\hat{y}_i^{(t-1)} + \epsilon f_t(x_i)$ $\epsilon$ 称为步伐大小（step-size）或者缩放（shrinkage），通常设置为大约 0.1 这意味着在每一步我们做完全优化，是为了给未来的轮次保留机会（去进一步优化），这样做有助于防止过拟合。 —————————————————————幻灯片内容结束———————————————————————- XGBoost 系统设计的精髓这部分内容主要来自陈天奇的论文 XGBoost: A Scalable Tree Boosting System 缩放和列抽样 shrinkage and column subsampling随机森林中的用法和目的一样，用来防止过拟合，主要参考论文2.3节 这个xgboost与现代的gbdt一样，都有shrinkage参数 （最原始的gbdt没有这个参数）类似于梯度下降算法中的学习速率，在每一步tree boosting之后增加了一个参数 $\eta$（被加入树的权重），通过这种方式来减小每棵树的影响力，给后面的树提供空间去优化模型。 column subsampling 列（特征）抽样，这个经常用在随机森林，不过据XGBoost的使用者反馈，列抽样防止过拟合的效果比传统的行抽样还好（xgboost也提供行抽样的参数供用户使用），并且有利于后面提到的并行化处理算法。 查找分裂点的近似算法 Approximate Algorithm主要参考论文3.2节 当数据量十分庞大，以致于不能全部放入内存时，精确的贪婪算法就不可能很有效率，通样的问题也出现在分布式的数据集中，为了高效的梯度提升算法，在这两种背景下，近似的算法被提出使用，算法的伪代码如下图所示 概括一下：枚举所有特征，根据特征，比如是第 $k$ 个特征的分布的分位数来决定出 $l$ 个候选切分点 $S_k = \{s_{k1},s_{k2},\cdots s_{kl}\}$ ，然后根据这些候选切分点把相应的样本映射到对应的桶中，对每个桶的 $G,H$ 进行累加。最后在候选切分点集合上贪心查找，和Exact Greedy Algorithm类似。 特征分布的分位数的理解 此图来自知乎weapon大神的《 GBDT算法原理与系统设计简介》 论文给出近似算法的2种变体，主要是根据候选点的来源不同区分： 在建树之前预先将数据进行全局（global）分桶，需要设置更小的分位数间隔，这里用 ϵ 表示，3分位的分位数间隔就是 $1/3$，产生更多的桶，特征分裂查找基于候选点多，计算较慢，但只需在全局执行一次，全局分桶多次使用。 每次分裂重新局部（local）分桶，可以设置较大的 $ϵ$ ，产生更少的桶，每次特征分裂查找基于较少的候选点，计算速度快，但是需要每次节点分裂后重新执行，论文中说该方案更适合树深的场景。 论文给出Higgs案例下，方案1全局分桶设置 $ϵ=0.05$ 与精确算法效果差不多，方案2局部分桶设置 $ϵ=0.3$ 与精确算法仅稍差点，方案1全局分桶设置 $ϵ=0.3$ 则效果极差，如下图： 由此可见，局部选择的近似算法的确比全局选择的近似算法优秀的多，所得出的结果和贪婪算法几乎不相上下。 最后很重的是：使用哪种方案，xgboost用户可以自由选择。 Notably, it is also possible to directly construct approximate histograms of gradient statistics. Our system efficiently supports exact greedy for the single machine setting, as well as approximate algorithm with both local and global proposal methods for all settings. Users can freely choose between the methods according to their needs. 这里直方图算法，常用于GPU的内存优化算法，leetcode上也有人总结出来：LeetCode Largest Rectangle in Histogram O(n) 解法详析， Maximal Rectangle 带权的分位方案 Weighted Quantile Sketch主要参考论文3.3节 在近似的分裂点查找算法中，一个步骤就是提出候选分裂点，通常情况下，一个特征的分位数使候选分裂点均匀地分布在数据集上，就像前文举的关于特征分位数的例子。 考虑 $\cal{D}_k = \lbrace (x_{1k},h_1), (x_{2k},h_2), (x_{3k},h_3), \cdot \cdot \cdot , (x_{nk},h_n)\rbrace$ 代表每个样本的第 $k$ 个特征和其对应的二阶梯度所组成的集合。那么我们现在就能用分位数来定义下面的这个排序函数 $r_k:\Bbb R \rightarrow [0,1]$$$r_k(z) = \frac 1 {\sum_{(x,h) \in \cal{D}_k}h} \sum_{(x,h)\in \cal{D}_k,x \lt z} h$$上式表示的就是该特征的值小于 $z$ 的样本所占总样本的比例。于是我们就能用下面这个不等式来寻找分裂候选点$\lbrace s_{k1},s_{k2},s_{k3}, \cdots, s_{kl} \rbrace$$$|r_k(s_{k,j}) - r_k(s_{k, j+1})| \lt \epsilon,\ s_{k1}=\underset{i}{min}\ x_{ik},s_{kl}=\underset{i}{max}\ x_{ik}$$上式中 $\epsilon$ 的作用：控制让相邻两个候选分裂点相差不超过某个值 $\epsilon$ ，那么 $1/\epsilon$ 的整数值就代表几分位，举例 $\epsilon=1/3$ ，那么就是三分位，即有 $3-1$ 个候选分裂点。数学上，从最小值开始，每次增加 $ϵ∗(\underset{i}\max x_{ik}−\underset{i}\min x_{ik})$ 作为分裂候选点。然后在这些分裂候选点中选择一个最大分数作为最后的分裂点，而且每个数据点的权重是 $h_i$ ，原因如下：$$\begin{align}Obj^{(t)} &amp;\approx \sum_{i = 1}^n [g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t) \\&amp;=\sum_{i=1}^N\frac{1}{2}h_i\left(2\frac{g_i}{h_i}f_t({\bf x_i}) + f_t^2({\bf x_i})\right) + \Omega(f_t) \\&amp;=\sum_{i=1}^N \frac{1}{2}h_i\left(\frac{g_i^2}{h_i^2} +2\frac{g_i}{h_i}f_t({\bf x_i}) + f_t^2({\bf x_i})\right) + \Omega(f_t) - \frac{g_i^2}{2h_i} \\&amp;=\sum_{i=1}^N \frac{1}{2}{\color{green}h_i}\left( f_t({\bf x_i}) – ({\color{green}- \frac{g_i}{h_i}})\right)^2 + \Omega(f_t) - \frac{g_i^2}{2h_i} \\&amp;=\sum_{i=1}^N \frac{1}{2}{\color{green}h_i}\left( f_t({\bf x_i}) – ({\color{green}- \frac{g_i}{h_i}})\right)^2 + \Omega(f_t) - constant\end{align}$$说明：这部分论文原文推导有些错误，国外问答网站 stack exchange 给出很明确的答复， 上式可以视为标签为 $-\frac{g_i}{h_i}$ 且权重为 $h_i$ 的平方误差，此时视 $\frac{g_i^2}{2h_i}$ 常数 （因为是来自上一轮的梯度和二阶梯度）。 现在应该明白 Weighted Quantile Sketch 带权的分位方案的由来，下面举个例子： 即要切分为3个，总和为1.8，因此第1个在0.6处，第2个在1.2处。此图来自知乎weapon大神的《 GBDT算法原理与系统设计简介》 注意稀疏问题的分裂点查找 Sparsity-aware Split Finding主要参考论文3.4节 对于数据缺失数据、one-hot编码等造成的特征稀疏现象，作者在论文中提出可以处理稀疏特征的分裂算法，主要是对稀疏特征值缺失的样本学习出默认节点分裂方向： 默认miss value进右子树，对non-missing value的样本在左子树的统计值 $G_L$ 与 $H_L$，右子树为 $G-G_L$ 与$H−H_L$，其中包含miss的样本，统计这种方案（默认miss value进右子树）的分数。 默认miss value进左子树，对non-missing value的样本在右子树的统计值 $G_R$ 与 $H_R$，左子树为 $G-G_R$ 与$H−H_R$ ，其中包含miss的样本，统计这种方案（默认miss value进左子树）的分数。 选择分数（即增益）比较大的方案。 这样最后求出增益最大的特征值以及 miss value 的分裂方向，作者在论文中提出基于稀疏分裂算法： （修正：下文 “Input: d feature dimension” 这里 “d” 应该改为 “m”） 使用了该方法，相当于比传统方法多遍历了一次，但是它只在非缺失值的样本上进行迭代，因此其复杂度与非缺失值的样本成线性关系。在 Allstate-10k 数据集上，比传统方法快了50倍： 旨在并行学习的列块结构 Column Block for Parallel Learning主要参考论文4.1节 CSR vs CSC 稀疏矩阵的压缩存储形式，比较常见的其中两种：压缩的稀疏行（Compressed Sparse Row）和 压缩的稀疏列（Compressed Sparse Row） CSR结构包含非0数据块values，行偏移offsets，列下标indices。offsets数组大小为（总行数目+1），CSR 是对稠密矩阵的压缩，实际上直接访问稠密矩阵元素 $(i,j)$ 并不高效，毕竟损失部分信息，访问过程如下： 根据行 $i$ 得到偏移区间开始位置 offsets[i]与区间结束位置 offsets[i+1]-1，得到 $i$ 行数据块 values[offsets[i]..(offsets[i+1]-1)]， 与非0的列下表indices[offsets[i]..(offsets[i+1]-1)] 在列下标数据块中二分查找 $j$，找不到则返回0，否则找到下标值 $k$，返回 values[offsets[i]+k] 从访问单个元素来说，相比坐标系的存储结构，那么从 $O(1)$ 时间复杂度升到 $O(\log N)$, N 为该行非稀疏数据项个数。但是如果要遍历访问整行非0数据，则无需访问indices数组，时间复杂度反而更低，因为少了大量的稀疏为0的数据访问。 CSC 与 CSR 变量结构上并无差别，只是变量意义不同 values仍然为矩阵的非0数据块 offsets为列偏移，即特征id对应数组 indices为行下标，对应样本id数组 XBGoost使用CSC 主要用于对特征的全局预排序。预先将 CSR 数据转化为无序的 CSC 数据，遍历每个特征，并对每个特征 $i$ 进行排序：sort(&amp;values[offsets[i]], &amp;values[offsets[i+1]-1])。全局特征排序后，后期节点分裂可以复用全局排序信息，而不需要重新排序。 矩阵的存储形式，参考此文：稀疏矩阵存储格式总结+存储效率对比:COO,CSR,DIA,ELL,HYB 采取这种存储结构的好处 未完待续。。。。。 关注缓存的存取 Cache-aware Access使用Block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的。这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。 因此，对于exact greedy算法中, 使用缓存预取。具体来说，对每个线程分配一个连续的buffer，读取梯度信息并存入Buffer中（这样就实现了非连续到连续的转化），然后再统计梯度信息。该方式在训练样本数大的时候特别有用，见下图： 在近似算法中，对块的大小进行了合理的设置。定义Block的大小为Block中最多的样本数。设置合适的大小是很重要的，设置过大则容易导致命中率低，过小则容易导致并行化效率不高。经过实验，发现 $2^{16}$ 比较好，那么上文提到CSC存储结构的 indices 数组（存储的行下表）的元素占用的字节数就是 16/8 = 2 。 核外块的计算 Blocks for Out-of-core ComputationXGBoost 中提出 Out-of-core Computation优化，解决了在硬盘上读取数据耗时过长，吞吐量不足 多线程对数据分块压缩 Block Compression 存储在硬盘上，再将数据传输到内存，最后再用独立的线程解压缩，核心思想：将磁盘的读取消耗转换为解压缩所消耗的计算资源。 分布式数据库系统的常见设计：Block Sharding 将数据分片到多块硬盘上，每块硬盘分配一个预取线程，将数据fetche到in-memory buffer中。训练线程交替读取多块缓存的同时，计算任务也在运转，提升了硬盘总体的吞吐量。 注：这部分内容属于外存算法External_memory_algorithm XGBoost 对 GBDT 实现的不同之处这部分内容主要参考了知乎上的一个问答 机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？ - 知乎 根据他们的总结和我自己对论文的理解和补充。 传统GBDT以CART作为基分类器，xgboost支持多种基础分类器。比如，线性分类器，这个时候xgboost相当于带 L1 和 L2正则化项 的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 可以通过booster [default=gbtree] 设置参数，详细参照官网 gbtree: tree-based models gblinear: linear models DART: Dropouts meet Multiple Additive Regression Trees dropout 在深度学习里面也经常使用，需要注意的是无论深度学习还是机器学习：使用droput训练出来的模型，预测的时候要使dropout失效。 传统GBDT在优化时只用到一阶导数信息，xgboost则对损失函数函数进行了二阶泰勒展开，同时用到了一阶和二阶导数，这样相对会精确地代表损失函数的值。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导，详细参照官网API。 并行处理，相比GBM有了速度的飞跃 借助 OpenMP ，自动利用单机CPU的多核进行并行计算 支持GPU加速 支持分布式 剪枝 当新增分裂带来负增益时，GBM会停止分裂（贪心策略，非全局的剪枝） XGBoost一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝（事后，进行全局剪枝） xgboost在代价函数里加入了显示的正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和，防止过拟合，这也是xgboost优于传统GBDT的一个特性。正则化的两个部分，都是为了防止过拟合，剪枝是都有的，叶子结点输出L2平滑是新增的。 Built-in Cross-Validation 内置交叉验证 XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run. This is unlike GBM where we have to run a grid-search and only a limited values can be tested. XGBoost允许在每一轮boosting迭代中使用交叉验证，这样可以方便地获得最优boosting迭代次数 GBM使用网格搜索，只能检测有限个值 continue on Existing Model 可以保存模型下次接着训练，方便在线学习 User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications. GBM implementation of sklearn also has this feature so they are even on this point. High Flexibility 可定制损失函数，只要这个损失函数2阶可导 XGBoost allow users to define custom optimization objectives and evaluation criteria. This adds a whole new dimension to the model and there is no limit to what we can do. 提供多语言接口 命令行（Command Line Interface， CLI） C++/Python（可以和scikit-learn结合）/R（可以和caret包结合）/Julia/JAVA和JVM语言（如Scala、 Hadoop平台等） xgboost工具支持并行，执行速度确实比其他Gradient Boosting实现快 模型性能：在结构化数据集上，在分类／回归/排序预测建模上表现突出，相比之下，神经网络尤其擅长非结构化的数据集（比如：图片，语音） 注意xgboost不同于随机森林中的并行粒度是：tree，xgboost与其他提升方法（比如GBDT）一样，也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。 我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 总体来说，这部分内容需要学习很多，特别是涉及到分布式地并发优化和资源调度算法，这就不仅仅是数学模型的问题了，还涉及到系统设计，程序运行性能的优化，本人实在是才疏学浅，这部分内容理解尚浅，进一步学习还需要其他论文和看XGBoost源码，有些优化的地方也不是作者首创，表示从附录的论文中得以学习集成到XGBoost中，真的是集万千之大作，作者不愧是上海交大ACM班出身。大神的访谈：https://cosx.org/2015/06/interview-of-tianqi/ 优化的角度马琳同学的回答 非常棒，真是让我感受到了：横看成岭侧成峰 高可用的xgboost由于xgboost发展平稳成熟，现在已经非常易用，下图来自官网 hello world来自官网，其他复杂的demo，参看github的demo目录 Python 12345678910import xgboost as xgb# read in datadtrain = xgb.DMatrix('demo/data/agaricus.txt.train')dtest = xgb.DMatrix('demo/data/agaricus.txt.test')# specify parameters via mapparam = &#123;'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' &#125;num_round = 2bst = xgb.train(param, dtrain, num_round)# make predictionpreds = bst.predict(dtest) 在jupter notebook中运行结果 树形提升器1234import xgboost as xgb# read in datadtrain = xgb.DMatrix('demo/data/agaricus.txt.train')dtest = xgb.DMatrix('demo/data/agaricus.txt.test') [18:22:42] 6513x127 matrix with 143286 entries loaded from demo/data/agaricus.txt.train [18:22:42] 1611x127 matrix with 35442 entries loaded from demo/data/agaricus.txt.test 1234# specify parameters via mapparam = &#123;'max_depth':3, 'eta':1, 'silent': 0, 'objective':'binary:logistic' &#125;num_round = 2bst = xgb.train(param, dtrain, num_round) [18:22:42] C:\Users\Administrator\Desktop\xgboost\src\tree\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3 [18:22:42] C:\Users\Administrator\Desktop\xgboost\src\tree\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3 1234# make predictionpreds = bst.predict(dtest)print(preds)print(bst.eval(dtest)) [0.10828121 0.85500014 0.10828121 ... 0.95467216 0.04156424 0.95467216] [0] eval-error:0.000000 DART提升器 Dropouts meet Multiple Additive Regression Trees123456789101112param = &#123;'booster': 'dart', 'max_depth': 4, 'eta': 0.001, 'objective': 'binary:logistic', 'silent': 0, 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.5, 'skip_drop': 0.0&#125;#Command Line Parameters: 提升的轮次数num_round = 2bst = xgb.train(param, dtrain, num_round) 1234[18:22:42] C:\Users\Administrator\Desktop\xgboost\src\tree\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=4[18:22:42] C:\Users\Administrator\Desktop\xgboost\src\gbm\gbtree.cc:494: drop 0 trees, weight = 1[18:22:42] C:\Users\Administrator\Desktop\xgboost\src\tree\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=4[18:22:42] C:\Users\Administrator\Desktop\xgboost\src\gbm\gbtree.cc:494: drop 1 trees, weight = 0.999001 1234# make predictionpreds = bst.predict(dtest, ntree_limit=num_round)print(preds)print(bst.eval(dtest)) [0.4990105 0.5009742 0.4990105 ... 0.5009742 0.4990054 0.5009742] [0] eval-error:0.007449 参数详解官网，看懂参数的前提是把前文数学公式和理论看懂，这部分内容主要是对官网的翻译。 运行XGBoost之前，我们必须设置3种类型的参数：通用参数（general parameters），提升器参数（booster paramter），任务参数（task parameter）。 通用参数：与我们所使用的提升器（通常是树型提升器或者线性提升器）的提升算法相关。 提升器参数：取决于你所选择的哪种提升器 学习任务的参数：这些参数决定了学习的方案（learning scenario）。例如：在排名任务场景下，回归任务可能使用不同的参数。 命令行参数：与 XGBoost 的命令行接口（CLI）版本的行为相关。 Note Parameters in R package In R-package, you can use . (dot) to replace underscore(与underline同义) in the parameters, for example, you can use max.depth to indicate max_depth. The underscore parameters are also valid in R. General Parameters Parameters for Tree Booster Additional parameters for Dart Booster (booster=dart) Parameters for Linear Booster (booster=gblinear) Parameters for Tweedie Regression (objective=reg:tweedie) Learning Task Parameters Command Line Parameters 通用参数 general parameters booster [default=gbtree] 设定基础提升器的参数 Which booster to use. Can be gbtree, gblinear or dart; gbtree and dart use tree based models while gblinear uses linear functions. silent [default=0]: 设置成1则没有运行信息的输出，最好是设置为0. nthread [default to maximum number of threads available if not set]：线程数 disable_default_eval_metric [default=0] Flag to disable default metric. Set to &gt;0 to disable. ，使默认的模型评估器失效的标识 num_pbuffer [set automatically by XGBoost, no need to be set by user] Size of prediction buffer, normally set to number of training instances. The buffers are used to save the prediction results of last boosting step. num_feature [set automatically by XGBoost, no need to be set by user] Feature dimension used in boosting, set to maximum dimension of the feature 提升器参数 Booster parameters树提升器参数 Parameters for Tree Booster eta [default=0.3], range $[0, 1]$ shrinkage参数，用于更新叶子节点权重时，乘以该系数，避免步长过大。参数值越大，越可能无法收敛。把学习率 eta 设置的小一些，小学习率可以使得后面的学习更加仔细。 gamma [default=0 alias: min_split_loss] , range $[0, \infty]$ 功能与min_split_loss 一样，（alias是“别名，又名”的意思，联想linux命令：alias就非常容易理解，即给相应的命令起了新的名字，引用同一个程序，功能是一样的），损失函数减少的最小量。 max_depth [default=6], range $[0, \infty]$ 每颗树的最大深度，树高越深，越容易过拟合。 min_child_weight [default=1], range: $[0, \infty]$ 这个参数默认是 1，是每个叶子里面loss函数二阶导（ hessian）的和至少是多少，对正负样本不均衡时的 0-1 分类而言，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 max_delta_step [default=0] , range: $[0, \infty]$ Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update. 这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。这个参数一般用不到，但是你可以挖掘出来它更多的用处。 subsample [default=1], range: $[0, 1]$ 训练实例的抽样率，较低的值使得算法更加保守，防止过拟合，但是太小的值也会造成欠拟合。如果设置0.5，那就意味着随机树的生长之前，随机抽取训练数据的50%做样本。 colsample_bytree [default=1], range: $[0, 1]$ 在构建每棵树的时候，特征（这里说是列，因为样本是按行存储的，那么列就是相应的特征）的采样率，用的特征进行列采样. colsample_bytree 表示的是每次分割节点时，抽取特征的比例。 lambda [default=1, alias: reg_lambda] 作用于权重值的 L2 正则化项参数，参数越大，模型越不容易过拟合。 alpha [default=0, alias: reg_alpha] 作用于权重值的 L1 正则项参数，参数值越大，模型越不容易过拟合。 tree_method string [default=auto] 用来设定树的构建算法，欲知详情请看陈天奇论文中的引用资料： reference paper. The tree construction algorithm used in XGBoost. See description in the reference paper. 分布式和外存版本仅仅支持 tree_method=approx Distributed and external memory version only support tree_method=approx. 选项：auto, exact, approx, hist, gpu_exact, gpu_hist, auto Choices: auto,exact,approx,hist,gpu_exact,gpu_hist,auto auto: Use heuristic to choose the fastest method. 启发式地选择快速算法 ​ - For small to medium dataset, exact greedy (exact) will be used. 中小数据量采用精确的贪婪搜索算法（指代前文说的树的生长过程中，节点分裂算法，所以很好理解） ​ - For very large dataset, approximate algorithm (approx) will be chosen. 非常大的数据集，近似算法将被选用。 ​ - Because old behavior is always use exact greedy in single machine, user will get a message when approximate algorithm is chosen to notify this choice. 因为旧的行为总是使用精确的贪婪算法，所以在近似算法被选用的时候，用户会收到一个通知消息，告诉用户近似算法被选用。 exact: Exact greedy algorithm. 精确地贪婪算法 approx: Approximate greedy algorithm using quantile sketch and gradient histogram. 近似算法采用分位方案和梯度直方图方案。 hist: Fast histogram optimized approximate greedy algorithm. It uses some performance improvements such as bins caching. 优化过的近似贪婪算法的快速算法，这个快速算法采用一些性能改善（的策略），例如桶的缓存（这里桶指的是直方图算法中所用的特征数据划分成不同的桶，欲知详情，查看陈天奇论文以及论文的引用资料） gpu_exact: GPU implementation of exact algorithm. gpu_hist: GPU implementation of hist algorithm. sketch_eps [default=0.03], range: (0, 1) 全称：sketch epsilon 即 分位算法中的 $\epsilon$ 参数 Only used for tree_method=approx. 仅仅用于近似算法 This roughly translates into O(1 / sketch_eps) number of bins. Compared to directly select number of bins, this comes with theoretical guarantee with sketch accuracy. 大致理解为桶数的倒数值。与直接给出桶数相比，这个与带权分位草案（Weighted Quantitle Sketch）能够保证理论上一致 Usually user does not have to tune this. But consider setting to a lower number for more accurate enumeration of split candidates. 通常情况下，不需要用户调试这个参数，但是考虑到设置一个更低的值能够枚举更精确的分割候选点。 scale_pos_weight [default=1] 正标签的权重缩放值 Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances). 控制样本正负标签的平衡，对于标签不平衡的样本有用，一个经典的值是：训练样本中具有负标签的实例数量/训练样本中正标签的实例数量。（举例：-1:2000个 +1:8000个，那么训练过程中每个正标签实例权重只有负标签实例的25%） See Parameters Tuning for more discussion. Also, see Higgs Kaggle competition demo for examples: R, py1, py2, py3. updater [default=grow_colmaker,prune] 逗号分割的字符串定义树的生成器和剪枝，注意这些生成器已经模块化，只要指定名字即可。 A comma separated string defining the sequence of tree updaters to run, providing a modular way to construct and to modify the trees. This is an advanced parameter that is usually set automatically, depending on some other parameters. However, it could be also set explicitly by a user. The following updater plugins exist: grow_colmaker: non-distributed column-based construction of trees. 单机版本下的基于列数据生长树，这里distributed tree 是xgboost有两种策略：单机版non-distributed和distributed分布式版本，比如单机版用的是精确贪婪的方式寻找分割数据点，分布式版本在采用的是近似直方图算法） distcol: distributed tree construction with column-based data splitting mode. 用基于列数据的分割模式来构建一个树（即：生长一棵树），且树是按照分布式版本的算法构建的。 grow_histmaker: distributed tree construction with row-based data splitting based on global proposal of histogram counting. 基于全局数据的直方图统计信息，并按照行分割的方式地进行树的生长。 grow_local_histmaker: based on local histogram counting. 基于局部数据（当前节点，非整棵树）的直方图统计 grow_skmaker: uses the approximate sketching algorithm. 使用近似草案算法。 sync: synchronizes trees in all distributed nodes. 在分布式地所有节点中同步树（的信息） refresh: refreshes tree’s statistics and/or leaf values based on the current data. Note that no random subsampling of data rows is performed. 刷新树的统计信息或者基于当前数据的叶子节点的值，注意：没有进行数据行的随机子抽样。 prune: prunes the splits where loss &lt; min_split_loss (or $\gamma$). 在当前节点小于被定义的最小分割损失时，那么进行剪枝。 In a distributed setting, the implicit updater sequence value would be adjusted to grow_histmaker,prune.在分布式环境下，这个参数值被显示地调整为grow_histmaker,prune refresh_leaf [default=1] This is a parameter of the refresh updater plugin. When this flag is 1, tree leafs as well as tree nodes’ stats are updated. When it is 0, only node stats are updated. 用来标记是否刷新叶子节点信息的标识。当这个标志位为0时，只有节点的统计信息被更新。 process_type [default=default] A type of boosting process to run. Choices:default,update default: The normal boosting process which creates new trees. update: Starts from an existing model and only updates its trees. In each boosting iteration, a tree from the initial model is taken, a specified sequence of updater plugins is run for that tree, and a modified tree is added to the new model. The new model would have either the same or smaller number of trees, depending on the number of boosting iteratons performed. Currently, the following built-in updater plugins could be meaningfully used with this process type: refresh, prune. With process_type=update, one cannot use updater plugins that create new trees. grow_policy [default=depthwise] 树的生长策略，基于深度或者基于最高损失变化 Controls a way new nodes are added to the tree. Currently supported only if tree_method is set to hist. Choices:depthwise, lossguide depthwise: split at nodes closest to the root. 按照离根节点最近的节点进行分裂 lossguide: split at nodes with highest loss change. max_leaves [default=0] 叶子节点的最大数目，只有当参数grow_policy=lossguide`才相关（起作用） Maximum number of nodes to be added. Only relevant when grow_policy=lossguide is set. max_bin, [default=256] 桶的最大数目 Only used if tree_method is set to hist.只有参数 tree_method=hist 时，这个参数才被使用。 Maximum number of discrete bins to bucket continuous features. 用来控制将连续特征离散化为多个直方图的直方图数目。 Increasing this number improves the optimality of splits at the cost of higher computation time. 增加此值提高了拆分的最优性, 但是是以更多的计算时间为代价的。 predictor , [default=cpu_predictor] 设定预测器算法的参数 The type of predictor algorithm to use. Provides the same results but allows the use of GPU or CPU. cpu_predictor: Multicore CPU prediction algorithm. 多核cpu预测器算法 gpu_predictor: Prediction using GPU. Default when tree_method is gpu_exact or gpu_hist. GPU预测器算法，当参数 tree_method = gpu_exact or gpu_hist 时，预测器算法默认采用 gpu_predictor 。 Additional parameters for Dart Booster (booster=dart)此部分可参考：原始论文 和 DART介绍 Note 在测试集上预测的时候，必须通过参数 ntree_limits 要关闭掉dropout功能 Using predict() with DART booster If the booster object is DART type, predict() will perform dropouts, i.e. only some of the trees will be evaluated. This will produce incorrect results if data is not the training data. To obtain correct results on test sets, set ntree_limit to a nonzero value, e.g. 12&gt;preds = bst.predict(dtest, ntree_limit=num_round)&gt; sample_type [default=uniform] 设定抽样算法的类型 Type of sampling algorithm. uniform: dropped trees are selected uniformly. 所有的树被统一处理，指的是权重一样，同样的几率被选为辍学树（被选为辍学的树，即不参与训练的学习过程） weighted: dropped trees are selected in proportion to weight. 选择辍学树的时候是正比于权重。 normalize_type [default=tree] 归一化（又名：标准化）算法的的类型，这个地方是与深度学习中的dropout不太一样。 Type of normalization algorithm. tree: new trees have the same weight of each of dropped trees. 新树拥有跟每一颗辍学树一样的权重 Weight of new trees are 1 / (k + learning_rate). Dropped trees are scaled by a factor of k / (k + learning_rate). forest: new trees have the same weight of sum of dropped trees (forest).新树的权重等于所有辍学树的权重总和 Weight of new trees are 1 / (1 + learning_rate). Dropped trees are scaled by a factor of 1 / (1 + learning_rate). rate_drop [default=0.0], range: [0.0, 1.0] 辍学率，与深度学习中的一样意思 Dropout rate (a fraction of previous trees to drop during the dropout). one_drop [default=0] 设置是否在选择辍学的过程中，至少一棵树被选为辍学树。 When this flag is enabled, at least one tree is always dropped during the dropout (allows Binomial-plus-one or epsilon-dropout from the original DART paper). skip_drop [default=0.0], range: [0.0, 1.0] 在提升迭代的过程中，跳过辍学过程的概率，即不执行dropout功能的概率 Probability of skipping the dropout procedure during a boosting iteration. If a dropout is skipped, new trees are added in the same manner as gbtree. Note that non-zero skip_drop has higher priority than rate_drop or one_drop. 注意到非0值得skip_drop参数比rate_drop和one_drop参数拥有更高的优先级。 学习任务的参数 Learning Task ParametersSpecify the learning task and the corresponding learning objective. The objective options are below: objective[default=reg:linear] 这个参数定义需要被最小化的损失函数 reg:linear: linear regression reg:logistic: logistic regression binary:logistic: logistic regression for binary classification, output probability binary:logitraw: logistic regression for binary classification, output score before logistic transformation binary:hinge: hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities. 2分类的链式损失 gpu:reg:linear, gpu:reg:logistic, gpu:binary:logistic, gpu:binary:logitraw: versions of the corresponding objective functions evaluated on the GPU; note that like the GPU histogram algorithm, they can only be used when the entire training session uses the same dataset count:poisson –poisson regression for count data, output mean of poisson distribution max_delta_step is set to 0.7 by default in poisson regression (used to safeguard optimization) survival:cox: Cox regression for right censored survival time data (negative values are considered right censored). Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function h(t) = h0(t) * HR). 比例风险回归模型(proportional hazards model，简称Cox模型)” 这块不太懂 multi:softmax: set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes) 多分类输出one-hot向量 multi:softprob: same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata * nclass matrix. The result contains predicted probability of each data point belonging to each class. 多分类输出各个类的概率向量 rank:pairwise: Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized rank:ndcg: Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized rank:map: Use LambdaMART to perform list-wise ranking where Mean Average Precision (MAP) is maximized reg:gamma: gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed. reg:tweedie: Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed. base_score [default=0.5] The initial prediction score of all instances, global bias For sufficient number of iterations, changing this value will not have too much effect. eval_metric [default according to objective] 对于有效数据的度量方法 Evaluation metrics for validation data, a default metric will be assigned according to objective (rmse for regression, and error for classification, mean average precision for ranking) User can add multiple evaluation metrics. Python users: remember to pass the metrics in as list of parameters pairs instead of map, so that latter eval_metric won’t override previous one The choices are listed below: rmse: root mean square error 均方根误差 mae: mean absolute error 平均绝对误差 logloss: negative log-likelihood 负对数似然函数值 error: Binary classification error rate. It is calculated as #(wrong cases)/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances. 二分类错误率(阈值为0.5) error@t: a different than 0.5 binary classification threshold value could be specified by providing a numerical value through ‘t’指定2分类误差率的阈值t merror: Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases). 多分类错误率 mlogloss: Multiclass logloss. 多分类的负对数似然函数值 auc: Area under the curve 曲线下面积 aucpr: Area under the PR curve 准确率和召回率曲线下的面积 ndcg: Normalized Discounted Cumulative Gain map: Mean Average Precision 主集合的平均准确率(MAP)是每个主题的平均准确率的平均值 ndcg@n, map@n: ‘n’ can be assigned as an integer to cut off the top positions in the lists for evaluation. ndcg-, map-, ndcg@n-, map@n-: In XGBoost, NDCG and MAP will evaluate the score of a list without any positive samples as 1. By adding “-” in the evaluation metric XGBoost will evaluate these score as 0 to be consistent under some conditions. poisson-nloglik: negative log-likelihood for Poisson regression gamma-nloglik: negative log-likelihood for gamma regression cox-nloglik: negative partial log-likelihood for Cox proportional hazards regression gamma-deviance: residual deviance for gamma regression tweedie-nloglik: negative log-likelihood for Tweedie regression (at a specified value of the tweedie_variance_power parameter) seed [default=0] 随机数的种子 Random number seed. 设置它可以复现随机数据的结果，也可以用于调整参数 命令行参数 Command Line ParametersThe following parameters are only used in the console version of XGBoost num_round The number of rounds for boosting data The path of training data test:data The path of test data to do prediction save_period [default=0] The period to save the model. Setting save_period=10 means that for every 10 rounds XGBoost will save the model. Setting it to 0 means not saving any model during the training. task [default=train] options:train,pred,eval,dump train: training using data pred: making prediction for test:data eval: for evaluating statistics specified by eval[name]=filename dump: for dump the learned model into text format model_in [default=NULL] Path to input model, needed for test, eval, dump tasks. If it is specified in training, XGBoost will continue training from the input model. model_out [default=NULL] Path to output model after training finishes. If not specified, XGBoost will output files with such names as 0003.model where 0003 is number of boosting rounds. model_dir [default=models/] The output directory of the saved models during training fmap Feature map, used for dumping model dump_format [default=text] options:text, json Format of model dump file name_dump [default=dump.txt] Name of model dump file name_pred [default=pred.txt] Name of prediction file, used in pred mode pred_margin [default=0] Predict margin instead of transformed probabilityXGBoost GPU SupportXGBoost Python Package 调参 Complete Guide to Parameter Tuning in XGBoost (with codes in Python) 主要 https://www.cnblogs.com/infaraway/p/7890558.html 引用 陈天奇的论文 XGBoost: A Scalable Tree Boosting System 陈天奇的演讲视频 XGBoost A Scalable Tree Boosting System June 02, 2016 演讲幻灯片 和 官网幻灯片 XGBoost 官网 XGBoost的源代码贡献者之一的 演讲 机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？ - 知乎]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《统计学习方法》第8章 提升方法之AdaBoost\BoostingTree\GBDT]]></title>
    <url>%2F2018%2F10%2F01%2F8.Booting-Methods_LiHang-Statistical-Learning-Methods%2F</url>
    <content type="text"><![CDATA[前言在深度学习火起来之前，提升方法（包括AdaBoost, GBDT, XGBoost）是 kaggle 等比赛中的利器，所以提升方法 （boosting） 是必备的知识点。李航《统计学习方法》第8章——提升方法主要内容：AdaBoost, Boosting Tree, GBDT(这一块原文不够详细，将补充一些)。写本文主要目的是复习（毕竟之前看纸质版做的笔记）， 对于证明比较跳跃和勘误的地方我都做了注解，以便初学者快速阅读理解不会卡住，另外本文拓展部分补充了：集成学习。另外李航这本书著作于2012年，陈天奇 的 XGBoost (eXtreme Gradient Boosting) 在2015年惊艳四方，本文暂时不叙述这个，将会另开一篇：XGBoost入门，包括微软的LightGBM，但是本文是 XGBoost 和 LightGBM 的基础，所以要先总结本文。初学者看到前言这么多名词不必畏惧，直接看正文，本人也是从本章正文学习，再拓展出这些的，初学直接看正文，一步一步往下顺。 正文提升（boosting） 方法是一种常用的统计学习方法， 应用广泛且 有效。 在分类问题中， 它通过改变训练样本的权重， 学习多个分类 器， 并将这些分类器进行线性组合， 提高分类的性能。 本章主要内容 提升方法的思路和代表性的提升算法AdaBoost； 通过训练误差分析探讨AdaBoost为什么能够提高学习精度； 并且从 前向分步加法模型的角度解释AdaBoost； 然后叙述提升方法更具体的 实例——提升树（boosting tree）和GBDT 。 8.1 提升方法AdaBoost算法8.1.1 提升方法的基本思路提升方法的思想对于一个复杂任务来说， 将多个专 家的判断进行适当的综合所得出的判断， 要比其中任何一个专家单独 的判断好。 实际上， 就是“三个臭皮匠顶个诸葛亮”的道理 历史背景历史上， Kearns和Valiant首先提出了“强可学习（strongly learnable） ”和“弱可学习（weakly learnable） ”的概念。 指出： 在概率近似正确（probably approximately correct， PAC） 学习的框架中， 一 个概念（一个类） ， 如果存在一个多项式的学习算法能够学习它， 并 且正确率很高， 那么就称这个概念是强可学习的； 一个概念， 如果存 在一个多项式的学习算法能够学习它， 学习的正确率仅比随机猜测略 好， 那么就称这个概念是弱可学习的。 非常有趣的是Schapire后来证 明强可学习与弱可学习是等价的， 也就是说， 在PAC学习的框架下， 一个概念是强可学习的充分必要条件是这个概念是弱可学习的。 这样一来， 问题便成为， 在学习中， 如果已经发现了“弱学习算 法”， 那么能否将它提升（boost） 为“强学习算法”。 大家知道， 发现 弱学习算法通常要比发现强学习算法容易得多。 那么如何具体实施提 升， 便成为开发提升方法时所要解决的问题。 关于提升方法的研究很 多， 有很多算法被提出。 最具代表性的是AdaBoost算法（AdaBoost algorithm） 。 对于分类问题而言， 给定一个训练样本集， 求比较粗糙的分类规 则（弱分类器） 要比求精确的分类规则（强分类器） 容易得多。提升方法就是从弱学习算法出发， 反复学习， 得到一系列弱分类器（又称 为基本分类器） ， 然后组合这些弱分类器， 构成一个强分类器。 大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分 布） ， 针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。 提升方法的核心问题和思想对提升方法来说， 有两个问题需要回答： 一是在每一轮如 何改变训练数据的权值或概率分布； 二是如何将弱分类器组合成一个强分类器。 关于第1个问题， AdaBoost的做法是提高那些被前一轮弱分类器错误分类样本的权值， 而降低那些被正确分类样本的权值。 这样一来， 那些没有得到正确分类的数据， 由于其权值的加大而受到后一轮的弱分类器的更大关注。 于是， 分类问题被一系列的弱分类器“分而治之”。 至于第2个问题， 即弱分类器的组合， AdaBoost采取加权多数表决的方法。 具体地， 加大分类误差率小的弱分类器的权值， 使其在表决中起较大的作用， 减小分类误差率大的弱分类器的权值， 使其在表决中起较小的作用。 AdaBoost的巧妙之处就在于它将这些想法自然且有效地实现在一 种算法里。 8.1.2 AdaBoost算法 8.1.3 AdaBoost的例子 8.2 AdaBoost算法的训练误差分析 8.3 AdaBoost算法的解释 8.3.1 前向分步算法 8.3.2 前向分步算法与AdaBoost 8.4 提升树 8.4.1 提升树模型 8.4.2 提升树算法 8.4.3 梯度提升Gradient Boosting有前面的例子非常容易理解这部分内容，不再赘述， GBDT如果看懂前文，那么理解GBDT就简单很多。有一篇博客，清晰易懂，总结很不错，推荐看一下： 梯度提升树(GBDT)原理小结 ，里面的这有一个提示：关于多分类问题：每轮都在拟合概率向量 [类别1概率，类别2概率…,类别k的概率] 的伪残差。 以下摘录自：大名鼎鼎的 Stanford 统计教材 《The Elements of Statistical Learning》。根据《李航统计学习方法》的提升方法这一章的参考附录正是此书，对比之下，异曲同工，这里不再赘述，快速总结一下。 仔细体会这句话：它和梯度下降十分相似，不同在于GBDT 就是在函数空间的“梯度下降”，以前学习ML和DL的梯度下降是在参数空间的梯度下降。 在梯度下降中，不断减去 $\frac{\partial{f(x)}}{\partial{\theta}}$，希望求得 $min_{\theta}f(x)$ ; 同理梯度提升中不断减去$\frac{\partial{L(y,f(x))}}{f(x)}$，希望求得 $min_{f(x)}L(y, f(x))$ 。这里引用 https://wepon.me 的介绍： 注意：《统计学习方法》这一章介绍的都是最原始的提升树算法，实际上有进一步改进的调整： 步进参数 $\nu$ ，专业名词为shrinkage，又称收缩，类似梯度下降算法的学习率，这里为什么称为步进参数，因为前文提到：提升方法是加法模型的前向分布算法，提升树也属于提升方法中一种（基学习器是决策树）。 例如：运用在GBDT中 $f_m(x)=f_{m-1}(x)+\nu\cdot \sum\limits_{j=1}^{J}\gamma_{jm}I(x\in R_{jm})$ 正则化手段：subsampling 子抽样（包括：样本子抽样和特征的子抽样），提高泛化能力。 例如：在随机森林中，每棵树的训练样本都是对原始训练集有放回的子抽样，好处如下： 如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，依然没有解决决策树过拟合问题。随机抽样是为了保证不同决策树之间的多样性，从而提高模型的泛化能力。使得随机森林不容易陷入过拟合，并且具有较好的抗噪能力（比如：对缺省值不敏感）。 而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是”求同”。如果是无放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是”有偏的”，从而影响最终的投票结果。为了保证最终结果的可靠性，同时又要保证模型的泛化能力，需要每一颗树既要“求同“ 又要 ”存异”。 深入理解请看GDBT的论文：Greedy Function Approximation：A Gradient Boosting Machine 拓展—集成学习实际上，提升方法（boosting） 属于集成学习（ensemble learning） 的一种，集成模型还有2类比较出名：bagging 方法（Bagging 由来：Bootstrap aggregating）和 Stacking方法，随机森林属于第二类。由于随机森林不像提升树不用等待上一轮（上一棵树）结果，各棵树都可以独立求解（包括独立进行子抽样），即可以在树这个粒度下并发进行运算求解，在大规模数据下并发性能良好，随机森林比较简单。后来 XGBoost 和 lightGBM 的良好地实现了算法模型GBDT，虽然依然不是在树粒度层面的并发，但是运行时间和效果在kaggle、KDD Cup等比赛中惊艳四方，将会在日后另一篇中总结xgboost。 进一步学习的经典资料 wiki关于集成模型的介绍 https://en.wikipedia.org/wiki/Ensemble_learning 周志华老师：《Ensemble Methods Foundations and Algorithms》 用 Stanford 的统计教材 The Elements of Statistical Learning 进一步学习与补充 第9章 Additive Models, Trees, and Related Methods 第10章 Boosting and Additive Trees 第14 章 Random Forests 第15 章 Ensemble learning 知乎探讨：为什么说bagging是减少variance，而boosting是减少bias? - 知乎 https://www.zhihu.com/question/26760839 为什么在实际的kaggle比赛中，GBDT和Random Forest效果非常好以下来自马超博士的回答 - 知乎 这是一个非常好，也非常值得思考的问题。换一个方式来问这个问题：为什么基于 tree-ensemble 的机器学习方法，在实际的 kaggle 比赛中效果非常好？ 通常，解释一个机器学习模型的表现是一件很复杂事情，而这篇文章尽可能用最直观的方式来解释这一问题。 我主要从三个方面来回答楼主这个问题。 理论模型 （站在 vc-dimension 的角度） 实际数据 系统的实现 （主要基于 xgboost） 通常决定一个机器学习模型能不能取得好的效果，以上三个方面的因素缺一不可。 站在理论模型的角度统计机器学习里经典的 vc-dimension 理论告诉我们：一个机器学习模型想要取得好的效果，这个模型需要满足以下两个条件： 模型在我们的训练数据上的表现要不错，也就是 trainning error 要足够小。 模型的 vc-dimension 要低。换句话说，就是模型的自由度不能太大，以防overfit. 当然，这是我用大白话描述出来的，真正的 vc-dimension 理论需要经过复杂的数学推导，推出 vc-bound. vc-dimension 理论其实是从另一个角度刻画了一个我们所熟知的概念，那就是 bias variance trade-off. 好，现在开始让我们想象一个机器学习任务。对于这个任务，一定会有一个 “上帝函数” 可以完美的拟合所有数据（包括训练数据，以及未知的测试数据）。很可惜，这个函数我们肯定是不知道的 （不然就不需要机器学习了）。我们只可能选择一个 “假想函数” 来 逼近 这个 “上帝函数”，我们通常把这个 “假想函数” 叫做 hypothesis. 在这些 hypothesis 里，我们可以选择 svm, 也可以选择 logistic regression. 可以选择单棵决策树，也可以选择 tree-ensemble (gbdt, random forest). 现在的问题就是，为什么 tree-ensemble 在实际中的效果很好呢？ 区别就在于 “模型的可控性”。 先说结论，tree-ensemble 这样的模型的可控性是好的，而像 LR 这样的模型的可控性是不够好的（或者说，可控性是没有 tree-ensemble 好的）。为什么会这样？别急，听我慢慢道来。 我们之前说，当我们选择一个 hypothsis 后，就需要在训练数据上进行训练，从而逼近我们的 “上帝函数”。我们都知道，对于 LR 这样的模型。如果 underfit，我们可以通过加 feature，或者通过高次的特征转换来使得我们的模型在训练数据上取得足够高的正确率。而对于 tree-enseble 来说，我们解决这一问题的方法是通过训练更多的 “弱弱” 的 tree. 所以，这两类模型都可以把 training error 做的足够低，也就是说模型的表达能力都是足够的。但是这样就完事了吗？没有，我们还需要让我们的模型的 vc-dimension 低一些。而这里，重点来了。在 tree-ensemble 模型中，通过加 tree 的方式，对于模型的 vc-dimension 的改变是比较小的。而在 LR 中，初始的维数设定，或者说特征的高次转换对于 vc-dimension 的影响都是更大的。换句话说，tree-ensemble 总是用一些 “弱弱” 的树联合起来去逼近 “上帝函数”，一次一小步，总能拟合的比较好。而对于 LR 这样的模型，我们很难去猜到这个“上帝函数”到底长什么样子（到底是2次函数还是3次函数？上帝函数如果是介于2次和3次之间怎么办呢？）。所以，一不小心我们设定的多项式维数高了，模型就 “刹不住车了”。俗话说的好，步子大了，总会扯着蛋。这也就是我们之前说的，tree-ensemble 模型的可控性更好，也即更不容易 overfit. 站在数据的角度除了理论模型之外, 实际的数据也对我们的算法最终能取得好的效果息息相关。kaggle 比赛选择的都是真实世界中的问题。所以数据多多少少都是有噪音的。而基于树的算法通常抗噪能力更强。比如在树模型中，我们很容易对缺失值进行处理。除此之外，基于树的模型对于 categorical feature 也更加友好。 除了数据噪音之外，feature 的多样性也是 tree-ensemble 模型能够取得更好效果的原因之一。通常在一个kaggle任务中，我们可能有年龄特征，收入特征，性别特征等等从不同 channel 获得的特征。而特征的多样性也正是为什么工业界很少去使用 svm 的一个重要原因之一，因为 svm 本质上是属于一个几何模型，这个模型需要去定义 instance 之间的 kernel 或者 similarity （对于linear svm 来说，这个similarity 就是内积）。这其实和我们在之前说过的问题是相似的，我们无法预先设定一个很好的similarity。这样的数学模型使得 svm 更适合去处理 “同性质”的特征，例如图像特征提取中的 lbp 。而从不同 channel 中来的 feature 则更适合 tree-based model, 这些模型对数据的 distributation 通常并不敏感。 站在系统实现的角度除了有合适的模型和数据，一个良好的机器学习系统实现往往也是算法最终能否取得好的效果的关键。一个好的机器学习系统实现应该具备以下特征： 正确高效的实现某种模型。我真的见过有些机器学习的库实现某种算法是错误的。而高效的实现意味着可以快速验证不同的模型和参数。 系统具有灵活、深度的定制功能。 系统简单易用。 系统具有可扩展性, 可以从容处理更大的数据。 到目前为止，xgboost 是我发现的唯一一个能够很好的满足上述所有要求的 machine learning package. 在此感谢青年才俊 陈天奇。 在效率方面，xgboost 高效的 c++ 实现能够通常能够比其它机器学习库更快的完成训练任务。在灵活性方面，xgboost 可以深度定制每一个子分类器，并且可以灵活的选择 loss function（logistic，linear，softmax 等等）。除此之外，xgboost还提供了一系列在机器学习比赛中十分有用的功能，例如 early-stop， cv 等等在易用性方面，xgboost 提供了各种语言的封装，使得不同语言的用户都可以使用这个优秀的系统。最后，在可扩展性方面，xgboost 提供了分布式训练（底层采用 rabit 接口），并且其分布式版本可以跑在各种平台之上，例如 mpi, yarn, spark 等等。 有了这么多优秀的特性，自然这个系统会吸引更多的人去使用它来参加 kaggle 比赛。 综上所述，理论模型，实际的数据，良好的系统实现，都是使得 tree-ensemble 在实际的 kaggle 比赛中“屡战屡胜”的原因。 用一句话与大家共勉：算法学习要学习算法之间的联系与区别，优缺点和适用场合，这样能做到融会贯通。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《统计学习方法》第4章 NaiveBayes]]></title>
    <url>%2F2018%2F10%2F01%2F4.Naive-Bayes_LiHang-Statistical-Learning-Methods%2F</url>
    <content type="text"><![CDATA[前言写本文章主要目的是复习（毕竟之前看纸质版做的笔记）， 对于证明比较跳跃和勘误的地方我都做了注解，以便初学者和以后复习地时候快速阅读理解不会卡住。 朴素贝叶斯法 4.1 朴素贝叶斯法的学习与分类4.1.1 基本方法 4.1.2 后验概率最大化的含义 4.2 朴素贝叶斯法的参数估计4.2.1 极大似然估计 4.2.2 学习与分类算法 例子 4.2.3 贝叶斯估计 本章概要 习题]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《统计学习方法总结》]]></title>
    <url>%2F2018%2F10%2F01%2Fsummary_of_LiHang_Statistical-learning-methods%2F</url>
    <content type="text"><![CDATA[这是个人学习完李航 《统计学习方法》 的总结笔记，之前是在纸质版书籍上做的笔记，接下来会陆续更新每一章总结，由于时间有限，将以PDF注解的方式梳理10个主要统计学习方法。那么注意勘误(errata)，首先将引入作者自己的总结，而后，我再对每个具体算法和关联性进行总结，逐渐形成系统性的理解，算法之间的分类，联系，优缺点对比，以及适用场景才是我们学习的重点。 总结 1 适用问题分类问题是从实例的特征向量到类标记的预测问题；标注问题是从观测序列到标记序列(或状态序列)的预测问题。可以认为分类问题是标注问题的特殊情况。 分类问题中可能的预测结果是二类或多类；而标注问题中可能的预测结果是所有的标记序列，其数目是指数级的。 感知机、k近邻法、朴素贝叶斯法、决策树是简单的分类方法，具有模型直观、方法简单、实现容易等特点； 逻辑斯谛回归与最大熵模型、支持向量机、提升方法是更复杂但更有效的分类方法，往往分类准确率更高； 隐马尔可夫模型、条件随机场是主要的标注方法。通常条件随机场的标注准确率更高。 2 模型分类问题与标注问题的预测模型都可以认为是表示从输入空间到输出空间的映射.它们可以写成条件概率分布 $P(Y|X)$ 或决策函数 $Y=f(X)$ 的形式。前者表示给定输入条件下输出的概率模型，后者表示输入到输出的非概率模型。有的模型只是其中一种，有的模型可以看成2者兼有。 朴素贝叶斯法、隐马尔可夫模型是概率模型；感知机、k近邻法、支持向量机、提升方法是非概率模型；而决策树、逻辑斯谛回归与最大熵模型、条件随机场既可以看作是概率模型，又可以看作是非概率模型。 直接学习条件概率分布 $P(Y | X)$ 或决策函数 $Y=f(X)$ 的方法为判别方法，对应的模型是判别模型：感知机、k近邻法、决策树、逻辑斯谛回归与最大熵模型、支持向量机、提升方法、条件随机场是判别方法。 首先学习联合概率分布 $P(X,Y)$，从而求得条件概率分布 $P(Y|X)$ 的方法是生成方法，对应的模型是生成模型：朴素贝叶斯法、隐马尔可夫模型是生成方法。 决策树是定义在一般的特征空间上的，可以含有连续变量或离散变量。感知机、支持向量机、k近邻法的特征空间是欧氏空间(更一般地，是希尔伯特空间)。提升方法的模型是弱分类器的线性组合，弱分类器的特征空间就是提升方法模型的特征空间。 感知机模型是线性模型；而逻辑斯谛回归与最大熵模型、条件随机场是对数线性模型；k近邻法、决策树、支持向量机(包含核函数)、提升方法使用的是非线性模型。 3 学习策略在二类分类的监督学习中，支持向量机、逻辑斯谛回归与最大熵模型、提升方法各自使用合页损失函数、逻辑斯谛损失函数、指数损失函数，分别写为 这3种损失函数都是0-1损失函数的上界，具有相似的形状。 从上图可以认为支持向量机、逻辑斯谛回归与最大熵模型、提升方法使用不同的代理损失函数(surrogateloas Punotion)表示分类的损失，定义经验风险或结构风险函数，实现二类分类学习任务。学习的策略是优化以下结构风险函数 第1项为经验风险(经验损失)，第2项为正则化项，L为损失函数，J(f)为模型的复杂度。 支持向量机用L2范数表示模型的复杂度。原始的逻辑斯谛回归与最大熵模型没有正则化项，可以给它们加上L2范数正则化项。提升方法没有显式的正则化项，通常通过早停止(early stopping)的方法达到正则化的效果。 概率模型的学习可以形式化为极大似然估计或贝叶斯估计的极大后验概率估计。学习的策略是极小化对数似然损失或极小化正则化的对数似然损失。极大后验概率估计时，正则化项是先验概率的负对数。 决策树学习的策略是正则化的极大似然估计，损失函数是对数似然损失，正则化项是决策树的复杂度。 逻辑斯谛回归与最大熵模型、条件随机场的学习策略既可以看成是极大似然估计(或正则化的极大似然估计)，又可以看成是极小化逻辑斯谛损失(或正则化的逻辑斯谛损失)。 朴素贝叶斯模型、隐马尔可夫模型的非监督学习也是极大似然估计或极大后验概率估计，但这时模型含有隐变量。 4 学习算法 朴素贝叶斯法与隐马尔可夫模型的监督学习，最优解即极大似然估计值，可以由概率计算公式直接计算。 感知机、逻辑斯谛回归与最大熵模型、条件随机场的学习利用梯度下降法、拟牛顿法等一般的无约束最优化问题的解法。 支持向量机学习，可以解凸二次规划的对偶问题。有序列最小最优化算法等方法。 决策树学习是基于启发式算法的典型例子。可以认为特征选择、生成、剪枝是启发式地进行正则化的极大似然估计。 提升方法利用学习的模型是加法模型、损失函数是指数损失函数的特点，启发式地从前向后逐步学习模型，以达到逼近优化目标函数的目的。 EM算法是一种迭代的求解含隐变量概率模型参数的方法，它的收敛性可以保证，但是不能保证收敛到全局最优。 支持向量机学习、逻辑斯谛回归与最大熵模型学习、条件随机场学习是凸优化问题，全局最优解保证存在。而其他学习问题则不是凸优化问题。 分章节总结数学基础不再重复，请参考我的数学笔记线性代数总结 和 MIT的概率论教材 《introduction to probability》 和 凸优化 第8章 提升方法之AdaBoost\BoostingTree\GBDT MaxEnt HMM CRF 第9章EM/GMM/F-MM/GEM SVM 第4章 朴素贝叶斯]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《统计学习方法》第9章 EM/GMM/F-MM/GEM]]></title>
    <url>%2F2018%2F10%2F01%2F9.EM_and_GEM_LiHang-Statistical-Learning-Methods%2F</url>
    <content type="text"><![CDATA[前言EM（期望最大）算法有很多的应用，最广泛的就是混合高斯模型、聚类、HMM等等，本质上就是一种优化算法，不断迭代，获得优值，与梯度下降、牛顿法、共轭梯度法都起到同一类的作用。 本文是对李航《统计学习方法》的第9章复习总结，主要内容如下 EM（期望最大）算法证明有跳跃性的地方全部事无巨细地写出来， 在 三硬币例子解析 这一节将会把这个例子跟公式一一对应起来 GMM（高斯混合模型）迭代公式证明 F函数的极大-极大算法（Maximization-Maximization-algorithm）和GEM 详细证明 当然大家也可以参考 Stanford 吴恩达主讲的 CS299 Machine Learning 的 EM课件 ，相比之下《统计学习方法》这本书在 Jensen‘s inequality（琴声不等式）讲的不够详细，其他都差不多，只是Q函数定义不同，这两种定义都很流行所以后文也会介绍区别。 正文9.1 EM算法的引入概率模型有时既含有观测变量（observable variable） ， 又含有隐变量（hidden variable）或潜在变量（latent variable） 。 如果概率模型的变量都是观测变量， 那么给定数据， 可以直接用极大似然估计法或贝叶斯估计法估计模型参数。 但是， 当模型含有隐变量时， 就不能简单地使用这些估计方法。 EM算法就是含有隐变量的概率模型参数的极大似然估计法， 或极大后验概率估计法。 我们仅讨论极大似然估计， 极大后验概率估计与其类似。 9.1.1 EM算法 这里， 随机变量 $y$ 是观测变量， 表示一次试验观测的结果是1或0； 随机变量 $z$ 是隐变量， 表示未观测到的掷硬币 $A$ 的结果； $\theta＝( \pi ,p， q)$ 是模型参数。 这一模型是以上数据的生成模型。 注意， 随机变量 $y$ 的数据可以观测， 随机变量 $z$ 的数据不可观测。$$\begin{align}P(y|\theta) &amp;= \sum\limits_{z}P(y,z|\theta)=\sum\limits_{z}\frac{P(z,\theta)}{P(\theta)}\cdot\frac{P(y,z,\theta)}{P(z, \theta)}=\sum\limits_{z}P(z|\theta)P(y|z,\theta) \\&amp;= P(z=1|\theta)P(y|z=1, \theta) + P(z=0|\theta)P(y|z=0, \theta)\\&amp;= \pi p^y(1-p)^{(1-y)} + (1 - \pi) q^y(1-q)^{(1-y)} \tag{9.1}\\&amp;= \begin{cases} \pi p + (1 - \pi) q, &amp;y=1\\ \pi (1-p) + (1-\pi)(1-q), &amp;y=0\end{cases}\end{align}$$将观测数据表示为 $Y＝(Y_1， Y_2,…,Y_n)^T$， 未观测数据表示为 $Z＝(Z_1,Z_2,…,Z_n)^T$， 则观测数据的似然函数为$$P(Y|\theta) = \sum\limits_{Z}P(Y,Z|\theta)=\sum\limits_{Z}P(Z|\theta)P(Y|Z,\theta) \tag{9.2}$$即：$$P(Y|\theta)= \prod_{j=1}^{n}\left\{\pi p^{y_j}(1-p)^{(1-y_j)} + (1 - \pi) q^{y_j}(1-q)^{(1-y_j)}\right\} \tag{9.3}$$考虑求模型参数 $\theta =(\pi, p, q) $ 的极大似然估计，即：$$\begin{align}\hat{\theta}&amp;=\mathop{\arg\max}_{\theta} \mathrm{log}P(Y|\theta) \\&amp;= \mathop{\arg\max}_{\theta}\log\prod_{j=1}^{n}P(Y|\theta) \Leftarrow\text{n次抛硬币试验都是独立} \\&amp;= \mathop{\arg\max}_{\theta}\sum\limits_{j=1}^{n}\log P(Y|\theta) \\&amp;= \mathop{\arg\max}_{\theta}\sum\limits_{j=1}^{n}\log\left\{\sum\limits_{Z}{P(Z|\theta)P(Y|Z,\theta)}\right\} \tag{9-3}\end{align}$$问题：这里为什么要取对数？ 取对数之后累积变为累和，求导更加方便（后面三硬币例子解析将会看到） 概率累积会出现数值非常小的情况，比如1e-30，由于计算机的精度是有限的，无法识别这一类数据，取对数之后，更易于计算机的识别(1e-30以10为底取对数后便得到-30)。 这个问题没有解析解，因为隐变量数据无法获得，只有通过迭代的方法求解。 EM算法就是可以用于求解这个问题的一种迭代算法。 一般地， 用 $Y$ 表示观测随机变量的数据， $Z$ 表示隐随机变量的数据。 $Y$ 和 $Z$ 连在一起称为完全数据（complete-data） ， 观测数据 $Y$ 又称为不完全数据（incomplete-data） 。 假设给定观测数据 $Y$， 其概率分布是 $P(Y|\theta)$， 其中是需要估计的模型参数， 那么不完全数据 $Y$ 的似然函数是 $P(Y|\theta)$， 对数似然函数 $L(\theta)＝\mathrm{log}P(Y|\theta)$ ； 假设 $Y$ 和 $Z$ 的联合概率分布是 $P(Y, Z|\theta)$， 那么完全数据的对数似然函数是 $\mathrm{log}P(Y, Z|\theta)$。 9.1.2 EM算法的导出 注：书上给出琴声不等式（$\ln\sum_j\lambda_jy_j\geq \sum_j\lambda_j\log y_j,\quad \lambda_j\ge 0,\sum_j\lambda_j=1$），自行维基百科一下了解详情。最后一步源自于 $Z$ 所有可能取值的概率和为1$$\mathrm{log}P(Y|\theta^{(i)})=\mathrm{log}P(Y|\theta^{(i)}) \cdot \sum\limits_{Z}P(Z|Y, \theta^{(i)})$$$$\begin{align}\theta^{(i+1)} &amp;= \mathop{\arg\max}_{\theta} \left\{ L(\theta^{(i)}) + \sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}\right\} \\&amp;= \mathop{\arg\max}_{\theta}\left\{ \mathrm{log}P(Y|\theta^{(i)})\sum\limits_{Z}P(Z|Y, \theta^{(i)}) + \sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})} \right\} \\\end{align}$$加号右边，利用对数函数的性质得到：$$\begin{align}&amp;\sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})} \\&amp;=\sum\limits_{Z}P(Z|Y,\theta^{(i)})\left\{\mathrm{log}[P(Y|Z,\theta)P(Z|\theta)] - \mathrm{log}[P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})]\right\} \\&amp;=\sum\limits_{Z}P(Z|Y,\theta^{(i)})\left\{\mathrm{log}[P(Y|Z,\theta)P(Z|\theta)] - \mathrm{log}P(Z|Y,\theta^{(i)})-\mathrm{log}P(Y|\theta^{(i)})\right\} \\&amp;= \sum\limits_{Z}P(Z|Y,\theta^{(i)})\mathrm{log}[P(Y|Z,\theta)P(Z|\theta)] - \sum\limits_{Z}P(Z|Y,\theta^{(i)})\mathrm{log}P(Z|Y,\theta^{(i)})-\sum\limits_{Z}P(Z|Y,\theta^{(i)})\mathrm{log}P(Y|\theta^{(i)}) \\\end{align}$$代入上式可得：$$\begin{align}\theta^{(i+1)} &amp;= \mathop{\arg\max}_{\theta} \left\{ \sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}[P(Y|Z,\theta)P(Z|\theta)]-\sum\limits_{Z}P(Z|Y,\theta^{(i)})\mathrm{log}P(Z|Y,\theta^{(i)}) \right\} \\\end{align}$$ 由于在迭代求第 $i+1$ 步时，$\theta^{(i)}$ 是已知的，那么由训练数据中可以求得 $P(Z|Y,\theta^{(i)})$ ，所以在 $\theta^{(i)}$ 值确定的情况下，$P(Z|Y,\theta^{(i)})$ 的值也是确定的而不是变量，那么对上式极大化等价求解对下面式子的极大化$$\begin{align}\theta^{(i+1)} &amp;= \mathop{\arg\max}_{\theta} \left\{ \sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}[P(Y|Z,\theta)P(Z|\theta)]\right\} \\&amp;= \mathop{\arg\max}_{\theta} \left\{ \sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}P(Y,Z|\theta)\right\} \\&amp;= \mathop{\arg\max}_{\theta}Q(\theta, \theta^{(i)}) \tag{9.17}\end{align}$$ Q函数 EM算法 EM算法解释 9.1.3 EM算法在非监督学习中的应用 9.2 EM算法的收敛性这一部分原书讲的比较详细，不画蛇添足，贴上来。 三硬币例子解析前文讲到抛硬币的例子，现在重新详细推导一下三硬币这个例子。 $j$ 是训练集中的数据编号，实际上书上这里求得是$$\begin{align}P(Z|y_j,\theta^{(i)}) = \cases{P(Z=1|y_j,\theta^{(i)})=\mu_{j}^{(i+1)} \\ P(Z=0|y_j,\theta^{(i)})=1- \mu_{j}^{(i+1)}}\end{align}$$前文已知Q函数：$$Q(\theta, \theta^{(i)})=\sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}P(Y,Z|\theta)$$ 第一步求期望即求Q函数，由本文开头的 9.1.1 EM算法 这一节的公式 (9-3) 和 Q函数得到，在多个样本情况下 Q 函数为：$$\begin{align}Q(\theta, \theta^{(i)}) &amp;= \sum\limits_{j=1}^{n}\sum\limits_{Z}P(Z|y_j, \theta^{(i)})\log P(y_j,Z|\theta)\\&amp;= \sum\limits_{j=1}^{n}\left\{ P(Z=1|y_j, \theta^{(i)})\mathrm{log}P(y_j,Z=1|\theta) + P(Z=0|y_j, \theta^{(i)})\mathrm{log}P(y_j,Z=0|\theta) \right\}\\&amp;= \sum\limits_{j=1}^{n}\left\{\mu_{j}^{(i+1)}log P(y_j,Z=1|\theta) + (1-\mu_{j}^{(i+1)})\mathrm{log}P(y_j,Z=0|\theta) \right\}\\&amp;= \sum\limits_{j=1}^{n}\left\{\mu_{j}^{(i+1)}\log [\pi p^{y_j}(1-p)^{1-y_j}]+(1-\mu_{j}^{(i+1)})\log [(1-\pi )q^{y_j}(1-q)^{1-y_j}] \right\}\\\end{align}$$ 第二步极大化Q函数$\begin{align}\theta^{(i+1)} = \mathop{\arg\max}_{\theta}Q(\theta, \theta^{(i)}) = \mathop{\arg\max}_{\theta} \left\{\sum\limits_{j=1}^{n} \sum\limits_{Z}P(Z|y_j, \theta^{(i)})\log P(y_j,Z|\theta)\right\}\end{align}$ 用微积分求解最大值，先求导数为0点（为了求导方便令对数的底数为e，即认为此处对数函数为自然对数）：$$\begin{aligned} \frac{\partial Q(\theta,\theta^{(i)})}{\partial \pi}&amp;=\sum_{j=1}^N\{\frac{\mu_{j}^{(i+1)}\ln [\pi p^{y_j}(1-p)^{1-y_j}]+(1-\mu_{j}^{(i+1)})\ln [(1-\pi )q^{y_j}(1-q)^{1-y_j}] }{\partial \pi}\}\\&amp;=\sum_{j=1}^N\{ \mu_{j}^{(i+1)}\frac{p^{y_j}(1-p)^{1-y_j}}{\pi p^{y_j}(1-p)^{1-y_j}}+(1-\mu_{j}^{(i+1)})\frac{-q^{y_j}(1-q)^{1-y_j}}{(1-\pi )q^{y_j}(1-q)^{1-y_j}} \}\\&amp;=\sum_{j=1}^N\{ \frac{\mu_{j}^{(i+1)}-\pi }{\pi (1-\pi)}\}\\&amp;=\frac{(\sum_{j=1}^N\mu_{j}^{(i+1)})-n\pi }{\pi (1-\pi)} \end{aligned}$$ $$\begin{aligned}\because \quad\frac{\partial Q(\theta,\theta^{(i)})}{\partial \pi}=0 &amp;\implies \pi =\frac 1n\sum_{j=1}^N\mu_{j}^{(i+1)}\\\therefore \quad \pi^{(i+1)}&amp;=\frac 1n\sum_{j=1}^N\mu_{j}^{(i+1)} \end{aligned}$$ $$\begin{aligned} \frac{\partial Q(\theta,\theta^{(i)})}{\partial p}&amp;=\sum_{j=1}^N\{\frac{\mu_{j}^{(i+1)}\ln [\pi p^{y_j}(1-p)^{1-y_j}]+(1-\mu_{j}^{(i+1)})\ln [(1-\pi )q^{y_j}(1-q)^{1-y_j}] }{\partial p}\}\\&amp;=\sum_{j=1}^N\{\mu_{j}^{(i+1)}\frac{\pi (y_jp^{y_j-1}(1-p)^{1-y_j}+p^{y_j}(-1)(1-y_j)(1-p)^{1-y_j-1})}{\pi p^{y_j}(1-p)^{1-y_j}}+0 \}\\&amp;=\sum_{j=1}^N\{ \frac{\mu_{j}^{(i+1)}(y_j-p) }{p(1-p)}\}\\&amp;=\frac{(\sum_{j=1}^N\mu_{j}^{(i+1)}y_j)-(p\sum_{j=1}^N\mu_{j}^{(i+1)}) }{p(1-p)} \end{aligned}$$ $$\begin{aligned}\because \quad \frac{\partial Q(\theta,\theta^{(i)})}{\partial p}=0 &amp;\implies p =\frac{\sum_{j=1}^N \mu^{(i+1)}_j y_j}{\sum_{j=1}^N\mu^{(i+1)}_j} \\\therefore \quad p^{(i+1)}&amp;=\frac{\sum_{j=1}^N\mu^{(i+1)}_j y_j}{\sum_{j=1}^N\mu^{(i+1)}_j} \\q^{(i+1)}&amp;=\frac{\sum_{j=1}^N(1-\mu^{(i+1)}_j)y_j}{\sum_{j=1}^N(1-\mu^{(i+1)}_j)}\end{aligned}$$可以参照书上的结果，一模一样： CS299 EM算法与《统计学习方法》的表述不同点 《统计学习方法》这部分术语源自于鼎鼎大名的ESL 全称：The Elements of Statistical Learning，这也是Stanford统计经典巨作。 Stanford 吴恩达主讲的 CS299 Machine Learning 的 EM课件 由本文的推导，易得 ESL 中的 $ Q_{ESL} = Q_{CS299}\frac{\log P(X,Z;\theta)}{Q_{CS299}} $ 9.3 EM算法在高斯混合模型学习中的应用EM算法的一个重要应用是高斯混合模型的参数估计。 高斯混合模型应用广泛， 在许多情况下， EM算法是学习高斯混合模型（Gaussian misture model） 的有效方法。 9.3.1 高斯混合模型 9.3.2 高斯混合模型参数估计的EM算法 注意：上面的极大化的求混合模型参数迭代公式的过程参考： 大牛JerryLead 的 （EM算法）The EM Algorithm 与K-means比较相同点：都是可用于聚类的算法；都需要指定K值。 不同点：GMM可以给出一个样本属于某类的概率是多少。 9.4 EM算法的推广EM算法还可以解释为F函数（F function） 的极大-极大算法（maximization maximization algorithm） ， 基于这个解释有若干变形与推广， 如广义期望极大（generalized expectation maximization，GEM） 算法。 注：原文引理(9.1)(9.2)的证明有坑需要注意，先看原文，后面列出详细过程 9.4.1 F函数的极大-极大算法 熵这块，不清楚的可以回顾一下我的另一篇总结：《机器学习中的信息论基础》 。 引理9.1需要更详细说明：$$L=E_{\tilde{p}}\log P(Y,Z|\theta) - E_{\tilde{p}}\log \tilde{P}(Z) + \lambda\left\{1-\sum\limits_{Z}\tilde{P}(Z)\right\}$$证明过程思路：拉格朗日求有约束的极大值。需要注意，由累加号和均值可以看出这里的 $Z$ 是指 $Z_i, i$ 这里是 $Z$ 的离散值的标号 ，因此需要重写公式 (9.35) 比较清楚：$$L=\sum\limits_{Z_i}{\tilde{P}(Z_i)}\log P(Y,Z_i|\theta) - \sum\limits_{Z_i}{\tilde{P}(Z_i)}\log \tilde{P}(Z_i)+\lambda\left\{1-\sum\limits_{Z_i}\tilde{P}(Z_i)\right\}$$所以这里其实是 $L$ 关于 $P(Z_i)$的求导（这里作者求导的时候把对数函数默认当做自然对数）：$$\begin{align}&amp;\frac{\partial{L}}{\partial{\tilde{P}(Z_i)}}=\log P(Y,Z_i|\theta)-\log \tilde{P}(Z_i)-1-\lambda \\&amp;\because\quad\frac{\partial{L}}{\partial{\tilde{P}(Z_i)}}=0\\ &amp;\therefore\quad \lambda=\log P(Y,Z_i|\theta)-\log \tilde{P}(Z_i)-1\end{align}$$上式两端同取对数：$$\begin{align}\lambda+1&amp;=\log P(Y,Z_i|\theta)-\log \tilde{P}(Z_i) \\ &amp;\Rightarrow e^{\lambda+1}=\frac{P(Y,Z_i|\theta)}{\tilde{P}(Z_i)} \\&amp;\Rightarrow\tilde{P}(Z_i)=\frac{P(Y,Z_i|\theta)}{e^{\lambda+1}} \tag{9-1}\end{align}$$由离散变量的概率和为1，得到：$$\begin{align}\sum\limits_{Z_i}e^{\lambda+1} &amp;= \frac{\sum\limits_{Z_i}P(Y,Z_i|\theta)}{\sum\limits_{Z_i}\tilde{P}(Z_i)} \Rightarrow\\e^{\lambda+1} &amp;= P(Y|\theta) \tag{9-2}\end{align}$$将 (9-2) 代入 (9-1)​ 式，得到$$\begin{align}\tilde{P}(Z_i)&amp;=\frac{P(Y,Z_i|\theta)}{P(Y|\theta)} \\&amp;=\frac{P(Y,Z_i,\theta)}{p(\theta)}\frac{P(\theta)}{P(Y,\theta)} \\&amp;= P(Z_i|Y,\theta)\end{align}$$这里前提条件是 $\theta$ 是固定情况下的推导过程，所以原文给上式标记出了 $\theta$ ，又因为每个 $Z_i$ 都符合这个式子，那么可重写上式：$$\tilde{P}_{\theta}(Z) = P(Z|Y,\theta)$$这样引理9.1证明完毕。 引理9.2如下 由公式 $(9.33)$ 和 $(9.34)$ :$$F(\tilde{P}, \theta)=E_{\tilde{p}}[\log P(Y,Z|\theta)] + H(\tilde{P}) \\\tilde{P}_{\theta}(Z) = P(Z|Y,\theta)$$得到：$$\begin{align}F(\tilde{P}, \theta)&amp;=\sum\limits_{Z}{\tilde{P}_{\theta}(Z)}\log P(Y,Z|\theta) - \sum\limits_{Z}{\tilde{P}_{\theta}(Z)}\log \tilde{P}_{\theta}(Z) \\&amp;=\sum\limits_{Z} P(Z|Y,\theta)\log P(Y,Z|\theta) - \sum\limits_{Z}P(Z|Y,\theta)\log P(Z|Y,\theta) \\&amp;=\sum\limits_{Z}P(Z|Y,\theta)[\log P(Y,Z|\theta) - \log P(Z|Y,\theta)] \\&amp;=\sum\limits_{Z}P(Z|Y,\theta)\log\frac{P(Y,Z|\theta)}{P(Z|Y,\theta)}\\&amp;=\sum\limits_{Z}P(Z|Y,\theta)\log\left\{\frac{P(Y,Z,\theta)}{p(\theta)}\frac{P(Y,\theta)}{P(Y,Z,\theta)}\right\}\\&amp;= \sum\limits_{Z}P(Z|Y,\theta)\log P(Y|\theta) \\&amp;= \log P(Y|\theta) \\ \end{align}$$引理9.2证明完毕 9.4.2 GEM算法 本章概要 引用 The Expectation Maximization Algorithm: A short tutorial - Sean Borman 李航《统计学习方法》 大牛JerryLead 的 （EM算法）The EM Algorithm 人人都懂EM算法 EM算法简述及简单示例（三硬币模型）]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[summary of learning Deep Learning Specialization]]></title>
    <url>%2F2018%2F06%2F28%2Fsummary_of_learning_of_Deep_Learning_Specializatio_on_Coursera%2F</url>
    <content type="text"><![CDATA[EnglishThis is my summary of learning Deep Learning Specialization on Coursera, which consists of 5 courses as following: 1st course: Neural Networks and Deep Learning 2nd course: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization 3rd course: Structuring Machine Learning Projects 4th course: Convolutional Neural Networks 5th course: Sequence Models And, here are my summaries of them: 1st course: summary_of_neural-networks-deep-learning 2nd course: summary_of_Improving-Deep-Neural-Networks 3rd course: summary_of_Structuring-Machine-Learning-Projects 4th course: summary_of_convolutional-neural-networks 5th course: summary_of_nlp-sequence-models I spent about 45 days in finishing this Deep learning Specialization and the personal lecture notes, summaries and assignments, but as the saying goes, “gain new knowledge by reviewing the old”. Therefore, I will stick at learning more about Deep Learning and renew the content of this specilization. if you need more details about this Deep Learning Specilization in English, please refer deeplearning.ai or the specialization on Coursera. Tip: if you are familiar with Chinese, you can read the content as following. 中文本文是我个人对吴恩达的深度学习专项课程的学习总结，此文有5个子课程，总结如下： 1st course: summary_of_neural-networks-deep-learning 2nd course: summary_of_Improving-Deep-Neural-Networks 3rd course: summary_of_Structuring-Machine-Learning-Projects 4th course: summary_of_convolutional-neural-networks 5th course: summary_of_nlp-sequence-models 此专项课程的中文目录如下: 如果需要详细视频内容和课程ppt，请参考网易云课堂：吴恩达给你的人工智能第一课， 但是网易并没有提供完成作业的平台，完成作业还需要到 Coursera。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[summary of nlp sequence models]]></title>
    <url>%2F2018%2F06%2F06%2Fsummary_of_nlp-sequence-models%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal summary after studying the course, nlp sequence models, which belongs to Deep Learning Specialization. and the copyright belongs to deeplearning.ai. My personal note$1_{st}$ week : Building a Recurrent Neural Network Step by Step 01_why-sequence-models 02_notation 03_recurrent-neural-network-model 04_backpropagation-through-time 05_different-types-of-rnns 06_language-model-and-sequence-generation 07_sampling-novel-sequences 08_vanishing-gradients-with-rnns 09_gated-recurrent-unit-gru 10_long-short-term-memory-lstm 11_bidirectional-rnn 12_deep-rnns $2_{nd}$ week : natural language processing word embeddings 01_introduction-to-word-embeddings 01_word-representation 02_using-word-embeddings 03_properties-of-word-embeddings 04_embedding-matrix 02_learning-word-embeddings-word2vec-glove 01_learning-word-embeddings 02_word2vec 03_negative-sampling 04_glove-word-vectors 03_applications-using-word-embeddings 01_sentiment-classification 02_debiasing-word-embeddings $3_{rd}$ week : sequence models attention mechanism 01_various-sequence-to-sequence-architectures 01_basic-models 02_picking-the-most-likely-sentence 03_beam-search 04_refinements-to-beam-search 05_error-analysis-in-beam-search 06_bleu-score-optional 07_attention-model-intuition 08_attention-model 02_speech-recognition-audio-data 01_speech-recognition 02_trigger-word-detection conclusion of Deep Learning Specialization and thank-you My personal programming assignments$1_{st}$ week: Building a Recurrent Neural Network Step by Step Dinosaurus Island Character level language model final Improvise a Jazz Solo with an LSTM Network $2_{nd}$ week: Word Vector Representation Emojify $3_{rd}$ week: machine translation Trigger word]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Trigger word]]></title>
    <url>%2F2018%2F06%2F06%2FTrigger_word_detection-v1%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 3rd week and the copyright belongs to deeplearning.ai. Trigger Word DetectionWelcome to the final programming assignment of this specialization! In this week’s videos, you learned about applying deep learning to speech recognition. In this assignment, you will construct a speech dataset and implement an algorithm for trigger word detection (sometimes also called keyword detection, or wakeword detection). Trigger word detection is the technology that allows devices like Amazon Alexa, Google Home, Apple Siri, and Baidu DuerOS to wake up upon hearing a certain word. For this exercise, our trigger word will be “Activate.” Every time it hears you say “activate,” it will make a “chiming” sound. By the end of this assignment, you will be able to record a clip of yourself talking, and have the algorithm trigger a chime when it detects you saying “activate.” After completing this assignment, perhaps you can also extend it to run on your laptop so that every time you say “activate” it starts up your favorite app, or turns on a network connected lamp in your house, or triggers some other event? In this assignment you will learn to: Structure a speech recognition project Synthesize and process audio recordings to create train/dev datasets Train a trigger word detection model and make predictions Lets get started! Run the following cell to load the package you are going to use. 12345678910import numpy as npfrom pydub import AudioSegmentimport randomimport sysimport ioimport osimport globimport IPythonfrom td_utils import *%matplotlib inline 1 - Data synthesis: Creating a speech datasetLet’s start by building a dataset for your trigger word detection algorithm. A speech dataset should ideally be as close as possible to the application you will want to run it on. In this case, you’d like to detect the word “activate” in working environments (library, home, offices, open-spaces …). You thus need to create recordings with a mix of positive words (“activate”) and negative words (random words other than activate) on different background sounds. Let’s see how you can create such a dataset. 1.1 - Listening to the dataOne of your friends is helping you out on this project, and they’ve gone to libraries, cafes, restaurants, homes and offices all around the region to record background noises, as well as snippets of audio of people saying positive/negative words. This dataset includes people speaking in a variety of accents. In the raw_data directory, you can find a subset of the raw audio files of the positive words, negative words, and background noise. You will use these audio files to synthesize a dataset to train the model. The “activate” directory contains positive examples of people saying the word “activate”. The “negatives” directory contains negative examples of people saying random words other than “activate”. There is one word per audio recording. The “backgrounds” directory contains 10 second clips of background noise in different environments. Run the cells below to listen to some examples. 1IPython.display.Audio("./raw_data/activates/1.wav") Your browser does not support the audio element. 1IPython.display.Audio("./raw_data/negatives/4.wav") Your browser does not support the audio element. 1IPython.display.Audio("./raw_data/backgrounds/1.wav") Your browser does not support the audio element. You will use these three type of recordings (positives/negatives/backgrounds) to create a labelled dataset. 1.2 - From audio recordings to spectrogramsWhat really is an audio recording? A microphone records little variations in air pressure over time, and it is these little variations in air pressure that your ear also perceives as sound. You can think of an audio recording is a long list of numbers measuring the little air pressure changes detected by the microphone. We will use audio sampled at 44100 Hz (or 44100 Hertz). This means the microphone gives us 44100 numbers per second. Thus, a 10 second audio clip is represented by 441000 numbers (= $10 \times 44100$). It is quite difficult to figure out from this “raw” representation of audio whether the word “activate” was said. In order to help your sequence model more easily learn to detect triggerwords, we will compute a spectrogram of the audio. The spectrogram tells us how much different frequencies are present in an audio clip at a moment in time. (If you’ve ever taken an advanced class on signal processing or on Fourier transforms, a spectrogram is computed by sliding a window over the raw audio signal, and calculates the most active frequencies in each window using a Fourier transform. If you don’t understand the previous sentence, don’t worry about it.) Lets see an example. 1IPython.display.Audio("audio_examples/example_train.wav") Your browser does not support the audio element. 1x = graph_spectrogram("audio_examples/example_train.wav") The graph above represents how active each frequency is (y axis) over a number of time-steps (x axis). Figure 1: Spectrogram of an audio recording, where the color shows the degree to which different frequencies are present (loud) in the audio at different points in time. Green squares means a certain frequency is more active or more present in the audio clip (louder); blue squares denote less active frequencies. The dimension of the output spectrogram depends upon the hyperparameters of the spectrogram software and the length of the input. In this notebook, we will be working with 10 second audio clips as the “standard length” for our training examples. The number of timesteps of the spectrogram will be 5511. You’ll see later that the spectrogram will be the input $x$ into the network, and so $T_x = 5511$. 123_, data = wavfile.read("audio_examples/example_train.wav")print("Time steps in audio recording before spectrogram", data[:,0].shape)print("Time steps in input after spectrogram", x.shape) Time steps in audio recording before spectrogram (441000,) Time steps in input after spectrogram (101, 5511) Now, you can define: 12Tx = 5511 # The number of time steps input to the model from the spectrogramn_freq = 101 # Number of frequencies input to the model at each time step of the spectrogram Note that even with 10 seconds being our default training example length, 10 seconds of time can be discretized to different numbers of value. You’ve seen 441000 (raw audio) and 5511 (spectrogram). In the former case, each step represents $10/441000 \approx 0.000023$ seconds. In the second case, each step represents $10/5511 \approx 0.0018$ seconds. For the 10sec of audio, the key values you will see in this assignment are: $441000$ (raw audio) $5511 = T_x$ (spectrogram output, and dimension of input to the neural network). $10000$ (used by the pydub module to synthesize audio) $1375 = T_y$ (the number of steps in the output of the GRU you’ll build). Note that each of these representations correspond to exactly 10 seconds of time. It’s just that they are discretizing them to different degrees. All of these are hyperparameters and can be changed (except the 441000, which is a function of the microphone). We have chosen values that are within the standard ranges uses for speech systems. Consider the $T_y = 1375$ number above. This means that for the output of the model, we discretize the 10s into 1375 time-intervals (each one of length $10/1375 \approx 0.0072$s) and try to predict for each of these intervals whether someone recently finished saying “activate.” Consider also the 10000 number above. This corresponds to discretizing the 10sec clip into 10/10000 = 0.001 second itervals. 0.001 seconds is also called 1 millisecond, or 1ms. So when we say we are discretizing according to 1ms intervals, it means we are using 10,000 steps. 1Ty = 1375 # The number of time steps in the output of our model 1.3 - Generating a single training exampleBecause speech data is hard to acquire and label, you will synthesize your training data using the audio clips of activates, negatives, and backgrounds. It is quite slow to record lots of 10 second audio clips with random “activates” in it. Instead, it is easier to record lots of positives and negative words, and record background noise separately (or download background noise from free online sources). To synthesize a single training example, you will: Pick a random 10 second background audio clip Randomly insert 0-4 audio clips of “activate” into this 10sec clip Randomly insert 0-2 audio clips of negative words into this 10sec clip Because you had synthesized the word “activate” into the background clip, you know exactly when in the 10sec clip the “activate” makes its appearance. You’ll see later that this makes it easier to generate the labels $y^{\langle t \rangle}$ as well. You will use the pydub package to manipulate audio. Pydub converts raw audio files into lists of Pydub data structures (it is not important to know the details here). Pydub uses 1ms as the discretization interval (1ms is 1 millisecond = 1/1000 seconds) which is why a 10sec clip is always represented using 10,000 steps. 123456# Load audio segments using pydub activates, negatives, backgrounds = load_raw_audio()print("background len: " + str(len(backgrounds[0]))) # Should be 10,000, since it is a 10 sec clipprint("activate[0] len: " + str(len(activates[0]))) # Maybe around 1000, since an "activate" audio clip is usually around 1 sec (but varies a lot)print("activate[1] len: " + str(len(activates[1]))) # Different "activate" clips can have different lengths background len: 10000 activate[0] len: 916 activate[1] len: 1579 Overlaying positive/negative words on the background: Given a 10sec background clip and a short audio clip (positive or negative word), you need to be able to “add” or “insert” the word’s short audio clip onto the background. To ensure audio segments inserted onto the background do not overlap, you will keep track of the times of previously inserted audio clips. You will be inserting multiple clips of positive/negative words onto the background, and you don’t want to insert an “activate” or a random word somewhere that overlaps with another clip you had previously added. For clarity, when you insert a 1sec “activate” onto a 10sec clip of cafe noise, you end up with a 10sec clip that sounds like someone sayng “activate” in a cafe, with “activate” superimposed on the background cafe noise. You do not end up with an 11 sec clip. You’ll see later how pydub allows you to do this. Creating the labels at the same time you overlay: Recall also that the labels $y^{\langle t \rangle}$ represent whether or not someone has just finished saying “activate.” Given a background clip, we can initialize $y^{\langle t \rangle}=0$ for all $t$, since the clip doesn’t contain any “activates.” When you insert or overlay an “activate” clip, you will also update labels for $y^{\langle t \rangle}$, so that 50 steps of the output now have target label 1. You will train a GRU to detect when someone has finished saying “activate”. For example, suppose the synthesized “activate” clip ends at the 5sec mark in the 10sec audio—exactly halfway into the clip. Recall that $T_y = 1375$, so timestep $687 = $ int(1375*0.5) corresponds to the moment at 5sec into the audio. So, you will set $y^{\langle 688 \rangle} = 1$. Further, you would quite satisfied if the GRU detects “activate” anywhere within a short time-internal after this moment, so we actually set 50 consecutive values of the label $y^{\langle t \rangle}$ to 1. Specifically, we have $y^{\langle 688 \rangle} = y^{\langle 689 \rangle} = \cdots = y^{\langle 737 \rangle} = 1$. This is another reason for synthesizing the training data: It’s relatively straightforward to generate these labels $y^{\langle t \rangle}$ as described above. In contrast, if you have 10sec of audio recorded on a microphone, it’s quite time consuming for a person to listen to it and mark manually exactly when “activate” finished. Here’s a figure illustrating the labels $y^{\langle t \rangle}$, for a clip which we have inserted “activate”, “innocent”, activate”, “baby.” Note that the positive labels “1” are associated only with the positive words. Figure 2 To implement the training set synthesis process, you will use the following helper functions. All of these function will use a 1ms discretization interval, so the 10sec of audio is alwsys discretized into 10,000 steps. get_random_time_segment(segment_ms) gets a random time segment in our background audio is_overlapping(segment_time, existing_segments) checks if a time segment overlaps with existing segments insert_audio_clip(background, audio_clip, existing_times) inserts an audio segment at a random time in our background audio using get_random_time_segment and is_overlapping insert_ones(y, segment_end_ms) inserts 1’s into our label vector y after the word “activate” The function get_random_time_segment(segment_ms) returns a random time segment onto which we can insert an audio clip of duration segment_ms. Read through the code to make sure you understand what it is doing. 123456789101112131415def get_random_time_segment(segment_ms): """ Gets a random time segment of duration segment_ms in a 10,000 ms audio clip. Arguments: segment_ms -- the duration of the audio clip in ms ("ms" stands for "milliseconds") Returns: segment_time -- a tuple of (segment_start, segment_end) in ms """ segment_start = np.random.randint(low=0, high=10000-segment_ms) # Make sure segment doesn't run past the 10sec background segment_end = segment_start + segment_ms - 1 return (segment_start, segment_end) Next, suppose you have inserted audio clips at segments (1000,1800) and (3400,4500). I.e., the first segment starts at step 1000, and ends at step 1800. Now, if we are considering inserting a new audio clip at (3000,3600) does this overlap with one of the previously inserted segments? In this case, (3000,3600) and (3400,4500) overlap, so we should decide against inserting a clip here. For the purpose of this function, define (100,200) and (200,250) to be overlapping, since they overlap at timestep 200. However, (100,199) and (200,250) are non-overlapping. Exercise: Implement is_overlapping(segment_time, existing_segments) to check if a new time segment overlaps with any of the previous segments. You will need to carry out 2 steps: Create a “False” flag, that you will later set to “True” if you find that there is an overlap. Loop over the previous_segments’ start and end times. Compare these times to the segment’s start and end times. If there is an overlap, set the flag defined in (1) as True. You can use:123for ....: if ... &lt;= ... and ... &gt;= ...: ... Hint: There is overlap if the segment starts before the previous segment ends, and the segment ends after the previous segment starts. 12345678910111213141516171819202122232425262728# GRADED FUNCTION: is_overlappingdef is_overlapping(segment_time, previous_segments): """ Checks if the time of a segment overlaps with the times of existing segments. Arguments: segment_time -- a tuple of (segment_start, segment_end) for the new segment previous_segments -- a list of tuples of (segment_start, segment_end) for the existing segments Returns: True if the time segment overlaps with any of the existing segments, False otherwise """ segment_start, segment_end = segment_time ### START CODE HERE ### (≈ 4 line) # Step 1: Initialize overlap as a "False" flag. (≈ 1 line) overlap = False; # Step 2: loop over the previous_segments start and end times. # Compare start/end times and set the flag to True if there is an overlap (≈ 3 lines) for previous_start, previous_end in previous_segments: if segment_end &gt;= previous_start and segment_start &lt;= previous_end: overlap = True; ### END CODE HERE ### return overlap 1234overlap1 = is_overlapping((950, 1430), [(2000, 2550), (260, 949)])overlap2 = is_overlapping((2305, 2950), [(824, 1532), (1900, 2305), (3424, 3656)])print("Overlap 1 = ", overlap1)print("Overlap 2 = ", overlap2) Overlap 1 = False Overlap 2 = True Expected Output: Overlap 1 False Overlap 2 True Now, lets use the previous helper functions to insert a new audio clip onto the 10sec background at a random time, but making sure that any newly inserted segment doesn’t overlap with the previous segments. Exercise: Implement insert_audio_clip() to overlay an audio clip onto the background 10sec clip. You will need to carry out 4 steps: Get a random time segment of the right duration in ms. Make sure that the time segment does not overlap with any of the previous time segments. If it is overlapping, then go back to step 1 and pick a new time segment. Add the new time segment to the list of existing time segments, so as to keep track of all the segments you’ve inserted. Overlay the audio clip over the background using pydub. We have implemented this for you. 12345678910111213141516171819202122232425262728293031323334353637# GRADED FUNCTION: insert_audio_clipdef insert_audio_clip(background, audio_clip, previous_segments): """ Insert a new audio segment over the background noise at a random time step, ensuring that the audio segment does not overlap with existing segments. Arguments: background -- a 10 second background audio recording. audio_clip -- the audio clip to be inserted/overlaid. previous_segments -- times where audio segments have already been placed Returns: new_background -- the updated background audio """ # Get the duration of the audio clip in ms segment_ms = len(audio_clip) ### START CODE HERE ### # Step 1: Use one of the helper functions to pick a random time segment onto which to insert # the new audio clip. (≈ 1 line) segment_time = get_random_time_segment(segment_ms); # Step 2: Check if the new segment_time overlaps with one of the previous_segments. If so, keep # picking new segment_time at random until it doesn't overlap. (≈ 2 lines) while is_overlapping(segment_time, previous_segments): segment_time = get_random_time_segment(segment_ms); # Step 3: Add the new segment_time to the list of previous_segments (≈ 1 line) previous_segments.append(segment_time); ### END CODE HERE ### # Step 4: Superpose audio segment and background new_background = background.overlay(audio_clip, position = segment_time[0]) return new_background, segment_time 12345np.random.seed(5)audio_clip, segment_time = insert_audio_clip(backgrounds[0], activates[0], [(3790, 4400)])audio_clip.export("insert_test.wav", format="wav")print("Segment Time: ", segment_time)IPython.display.Audio("insert_test.wav") Segment Time: (2254, 3169) Your browser does not support the audio element. Expected Output Segment Time (2254, 3169) 12# Expected audioIPython.display.Audio("audio_examples/insert_reference.wav") Your browser does not support the audio element. Finally, implement code to update the labels $y^{\langle t \rangle}$, assuming you just inserted an “activate.” In the code below, y is a (1,1375) dimensional vector, since $T_y = 1375$. If the “activate” ended at time step $t$, then set $y^{\langle t+1 \rangle} = 1$ as well as for up to 49 additional consecutive values. However, make sure you don’t run off the end of the array and try to update y[0][1375], since the valid indices are y[0][0] through y[0][1374] because $T_y = 1375$. So if “activate” ends at step 1370, you would get only y[0][1371] = y[0][1372] = y[0][1373] = y[0][1374] = 1 Exercise: Implement insert_ones(). You can use a for loop. (If you are an expert in python’s slice operations, feel free also to use slicing to vectorize this.) If a segment ends at segment_end_ms (using a 10000 step discretization), to convert it to the indexing for the outputs $y$ (using a $1375$ step discretization), we will use this formula:1segment_end_y = int(segment_end_ms * Ty / 10000.0) 12345678910111213141516171819202122232425262728# GRADED FUNCTION: insert_onesdef insert_ones(y, segment_end_ms): """ Update the label vector y. The labels of the 50 output steps strictly after the end of the segment should be set to 1. By strictly we mean that the label of segment_end_y should be 0 while, the 50 followinf labels should be ones. Arguments: y -- numpy array of shape (1, Ty), the labels of the training example segment_end_ms -- the end time of the segment in ms Returns: y -- updated labels """ # duration of the background (in terms of spectrogram time-steps) segment_end_y = int(segment_end_ms * Ty / 10000.0) # Add 1 to the correct index in the background label (y) ### START CODE HERE ### (≈ 3 lines) for i in range(segment_end_y + 1, segment_end_y + 51): if i &lt; Ty: y[0, i] = 1 ### END CODE HERE ### return y 123arr1 = insert_ones(np.zeros((1, Ty)), 9700)plt.plot(insert_ones(arr1, 4251)[0,:])print("sanity checks:", arr1[0][1333], arr1[0][634], arr1[0][635]) sanity checks: 0.0 1.0 0.0 Expected Output sanity checks: 0.0 1.0 0.0 Finally, you can use insert_audio_clip and insert_ones to create a new training example.Exercise: Implement create_training_example(). You will need to carry out the following steps:1. Initialize the label vector $y$ as a numpy array of zeros and shape $(1, T_y)$.2. Initialize the set of existing segments to an empty list.3. Randomly select 0 to 4 “activate” audio clips, and insert them onto the 10sec clip. Also insert labels at the correct position in the label vector $y$.4. Randomly select 0 to 2 negative audio clips, and insert them into the 10sec clip.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# GRADED FUNCTION: create_training_exampledef create_training_example(background, activates, negatives): """ Creates a training example with a given background, activates, and negatives. Arguments: background -- a 10 second background audio recording activates -- a list of audio segments of the word "activate" negatives -- a list of audio segments of random words that are not "activate" Returns: x -- the spectrogram of the training example y -- the label at each time step of the spectrogram """ # Set the random seed np.random.seed(18) # Make background quieter background = background - 20 ### START CODE HERE ### # Step 1: Initialize y (label vector) of zeros (≈ 1 line) y = np.zeros((1, Ty)); # Step 2: Initialize segment times as empty list (≈ 1 line) previous_segments = []; ### END CODE HERE ### # Select 0-4 random "activate" audio clips from the entire list of "activates" recordings number_of_activates = np.random.randint(0, 5) random_indices = np.random.randint(len(activates), size=number_of_activates) random_activates = [activates[i] for i in random_indices] ### START CODE HERE ### (≈ 3 lines) # Step 3: Loop over randomly selected "activate" clips and insert in background for random_activate in random_activates: # Insert the audio clip on the background background, segment_time = insert_audio_clip(background, random_activate, previous_segments); # Retrieve segment_start and segment_end from segment_time segment_start, segment_end = segment_time; # Insert labels in "y" y = insert_ones(y, segment_end); ### END CODE HERE ### # Select 0-2 random negatives audio recordings from the entire list of "negatives" recordings number_of_negatives = np.random.randint(0, 3) random_indices = np.random.randint(len(negatives), size=number_of_negatives) random_negatives = [negatives[i] for i in random_indices] ### START CODE HERE ### (≈ 2 lines) # Step 4: Loop over randomly selected negative clips and insert in background for random_negative in random_negatives: # Insert the audio clip on the background background, _ = insert_audio_clip(background, random_negative, previous_segments); ### END CODE HERE ### # Standardize the volume of the audio clip background = match_target_amplitude(background, -20.0) # Export new training example file_handle = background.export("train" + ".wav", format="wav") print("File (train.wav) was saved in your directory.") # Get and plot spectrogram of the new recording (background with superposition of positive and negatives) x = graph_spectrogram("train.wav") return x, y1x, y = create_training_example(backgrounds[0], activates, negatives) File (train.wav) was saved in your directory.Expected OutputNow you can listen to the training example you created and compare it to the spectrogram generated above.1IPython.display.Audio("train.wav") Your browser does not support the audio element.Expected Output1IPython.display.Audio("audio_examples/train_reference.wav") Your browser does not support the audio element.Finally, you can plot the associated labels for the generated training example.1plt.plot(y[0]) [&lt;matplotlib.lines.Line2D at 0x7efcb81a8da0&gt;]Expected Output## 1.4 - Full training setYou’ve now implemented the code needed to generate a single training example. We used this process to generate a large training set. To save time, we’ve already generated a set of training examples.123# Load preprocessed training examplesX = np.load("./XY_train/X.npy")Y = np.load("./XY_train/Y.npy")## 1.5 - Development setTo test our model, we recorded a development set of 25 examples. While our training data is synthesized, we want to create a development set using the same distribution as the real inputs. Thus, we recorded 25 10-second audio clips of people saying “activate” and other random words, and labeled them by hand. This follows the principle described in Course 3 that we should create the dev set to be as similar as possible to the test set distribution; that’s why our dev set uses real rather than synthesized audio.123# Load preprocessed dev set examplesX_dev = np.load("./XY_dev/X_dev.npy")Y_dev = np.load("./XY_dev/Y_dev.npy")# 2 - ModelNow that you’ve built a dataset, lets write and train a trigger word detection model!The model will use 1-D convolutional layers, GRU layers, and dense layers. Let’s load the packages that will allow you to use these layers in Keras. This might take a minute to load.12345from keras.callbacks import ModelCheckpointfrom keras.models import Model, load_model, Sequentialfrom keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1Dfrom keras.layers import GRU, Bidirectional, BatchNormalization, Reshapefrom keras.optimizers import Adam Using TensorFlow backend.## 2.1 - Build the modelHere is the architecture we will use. Take some time to look over the model and see if it makes sense. Figure 3 One key step of this model is the 1D convolutional step (near the bottom of Figure 3). It inputs the 5511 step spectrogram, and outputs a 1375 step output, which is then further processed by multiple layers to get the final $T_y = 1375$ step output. This layer plays a role similar to the 2D convolutions you saw in Course 4, of extracting low-level features and then possibly generating an output of a smaller dimension.Computationally, the 1-D conv layer also helps speed up the model because now the GRU has to process only 1375 timesteps rather than 5511 timesteps. The two GRU layers read the sequence of inputs from left to right, then ultimately uses a dense+sigmoid layer to make a prediction for $y^{\langle t \rangle}$. Because $y$ is binary valued (0 or 1), we use a sigmoid output at the last layer to estimate the chance of the output being 1, corresponding to the user having just said “activate.”Note that we use a uni-directional RNN rather than a bi-directional RNN. This is really important for trigger word detection, since we want to be able to detect the trigger word almost immediately after it is said. If we used a bi-directional RNN, we would have to wait for the whole 10sec of audio to be recorded before we could tell if “activate” was said in the first second of the audio clip.Implementing the model can be done in four steps:Step 1: CONV layer. Use Conv1D() to implement this, with 196 filters,a filter size of 15 (kernel_size=15), and stride of 4. [See documentation.]Step 2: First GRU layer. To generate the GRU layer, use:1X = GRU(units = 128, return_sequences = True)(X)Setting return_sequences=True ensures that all the GRU’s hidden states are fed to the next layer. Remember to follow this with Dropout and BatchNorm layers.Step 3: Second GRU layer. This is similar to the previous GRU layer (remember to use return_sequences=True), but has an extra dropout layer.Step 4: Create a time-distributed dense layer as follows:1X = TimeDistributed(Dense(1, activation = "sigmoid"))(X)This creates a dense layer followed by a sigmoid, so that the parameters used for the dense layer are the same for every time step. [See documentation.]Exercise: Implement model(), the architecture is presented in Figure 3.123456789101112131415161718192021222324252627282930313233343536373839404142# GRADED FUNCTION: modeldef model(input_shape): """ Function creating the model's graph in Keras. Argument: input_shape -- shape of the model's input data (using Keras conventions) Returns: model -- Keras model instance """ X_input = Input(shape = input_shape) ### START CODE HERE ### # Step 1: CONV layer (≈4 lines) X = Conv1D(196, 15, strides = 4)(X_input); # CONV1D X = BatchNormalization()(X); # Batch normalization X = Activation('relu')(X); # ReLu activation X = Dropout(0.8)(X); # dropout (use 0.8) # Step 2: First GRU Layer (≈4 lines) X = GRU(128, return_sequences = True)(X); # GRU (use 128 units and return the sequences) X = Dropout(0.8)(X); # dropout (use 0.8) X = BatchNormalization()(X); # Batch normalization # Step 3: Second GRU Layer (≈4 lines) X = GRU(128, return_sequences = True)(X); # GRU (use 128 units and return the sequences) X = Dropout(0.8)(X); # dropout (use 0.8) X = BatchNormalization()(X); # Batch normalization X = Dropout(0.8)(X); # dropout (use 0.8) # Step 4: Time-distributed dense layer (≈1 line) X = TimeDistributed(Dense(1, activation = "sigmoid"))(X) # time distributed (sigmoid) ### END CODE HERE ### model = Model(inputs = X_input, outputs = X) return model1model = model(input_shape = (Tx, n_freq))Let’s print the model summary to keep track of the shapes.1model.summary() _ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 5511, 101) 0 _ conv1d_1 (Conv1D) (None, 1375, 196) 297136 _ batch_normalization_1 (Batch (None, 1375, 196) 784 _ activation_1 (Activation) (None, 1375, 196) 0 _ dropout_1 (Dropout) (None, 1375, 196) 0 _ gru_1 (GRU) (None, 1375, 128) 124800 _ dropout_2 (Dropout) (None, 1375, 128) 0 _ batch_normalization_2 (Batch (None, 1375, 128) 512 _ gru_2 (GRU) (None, 1375, 128) 98688 _ dropout_3 (Dropout) (None, 1375, 128) 0 _ batch_normalization_3 (Batch (None, 1375, 128) 512 _ dropout_4 (Dropout) (None, 1375, 128) 0 _ time_distributed_1 (TimeDist (None, 1375, 1) 129 ================================================================= Total params: 522,561 Trainable params: 521,657 Non-trainable params: 904 _Expected Output: Total params 522,561 Trainable params 521,657 Non-trainable params 904 The output of the network is of shape (None, 1375, 1) while the input is (None, 5511, 101). The Conv1D has reduced the number of steps from 5511 at spectrogram to 1375. 2.2 - Fit the modelTrigger word detection takes a long time to train. To save time, we’ve already trained a model for about 3 hours on a GPU using the architecture you built above, and a large training set of about 4000 examples. Let’s load the model. 1model = load_model('./models/tr_model.h5') You can train the model further, using the Adam optimizer and binary cross entropy loss, as follows. This will run quickly because we are training just for one epoch and with a small training set of 26 examples. 12opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)model.compile(loss='binary_crossentropy', optimizer=opt, metrics=["accuracy"]) 1model.fit(X, Y, batch_size = 5, epochs=1) Epoch 1/1 26/26 [==============================] - 23s - loss: 0.0727 - acc: 0.9806 &lt;keras.callbacks.History at 0x7efc4e3727f0&gt; 2.3 - Test the modelFinally, let’s see how your model performs on the dev set. 12loss, acc = model.evaluate(X_dev, Y_dev)print("Dev set accuracy = ", acc) 25/25 [==============================] - 4s Dev set accuracy = 0.946036338806 This looks pretty good! However, accuracy isn’t a great metric for this task, since the labels are heavily skewed to 0’s, so a neural network that just outputs 0’s would get slightly over 90% accuracy. We could define more useful metrics such as F1 score or Precision/Recall. But let’s not bother with that here, and instead just empirically see how the model does. 3 - Making PredictionsNow that you have built a working model for trigger word detection, let’s use it to make predictions. This code snippet runs audio (saved in a wav file) through the network. 1234567891011121314def detect_triggerword(filename): plt.subplot(2, 1, 1) x = graph_spectrogram(filename) # the spectogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model x = x.swapaxes(0,1) x = np.expand_dims(x, axis=0) predictions = model.predict(x) plt.subplot(2, 1, 2) plt.plot(predictions[0,:,0]) plt.ylabel('probability') plt.show() return predictions Once you’ve estimated the probability of having detected the word “activate” at each output step, you can trigger a “chiming” sound to play when the probability is above a certain threshold. Further, $y^{\langle t \rangle}$ might be near 1 for many values in a row after “activate” is said, yet we want to chime only once. So we will insert a chime sound at most once every 75 output steps. This will help prevent us from inserting two chimes for a single instance of “activate”. (This plays a role similar to non-max suppression from computer vision.) 12345678910111213141516171819chime_file = "audio_examples/chime.wav"def chime_on_activate(filename, predictions, threshold): audio_clip = AudioSegment.from_wav(filename) chime = AudioSegment.from_wav(chime_file) Ty = predictions.shape[1] # Step 1: Initialize the number of consecutive output steps to 0 consecutive_timesteps = 0 # Step 2: Loop over the output steps in the y for i in range(Ty): # Step 3: Increment consecutive output steps consecutive_timesteps += 1 # Step 4: If prediction is higher than the threshold and more than 75 consecutive output steps have passed if predictions[0,i,0] &gt; threshold and consecutive_timesteps &gt; 75: # Step 5: Superpose audio and background using pydub audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds)*1000) # Step 6: Reset consecutive output steps to 0 consecutive_timesteps = 0 audio_clip.export("chime_output.wav", format='wav') 3.3 - Test on dev examplesLet’s explore how our model performs on two unseen audio clips from the development set. Lets first listen to the two dev set clips. 1IPython.display.Audio("./raw_data/dev/1.wav") Your browser does not support the audio element. 1IPython.display.Audio("./raw_data/dev/2.wav") Your browser does not support the audio element. Now lets run the model on these audio clips and see if it adds a chime after “activate”! 1234filename = "./raw_data/dev/1.wav"prediction = detect_triggerword(filename)chime_on_activate(filename, prediction, 0.5)IPython.display.Audio("./chime_output.wav") Your browser does not support the audio element. 1234filename = "./raw_data/dev/2.wav"prediction = detect_triggerword(filename)chime_on_activate(filename, prediction, 0.5)IPython.display.Audio("./chime_output.wav") Your browser does not support the audio element. CongratulationsYou’ve come to the end of this assignment! Here’s what you should remember: Data synthesis is an effective way to create a large training set for speech problems, specifically trigger word detection. Using a spectrogram and optionally a 1D conv layer is a common pre-processing step prior to passing audio data to an RNN, GRU or LSTM. An end-to-end deep learning approach can be used to built a very effective trigger word detection system. Congratulations on finishing the fimal assignment! Thank you for sticking with us through the end and for all the hard work you’ve put into learning deep learning. We hope you have enjoyed the course! 4 - Try your own example! (OPTIONAL/UNGRADED)In this optional and ungraded portion of this notebook, you can try your model on your own audio clips! Record a 10 second audio clip of you saying the word “activate” and other random words, and upload it to the Coursera hub as myaudio.wav. Be sure to upload the audio as a wav file. If your audio is recorded in a different format (such as mp3) there is free software that you can find online for converting it to wav. If your audio recording is not 10 seconds, the code below will either trim or pad it as needed to make it 10 seconds. 12345678910# Preprocess the audio to the correct formatdef preprocess_audio(filename): # Trim or pad audio segment to 10000ms padding = AudioSegment.silent(duration=10000) segment = AudioSegment.from_wav(filename)[:10000] segment = padding.overlay(segment) # Set frame rate to 44100 segment = segment.set_frame_rate(44100) # Export as wav segment.export(filename, format='wav') Once you’ve uploaded your audio file to Coursera, put the path to your file in the variable below. 1your_filename = "audio_examples/my_audio.wav" 12preprocess_audio(your_filename)IPython.display.Audio(your_filename) # listen to the audio you uploaded Your browser does not support the audio element. Finally, use the model to predict when you say activate in the 10 second audio clip, and trigger a chime. If beeps are not being added appropriately, try to adjust the chime_threshold. 1234chime_threshold = 0.5prediction = detect_triggerword(your_filename)chime_on_activate(your_filename, prediction, chime_threshold)IPython.display.Audio("./chime_output.wav") Your browser does not support the audio element.]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural machine translation with attention]]></title>
    <url>%2F2018%2F06%2F05%2FNeural%2Bmachine%2Btranslation%2Bwith%2Battention%2B-%2Bv4%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 3rd week and the copyright belongs to deeplearning.ai. Neural Machine TranslationWelcome to your first programming assignment for this week! You will build a Neural Machine Translation (NMT) model to translate human readable dates (“25th of June, 2009”) into machine readable dates (“2009-06-25”). You will do this using an attention model, one of the most sophisticated sequence to sequence models. This notebook was produced together with NVIDIA’s Deep Learning Institute. Let’s load all the packages you will need for this assignment. 123456789101112131415from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiplyfrom keras.layers import RepeatVector, Dense, Activation, Lambdafrom keras.optimizers import Adamfrom keras.utils import to_categoricalfrom keras.models import load_model, Modelimport keras.backend as Kimport numpy as npfrom faker import Fakerimport randomfrom tqdm import tqdmfrom babel.dates import format_datefrom nmt_utils import *import matplotlib.pyplot as plt%matplotlib inline Using TensorFlow backend. 1 - Translating human readable dates into machine readable datesThe model you will build here could be used to translate from one language to another, such as translating from English to Hindi. However, language translation requires massive datasets and usually takes days of training on GPUs. To give you a place to experiment with these models even without using massive datasets, we will instead use a simpler “date translation” task. The network will input a date written in a variety of possible formats (e.g. “the 29th of August 1958”, “03/30/1968”, “24 JUNE 1987”) and translate them into standardized, machine readable dates (e.g. “1958-08-29”, “1968-03-30”, “1987-06-24”). We will have the network learn to output dates in the common machine-readable format YYYY-MM-DD. 1.1 - DatasetWe will train the model on a dataset of 10000 human readable dates and their equivalent, standardized, machine readable dates. Let’s run the following cells to load the dataset and print some examples. 12m = 10000dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m) 100%|██████████| 10000/10000 [00:01&lt;00:00, 8435.76it/s] 1dataset[:10] [(&apos;9 may 1998&apos;, &apos;1998-05-09&apos;), (&apos;10.09.70&apos;, &apos;1970-09-10&apos;), (&apos;4/28/90&apos;, &apos;1990-04-28&apos;), (&apos;thursday january 26 1995&apos;, &apos;1995-01-26&apos;), (&apos;monday march 7 1983&apos;, &apos;1983-03-07&apos;), (&apos;sunday may 22 1988&apos;, &apos;1988-05-22&apos;), (&apos;tuesday july 8 2008&apos;, &apos;2008-07-08&apos;), (&apos;08 sep 1999&apos;, &apos;1999-09-08&apos;), (&apos;1 jan 1981&apos;, &apos;1981-01-01&apos;), (&apos;monday may 22 1995&apos;, &apos;1995-05-22&apos;)] You’ve loaded: dataset: a list of tuples of (human readable date, machine readable date) human_vocab: a python dictionary mapping all characters used in the human readable dates to an integer-valued index machine_vocab: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. These indices are not necessarily consistent with human_vocab. inv_machine_vocab: the inverse dictionary of machine_vocab, mapping from indices back to characters. Let’s preprocess the data and map the raw text data into the index values. We will also use Tx=30 (which we assume is the maximum length of the human readable date; if we get a longer input, we would have to truncate it) and Ty=10 (since “YYYY-MM-DD” is 10 characters long). 12345678Tx = 30Ty = 10X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)print("X.shape:", X.shape)print("Y.shape:", Y.shape)print("Xoh.shape:", Xoh.shape)print("Yoh.shape:", Yoh.shape) X.shape: (10000, 30) Y.shape: (10000, 10) Xoh.shape: (10000, 30, 37) Yoh.shape: (10000, 10, 11) You now have: X: a processed version of the human readable dates in the training set, where each character is replaced by an index mapped to the character via human_vocab. Each date is further padded to $T_x$ values with a special character (&lt; pad &gt;). X.shape = (m, Tx) Y: a processed version of the machine readable dates in the training set, where each character is replaced by the index it is mapped to in machine_vocab. You should have Y.shape = (m, Ty). Xoh: one-hot version of X, the “1” entry’s index is mapped to the character thanks to human_vocab. Xoh.shape = (m, Tx, len(human_vocab)) Yoh: one-hot version of Y, the “1” entry’s index is mapped to the character thanks to machine_vocab. Yoh.shape = (m, Tx, len(machine_vocab)). Here, len(machine_vocab) = 11 since there are 11 characters (‘-‘ as well as 0-9). Lets also look at some examples of preprocessed training examples. Feel free to play with index in the cell below to navigate the dataset and see how source/target dates are preprocessed. 123456789index = 0print("Source date:", dataset[index][0])print("Target date:", dataset[index][1])print()print("Source after preprocessing (indices):", X[index])print("Target after preprocessing (indices):", Y[index])print()print("Source after preprocessing (one-hot):", Xoh[index])print("Target after preprocessing (one-hot):", Yoh[index]) Source date: 9 may 1998 Target date: 1998-05-09 Source after preprocessing (indices): [12 0 24 13 34 0 4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36] Target after preprocessing (indices): [ 2 10 10 9 0 1 6 0 1 10] Source after preprocessing (one-hot): [[ 0. 0. 0. ..., 0. 0. 0.] [ 1. 0. 0. ..., 0. 0. 0.] [ 0. 0. 0. ..., 0. 0. 0.] ..., [ 0. 0. 0. ..., 0. 0. 1.] [ 0. 0. 0. ..., 0. 0. 1.] [ 0. 0. 0. ..., 0. 0. 1.]] Target after preprocessing (one-hot): [[ 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [ 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [ 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 2 - Neural machine translation with attentionIf you had to translate a book’s paragraph from French to English, you would not read the whole paragraph, then close the book and translate. Even during the translation process, you would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English you are writing down. The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. 2.1 - Attention mechanismIn this part, you will implement the attention mechanism presented in the lecture videos. Here is a figure to remind you how the model works. The diagram on the left shows the attention model. The diagram on the right shows what one “Attention” step does to calculate the attention variables $\alpha^{\langle t, t’ \rangle}$, which are used to compute the context variable $context^{\langle t \rangle}$ for each timestep in the output ($t=1, \ldots, T_y$). Figure 1: Neural machine translation with attentionHere are some properties of the model that you may notice:1. There are two separate LSTMs in this model (see diagram on the left). Because the one at the bottom of the picture is a Bi-directional LSTM and comes before the attention mechanism, we will call it pre-attention Bi-LSTM. The LSTM at the top of the diagram comes after the attention mechanism, so we will call it the post-attention LSTM. The pre-attention Bi-LSTM goes through $T_x$ time steps; the post-attention LSTM goes through $T_y$ time steps.2. The post-attention LSTM passes $s^{\langle t \rangle}, c^{\langle t \rangle}$ from one time step to the next. In the lecture videos, we were using only a basic RNN for the post-activation sequence model, so the state captured by the RNN output activations $s^{\langle t\rangle}$. But since we are using an LSTM here, the LSTM has both the output activation $s^{\langle t\rangle}$ and the hidden cell state $c^{\langle t\rangle}$. However, unlike previous text generation examples (such as Dinosaurus in week 1), in this model the post-activation LSTM at time $t$ does will not take the specific generated $y^{\langle t-1 \rangle}$ as input; it only takes $s^{\langle t\rangle}$ and $c^{\langle t\rangle}$ as input. We have designed the model this way, because (unlike language generation where adjacent characters are highly correlated) there isn’t as strong a dependency between the previous character and the next character in a YYYY-MM-DD date.3. We use $a^{\langle t \rangle} = [\overrightarrow{a}^{\langle t \rangle}; \overleftarrow{a}^{\langle t \rangle}]$ to represent the concatenation of the activations of both the forward-direction and backward-directions of the pre-attention Bi-LSTM.4. The diagram on the right uses a RepeatVector node to copy $s^{\langle t-1 \rangle}$’s value $T_x$ times, and then Concatenation to concatenate $s^{\langle t-1 \rangle}$ and $a^{\langle t \rangle}$ to compute $e^{\langle t, t’}$, which is then passed through a softmax to compute $\alpha^{\langle t, t’ \rangle}$. We’ll explain how to use RepeatVector and Concatenation in Keras below.Lets implement this model. You will start by implementing two functions: one_step_attention() and model().1) one_step_attention(): At step $t$, given all the hidden states of the Bi-LSTM ($[a^{},a^{}, …, a^{&lt;T_x&gt;}]$) and the previous hidden state of the second LSTM ($s^{}$), one_step_attention() will compute the attention weights ($[\alpha^{&lt;t,1&gt;},\alpha^{&lt;t,2&gt;}, …, \alpha^{&lt;t,T_x&gt;}]$) and output the context vector (see Figure 1 (right) for details): $$context^{} = \sum_{t' = 0}^{T_x} \alpha^{}\tag{1}$$ Note that we are denoting the attention in this notebook $context^{\langle t \rangle}$. In the lecture videos, the context was denoted $c^{\langle t \rangle}$, but here we are calling it $context^{\langle t \rangle}$ to avoid confusion with the (post-attention) LSTM’s internal memory cell variable, which is sometimes also denoted $c^{\langle t \rangle}$.2) model(): Implements the entire model. It first runs the input through a Bi-LSTM to get back $[a^{},a^{}, …, a^{&lt;T_x&gt;}]$. Then, it calls one_step_attention() $T_y$ times (for loop). At each iteration of this loop, it gives the computed context vector $c^{}$ to the second LSTM, and runs the output of the LSTM through a dense layer with softmax activation to generate a prediction $\hat{y}^{}$.Exercise: Implement one_step_attention(). The function model() will call the layers in one_step_attention() $T_y$ using a for-loop, and it is important that all $T_y$ copies have the same weights. I.e., it should not re-initiaiize the weights every time. In other words, all $T_y$ steps should have shared weights. Here’s how you can implement layers with shareable weights in Keras:1. Define the layer objects (as global variables for examples).2. Call these objects when propagating the input.We have defined the layers you need as global variables. Please run the following cells to create them. Please check the Keras documentation to make sure you understand what these layers are: RepeatVector(), Concatenate(), Dense(), Activation(), Dot().1234567# Defined shared layers as global variablesrepeator = RepeatVector(Tx)concatenator = Concatenate(axis=-1)densor1 = Dense(10, activation = "tanh")densor2 = Dense(1, activation = "relu")activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebookdotor = Dot(axes = 1)Now you can use these layers to implement one_step_attention(). In order to propagate a Keras tensor object X through one of these layers, use layer(X) (or layer([X,Y]) if it requires multiple inputs.), e.g. densor(X) will propagate X through the Dense(1) layer defined above.12345678910111213141516171819202122232425262728293031# GRADED FUNCTION: one_step_attentiondef one_step_attention(a, s_prev): """ Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights "alphas" and the hidden states "a" of the Bi-LSTM. Arguments: a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a) s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s) Returns: context -- context vector, input of the next (post-attetion) LSTM cell """ ### START CODE HERE ### # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states "a" (≈ 1 line) s_prev = repeator(s_prev); # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line) concat = concatenator([a, s_prev]); # Use densor1 to propagate concat through a small fully-connected neural network to compute the "intermediate energies" variable e. (≈1 lines) e = densor1(concat); # Use densor2 to propagate e through a small fully-connected neural network to compute the "energies" variable energies. (≈1 lines) energies = densor2(e); # Use "activator" on "energies" to compute the attention weights "alphas" (≈ 1 line) alphas = activator(energies); # Use dotor together with "alphas" and "a" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line) context = dotor([alphas, a]); ### END CODE HERE ### return contextYou will be able to check the expected output of one_step_attention() after you’ve coded the model() function.Exercise: Implement model() as explained in figure 2 and the text above. Again, we have defined global layers that will share weights to be used in model().1234n_a = 32n_s = 64post_activation_LSTM_cell = LSTM(n_s, return_state = True)output_layer = Dense(len(machine_vocab), activation=softmax)Now you can use these layers $T_y$ times in a for loop to generate the outputs, and their parameters will not be reinitialized. You will have to carry out the following steps:1. Propagate the input into a Bidirectional LSTM2. Iterate for $t = 0, \dots, T_y-1$: 1. Call one_step_attention() on $[\alpha^{&lt;t,1&gt;},\alpha^{&lt;t,2&gt;}, …, \alpha^{&lt;t,T_x&gt;}]$ and $s^{}$ to get the context vector $context^{}$. 2. Give $context^{}$ to the post-attention LSTM cell. Remember pass in the previous hidden-state $s^{\langle t-1\rangle}$ and cell-states $c^{\langle t-1\rangle}$ of this LSTM using initial_state= [previous hidden state, previous cell state]. Get back the new hidden state $s^{}$ and the new cell state $c^{}$. 3. Apply a softmax layer to $s^{}$, get the output. 4. Save the output by adding it to the list of outputs.3. Create your Keras model instance, it should have three inputs (“inputs”, $s^{}$ and $c^{}$) and output the list of “outputs”.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# GRADED FUNCTION: modeldef model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size): """ Arguments: Tx -- length of the input sequence Ty -- length of the output sequence n_a -- hidden state size of the Bi-LSTM n_s -- hidden state size of the post-attention LSTM human_vocab_size -- size of the python dictionary "human_vocab" machine_vocab_size -- size of the python dictionary "machine_vocab" Returns: model -- Keras model instance """ # Define the inputs of your model with a shape (Tx,) # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,) X = Input(shape=(Tx, human_vocab_size)) s0 = Input(shape=(n_s,), name='s0') c0 = Input(shape=(n_s,), name='c0') s = s0 c = c0 # Initialize empty list of outputs outputs = [] ### START CODE HERE ### # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line) a = Bidirectional(LSTM(n_a, return_sequences = True))(X); # Step 2: Iterate for Ty steps for t in range(Ty): # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line) context = one_step_attention(a, s); # Step 2.B: Apply the post-attention LSTM cell to the "context" vector. # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line) s, _, c = post_activation_LSTM_cell(context, initial_state= [s, c]); # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line) out = output_layer(s); # Step 2.D: Append "out" to the "outputs" list (≈ 1 line) outputs.append(out); # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line) model = Model(inputs = [X, s0, c0], outputs = outputs); ### END CODE HERE ### return modelRun the following cell to create your model.1model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))Let’s get a summary of the model to check if it matches the expected output.1model.summary() ____ Layer (type) Output Shape Param # Connected to ==================================================================================================== input_1 (InputLayer) (None, 30, 37) 0 ____ s0 (InputLayer) (None, 64) 0 ____ bidirectional_1 (Bidirectional) (None, 30, 64) 17920 input_1[0][0] ____ repeat_vector_1 (RepeatVector) (None, 30, 64) 0 s0[0][0] lstm_1[0][0] lstm_1[1][0] lstm_1[2][0] lstm_1[3][0] lstm_1[4][0] lstm_1[5][0] lstm_1[6][0] lstm_1[7][0] lstm_1[8][0] ____ concatenate_1 (Concatenate) (None, 30, 128) 0 bidirectional_1[0][0] repeat_vector_1[0][0] bidirectional_1[0][0] repeat_vector_1[1][0] bidirectional_1[0][0] repeat_vector_1[2][0] bidirectional_1[0][0] repeat_vector_1[3][0] bidirectional_1[0][0] repeat_vector_1[4][0] bidirectional_1[0][0] repeat_vector_1[5][0] bidirectional_1[0][0] repeat_vector_1[6][0] bidirectional_1[0][0] repeat_vector_1[7][0] bidirectional_1[0][0] repeat_vector_1[8][0] bidirectional_1[0][0] repeat_vector_1[9][0] ____ dense_1 (Dense) (None, 30, 10) 1290 concatenate_1[0][0] concatenate_1[1][0] concatenate_1[2][0] concatenate_1[3][0] concatenate_1[4][0] concatenate_1[5][0] concatenate_1[6][0] concatenate_1[7][0] concatenate_1[8][0] concatenate_1[9][0] ____ dense_2 (Dense) (None, 30, 1) 11 dense_1[0][0] dense_1[1][0] dense_1[2][0] dense_1[3][0] dense_1[4][0] dense_1[5][0] dense_1[6][0] dense_1[7][0] dense_1[8][0] dense_1[9][0] ____ attention_weights (Activation) (None, 30, 1) 0 dense_2[0][0] dense_2[1][0] dense_2[2][0] dense_2[3][0] dense_2[4][0] dense_2[5][0] dense_2[6][0] dense_2[7][0] dense_2[8][0] dense_2[9][0] ____ dot_1 (Dot) (None, 1, 64) 0 attention_weights[0][0] bidirectional_1[0][0] attention_weights[1][0] bidirectional_1[0][0] attention_weights[2][0] bidirectional_1[0][0] attention_weights[3][0] bidirectional_1[0][0] attention_weights[4][0] bidirectional_1[0][0] attention_weights[5][0] bidirectional_1[0][0] attention_weights[6][0] bidirectional_1[0][0] attention_weights[7][0] bidirectional_1[0][0] attention_weights[8][0] bidirectional_1[0][0] attention_weights[9][0] bidirectional_1[0][0] ____ c0 (InputLayer) (None, 64) 0 ____ lstm_1 (LSTM) [(None, 64), (None, 6 33024 dot_1[0][0] s0[0][0] c0[0][0] dot_1[1][0] lstm_1[0][0] lstm_1[0][2] dot_1[2][0] lstm_1[1][0] lstm_1[1][2] dot_1[3][0] lstm_1[2][0] lstm_1[2][2] dot_1[4][0] lstm_1[3][0] lstm_1[3][2] dot_1[5][0] lstm_1[4][0] lstm_1[4][2] dot_1[6][0] lstm_1[5][0] lstm_1[5][2] dot_1[7][0] lstm_1[6][0] lstm_1[6][2] dot_1[8][0] lstm_1[7][0] lstm_1[7][2] dot_1[9][0] lstm_1[8][0] lstm_1[8][2] ____ dense_3 (Dense) (None, 11) 715 lstm_1[0][0] lstm_1[1][0] lstm_1[2][0] lstm_1[3][0] lstm_1[4][0] lstm_1[5][0] lstm_1[6][0] lstm_1[7][0] lstm_1[8][0] lstm_1[9][0] ==================================================================================================== Total params: 52,960 Trainable params: 52,960 Non-trainable params: 0 ____Expected Output:Here is the summary you should see Total params: 52,960 Trainable params: 52,960 Non-trainable params: 0 bidirectional_1’s output shape (None, 30, 64) repeat_vector_1’s output shape (None, 30, 64) concatenate_1’s output shape (None, 30, 128) attention_weights’s output shape (None, 30, 1) dot_1’s output shape (None, 1, 64) dense_3’s output shape (None, 11) As usual, after creating your model in Keras, you need to compile it and define what loss, optimizer and metrics your are want to use. Compile your model using categorical_crossentropy loss, a custom Adam optimizer (learning rate = 0.005, $\beta_1 = 0.9$, $\beta_2 = 0.999$, decay = 0.01) and [&#39;accuracy&#39;] metrics: 1234### START CODE HERE ### (≈2 lines)opt = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, decay=0.01);model.compile(loss = 'categorical_crossentropy',optimizer=opt, metrics = ['accuracy']);### END CODE HERE ### The last step is to define all your inputs and outputs to fit the model: You already have X of shape $(m = 10000, T_x = 30)$ containing the training examples. You need to create s0 and c0 to initialize your post_activation_LSTM_cell with 0s. Given the model() you coded, you need the “outputs” to be a list of 11 elements of shape (m, T_y). So that: outputs[i][0], ..., outputs[i][Ty] represent the true labels (characters) corresponding to the $i^{th}$ training example (X[i]). More generally, outputs[i][j] is the true label of the $j^{th}$ character in the $i^{th}$ training example. 123s0 = np.zeros((m, n_s))c0 = np.zeros((m, n_s))outputs = list(Yoh.swapaxes(0,1)) Let’s now fit the model and run it for one epoch. 1model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100) Epoch 1/1 10000/10000 [==============================] - 31s - loss: 22.1424 - dense_3_loss_1: 2.3001 - dense_3_loss_2: 2.2528 - dense_3_loss_3: 2.3607 - dense_3_loss_4: 2.5894 - dense_3_loss_5: 1.6743 - dense_3_loss_6: 1.9239 - dense_3_loss_7: 2.6330 - dense_3_loss_8: 1.5383 - dense_3_loss_9: 2.0970 - dense_3_loss_10: 2.7730 - dense_3_acc_1: 0.0035 - dense_3_acc_2: 0.0309 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 0.0045 - dense_3_acc_5: 0.9581 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 0.0027 - dense_3_acc_8: 0.9599 - dense_3_acc_9: 0.0051 - dense_3_acc_10: 0.0088 &lt;keras.callbacks.History at 0x7f8fbd556f60&gt; While training you can see the loss as well as the accuracy on each of the 10 positions of the output. The table below gives you an example of what the accuracies could be if the batch had 2 examples: Thus, dense_2_acc_8: 0.89 means that you are predicting the 7th character of the output correctly 89% of the time in the current batch of data. We have run this model for longer, and saved the weights. Run the next cell to load our weights. (By training a model for several minutes, you should be able to obtain a model of similar accuracy, but loading our model will save you time.) 1model.load_weights('models/model.h5') You can now see the results on new examples. 1234567891011EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']for example in EXAMPLES: source = string_to_int(example, Tx, human_vocab) source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1) prediction = model.predict([source, s0, c0]) prediction = np.argmax(prediction, axis = -1) output = [inv_machine_vocab[int(i)] for i in prediction] print("source:", example) print("output:", ''.join(output)) source: 3 May 1979 output: 1979-05-03 source: 5 April 09 output: 2009-05-05 source: 21th of August 2016 output: 2016-08-21 source: Tue 10 Jul 2007 output: 2007-07-10 source: Saturday May 9 2018 output: 2018-05-09 source: March 3 2001 output: 2001-03-03 source: March 3rd 2001 output: 2001-03-03 source: 1 March 2001 output: 2001-03-01 You can also change these examples to test with your own examples. The next part will give you a better sense on what the attention mechanism is doing–i.e., what part of the input the network is paying attention to when generating a particular output character. 3 - Visualizing Attention (Optional / Ungraded)Since the problem has a fixed output length of 10, it is also possible to carry out this task using 10 different softmax units to generate the 10 characters of the output. But one advantage of the attention model is that each part of the output (say the month) knows it needs to depend only on a small part of the input (the characters in the input giving the month). We can visualize what part of the output is looking at what part of the input. Consider the task of translating “Saturday 9 May 2018” to “2018-05-09”. If we visualize the computed $\alpha^{\langle t, t’ \rangle}$ we get this: Figure 8: Full Attention Map Notice how the output ignores the “Saturday” portion of the input. None of the output timesteps are paying much attention to that portion of the input. We see also that 9 has been translated as 09 and May has been correctly translated into 05, with the output paying attention to the parts of the input it needs to to make the translation. The year mostly requires it to pay attention to the input’s “18” in order to generate “2018.” 3.1 - Getting the activations from the networkLets now visualize the attention values in your network. We’ll propagate an example through the network, then visualize the values of $\alpha^{\langle t, t’ \rangle}$. To figure out where the attention values are located, let’s start by printing a summary of the model . 1model.summary() ____________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ==================================================================================================== input_1 (InputLayer) (None, 30, 37) 0 ____________________________________________________________________________________________________ s0 (InputLayer) (None, 64) 0 ____________________________________________________________________________________________________ bidirectional_1 (Bidirectional) (None, 30, 64) 17920 input_1[0][0] ____________________________________________________________________________________________________ repeat_vector_1 (RepeatVector) (None, 30, 64) 0 s0[0][0] lstm_1[0][0] lstm_1[1][0] lstm_1[2][0] lstm_1[3][0] lstm_1[4][0] lstm_1[5][0] lstm_1[6][0] lstm_1[7][0] lstm_1[8][0] ____________________________________________________________________________________________________ concatenate_1 (Concatenate) (None, 30, 128) 0 bidirectional_1[0][0] repeat_vector_1[0][0] bidirectional_1[0][0] repeat_vector_1[1][0] bidirectional_1[0][0] repeat_vector_1[2][0] bidirectional_1[0][0] repeat_vector_1[3][0] bidirectional_1[0][0] repeat_vector_1[4][0] bidirectional_1[0][0] repeat_vector_1[5][0] bidirectional_1[0][0] repeat_vector_1[6][0] bidirectional_1[0][0] repeat_vector_1[7][0] bidirectional_1[0][0] repeat_vector_1[8][0] bidirectional_1[0][0] repeat_vector_1[9][0] ____________________________________________________________________________________________________ dense_1 (Dense) (None, 30, 10) 1290 concatenate_1[0][0] concatenate_1[1][0] concatenate_1[2][0] concatenate_1[3][0] concatenate_1[4][0] concatenate_1[5][0] concatenate_1[6][0] concatenate_1[7][0] concatenate_1[8][0] concatenate_1[9][0] ____________________________________________________________________________________________________ dense_2 (Dense) (None, 30, 1) 11 dense_1[0][0] dense_1[1][0] dense_1[2][0] dense_1[3][0] dense_1[4][0] dense_1[5][0] dense_1[6][0] dense_1[7][0] dense_1[8][0] dense_1[9][0] ____________________________________________________________________________________________________ attention_weights (Activation) (None, 30, 1) 0 dense_2[0][0] dense_2[1][0] dense_2[2][0] dense_2[3][0] dense_2[4][0] dense_2[5][0] dense_2[6][0] dense_2[7][0] dense_2[8][0] dense_2[9][0] ____________________________________________________________________________________________________ dot_1 (Dot) (None, 1, 64) 0 attention_weights[0][0] bidirectional_1[0][0] attention_weights[1][0] bidirectional_1[0][0] attention_weights[2][0] bidirectional_1[0][0] attention_weights[3][0] bidirectional_1[0][0] attention_weights[4][0] bidirectional_1[0][0] attention_weights[5][0] bidirectional_1[0][0] attention_weights[6][0] bidirectional_1[0][0] attention_weights[7][0] bidirectional_1[0][0] attention_weights[8][0] bidirectional_1[0][0] attention_weights[9][0] bidirectional_1[0][0] ____________________________________________________________________________________________________ c0 (InputLayer) (None, 64) 0 ____________________________________________________________________________________________________ lstm_1 (LSTM) [(None, 64), (None, 6 33024 dot_1[0][0] s0[0][0] c0[0][0] dot_1[1][0] lstm_1[0][0] lstm_1[0][2] dot_1[2][0] lstm_1[1][0] lstm_1[1][2] dot_1[3][0] lstm_1[2][0] lstm_1[2][2] dot_1[4][0] lstm_1[3][0] lstm_1[3][2] dot_1[5][0] lstm_1[4][0] lstm_1[4][2] dot_1[6][0] lstm_1[5][0] lstm_1[5][2] dot_1[7][0] lstm_1[6][0] lstm_1[6][2] dot_1[8][0] lstm_1[7][0] lstm_1[7][2] dot_1[9][0] lstm_1[8][0] lstm_1[8][2] ____________________________________________________________________________________________________ dense_3 (Dense) (None, 11) 715 lstm_1[0][0] lstm_1[1][0] lstm_1[2][0] lstm_1[3][0] lstm_1[4][0] lstm_1[5][0] lstm_1[6][0] lstm_1[7][0] lstm_1[8][0] lstm_1[9][0] ==================================================================================================== Total params: 52,960 Trainable params: 52,960 Non-trainable params: 0 ____________________________________________________________________________________________________ Navigate through the output of model.summary() above. You can see that the layer named attention_weights outputs the alphas of shape (m, 30, 1) before dot_2 computes the context vector for every time step $t = 0, \ldots, T_y-1$. Lets get the activations from this layer. The function attention_map() pulls out the attention values from your model and plots them. 1attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, "Tuesday 09 Oct 1993", num = 7, n_s = 64) &lt;matplotlib.figure.Figure at 0x7f8fcf43c748&gt; On the generated plot you can observe the values of the attention weights for each character of the predicted output. Examine this plot and check that where the network is paying attention makes sense to you. In the date translation application, you will observe that most of the time attention helps predict the year, and hasn’t much impact on predicting the day/month. Congratulations!You have come to the end of this assignment Here’s what you should remember from this notebook: Machine translation models can be used to map from one sequence to another. They are useful not just for translating human languages (like French-&gt;English) but also for tasks like date format translation. An attention mechanism allows a network to focus on the most relevant parts of the input when producing a specific part of the output. A network using an attention mechanism can translate from inputs of length $T_x$ to outputs of length $T_y$, where $T_x$ and $T_y$ can be different. You can visualize attention weights $\alpha^{\langle t,t’ \rangle}$ to see what the network is paying attention to while generating each output. Congratulations on finishing this assignment! You are now able to implement an attention model and use it to learn complex mappings from one sequence to another.]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Emojify]]></title>
    <url>%2F2018%2F06%2F03%2FEmojify%2B-%2Bv2%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 2nd week and the copyright belongs to deeplearning.ai. Emojify!Welcome to the second assignment of Week 2. You are going to use word vector representations to build an Emojifier. Have you ever wanted to make your text messages more expressive? Your emojifier app will help you do that. So rather than writing “Congratulations on the promotion! Lets get coffee and talk. Love you!” the emojifier can automatically turn this into “Congratulations on the promotion! 👍 Lets get coffee and talk. ☕️ Love you! ❤️” You will implement a model which inputs a sentence (such as “Let’s go see the baseball game tonight!”) and finds the most appropriate emoji to be used with this sentence (⚾️). In many emoji interfaces, you need to remember that ❤️ is the “heart” symbol rather than the “love” symbol. But using word vectors, you’ll see that even if your training set explicitly relates only a few words to a particular emoji, your algorithm will be able to generalize and associate words in the test set to the same emoji even if those words don’t even appear in the training set. This allows you to build an accurate classifier mapping from sentences to emojis, even using a small training set. In this exercise, you’ll start with a baseline model (Emojifier-V1) using word embeddings, then build a more sophisticated model (Emojifier-V2) that further incorporates an LSTM. Lets get started! Run the following cell to load the package you are going to use. 123456import numpy as npfrom emo_utils import *import emojiimport matplotlib.pyplot as plt%matplotlib inline 1 - Baseline model: Emojifier-V11.1 - Dataset EMOJISETLet’s start by building a simple baseline classifier. You have a tiny dataset (X, Y) where: X contains 127 sentences (strings) Y contains a integer label between 0 and 4 corresponding to an emoji for each sentence Figure 1: EMOJISET - a classification problem with 5 classes. A few examples of sentences are given here. Let’s load the dataset using the code below. We split the dataset between training (127 examples) and testing (56 examples). 12X_train, Y_train = read_csv('data/train_emoji.csv')X_test, Y_test = read_csv('data/tesss.csv') 1maxLen = len(max(X_train, key=len).split()) Run the following cell to print sentences from X_train and corresponding labels from Y_train. Change index to see different examples. Because of the font the iPython notebook uses, the heart emoji may be colored black rather than red. 12index = 1print(X_train[index], label_to_emoji(Y_train[index])) I am proud of your achievements 😄 1.2 - Overview of the Emojifier-V1In this part, you are going to implement a baseline model called “Emojifier-v1”. Figure 2: Baseline model (Emojifier-V1). The input of the model is a string corresponding to a sentence (e.g. “I love you). In the code, the output will be a probability vector of shape (1,5), that you then pass in an argmax layer to extract the index of the most likely emoji output. To get our labels into a format suitable for training a softmax classifier, lets convert $Y$ from its current shape current shape $(m, 1)$ into a “one-hot representation” $(m, 5)$, where each row is a one-hot vector giving the label of one example, You can do so using this next code snipper. Here, Y_oh stands for “Y-one-hot” in the variable names Y_oh_train and Y_oh_test: 12Y_oh_train = convert_to_one_hot(Y_train, C = 5)Y_oh_test = convert_to_one_hot(Y_test, C = 5) Let’s see what convert_to_one_hot() did. Feel free to change index to print out different values. 12index = 50print(Y_train[index], "is converted into one hot", Y_oh_train[index]) 0 is converted into one hot [ 1. 0. 0. 0. 0.] All the data is now ready to be fed into the Emojify-V1 model. Let’s implement the model! 1.3 - Implementing Emojifier-V1As shown in Figure (2), the first step is to convert an input sentence into the word vector representation, which then get averaged together. Similar to the previous exercise, we will use pretrained 50-dimensional GloVe embeddings. Run the following cell to load the word_to_vec_map, which contains all the vector representations. 1word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt') You’ve loaded: word_to_index: dictionary mapping from words to their indices in the vocabulary (400,001 words, with the valid indices ranging from 0 to 400,000) index_to_word: dictionary mapping from indices to their corresponding words in the vocabulary word_to_vec_map: dictionary mapping words to their GloVe vector representation. Run the following cell to check if it works. 1234word = "cucumber"index = 289846print("the index of", word, "in the vocabulary is", word_to_index[word])print("the", str(index) + "th word in the vocabulary is", index_to_word[index]) the index of cucumber in the vocabulary is 113317 the 289846th word in the vocabulary is potatos Exercise: Implement sentence_to_avg(). You will need to carry out two steps: Convert every sentence to lower-case, then split the sentence into a list of words. X.lower() and X.split() might be useful. For each word in the sentence, access its GloVe representation. Then, average all these values. 123456789101112131415161718192021222324252627282930# GRADED FUNCTION: sentence_to_avgdef sentence_to_avg(sentence, word_to_vec_map): """ Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word and averages its value into a single vector encoding the meaning of the sentence. Arguments: sentence -- string, one training example from X word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation Returns: avg -- average vector encoding information about the sentence, numpy-array of shape (50,) """ ### START CODE HERE ### # Step 1: Split sentence into list of lower case words (≈ 1 line) words = sentence.lower().split(); # Initialize the average word vector, should have the same shape as your word vectors. avg = np.zeros((word_to_vec_map[words[0]].shape)); # Step 2: average the word vectors. You can loop over the words in the list "words". for w in words: avg += word_to_vec_map[w]; avg = avg / len(words); ### END CODE HERE ### return avg 12avg = sentence_to_avg("Morrocan couscous is my favorite dish", word_to_vec_map)print("avg = ", avg) avg = [-0.008005 0.56370833 -0.50427333 0.258865 0.55131103 0.03104983 -0.21013718 0.16893933 -0.09590267 0.141784 -0.15708967 0.18525867 0.6495785 0.38371117 0.21102167 0.11301667 0.02613967 0.26037767 0.05820667 -0.01578167 -0.12078833 -0.02471267 0.4128455 0.5152061 0.38756167 -0.898661 -0.535145 0.33501167 0.68806933 -0.2156265 1.797155 0.10476933 -0.36775333 0.750785 0.10282583 0.348925 -0.27262833 0.66768 -0.10706167 -0.283635 0.59580117 0.28747333 -0.3366635 0.23393817 0.34349183 0.178405 0.1166155 -0.076433 0.1445417 0.09808667] Expected Output: avg= [-0.008005 0.56370833 -0.50427333 0.258865 0.55131103 0.03104983 -0.21013718 0.16893933 -0.09590267 0.141784 -0.15708967 0.18525867 0.6495785 0.38371117 0.21102167 0.11301667 0.02613967 0.26037767 0.05820667 -0.01578167 -0.12078833 -0.02471267 0.4128455 0.5152061 0.38756167 -0.898661 -0.535145 0.33501167 0.68806933 -0.2156265 1.797155 0.10476933 -0.36775333 0.750785 0.10282583 0.348925 -0.27262833 0.66768 -0.10706167 -0.283635 0.59580117 0.28747333 -0.3366635 0.23393817 0.34349183 0.178405 0.1166155 -0.076433 0.1445417 0.09808667] ModelYou now have all the pieces to finish implementing the model() function. After using sentence_to_avg() you need to pass the average through forward propagation, compute the cost, and then backpropagate to update the softmax’s parameters. Exercise: Implement the model() function described in Figure (2). Assuming here that $Yoh$ (“Y one hot”) is the one-hot encoding of the output labels, the equations you need to implement in the forward pass and to compute the cross-entropy cost are:$$ z^{(i)} = W . avg^{(i)} + b$$$$ a^{(i)} = softmax(z^{(i)})$$$$ \mathcal{L}^{(i)} = - \sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)$$ It is possible to come up with a more efficient vectorized implementation. But since we are using a for-loop to convert the sentences one at a time into the avg^{(i)} representation anyway, let’s not bother this time. We provided you a function softmax(). 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# GRADED FUNCTION: modeldef model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400): """ Model to train word vector representations in numpy. Arguments: X -- input data, numpy array of sentences as strings, of shape (m, 1) Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1) word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation learning_rate -- learning_rate for the stochastic gradient descent algorithm num_iterations -- number of iterations Returns: pred -- vector of predictions, numpy-array of shape (m, 1) W -- weight matrix of the softmax layer, of shape (n_y, n_h) b -- bias of the softmax layer, of shape (n_y,) """ np.random.seed(1) # Define number of training examples m = Y.shape[0] # number of training examples n_y = 5 # number of classes n_h = 50 # dimensions of the GloVe vectors # Initialize parameters using Xavier initialization W = np.random.randn(n_y, n_h) / np.sqrt(n_h) b = np.zeros((n_y,)) # Convert Y to Y_onehot with n_y classes Y_oh = convert_to_one_hot(Y, C = n_y) # Optimization loop for t in range(num_iterations): # Loop over the number of iterations for i in range(m): # Loop over the training examples ### START CODE HERE ### (≈ 4 lines of code) # Average the word vectors of the words from the i'th training example avg = sentence_to_avg(X[i], word_to_vec_map); # Forward propagate the avg through the softmax layer z = np.dot(W, avg) + b; a = softmax(z); # Compute cost using the i'th training label's one hot representation and "A" (the output of the softmax) cost = np.sum(-Y_oh[i] * np.log(a)); ### END CODE HERE ### # Compute gradients dz = a - Y_oh[i] dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h)) db = dz # Update parameters with Stochastic Gradient Descent W = W - learning_rate * dW b = b - learning_rate * db if t % 100 == 0: print("Epoch: " + str(t) + " --- cost = " + str(cost)) pred = predict(X, Y, W, b, word_to_vec_map) return pred, W, b 123456789101112131415161718192021print(X_train.shape)print(Y_train.shape)print(np.eye(5)[Y_train.reshape(-1)].shape)print(X_train[0])print(type(X_train))Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])print(Y.shape)X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear', 'Lets go party and drinks','Congrats on the new job','Congratulations', 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you', 'You totally deserve this prize', 'Let us go play football', 'Are you down for football this afternoon', 'Work hard play harder', 'It is suprising how people can be dumb sometimes', 'I am very disappointed','It is the best day in my life', 'I think I will end up alone','My life is so boring','Good job', 'Great so awesome'])print(X.shape)print(np.eye(5)[Y_train.reshape(-1)].shape)print(type(X_train)) (132,) (132,) (132, 5) never talk to me again &lt;class &apos;numpy.ndarray&apos;&gt; (20,) (20,) (132, 5) &lt;class &apos;numpy.ndarray&apos;&gt; Run the next cell to train your model and learn the softmax parameters (W,b). 12pred, W, b = model(X_train, Y_train, word_to_vec_map)print(pred) Epoch: 0 --- cost = 1.95204988128 Accuracy: 0.348484848485 Epoch: 100 --- cost = 0.0797181872601 Accuracy: 0.931818181818 Epoch: 200 --- cost = 0.0445636924368 Accuracy: 0.954545454545 Epoch: 300 --- cost = 0.0343226737879 Accuracy: 0.969696969697 [[ 3.] [ 2.] [ 3.] [ 0.] [ 4.] [ 0.] [ 3.] [ 2.] [ 3.] [ 1.] [ 3.] [ 3.] [ 1.] [ 3.] [ 2.] [ 3.] [ 2.] [ 3.] [ 1.] [ 2.] [ 3.] [ 0.] [ 2.] [ 2.] [ 2.] [ 1.] [ 4.] [ 3.] [ 3.] [ 4.] [ 0.] [ 3.] [ 4.] [ 2.] [ 0.] [ 3.] [ 2.] [ 2.] [ 3.] [ 4.] [ 2.] [ 2.] [ 0.] [ 2.] [ 3.] [ 0.] [ 3.] [ 2.] [ 4.] [ 3.] [ 0.] [ 3.] [ 3.] [ 3.] [ 4.] [ 2.] [ 1.] [ 1.] [ 1.] [ 2.] [ 3.] [ 1.] [ 0.] [ 0.] [ 0.] [ 3.] [ 4.] [ 4.] [ 2.] [ 2.] [ 1.] [ 2.] [ 0.] [ 3.] [ 2.] [ 2.] [ 0.] [ 3.] [ 3.] [ 1.] [ 2.] [ 1.] [ 2.] [ 2.] [ 4.] [ 3.] [ 3.] [ 2.] [ 4.] [ 0.] [ 0.] [ 3.] [ 3.] [ 3.] [ 3.] [ 2.] [ 0.] [ 1.] [ 2.] [ 3.] [ 0.] [ 2.] [ 2.] [ 2.] [ 3.] [ 2.] [ 2.] [ 2.] [ 4.] [ 1.] [ 1.] [ 3.] [ 3.] [ 4.] [ 1.] [ 2.] [ 1.] [ 1.] [ 3.] [ 1.] [ 0.] [ 4.] [ 0.] [ 3.] [ 3.] [ 4.] [ 4.] [ 1.] [ 4.] [ 3.] [ 0.] [ 2.]] Expected Output (on a subset of iterations): Epoch: 0 cost = 1.95204988128 Accuracy: 0.348484848485 Epoch: 100 cost = 0.0797181872601 Accuracy: 0.931818181818 Epoch: 200 cost = 0.0445636924368 Accuracy: 0.954545454545 Epoch: 300 cost = 0.0343226737879 Accuracy: 0.969696969697 Great! Your model has pretty high accuracy on the training set. Lets now see how it does on the test set. 1.4 - Examining test set performance1234print("Training set:")pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)print('Test set:')pred_test = predict(X_test, Y_test, W, b, word_to_vec_map) Training set: Accuracy: 0.977272727273 Test set: Accuracy: 0.857142857143 Expected Output: Train set accuracy 97.7 Test set accuracy 85.7 Random guessing would have had 20% accuracy given that there are 5 classes. This is pretty good performance after training on only 127 examples. In the training set, the algorithm saw the sentence “I love you“ with the label ❤️. You can check however that the word “adore” does not appear in the training set. Nonetheless, lets see what happens if you write “I adore you.” 12345X_my_sentences = np.array(["i adore you", "i love you", "funny lol", "lets play with a ball", "food is ready", "not feeling happy"])Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)print_predictions(X_my_sentences, pred) Accuracy: 0.833333333333 i adore you ❤️ i love you ❤️ funny lol 😄 lets play with a ball ⚾ food is ready 🍴 not feeling happy 😄 Amazing! Because adore has a similar embedding as love, the algorithm has generalized correctly even to a word it has never seen before. Words such as heart, dear, beloved or adore have embedding vectors similar to love, and so might work too—feel free to modify the inputs above and try out a variety of input sentences. How well does it work? Note though that it doesn’t get “not feeling happy” correct. This algorithm ignores word ordering, so is not good at understanding phrases like “not happy.” Printing the confusion matrix can also help understand which classes are more difficult for your model. A confusion matrix shows how often an example whose label is one class (“actual” class) is mislabeled by the algorithm with a different class (“predicted” class). 1234print(Y_test.shape)print(' '+ label_to_emoji(0)+ ' ' + label_to_emoji(1) + ' ' + label_to_emoji(2)+ ' ' + label_to_emoji(3)+' ' + label_to_emoji(4))print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))plot_confusion_matrix(Y_test, pred_test) (56,) ❤️ ⚾ 😄 😞 🍴 Predicted 0.0 1.0 2.0 3.0 4.0 All Actual 0 6 0 0 1 0 7 1 0 8 0 0 0 8 2 2 0 16 0 0 18 3 1 1 2 12 0 16 4 0 0 1 0 6 7 All 9 9 19 13 6 56 What you should remember from this part: Even with a 127 training examples, you can get a reasonably good model for Emojifying. This is due to the generalization power word vectors gives you. Emojify-V1 will perform poorly on sentences such as “This movie is not good and not enjoyable” because it doesn’t understand combinations of words–it just averages all the words’ embedding vectors together, without paying attention to the ordering of words. You will build a better algorithm in the next part. 2 - Emojifier-V2: Using LSTMs in Keras:Let’s build an LSTM model that takes as input word sequences. This model will be able to take word ordering into account. Emojifier-V2 will continue to use pre-trained word embeddings to represent words, but will feed them into an LSTM, whose job it is to predict the most appropriate emoji. Run the following cell to load the Keras packages. 12345678import numpy as npnp.random.seed(0)from keras.models import Modelfrom keras.layers import Dense, Input, Dropout, LSTM, Activationfrom keras.layers.embeddings import Embeddingfrom keras.preprocessing import sequencefrom keras.initializers import glorot_uniformnp.random.seed(1) Using TensorFlow backend. 2.1 - Overview of the modelHere is the Emojifier-v2 you will implement: Figure 3: Emojifier-V2. A 2-layer LSTM sequence classifier. 2.2 Keras and mini-batchingIn this exercise, we want to train Keras using mini-batches. However, most deep learning frameworks require that all sequences in the same mini-batch have the same length. This is what allows vectorization to work: If you had a 3-word sentence and a 4-word sentence, then the computations needed for them are different (one takes 3 steps of an LSTM, one takes 4 steps) so it’s just not possible to do them both at the same time. The common solution to this is to use padding. Specifically, set a maximum sequence length, and pad all sequences to the same length. For example, of the maximum sequence length is 20, we could pad every sentence with “0”s so that each input sentence is of length 20. Thus, a sentence “i love you” would be represented as $(e_{i}, e_{love}, e_{you}, \vec{0}, \vec{0}, \ldots, \vec{0})$. In this example, any sentences longer than 20 words would have to be truncated. One simple way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set. 2.3 - The Embedding layerIn Keras, the embedding matrix is represented as a “layer”, and maps positive integers (indices corresponding to words) into dense vectors of fixed size (the embedding vectors). It can be trained or initialized with a pretrained embedding. In this part, you will learn how to create an Embedding() layer in Keras, initialize it with the GloVe 50-dimensional vectors loaded earlier in the notebook. Because our training set is quite small, we will not update the word embeddings but will instead leave their values fixed. But in the code below, we’ll show you how Keras allows you to either train or leave fixed this layer. The Embedding() layer takes an integer matrix of size (batch size, max input length) as input. This corresponds to sentences converted into lists of indices (integers), as shown in the figure below. Figure 4: Embedding layer. This example shows the propagation of two examples through the embedding layer. Both have been zero-padded to a length of max_len=5. The final dimension of the representation is (2,max_len,50) because the word embeddings we are using are 50 dimensional. The largest integer (i.e. word index) in the input should be no larger than the vocabulary size. The layer outputs an array of shape (batch size, max input length, dimension of word vectors). The first step is to convert all your training sentences into lists of indices, and then zero-pad all these lists so that their length is the length of the longest sentence. Exercise: Implement the function below to convert X (array of sentences as strings) into an array of indices corresponding to words in the sentences. The output shape should be such that it can be given to Embedding() (described in Figure 4). 12345678910111213141516171819202122232425262728293031323334353637383940# GRADED FUNCTION: sentences_to_indicesdef sentences_to_indices(X, word_to_index, max_len): """ Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences. The output shape should be such that it can be given to `Embedding()` (described in Figure 4). Arguments: X -- array of sentences (strings), of shape (m, 1) word_to_index -- a dictionary containing the each word mapped to its index max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. Returns: X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len) """ m = X.shape[0] # number of training examples ### START CODE HERE ### # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line) X_indices = np.zeros((m, max_len)); for i in range(m): # loop over training examples # Convert the ith training sentence in lower case and split is into words. You should get a list of words. sentence_words = X[i].lower().split(); # Initialize j to 0 j = 0 # Loop over the words of sentence_words for w in sentence_words: # Set the (i,j)th entry of X_indices to the index of the correct word. X_indices[i, j] = word_to_index[w]; # Increment j to j + 1 j = j + 1; ### END CODE HERE ### return X_indices Run the following cell to check what sentences_to_indices() does, and check your results. 1234X1 = np.array(["funny lol", "lets play baseball", "food is ready for you"])X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)print("X1 =", X1)print("X1_indices =", X1_indices) X1 = [&apos;funny lol&apos; &apos;lets play baseball&apos; &apos;food is ready for you&apos;] X1_indices = [[ 155345. 225122. 0. 0. 0.] [ 220930. 286375. 69714. 0. 0.] [ 151204. 192973. 302254. 151349. 394475.]] Expected Output: X1 = [‘funny lol’ ‘lets play football’ ‘food is ready for you’] X1_indices = [[ 155345. 225122. 0. 0. 0.] [ 220930. 286375. 151266. 0. 0.] [ 151204. 192973. 302254. 151349. 394475.]] Let’s build the Embedding() layer in Keras, using pre-trained word vectors. After this layer is built, you will pass the output of sentences_to_indices() to it as an input, and the Embedding() layer will return the word embeddings for a sentence. Exercise: Implement pretrained_embedding_layer(). You will need to carry out the following steps: Initialize the embedding matrix as a numpy array of zeroes with the correct shape. Fill in the embedding matrix with all the word embeddings extracted from word_to_vec_map. Define Keras embedding layer. Use Embedding(). Be sure to make this layer non-trainable, by setting trainable = False when calling Embedding(). If you were to set trainable = True, then it will allow the optimization algorithm to modify the values of the word embeddings. Set the embedding weights to be equal to the embedding matrix 123456789101112131415161718192021222324252627282930313233343536# GRADED FUNCTION: pretrained_embedding_layerdef pretrained_embedding_layer(word_to_vec_map, word_to_index): """ Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors. Arguments: word_to_vec_map -- dictionary mapping words to their GloVe vector representation. word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words) Returns: embedding_layer -- pretrained layer Keras instance """ vocab_len = len(word_to_index) + 1 # adding 1 to fit Keras embedding (requirement) emb_dim = word_to_vec_map["cucumber"].shape[0] # define dimensionality of your GloVe word vectors (= 50) ### START CODE HERE ### # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim) emb_matrix = np.zeros((vocab_len, emb_dim)); # Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary for word, index in word_to_index.items(): emb_matrix[index, :] = word_to_vec_map[word]; # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. embedding_layer = Embedding(vocab_len, emb_dim, trainable = False); ### END CODE HERE ### # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None". embedding_layer.build((None,)) # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained. embedding_layer.set_weights([emb_matrix]) return embedding_layer 12embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)print("weights[0][1][3] =", embedding_layer.get_weights()[0][1][3]) weights[0][1][3] = -0.3403 Expected Output: weights[0][1][3] = -0.3403 2.3 Building the Emojifier-V2Lets now build the Emojifier-V2 model. You will do so using the embedding layer you have built, and feed its output to an LSTM network. Figure 3: Emojifier-v2. A 2-layer LSTM sequence classifier. Exercise: Implement Emojify_V2(), which builds a Keras graph of the architecture shown in Figure 3. The model takes as input an array of sentences of shape (m, max_len, ) defined by input_shape. It should output a softmax probability vector of shape (m, C = 5). You may need Input(shape = ..., dtype = &#39;...&#39;), LSTM(), Dropout(), Dense(), and Activation(). 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# GRADED FUNCTION: Emojify_V2def Emojify_V2(input_shape, word_to_vec_map, word_to_index): """ Function creating the Emojify-v2 model's graph. Arguments: input_shape -- shape of the input, usually (max_len,) word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words) Returns: model -- a model instance in Keras """ ### START CODE HERE ### # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices). sentence_indices = Input(shape = input_shape, dtype = 'int32'); # Create the embedding layer pretrained with GloVe Vectors (≈1 line) embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index); # Propagate sentence_indices through your embedding layer, you get back the embeddings embeddings = embedding_layer(sentence_indices); # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state # Be careful, the returned output should be a batch of sequences. X = LSTM(128, return_sequences = True)(embeddings); # Add dropout with a probability of 0.5 X = Dropout(0.5)(X); # Propagate X trough another LSTM layer with 128-dimensional hidden state # Be careful, the returned output should be a single hidden state, not a batch of sequences. X = LSTM(128, return_sequences = False)(X); # Add dropout with a probability of 0.5 X = Dropout(0.5)(X); # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors. X = Dense(5, activation = 'softmax')(X); # Add a softmax activation X = Activation('softmax')(X); # Create Model instance which converts sentence_indices into X. model = Model(inputs = sentence_indices, outputs = X); ### END CODE HERE ### return model Run the following cell to create your model and check its summary. Because all sentences in the dataset are less than 10 words, we chose max_len = 10. You should see your architecture, it uses “20,223,927” parameters, of which 20,000,050 (the word embeddings) are non-trainable, and the remaining 223,877 are. Because our vocabulary size has 400,001 words (with valid indices from 0 to 400,000) there are 400,001*50 = 20,000,050 non-trainable parameters. 12model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 10) 0 _________________________________________________________________ embedding_2 (Embedding) (None, 10, 50) 20000050 _________________________________________________________________ lstm_1 (LSTM) (None, 10, 128) 91648 _________________________________________________________________ dropout_1 (Dropout) (None, 10, 128) 0 _________________________________________________________________ lstm_2 (LSTM) (None, 128) 131584 _________________________________________________________________ dropout_2 (Dropout) (None, 128) 0 _________________________________________________________________ dense_1 (Dense) (None, 5) 645 _________________________________________________________________ activation_1 (Activation) (None, 5) 0 ================================================================= Total params: 20,223,927 Trainable params: 223,877 Non-trainable params: 20,000,050 _________________________________________________________________ As usual, after creating your model in Keras, you need to compile it and define what loss, optimizer and metrics your are want to use. Compile your model using categorical_crossentropy loss, adam optimizer and [&#39;accuracy&#39;] metrics: 1model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) It’s time to train your model. Your Emojifier-V2 model takes as input an array of shape (m, max_len) and outputs probability vectors of shape (m, number of classes). We thus have to convert X_train (array of sentences as strings) to X_train_indices (array of sentences as list of word indices), and Y_train (labels as indices) to Y_train_oh (labels as one-hot vectors). 12X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)Y_train_oh = convert_to_one_hot(Y_train, C = 5) Fit the Keras model on X_train_indices and Y_train_oh. We will use epochs = 50 and batch_size = 32. 1model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True) Epoch 1/50 132/132 [==============================] - 0s - loss: 1.6086 - acc: 0.1818 Epoch 2/50 132/132 [==============================] - 0s - loss: 1.5867 - acc: 0.3409 Epoch 3/50 132/132 [==============================] - 0s - loss: 1.5721 - acc: 0.2652 Epoch 4/50 132/132 [==============================] - 0s - loss: 1.5540 - acc: 0.3485 Epoch 5/50 132/132 [==============================] - 0s - loss: 1.5413 - acc: 0.3030 Epoch 6/50 132/132 [==============================] - 0s - loss: 1.5195 - acc: 0.3712 Epoch 7/50 132/132 [==============================] - 0s - loss: 1.5275 - acc: 0.3258 Epoch 8/50 132/132 [==============================] - 0s - loss: 1.4633 - acc: 0.4545 Epoch 9/50 132/132 [==============================] - 0s - loss: 1.4320 - acc: 0.4924 Epoch 10/50 132/132 [==============================] - 0s - loss: 1.3712 - acc: 0.6136 Epoch 11/50 132/132 [==============================] - 0s - loss: 1.3441 - acc: 0.6136 Epoch 12/50 132/132 [==============================] - 0s - loss: 1.2784 - acc: 0.6894 Epoch 13/50 132/132 [==============================] - 0s - loss: 1.2723 - acc: 0.6364 Epoch 14/50 132/132 [==============================] - 0s - loss: 1.2651 - acc: 0.6667 Epoch 15/50 132/132 [==============================] - 0s - loss: 1.2106 - acc: 0.6970 Epoch 16/50 132/132 [==============================] - 0s - loss: 1.2334 - acc: 0.7197 Epoch 17/50 132/132 [==============================] - 0s - loss: 1.2150 - acc: 0.7045 Epoch 18/50 132/132 [==============================] - 0s - loss: 1.1613 - acc: 0.7803 Epoch 19/50 132/132 [==============================] - 0s - loss: 1.1587 - acc: 0.7576 Epoch 20/50 132/132 [==============================] - 0s - loss: 1.1129 - acc: 0.8182 Epoch 21/50 132/132 [==============================] - 0s - loss: 1.1016 - acc: 0.8030 Epoch 22/50 132/132 [==============================] - 0s - loss: 1.1939 - acc: 0.6970 Epoch 23/50 132/132 [==============================] - 0s - loss: 1.2618 - acc: 0.6288 Epoch 24/50 132/132 [==============================] - 0s - loss: 1.2123 - acc: 0.6818 Epoch 25/50 132/132 [==============================] - 0s - loss: 1.1606 - acc: 0.7652 Epoch 26/50 132/132 [==============================] - 0s - loss: 1.1066 - acc: 0.8030 Epoch 27/50 132/132 [==============================] - 0s - loss: 1.1312 - acc: 0.7727 Epoch 28/50 132/132 [==============================] - 0s - loss: 1.1400 - acc: 0.7652 Epoch 29/50 132/132 [==============================] - 0s - loss: 1.1107 - acc: 0.8030 Epoch 30/50 132/132 [==============================] - 0s - loss: 1.0676 - acc: 0.8485 Epoch 31/50 132/132 [==============================] - 0s - loss: 1.0660 - acc: 0.8258 Epoch 32/50 132/132 [==============================] - 0s - loss: 1.0450 - acc: 0.8712 Epoch 33/50 132/132 [==============================] - 0s - loss: 1.0246 - acc: 0.8939 Epoch 34/50 132/132 [==============================] - 0s - loss: 1.0163 - acc: 0.8939 Epoch 35/50 132/132 [==============================] - 0s - loss: 1.0080 - acc: 0.9015 Epoch 36/50 132/132 [==============================] - 0s - loss: 1.0144 - acc: 0.9015 Epoch 37/50 132/132 [==============================] - 0s - loss: 1.0861 - acc: 0.8106 Epoch 38/50 132/132 [==============================] - 0s - loss: 1.0484 - acc: 0.8561 Epoch 39/50 132/132 [==============================] - 0s - loss: 1.1126 - acc: 0.7955 Epoch 40/50 132/132 [==============================] - 0s - loss: 1.0712 - acc: 0.8561 Epoch 41/50 132/132 [==============================] - 0s - loss: 1.0277 - acc: 0.8864 Epoch 42/50 132/132 [==============================] - 0s - loss: 1.0459 - acc: 0.8561 Epoch 43/50 132/132 [==============================] - 0s - loss: 1.0214 - acc: 0.8864 Epoch 44/50 132/132 [==============================] - 0s - loss: 1.0012 - acc: 0.9091 Epoch 45/50 132/132 [==============================] - 0s - loss: 0.9877 - acc: 0.9242 Epoch 46/50 132/132 [==============================] - 0s - loss: 0.9827 - acc: 0.9167 Epoch 47/50 132/132 [==============================] - 0s - loss: 0.9835 - acc: 0.9167 Epoch 48/50 132/132 [==============================] - 0s - loss: 0.9817 - acc: 0.9242 Epoch 49/50 132/132 [==============================] - 0s - loss: 0.9894 - acc: 0.9167 Epoch 50/50 132/132 [==============================] - 0s - loss: 0.9780 - acc: 0.9318 &lt;keras.callbacks.History at 0x7f49ffd55e48&gt; Your model should perform close to 100% accuracy on the training set. The exact accuracy you get may be a little different. Run the following cell to evaluate your model on the test set. 12345X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)Y_test_oh = convert_to_one_hot(Y_test, C = 5)loss, acc = model.evaluate(X_test_indices, Y_test_oh)print()print("Test accuracy = ", acc) 32/56 [================&gt;.............] - ETA: 0s Test accuracy = 0.839285714286 You should get a test accuracy between 80% and 95%. Run the cell below to see the mislabelled examples. 12345678910# This code allows you to see the mislabelled examplesC = 5y_test_oh = np.eye(C)[Y_test.reshape(-1)]X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)pred = model.predict(X_test_indices)for i in range(len(X_test)): x = X_test_indices num = np.argmax(pred[i]) if(num != Y_test[i]): print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip()) Expected emoji:😄 prediction: she got me a nice present ❤️ Expected emoji:😞 prediction: work is hard 😄 Expected emoji:😞 prediction: This girl is messing with me ❤️ Expected emoji:😞 prediction: work is horrible 😄 Expected emoji:😄 prediction: you brighten my day ❤️ Expected emoji:😞 prediction: she is a bully 😄 Expected emoji:😞 prediction: My life is so boring ❤️ Expected emoji:😄 prediction: will you be my valentine 😞 Expected emoji:😄 prediction: What you did was awesome 😞 Now you can try it on your own example. Write your own sentence below. 1234# Change the sentence below to see your prediction. Make sure all the words are in the Glove embeddings. x_test = np.array(['not feeling happy'])X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)print(x_test[0] +' '+ label_to_emoji(np.argmax(model.predict(X_test_indices)))) not feeling happy 😄 Previously, Emojify-V1 model did not correctly label “not feeling happy,” but our implementation of Emojiy-V2 got it right. (Keras’ outputs are slightly random each time, so you may not have obtained the same result.) The current model still isn’t very robust at understanding negation (like “not happy”) because the training set is small and so doesn’t have a lot of examples of negation. But if the training set were larger, the LSTM model would be much better than the Emojify-V1 model at understanding such complex sentences. Congratulations!You have completed this notebook! ❤️❤️❤️ What you should remember: If you have an NLP task where the training set is small, using word embeddings can help your algorithm significantly. Word embeddings allow your model to work on words in the test set that may not even have appeared in your training set. Training sequence models in Keras (and in most other deep learning frameworks) requires a few important details: To use mini-batches, the sequences need to be padded so that all the examples in a mini-batch have the same length. An Embedding() layer can be initialized with pretrained values. These values can be either fixed or trained further on your dataset. If however your labeled dataset is small, it’s usually not worth trying to train a large pre-trained set of embeddings. LSTM() has a flag called return_sequences to decide if you would like to return every hidden states or only the last one. You can use Dropout() right after LSTM() to regularize your network. Congratulations on finishing this assignment and building an Emojifier. We hope you’re happy with what you’ve accomplished in this notebook! 😀😀😀😀😀😀AcknowledgmentsThanks to Alison Darcy and the Woebot team for their advice on the creation of this assignment. Woebot is a chatbot friend that is ready to speak with you 24/7. As part of Woebot’s technology, it uses word embeddings to understand the emotions of what you say. You can play with it by going to http://woebot.io]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sequence models attention mechanism]]></title>
    <url>%2F2018%2F06%2F03%2F03_sequence-models-attention-mechanism%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal lecture note after studying the course nlp sequence models at the 3rd week and the copyright belongs to deeplearning.ai.## 01_various-sequence-to-sequence-architectures 01_basic-modelsHello, and welcome to this final week of this course, as well as to the final week of this sequence of five courses in the deep learning specialization. You’re nearly at the finish line. In this week, you hear about sequence-to-sequence models, which are useful for everything from machine translation to speech recognition. Let’s start with the basic models and then later this week you, hear about beam search, the attention model, and we’ll wrap up the discussion of models for audio data, like speech. Let’s get started. Let’s say you want to input a French sentence like Jane visite l’Afrique en septembre, and you want to translate it to the English sentence, Jane is visiting Africa in September. As usual, let’s use x through x, in this case , to represent the words in the input sequence, and we’ll use y through y to represent the words in the output sequence. So, how can you train a new network to input the sequence x and output the sequence y? Well, here’s something you could do, and the ideas I’m about to present are mainly from these two papers due to Sutskever, Oriol Vinyals, and Quoc Le, and that one by Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwen, and Yoshua Bengio. First, let’s have a network, which we’re going to call the encoder network be built as a RNN, and this could be a GRU and LSTM, feed in the input French words one word at a time. And after ingesting the input sequence, the RNN then offers a vector that represents the input sentence. After that, you can build a decoder network which I’m going to draw here, which takes as input the encoding output by the encoder network shown in black on the left, and then can be trained to output the translation one word at a time until eventually it outputs say, the end of sequence or end the sentence token upon which the decoder stops and as usual we could take the generated tokens and feed them to the next [inaudible] in the sequence like we ‘re doing before when synthesizing text using the language model. One of the most remarkable recent results in deep learning is that this model works, given enough pairs of French and English sentences. If you train the model to input a French sentence and output the corresponding English translation, this will actually work decently well. And this model simply uses an encoder network, whose job it is to find an encoding of the input French sentence and then use a decoder network to then generate the corresponding English translation. An architecture very similar to this also works for image captioning so given an image like the one shown here, maybe wanted to be captioned automatically as a cat sitting on a chair. So how do you train a new network to input an image and output a caption like that phrase up there? Here’s what you can do. From the earlier course on ConvNet you’ve seen how you can input an image into a convolutional network, maybe a pre-trained AlexNet, and have that learn an encoding or learn a set of features of the input image. So, this is actually the AlexNet architecture and if we get rid of this final Softmax unit, the pre-trained AlexNet can give you a 4096-dimensional feature vector of which to represent this picture of a cat. And so this pre-trained network can be the encoder network for the image and you now have a 4096-dimensional vector that represents the image. You can then take this and feed it to an RNN, whose job it is to generate the caption one word at a time. So similar to what we saw with machine translation translating from French to English, you can now input a feature vector describing the input and then have it generate an output sequence or output set of words one word at a time. And this actually works pretty well for image captioning, especially if the caption you want to generate is not too long. As far as I know, this type of model was first proposed by Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille, although it turns out there were multiple groups coming up with very similar models independently and at about the same time. So two other groups that had done very similar work at about the same time and I think independently of Mao et al were Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan, as well as Andrej Karpathy and Fei-Fei Yi. So, you’ve now seen how a basic sequence-to-sequence model works, or how a basic image-to-sequence or image captioning model works, but there are some differences between how you would run a model like this, so generating a sequence compared to how you were synthesizing novel text using a language model. One of the key differences is, you don’t want a randomly chosen translation, you maybe want the most likely translation, or you don’t want a randomly chosen caption, maybe not, but you might want the best caption and most likely caption. So let’s see in the next video how you go about generating that. 02_picking-the-most-likely-sentenceThere are some similarities between the sequence to sequence machine translation model and the language models that you have worked within the first week of this course, but there are some significant differences as well. Let’s take a look. So, you can think of machine translation as building a conditional language model. Here’s what I mean, in language modeling, this was the network we had built in the first week. And this model allows you to estimate the probability of a sentence. That’s what a language model does. And you can also use this to generate novel sentences, and sometimes when you are writing x1 and x2 here, where in this example, x2 would be equal to y1 or equal to y and one is just a feedback. But x1, x2, and so on were not important. So just to clean this up for this slide, I’m going to just cross these off. X1 could be the vector of all zeros and x2, x3 are just the previous output you are generating. So that was the language model. The machine translation model looks as follows, and I am going to use a couple different colors, green and purple, to denote respectively the coded network in green and the decoded network in purple. And you notice that the decoded network looks pretty much identical to the language model that we had up there. So what the machine translation model is, is very similar to the language model, except that instead of always starting along with the vector of all zeros, it instead has an encoded network that figures out some representation for the input sentence, and it takes that input sentence and starts off the decoded network with representation of the input sentence rather than with the representation of all zeros. So, that’s why I call this a conditional language model, and instead of modeling the probability of any sentence, it is now modeling the probability of, say, the output English translation, conditions on some input French sentence. So in other words, you’re trying to estimate the probability of an English translation. Like, what’s the chance that the translation is “Jane is visiting Africa in September,” but conditions on the input French censors like, “Jane visite I’Afrique en septembre.” So, this is really the probability of an English sentence conditions on an input French sentence which is why it is a conditional language model. Now, if you want to apply this model to actually translate a sentence from French into English, given this input French sentence, the model might tell you what is the probability of difference in corresponding English translations. So, x is the French sentence, “Jane visite l’Afrique en septembre.” And, this now tells you what is the probability of different English translations of that French input. And, what you do not want is to sample outputs at random. If you sample words from this distribution, p of y given x, maybe one time you get a pretty good translation, “Jane is visiting Africa in September.” But, maybe another time you get a different translation, “Jane is going to be visiting Africa in September. “ Which sounds a little awkward but is not a terrible translation, just not the best one. And sometimes, just by chance, you get, say, others: “In September, Jane will visit Africa.” And maybe, just by chance, sometimes you sample a really bad translation: “Her African friend welcomed Jane in September.” So, when you’re using this model for machine translation, you’re not trying to sample at random from this distribution. Instead, what you would like is to find the English sentence, y, that maximizes that conditional probability. So in developing a machine translation system, one of the things you need to do is come up with an algorithm that can actually find the value of y that maximizes this term over here. The most common algorithm for doing this is called beam search, and it’s something you’ll see in the next video. But, before moving on to describe beam search, you might wonder, why not just use greedy search? So, what is greedy search? Well, greedy search is an algorithm from computer science which says to generate the first word just pick whatever is the most likely first word according to your conditional language model. Going to your machine translation model and then after having picked the first word, you then pick whatever is the second word that seems most likely, then pick the third word that seems most likely. This algorithm is called greedy search. And, what you would really like is to pick the entire sequence of words, $y^{}, y^{}$ , up to $y^{&lt;T_y&gt;}$, that’s there, that maximizes the joint probability of that whole thing. And it turns out that the greedy approach, where you just pick the best first word, and then, after having picked the best first word, try to pick the best second word, and then, after that, try to pick the best third word, that approach doesn’t really work. To demonstrate that, let’s consider the following two translations. The first one is a better translation, so hopefully, in our machine translation model, it will say that p of y given x is higher for the first sentence. It’s just a better, more succinct translation of the French input. The second one is not a bad translation, it’s just more verbose, it has more unnecessary words. But, if the algorithm has picked “Jane is” as the first two words, because “going” is a more common English word, probably the chance of “Jane is going,” given the French input, this might actually be higher than the chance of “Jane is visiting,” given the French sentence. So, it’s quite possible that if you just pick the third word based on whatever maximizes the probability of just the first three words, you end up choosing option number two. But, this ultimately ends up resulting in a less optimal sentence, in a less good sentence as measured by this model for p of y given x. I know this was may be a slightly hand-wavey argument, but, this is an example of a broader phenomenon, where if you want to find the sequence of words, y1, y2, all the way up to the final word that together maximize the probability, it’s not always optimal to just pick one word at a time. And, of course, the total number of combinations of words in the English sentence is exponentially larger. So, if you have just 10,000 words in a dictionary and if you’re contemplating translations that are up to ten words long, then there are 10000 to the tenth possible sentences that are ten words long. Picking words from the vocabulary size, the dictionary size of 10000 words. So, this is just a huge space of possible sentences, and it’s impossible to rate them all, which is why the most common thing to do is use an approximate search out of them. And, what an approximate search algorithm does, is it will try, it won’t always succeed, but it will to pick the sentence, y, that maximizes that conditional probability. And, even though it’s not guaranteed to find the value of y that maximizes this, it usually does a good enough job. So, to summarize, in this video, you saw how machine translation can be posed as a conditional language modeling problem. But one major difference between this and the earlier language modeling problems is rather than wanting to generate a sentence at random, you may want to try to find the most likely English sentence, most likely English translation. But the set of all English sentences of a certain length is too large to exhaustively enumerate. So, we have to resort to a search algorithm. So, with that, let’s go onto the next video where you’ll learn about beam search algorithm. 03_beam-searchIn this video, you learn about the beam search algorithm. In the last video, you remember how for machine translation given an input French sentence, you don’t want to output a random English translation, you want to output the best and the most likely English translation. The same is also true for speech recognition where given an input audio clip, you don’t want to output a random text transcript of that audio, you want to output the best, maybe the most likely, text transcript. Beam search is the most widely used algorithm to do this. And in this video, you see how to get beam search to work for yourself. Let’s just try Beam Search using our running example of the French sentence, “Jane, visite l’Afrique en Septembre”. Hopefully being translated into, “Jane, visits Africa in September”. The first thing Beam search has to do is try to pick the first words of the English translation, that’s going to operate. So here I’ve listed, say, 10,000 words into vocabulary. And to simplify the problem a bit, I’m going to ignore capitalization. So I’m just listing all the words in lower case. So, in the first step of Beam Search, I use this network fragment with the coalition in green and decoalition in purple, to try to evaluate what is the probability of that for a square. So, what’s the probability of the first output y, given the input sentence x gives the French input. So, whereas greedy search will pick only the one most likely words and move on, Beam Search instead can consider multiple alternatives. So, the Beam Search algorithm has a parameter called B, which is called the beam width and for this example I’m going to set the beam width to be with the three. And what this means is Beam search will cause that not just one possibility but consider three at the time. So in particular, let’s say evaluating this probability over different choices the first words, it finds that the choices in, Jane and September are the most likely three possibilities for the first words in the English outputs. Then Beam search will stowaway in computer memory that it wants to try all of three of these words, and if the beam width parameter were said differently, the beam width parameter was 10, then we keep track of not just three but of the ten, most likely possible choices for the first word. So, to be clear in order to perform this first step of Beam search, what you need to do is run the input French sentence through this encoder network and then this first step will then decode the network, this is a softmax output overall 10,000 possibilities. Then you would take those 10,000 possible outputs and keep in memory which were the top three. Let’s go into the second step of Beam search. Having picked in, Jane and September as the three most likely choice of the first word, what Beam search will do now, is for each of these three choices consider what should be the second word, so after “in” maybe a second word is “a” or maybe as Aaron, I’m just listing words from the vocabulary, from the dictionary or somewhere down the list will be September, somewhere down the list there’s visit and then all the way to z and then the last word is zulu. So, to evaluate the probability of second word, it will use this new network fragments where is coder in green and for the decoder portion when trying to decide what comes after in. Remember the decoder first outputs, y hat one. So, I’m going to set to this y hat one to the word “in” as it goes back in. So there’s the word “in” because it decided for now. That’s because It trying to figure out that the first word was “in”, what is the second word, and then this will output I guess y hat two. And so by hard wiring y hat one here, really the inputs here to be the first words “in” this time were fragment can be used to evaluate whether it’s the probability of the second word given the input french sentence and that the first words of the translation has been the word “in”. Now notice that what we also need help out in this second step would be assertions to find the pair of the first and second words that is most likely it’s not just a second where is most likely that the pair of the first and second whereas the most likely and by the rules of conditional probability. This can be expressed as P of the first words times P of probability of the second words. Which you are getting from this network fragment and so if for each of the three words you’ve chosen “in”, “Jane,” and “September” you save away this probability then you can multiply them by this second probabilities to get the probability of the first and second words. So now you’ve seen how if the first word was “in” how you can evaluate the probability of the second word. Now at first it was “Jane” you do the same thing. The sentence could be “Jane a”,” Jane Aaron”, and so on down to “Jane is”, “Jane visits” and so on. And you will use this in your network fragments let me draw this in as well where here you will hardwire, Y hat One to be Jane. And so with the First word y one hat’s hard wired as Jane than just the network fragments can tell you what’s the probability of the second words to me. And given that the first word is “Jane”. And then same as above you can multiply with P of Y1 to get the probability of Y1 and Y2 for each of these 10,000 different possible choices for the second word. And then finally do the same thing for September although words from a down to Zulu and use this network fragment. That just goes in as well to see if the first word was September. What was the most likely options for the second words. So for this second step of beam search because we’re continuing to use a beam width of three and because there are 10,000 words in the vocabulary you’d end up considering three times 10000 or thirty thousand possibilities because there are 10,000 here, 10,000 here, 10,000 here as the beam width times the number of words in the vocabulary and what you do is you evaluate all of these 30000 options according to the probably the first and second words and then pick the top three. So with a cut down, these 30,000 possibilities down to three again down the beam width rounded again so let’s say that 30,000 choices, the most likely were in September and say Jane is, and Jane visits sorry this bit messy but those are the most likely three out of the 30,000 choices then that’s what Beam’s search would memorize away and take on to the next step being surge. So notice one thing if beam search decides that the most likely choices are the first and second words are in September, or Jane is, or Jane visits. Then what that means is that it is now rejecting September as a candidate for the first words of the output English translation so we’re now down to two possibilities for the first words but we still have a beam width of three keeping track of three choices for pairs of Y1, Y2 before going onto the third step of beam search. Just want to notice that because of beam width is equal to three, every step you instantiate three copies of the network to evaluate these partial sentence fragments and the output. And it’s because of beam width is equal to three that you have three copies of the network with different choices for the first words, but these three copies of the network can be very efficiently used to evaluate all 30,000 options for the second word. So just don’t instantiate 30,000 copies of the network or three copies of the network to very quickly evaluate all 10,000 possible outputs at that softmax output say for Y2. Let’s just quickly illustrate one more step of beam search. So said that the most likely choices for first two words were in September, Jane is, and Jane visits and for each of these pairs of words which we should have saved the way in computer memory the probability of Y1 and Y2 given the input X given the French sentence X. So similar to before, we now want to consider what is the third word. So in September a? In September Aaron? All the way down to is in September Zulu and to evaluate possible choices for the third word, you use this network fragments where you Hardwire the first word here to be in the second word to be September. And so this network fragment allows you to evaluate what’s the probability of the third word given the input French sentence X and given that the first two words are in September and English output. And then you do the same thing for the second fragment. So like so. And same thing for Jane visits and so beam search will then once again pick the top three possibilities may be that things in September. Jane is a likely outcome or Jane is visiting is likely or maybe Jane visits Africa is likely for that first three words and then it keeps going and then you go onto the fourth step of beam search hat one more word and on it goes. And the outcome of this process hopefully will be that adding one word at a time that Beam search will decide that. Jane visits Africa in September will be terminated by the end of sentence symbol using that system is quite common. They’ll find that this is a likely output English sentence and you’ll see more details of this yourself. In this week’s exercise as well where you get to play with beam search yourself. So with a beam of three being searched considers three possibilities at a time. Notice that if the beam width was said to be equal to one, say cause there’s only one, then this essentially becomes the greedy search algorithm which we had discussed in the last video but by considering multiple possibilities say three or ten or some other number at the same time beam search will usually find a much better output sentence than greedy search. You’ve now seen how Beam Search works but it turns out there’s some additional tips and tricks refinements that help you to make beam search work even better. Let’s go onto the next video to take a look. 04_refinements-to-beam-searchIn the last video, you saw the basic beam search algorithm. In this video, you’ll learn some little changes that make it work even better. Length normalization is a small change to the beam search algorithm that can help you get much better results. Here’s what it is. Beam search is maximizing this probability. And this product here is just expressing the observation that P(y1) up to y(Ty), given x, can be expressed as P(y1) given x times P(y2), given x and y1 times dot dot dot, up to I guess p of y Ty given x and y1 up to y t1-1. Maybe this notation is a bit more scary and more intimidating than it needs to be, but this is that probabilities that you see previously. Now, if you’re implementing these, these probabilities are all numbers less than 1. Often they’re much less than 1. And multiplying a lot of numbers less than 1 will result in a tiny, tiny, tiny number, which can result in numerical underflow. Meaning that it’s too small for the floating part representation in your computer to store accurately. So in practice, instead of maximizing this product, we will take logs. And if you insert a log there, then log of a product becomes a sum of a log, and maximizing this sum of log probabilities should give you the same results in terms of selecting the most likely sentence y. So by taking logs, you end up with a more numerically stable algorithm that is less prone to rounding errors, numerical rounding errors, or to really numerical underflow. And because the log function, that’s the logarithmic function, this is strictly monotonically increasing function, maximizing P(y). And because the logarithmic function, here’s the log function, is a strictly monotonically increasing function, we know that maximizing log P(y) given x should give you the same result as maximizing P(y) given x. As in the same value of y that maximizes this should also maximize that. So in most implementations, you keep track of the sum of logs of the probabilities rather than the protocol of probabilities. Now, there’s one other change to this objective function that makes the machine translation algorithm work even better. Which is that, if you referred to this original objective up here, if you have a very long sentence, the probability of that sentence is going to be low, because you’re multiplying as many terms here. Lots of numbers are less than 1 to estimate the probability of that sentence. And so if you multiply all the numbers that are less than 1 together, you just tend to end up with a smaller probability. And so this objective function has an undesirable effect, that maybe it unnaturally tends to prefer very short translations. It tends to prefer very short outputs. Because the probability of a short sentence is determined just by multiplying fewer of these numbers are less than 1. And so the product would just be not quite as small. And by the way, the same thing is true for this. The log of our probability is always less than or equal to 1. You’re actually in this range of the log. So the more terms you have together, the more negative this thing becomes. So there’s one other change to the algorithm that makes it work better, which is instead of using this as the objective you’re trying to maximize, one thing you could do is normalize this by the number of words in your translation. And so this takes the average of the log of the probability of each word. And this significantly reduces the penalty for outputting longer translations. And in practice, as a heuristic instead of dividing by Ty, by the number of words in the output sentence, sometimes you use a softer approach. We have Ty to the power of alpha, where maybe alpha is equal to 0.7. So if alpha was equal to 1, then yeah, completely normalizing by length. If alpha was equal to 0, then, well, Ty to the 0 would be 1, then you’re just not normalizing at all. And this is somewhat in between full normalization, and no normalization, and alpha’s another hyper parameter you have within that you can tune to try to get the best results. And have to admit, using alpha this way, this is a heuristic or this is a hack. There isn’t a great theoretical justification for it, but people have found this works well. People have found that it works well in practice, so many groups will do this. And you can try out different values of alpha and see which one gives you the best result. So just to wrap up how you run beam search, as you run beam search you see a lot of sentences with length equal 1, a lot of sentences with length equal 2, a lot of sentences with length equals 3. And so on, and maybe you run beam search for 30 steps and you consider output sentences up to length 30, let’s say. And so with beam with a 3, you will be keeping track of the top three possibilities for each of these possible sentence lengths, 1, 2, 3, 4 and so on, up to 30. Then, you would look at all of the output sentences and score them against this score. And so you can take your top sentences and just compute this objective function onto sentences that you have seen through the beam search process. And then finally, of all of these sentences that you validate this way, you pick the one that achieves the highest value on this normalized log probability objective. Sometimes it’s called a normalized log likelihood objective. And then that would be the final translation, your outputs. So that’s how you implement beam search, and you get to play this yourself in this week’s problem exercise. Finally, a few implementational details, how do you choose the beam width B? The larger B is, the more possibilities you’re considering, and does the better the sentence you probably find. But the larger B is, the more computationally expensive your algorithm is, because you’re also keeping a lot more possibilities around. All right, so finally, let’s just wrap up with some thoughts on how to choose the beam width B. So here are the pros and cons of setting B to be very large versus very small. If the beam width is very large, then you consider a lot of possibilities, and so you tend to get a better result because you are consuming a lot of different options, but it will be slower. And the memory requirements will also grow, will also be compositionally slower. Whereas if you use a very small beam width, then you get a worse result because you’re just keeping less possibilities in mind as the algorithm is running. But you get a result faster and the memory requirements will also be lower. So in the previous video, we used in our running example a beam width of three, so we’re keeping three possibilities in mind. In practice, that is on the small side. In production systems, it’s not uncommon to see a beam width maybe around 10, and I think beam width of 100 would be considered very large for a production system, depending on the application. But for research systems where people want to squeeze out every last drop of performance in order to publish the paper with the best possible result. It’s not uncommon to see people use beam widths of 1,000 or 3,000, but this is very application, that’s why it’s a domain dependent. So I would say try other variety of values of B as you work through your application. But when B gets very large, there is often diminishing returns. So for many applications, I would expect to see a huge gain as you go from a beam widht of 1, which is very greedy search, to 3, to maybe 10. But the gains as you go from 1,000 to 3,000 in beam width might not be as big. And for those of you that have taken maybe a lot of computer science courses before, if you’re familiar with computer science search algorithms like BFS, Breadth First Search, or DFS, Depth First Search. The way to think about beam search is that, unlike those other algorithms which you have learned about in a computer science algorithms course, and don’t worry about it if you’ve not heard of these algorithms. But if you’ve heard of Breadth First Search and Depth First Search then unlike those algorithms, which are exact search algorithms. Beam search runs much faster but does not guarantee to find the exact maximum for this argmax that you would like to find. If you haven’t heard of breadth first search or depth first search, don’t worry about it, it’s not important for our purposes. But if you have, this is how beam search relates to those algorithms. So that’s it for beam search, which is a widely used algorithm in many production systems, or in many commercial systems. Now, in the circles in the sequence of courses of deep learning, we talked a lot about error analysis. It turns out, one of the most useful tools I’ve found is to be able to do error analysis on beam search. So you sometimes wonder, should I increase my beam width? Is my beam width working well enough? And there’s some simple things you can compute to give you guidance on whether you need to work on improving your search algorithm. Let’s talk about that in the next video. 05_error-analysis-in-beam-searchIn the third course of this sequence of five courses, you saw how error analysis can help you focus your time on doing the most useful work for your project. Now, beam search is an approximate search algorithm, also called a heuristic search algorithm. And so it doesn’t always output the most likely sentence. It’s only keeping track of B equals 3 or 10 or 100 top possibilities. So what if beam search makes a mistake? In this video, you’ll learn how error analysis interacts with beam search and how you can figure out whether it is the beam search algorithm that’s causing problems and worth spending time on. Or whether it might be your RNN model that is causing problems and worth spending time on. Let’s take a look at how to do error analysis with beam search. Let’s use this example of Jane visite l’Afrique en septembre. So let’s say that in your machine translation dev set, your development set, the human provided this translation and Jane visits Africa in September, and I’m going to call this y. So it is a pretty good translation written by a human. Then let’s say that when you run beam search on your learned RNN model and your learned translation model, it ends up with this translation, which we will call y-hat, Jane visited Africa last September, which is a much worse translation of the French sentence. It actually changes the meaning, so it’s not a good translation. Now, your model has two main components. There is a neural network model, the sequence to sequence model. We shall just call this your RNN model. It’s really an encoder and a decoder. And you have your beam search algorithm, which you’re running with some beam width b. And wouldn’t it be nice if you could attribute this error, this not very good translation, to one of these two components? Was it the RNN or really the neural network that is more to blame, or is it the beam search algorithm, that is more to blame? And what you saw in the third course of the sequence is that it’s always tempting to collect more training data that never hurts. So in similar way, it’s always tempting to increase the beam width that never hurts or pretty much never hurts. But just as getting more training data by itself might not get you to the level of performance you want. In the same way, increasing the beam width by itself might not get you to where you want to go. But how do you decide whether or not improving the search algorithm is a good use of your time? So just how you can break the problem down and figure out what’s actually a good use of your time. Now, the RNN, the neural network, what was called RNN really means the encoder and the decoder. It computes P(y given x). So for example, for a sentence, Jane visits Africa in September, you plug in Jane visits Africa. Again, I’m ignoring upper versus lowercase now, right, and so on. And this computes P(y given x). So it turns out that the most useful thing for you to do at this point is to compute using this model to compute P(y given x) as well as to compute P(y-hat given x) using your RNN model. And then to see which of these two is bigger. So it’s possible that the left side is bigger than the right hand side. It’s also possible that P(y*) is less than P(y-hat) actually, or less than or equal to, right? Depending on which of these two cases hold true, you’d be able to more clearly ascribe this particular error, this particular bad translation to one of the RNN or the beam search algorithm being had greater fault. So let’s take out the logic behind this. Here are the two sentences from the previous slide. And remember, we’re going to compute P(y given x) and P(y-hat given x) and see which of these two is bigger. So there are going to be two cases. In case 1, P(y given x) as output by the RNN model is greater than P(y-hat given x). What does this mean? Well, the beam search algorithm chose y-hat, right? The way you got y-hat was you had an RNN that was computing P(y given x). And beam search’s job was to try to find a value of y that gives that arg max. But in this case, y actually attains a higher value for P(y given x) than the y-hat. So what this allows you to conclude is beam search is failing to actually give you the value of y that maximizes P(y given x) because the one job that beam search had was to find the value of y that makes this really big. But it chose y-hat, the y actually gets a much bigger value. So in this case, you could conclude that beam search is at fault. Now, how about the other case? In case 2, P(y given x) is less than or equal to P(y-hat given x), right? And then either this or this has gotta be true. So either case 1 or case 2 has to hold true. What do you conclude under case 2? Well, in our example, y is a better translation than y-hat. But according to the RNN, P(y) is less than P(y-hat), so saying that y is a less likely output than y-hat. So in this case, it seems that the RNN model is at fault and it might be worth spending more time working on the RNN. There’s some subtleties here pertaining to length normalizations that I’m glossing over. There’s some subtleties pertaining to length normalizations that I’m glossing over. And if you are using some sort of length normalization, instead of evaluating these probabilities, you should be evaluating the optimization objective that takes into account length normalization. But ignoring that complication for now, in this case, what this tells you is that even though y is a better translation, the RNN ascribed y in lower probability than the inferior translation. So in this case, I will say the RNN model is at fault. So the error analysis process looks as follows. You go through the development set and find the mistakes that the algorithm made in the development set. And so in this example, let’s say that P(y given x) was 2 x 10 to the -10, whereas, P(y-hat given x) was 1 x 10 to the -10. Using the logic from the previous slide, in this case, we see that beam search actually chose y-hat, which has a lower probability than y. So I will say beam search is at fault. So I’ll abbreviate that B. And then you go through a second mistake or second bad output by the algorithm, look at these probabilities. And maybe for the second example, you think the model is at fault. I’m going to abbreviate the RNN model with R. And you go through more examples. And sometimes the beam search is at fault, sometimes the model is at fault, and so on. And through this process, you can then carry out error analysis to figure out what fraction of errors are due to beam search versus the RNN model. And with an error analysis process like this, for every example in your dev sets, where the algorithm gives a much worse output than the human translation, you can try to ascribe the error to either the search algorithm or to the objective function, or to the RNN model that generates the objective function that beam search is supposed to be maximizing. And through this, you can try to figure out which of these two components is responsible for more errors. And only if you find that beam search is responsible for a lot of errors, then maybe is we’re working hard to increase the beam width. Whereas in contrast, if you find that the RNN model is at fault, then you could do a deeper layer of analysis to try to figure out if you want to add regularization, or get more training data, or try a different network architecture, or something else. And so a lot of the techniques that you saw in the third course in the sequence will be applicable there. So that’s it for error analysis using beam search. I found this particular error analysis process very useful whenever you have an approximate optimization algorithm, such as beam search that is working to optimize some sort of objective, some sort of cost function that is output by a learning algorithm, such as a sequence-to-sequence model or a sequence-to-sequence RNN that we’ve been discussing in these lectures. So with that, I hope that you’ll be more efficient at making these types of models work well for your applications. 06_bleu-score-optionalOne of the challenges of machine translation is that, given a French sentence, there could be multiple English translations that are equally good translations of that French sentence. So how do you evaluate a machine translation system if there are multiple equally good answers, unlike, say, image recognition where there’s one right answer? You just measure accuracy. If there are multiple great answers, how do you measure accuracy? The way this is done conventionally is through something called the BLEU score. So, in this optional video, I want to share with you, I want to give you a sense of how the BLEU score works. Let’s say you are given a French sentence Le chat est sur le tapis. And you are given a reference, human generated translation of this, which is the the cat is on the mat. But there are multiple, pretty good translations of this. So a different human, different person might translate it as there is a cat on the mat. And both of these are actually just perfectly fine translations of the French sentence. What the BLEU score does is given a machine generated translation, it allows you to automatically compute a score that measures how good is that machine translation. And the intuition is so long as the machine generated translation is pretty close to any of the references provided by humans, then it will get a high BLEU score. BLEU, by the way, stands for bilingual evaluation, Understudy. So in the theater world, an understudy is someone that learns the role of a more senior actor so they can take over the role of the more senior actor, if necessary. And motivation for BLEU is that, whereas you could ask human evaluators to evaluate the machine translation system, the BLEU score is an understudy, could be a substitute for having humans evaluate every output of a machine translation system. So the BLEU score was due to Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. This paper has been incredibly influential, and is, actually, quite a readable paper. So I encourage you to take a look if you have time. So, the intuition behind the BLEU score is we’re going to look at the machine generated output and see if the types of words it generates appear in at least one of the human generated references. And so these human generated references would be provided as part of the depth set or as part of the test set. Now, let’s look at a somewhat extreme example. Let’s say that the machine translation system abbreviating machine translation is MT. So the machine translation, or the MT output, is the the the the the the the. So this is clearly a pretty terrible translation. So one way to measure how good the machine translation output is, is to look at each the words in the output and see if it appears in the references. And so, this would be called a precision of the machine translation output. And in this case, there are seven words in the machine translation output. And every one of these 7 words appears in either Reference 1 or Reference 2, right? So the word the appears in both references. So each of these words looks like a pretty good word to include. So this will have a precision of 7 over 7. It looks like it was a great precision. So this is why the basic precision measure of what fraction of the words in the MT output also appear in the references. This is not a particularly useful measure, because it seems to imply that this MT output has very high precision. So instead, what we’re going to use is a modified precision measure in which we will give each word credit only up to the maximum number of times it appears in the reference sentences. So in Reference 1, the word, the, appears twice. In Reference 2, the word, the, appears just once. So 2 is bigger than 1, and so we’re going to say that the word, the, gets credit up to twice. So, with a modified precision, we will say that, it gets a score of 2 out of 7, because out of 7 words, we’ll give it a 2 credits for appearing. So here, the denominator is the count of the number of times the word, the, appears of 7 words in total. And the numerator is the count of the number of times the word, the, appears. We clip this count, we take a max, or we clip this count, at 2. So this gives us the modified precision measure. Now, so far, we’ve been looking at words in isolation. In the BLEU score, you don’t want to just look at isolated words. You maybe want to look at pairs of words as well. Let’s define a portion of the BLEU score on bigrams. And bigrams just means pairs of words appearing next to each other. So now, let’s see how we could use bigrams to define the BLEU score. And this will just be a portion of the final BLEU score. And we’ll take unigrams, or single words, as well as bigrams, which means pairs of words into account as well as maybe even longer sequences of words, such as trigrams, which means three words pairing together. So, let’s continue our example from before. We have to same Reference 1 and Reference 2. But now let’s say the machine translation or the MT System has a slightly better output. The cat the cat on the mat. Still not a great translation, but maybe better than the last one. So here, the possible bigrams are, well there’s the cat, but ignore case. And then there’s cat the, that’s another bigram. And then there’s the cat again, but I’ve already had that, so let’s skip that. And then cat on is the next one. And then on the, and the mat. So these are the bigrams in the machine translation output. And so let’s count up, How many times each of these bigrams appear. The cat appears twice, cat the appears once, and the others all appear just once. And then finally, let’s define the clipped count, so count, and then subscript clip. And to define that, let’s take this column of numbers, but give our algorithm credit only up to the maximum number of times that that bigram appears in either Reference 1 or Reference 2. So the cat appears a maximum of once in either of the references. So I’m going to clip that count to 1. Cat the, well, it doesn’t appear in Reference 1 or Reference 2, so I clip that to 0. Cat on, yep, that appears once. We give it credit for once. On the appears once, give that credit for once, and the mat appears once. So these are the clipped counts. We’re taking all the counts and clipping them, really reducing them to be no more than the number of times that bigram appears in at least one of the references. And then, finally, our modified bigram precision will be the sum of the count clipped. So that’s 1, 2, 3, 4 divided by the total number of bigrams. That’s 2, 3, 4, 5, 6, so 4 out of 6 or two-thirds is the modified precision on bigrams. So let’s just formalize this a little bit further. With what we had developed on unigrams, we defined this modified precision computed on unigrams as P subscript 1. The P stands for precision and the subscript 1 here means that we’re referring to unigrams. But that is defined as sum over the unigrams. So that just means sum over the words that appear in the machine translation output. So this is called y hat of count clip, Of that unigram. Divided by sum of our unigrams in the machine translation output of count, number of counts of that unigram, right? And so this is what we had gotten I guess is 2 out of 7, 2 slides back. So the 1 here refers to unigram, meaning we’re looking at single words in isolation. You can also define Pn as the n-gram version, Instead of unigram, for n-gram. So this would be sum over the n-grams in the machine translation output of count clip of that n-gram divided by sum over n-grams of the count of that n-gram. And so these precisions, or these modified precision scores, measured on unigrams or on bigrams, which we did on a previous slide, or on trigrams, which are triples of words, or even higher values of n for other n-grams. This allows you to measure the degree to which the machine translation output is similar or maybe overlaps with the references. And one thing that you could probably convince yourself of is if the MT output is exactly the same as either Reference 1 or Reference 2, then all of these values P1, and P2 and so on, they’ll all be equal to 1.0. So to get a modified precision of 1.0, you just have to be exactly equal to one of the references. And sometimes it’s possible to achieve this even if you aren’t exactly the same as any of the references. But you kind of combine them in a way that hopefully still results in a good translation. Finally, Finally, let’s put this together to form the final BLEU score. So P subscript n is the BLEU score computed on n-grams only. Also the modified precision computed on n-grams only. And by convention to compute one number, you compute P1, P2, P3 and P4, and combine them together using the following formula. It’s going to be the average, so sum from n = 1 to 4 of Pn and divide that by 4. So basically taking the average. By convention the BLEU score is defined as, e to the this, then exponentiations, and linear operate, exponentiation is strictly monotonically increasing operation and then we actually adjust this with one more factor called the, BP penalty. So BP, Stands for brevity penalty. The details maybe aren’t super important. But to just give you a sense, it turns out that if you output very short translations, it’s easier to get high precision. Because probably most of the words you output appear in the references. But we don’t want translations that are very short. So the BP, or the brevity penalty, is an adjustment factor that penalizes translation systems that output translations that are too short. So the formula for the brevity penalty is the following. $${P_n}{\rm{ = }}\frac{{\sum\limits_{n - gram \in \widehat y} {Coun{t_{clip}}(n - gram)} }}{{\sum\limits_{n - gram \in \widehat y} {Count(n - gram)} }}$$ $$BP \exp(\dfrac{1}{4}\sum_{n=1}^{4}P_{n})$$ $$BP = \left\{ \begin{array}{l} 1, if{\kern 1pt} {\kern 1pt} MT\_length > reference\_length{\kern 1pt} {\kern 1pt} \\ \exp (1 - MT\_length/reference\_length), otherwise \end{array} \right.$$ It’s equal to 1 if your machine translation system actually outputs things that are longer than the human generated reference outputs. And otherwise is some formula like that that overall penalizes shorter translations. So, in the details you can find in this paper. So, once again, earlier in this set of courses, you saw the importance of having a single real number evaluation metric. Because it allows you to try out two ideas, see which one achieves a higher score, and then try to stick with the one that achieved the higher score. So the reason the BLEU score was revolutionary for machine translation was because this gave a pretty good, by no means perfect, but pretty good single real number evaluation metric. And so that accelerated the progress of the entire field of machine translation. I hope this video gave you a sense of how the BLEU score works. In practice, few people would implement a BLEU score from scratch. There are open source implementations that you can download and just use to evaluate your own system. But today, BLEU score is used to evaluate many systems that generate text, such as machine translation systems, as well as the example I showed briefly earlier of image captioning systems where you would have a system, have a neural network generated image caption. And then use the BLEU score to see how much that overlaps with maybe a reference caption or multiple reference captions that were generated by people. So the BLEU score is a useful single real number evaluation metric to use whenever you want your algorithm to generate a piece of text. And you want to see whether it has similar meaning as a reference piece of text generated by humans. This is not used for speech recognition, because in speech recognition, there’s usually one ground truth. And you just use other measures to see if you got the speech transcription on pretty much, exactly word for word correct. But for things like image captioning, and multiple captions for a picture, it could be about equally good or for machine translations. There are multiple translations, but equally good. The BLEU score gives you a way to evaluate that automatically and therefore speed up your development. So with that, I hope you have a sense of how the BLEU score works. 07_attention-model-intuitionFor most of this week, you’ve been using a Encoder-Decoder architecture for machine translation. Where one RNN reads in a sentence and then different one outputs a sentence. There’s a modification to this called the Attention Model, that makes all this work much better. The attention algorithm, the attention idea has been one of the most influential ideas in deep learning. Let’s take a look at how that works. Get a very long French sentence like this. What we are asking this green encoder in your network to do is, to read in the whole sentence and then memorize the whole sentences and store it in the activations conveyed here. Then for the purple network, the decoder network till then generate the English translation. Jane went to Africa last September and enjoyed the culture and met many wonderful people; she came back raving about how wonderful her trip was, and is tempting me to go too. Now, the way a human translator would translate this sentence is not to first read the whole French sentence and then memorize the whole thing and then regurgitate an English sentence from scratch. Instead, what the human translator would do is read the first part of it, maybe generate part of the translation. Look at the second part, generate a few more words, look at a few more words, generate a few more words and so on. You kind of work part by part through the sentence, because it’s just really difficult to memorize the whole long sentence like that. What you see for the Encoder-Decoder architecture above is that, it works quite well for short sentences, so we might achieve a relatively high Bleu score, but for very long sentences, maybe longer than 30 or 40 words, the performance comes down. The Bleu score might look like this as the sentence that varies and short sentences are just hard to translate, hard to get all the words, right? Long sentences, it doesn’t do well on because it’s just difficult to get in your network to memorize a super long sentence. In this and the next video, you’ll see the Attention Model which translates maybe a bit more like humans might, looking at part of the sentence at a time and with an Attention Model, machine translation systems performance can look like this, because by working one part of the sentence at a time, you don’t see this huge dip which is really measuring the ability of a neural network to memorize a long sentence which maybe isn’t what we most badly need a neural network to do. In this video, I want to just give you some intuition about how attention works and then we’ll flesh out the details in the next video. The Attention Model was due to Dimitri, Bahdanau, Camcrun Cho, Yoshe Bengio and even though it was obviously developed for machine translation, it spread to many other application areas as well. This is really a very influential, I think very seminal paper in the deep learning literature. Let’s illustrate this with a short sentence, even though these ideas were maybe developed more for long sentences, but it’ll be easier to illustrate these ideas with a simpler example. We have our usual sentence, Jane visite l’Afrique en Septembre. Let’s say that we use a R and N, and in this case, I’m going to use a bidirectional R and N, in order to compute some set of features for each of the input words and you have to understand it, bidirectional RNN with outputs Y1 to Y3 and so on up to Y5 but we’re not doing a word for word translation, let me get rid of the Y’s on top. But using a bidirectional R and N, what we’ve done is for each other words, really for each of the five positions into sentence, you can compute a very rich set of features about the words in the sentence and maybe surrounding words in every position. Now, let’s go ahead and generate the English translation. We’re going to use another RNN to generate the English translations. Here’s my RNN note as usual and instead of using A to denote the activation, in order to avoid confusion with the activations down here, I’m just going to use a different notation, I’m going to use S to denote the hidden state in this RNN up here, so instead of writing A1 I’m going to right S1 and so we hope in this model that the first word it generates will be Jane, to generate Jane visits Africa in September. Now, the question is, when you’re trying to generate this first word, this output, what part of the input French sentence should you be looking at? Seems like you should be looking primarily at this first word, maybe a few other words close by, but you don’t need to be looking way at the end of the sentence. What the Attention Model would be computing is a set of attention weights and we’re going to use $\alpha^{&lt;1, 1&gt;}$ to denote when you’re generating the first words, how much should you be paying attention to this first piece of information here. And then we’ll also come up with a second that’s called Attention Weight, $\alpha^{&lt;1, 2&gt;}$ which tells us what we’re trying to compute the first work of Jane, how much attention we’re paying to this second work from the inputs and so on and the $\alpha^{&lt;1, 3&gt;}$ and so on, and together this will tell us what is exactly the context from denoter C that we should be paying attention to, and that is input to this RNN unit to then try to generate the first words. That’s one step of the R and N, we will flesh out all these details in the next video. For the second step of this R and N, we’re going to have a new hidden state S two and we’re going to have a new set of the attention weights. We’re going to have $\alpha^{&lt;2, 1&gt;}$ to tell us when we generate in the second word. I guess this will be visits maybe that being the ground trip label. How much should we paying attention to the first word in the french input and also, $\alpha^{&lt;2, 2&gt;}$ and so on. How much should we paying attention the word visite, how much should we pay attention to the free and so on. And of course, the first word we generate in Jane is also an input to this, and then we have some context that we’re paying attention to and the second step, there’s also an input and that together will generate the second word and that leads us to the third step, S three, where this is an input and we have some new context C that depends on the various $\alpha^{&lt;3, t&gt;}$ for the different time sets, that tells us how much should we be paying attention to the different words from the input French sentence and so on. So, some things I haven’t specified yet, but that will go further into detail in the next video of this, how exactly this context defines and the goal of the context is for the third word is really should capture that maybe we should be looking around this part of the sentence. The formula you use to do that will defer to the next video as well as how do you compute these attention weights. And you see in the next video that $\alpha^{&lt;3, t&gt;}$, which is, when you’re trying to generate the third word, I guess this would be the Africa, just getting the right output. The amounts that this RNN step should be paying attention to the French word that time T, that depends on the activations of the bidirectional RNN at time T, I guess it depends on the fourth activations and the, backward activations at time T and it will depend on the state from the previous steps, it will depend on S two, and these things together will influence, how much you pay attention to a specific word in the input French sentence. But we’ll flesh out all these details in the next video. But the key intuition to take away is that this way the RNN marches forward generating one word at a time, until eventually it generates maybe the EOS and at every step, there are these attention weighs. $\alpha^{&lt;t, t’&gt;}$ that tells it, when you’re trying to generate the T, English word, how much should you be paying attention to the T prime French words.And this allows it on every time step to look only maybe within a local window of the French sentence to pay attention to, when generating a specific English word. I hope this video conveys some intuition about Attention Model and that we now have a rough sense of, maybe how the algorithm works. Let’s go to the next video to flesh out the details of the Attention Model. 08_attention-modelIn the last video, you saw how the attention model allows a neural network to pay attention to only part of an input sentence while it’s generating a translation, much like a human translator might. Let’s now formalize that intuition into the exact details of how you would implement an attention model. So same as in the previous video, let’s assume you have an input sentence and you use a bidirectional RNN, or bidirectional GRU, or bidirectional LSTM to compute features on every word. In practice, GRUs and LSTMs are often used for this, with maybe LSTMs be more common. And so for the forward occurrence, you have a forward occurrence first time step. Activation backward occurrence, first time step. Activation forward occurrence, second time step. Activation backward and so on. For all of them in just a forward fifth time step a backwards fifth time step. We had a zero here technically we can also have I guess a backwards sixth as a factor of all zero, actually that’s a factor of all zeroes. And then to simplify the notation going forwards at every time step, even though you have the features computed from the forward occurrence and from the backward occurrence in the bidirectional RNN. I’m just going to use $a^{t’}$ to represent both of these concatenated together, $a^{&lt;t^{&lt;\prime&gt;}&gt;}=({\overrightarrow a^{&lt;t^{\prime}&gt;}},{\overleftarrow a^{&lt;t^{\prime}&gt;}})$. So a of t is going to be a feature vector for time step t. Although to be consistent with notation, we’re using second, I’m going to call this $t^\prime$. Actually, I’m going to use $t^{\prime}$ to index into the words in the French sentence. Next, we have our forward only, so it’s a single direction RNN with state s to generate the translation. And so the first time step, it should generate $y^{}$ and just will have as input some context C. And if you want to index it with time I guess you could write a $C^{}$ but sometimes I just right C without the superscript one. And this will depend on the attention parameters so $\alpha^{&lt;1,1&gt;}$, $\alpha^{&lt;1,2&gt;}$ and so on tells us how much attention. And so these alpha parameters tells us how much the context would depend on the features we’re getting or the activations we’re getting from the different time steps. And so the way we define the context is actually be a way to some of the features from the different time steps weighted by these attention weights. So more formally the attention weights will satisfy this that they are all be non-negative, so it will be a zero positive and they’ll sum to one. We’ll see later how to make sure this is true. And we will have the context or the context at time one often drop that superscript that’s going to be sum over $t^{\prime}$, all the values of $t^{\prime}$ of this weighted sum of these activations $c^{} = \sum\alpha^{&lt;1, t^{\prime}&gt;}a^{&lt;t^{\prime}&gt;}$. So this term, $\alpha^{&lt;1, t^{\prime}&gt;}$, here are the attention weights and this term, $a^{&lt;t^{\prime}&gt;}$, here comes from here $a^{&lt;t^{\prime}&gt;}=({\overrightarrow a^{&lt;t^{\prime}&gt;}},{\overleftarrow a ^{&lt;t^{\prime}&gt;}})$. So $\alpha^{&lt;t, t^{\prime}&gt;}$ is the amount of attention that’s $y^t$ should pay to $a^{t^{\prime}}$. So in other words, when you’re generating the t of the output words, how much you should be paying attention to the $t^{\prime}$th input to word. So that’s one step of generating the output and then at the next time step, you generate the second output and is again done some of where now you have a new set of attention weights on they to find a new way to sum. That generates a new context. This, $y^{}$, is also input and that allows you to generate the second word. Only now just this way to sum becomes the context of the second time step is $c^{} = \sum\alpha^{&lt;2, t^{\prime}&gt;}a^{&lt;t^{\prime}&gt;}$. So using these context vectors. $c^{}$ right there back, $c^{}$, and so on. This network uo here, which circled in purple color, here looks like a pretty standard RNN sequence with the context vectors as output and we can just generate the translation one word at a time. We have also define how to compute the context vectors in terms of these attention ways and those features of the input sentence. So the only remaining thing to do is to define how to actually compute these attention weights. Let’s do that on the next slide. So just to recap, $\alpha^{&lt;t, t^{\prime}&gt;}$ is the amount of attention you should paid to $a^{&lt;t^{\prime}&gt;}$ when you’re trying to generate the $t^{th}$ words in the output translation. So let me just write down the formula and we talk of how this works. This is formula you could use the compute $\alpha^{&lt;t, t^{\prime}&gt;}$ which is going to compute these terms $e^{&lt;t, t^{\prime}&gt;}$ and then use essentially a softmax to make sure that these weights sum to one if you sum over $t^{\prime}$. So for every fix value of t, these things, ${\alpha^{}} =\frac{{\exp({e^{}})}}{{\sum\limits_{t^{\prime} = 1}^{{T_x}} {\exp({e^{}})}}}$ , sum to one if you’re summing over $t^{\prime}$. And using this softmax prioritization, just ensures this properly sums to one. Now how do we compute these factors e. Well, one way to do so is to use a small neural network as follows. So $s^{}$ was the neural network state from the previous time step. So here is the network we have.If you’re trying to generate $y^t$ then $s^{}$ was the hidden state from the previous step that just fell into $s^t$ and that’s one input to very small neural network. Usually, one hidden layer in neural network because you need to compute these a lot. And then $a^{&lt;t^{\prime}&gt;}$ the features from time step $t^{\prime}$ is the other inputs. And the intuition is, if you want to decide how much attention to pay to the activation of $t^{\prime}$. Well, the things that seems like it should depend the most on is what is your own hidden state activation from the previous time step. You don’t have the current state activation yet because of context feeds into this so you haven’t computed that. But look at whatever you’re hidden stages of this RNN generating the upper translation and then for each of the positions, each of the words look at their features. So it seems pretty natural that $\alpha^{&lt;t, t^{\prime}&gt;}$ and $e^{&lt;t, t^{\prime}&gt;}$ should depend on these two quantities. But we don’t know what the function is. So one thing you could do is just train a very small neural network to learn whatever this function should be. And trust that back propagation trust gradient descent to learn the right function. And it turns out that if you implemented this whole model and train it with gradient descent, the whole thing actually works. This little neural network does a pretty decent job telling you how much attention $y^t$ should pay to $a^{&lt;t^{\prime}&gt;}$and this formula ${\alpha^{}} =\frac{{\exp({e^{}})}}{{\sum\limits_{t^{\prime} = 1}^{{T_x}} {\exp({e^{}})}}}$ makes sure that the attention weights sum to one and then as you chug along generating one word at a time, this neural network actually pays attention to the right parts of the input sentence that learns all this automatically using gradient descent.Now, one downside to this algorithm is that it does take quadratic time or quadratic cost to run this algorithm. If you have $T_x$ words in the input and $T_y$ words in the output then the total number of these attention parameters are going to be $T_x$ times $T_y$. And so this algorithm runs in quadratic cost. Although in machine translation applications where neither input nor output sentences is usually that long maybe quadratic cost is actually acceptable. Although, there is some research work on trying to reduce costs as well. Now, so far up in describing the attention idea in the context of machine translation. Without going too much into detail this idea has been applied to other problems as well. So just image captioning. So in the image capturing problem the task is to look at the picture and write a caption for that picture. So in this paper set to the bottom by Kevin Chu, Jimmy Barr, Ryan Kiros, Kelvin Shaw, Aaron Korver, Russell Zarkutnov, Virta Zemo, and Andrew Benjo they also showed that you could have a very similar architecture. Look at the picture and pay attention only to parts of the picture at a time while you’re writing a caption for a picture. So if you’re interested, then I encourage you to take a look at that paper as well. And you get to play with all this and more in the programming exercise.Whereas machine translation is a very complicated problem in the prior exercise you get to implement and play of the attention while you yourself for the date normalization problem. So the problem inputting a date like this. This actually has a date of the Apollo Moon landing and normalizing it into standard formats or a date like this and having a neural network a sequence, sequence model normalize it to this format. This by the way is the birthday of William Shakespeare. Also it’s believed to be. And what you see in prior exercises as you can train a neural network to input dates in any of these formats and have it use an attention model to generate a normalized format for these dates. One other thing that sometimes fun to do is to look at the visualizations of the attention weights. So here’s a machine translation example and here were plotted in different colors. the magnitude of the different attention weights. I don’t want to spend too much time on this but you find that the corresponding input and output words you find that the attention weights will tend to be high. Thus, suggesting that when it’s generating a specific word in output is, usually paying attention to the correct words in the input and all this including learning where to pay attention when was all learned using propagation with an attention model. So that’s it for the attention model really one of the most powerful ideas in deep learning. I hope you enjoy implementing and playing with these ideas yourself later in this week’s programming exercises. 02_speech-recognition-audio-data01_speech-recognitionOne of the most exciting developments were sequence-to-sequence models has been the rise of very accurate speech recognition. We’re nearing the end of the course, we want to take just a couple of videos to give you a sense of how these sequence-to-sequence models are applied to audio data, such as the speech. So, what is the speech recognition problem? You’re given an audio clip, x, and your job is to automatically find a text transcript, y. So, an audio clip, if you plot it looks like this, the horizontal axis here is time, and what a microphone does is it really measures minuscule changes in air pressure, and the way you’re hearing my voice right now is that your ear is detecting little changes in air pressure, probably generated either by your speakers or by a headset. And some audio clips like this plots with the air pressure against time. And, if this audio clip is of me saying, “the quick brown fox”, then hopefully, a speech recognition algorithm can input that audio clip and output that transcript. And because even the human ear doesn’t process raw wave forms, but the human ear has physical structures that measures the amounts of intensity of different frequencies, there is, a common pre-processing step for audio data is to run your raw audio clip and generate a spectrogram. So, this is the plots where the horizontal axis is time, and the vertical axis is frequencies, and intensity of different colors shows the amount of energy. So, how loud is the sound at different frequencies? At different times? And so, these types of spectrograms, or you might also hear people talk about false blank outputs, is often commonly applied pre-processing step before audio is pass into in the running algorithm. And the human ear does a computation pretty similar to this pre-processing step. So, one of the most exciting trends in speech recognition is that, once upon a time, speech recognition systems used to be built using phonemes and this where, I want to say hand-engineered basic units of cells. So, the quick brown fox represented as phonemes. I’m going to simplify a bit, let say, “The” has a “de” and “e” sound and Quick, has a “ku” and “wu”, “ik”, “k” sound, and linguist used to write off these basic units of sound, and try the Greek language down to these basic units of sound. So, brown, this aren’t the official phonemes which are written with more complicated notation, but linguists use to hypothesize that writing down audio in terms of these basic units of sound called phonemes would be the best way to do speech recognition. But with end-to-end deep learning, we’re finding that phonemes representations are no longer necessary. But instead, you can built systems that input an audio clip and directly output a transcript without needing to use hand-engineered representations like these. One of the things that made this possible was going to much larger data sets. So, academic data sets on speech recognition might be as a 300 hours, and in academia, 3000 hour data sets of transcribed audio would be considered reasonable size, so lot of research has been done, a lot of research papers that are written on data sets there are several thousand voice. But, the best commercial systems are now trains on over 10,000 hours and sometimes over a 100,000 hours of audio. And, it’s really moving to a much larger audio data sets, transcribe audio data sets were both x and y, together with deep learning algorithm, that has driven a lot of progress is speech recognition. So, how do you build a speech recognition system? In the last video, we’re talking about the attention model. So, one thing you could do is actually do that, where on the horizontal axis, you take in different time frames of the audio input, and then you have an attention model try to output the transcript like, “the quick brown fox”, or what it was said. One other method that seems to work well is to use the CTC cost for speech recognition. CTC stands for Connection is Temporal Classification and is due to Alex Graves, Santiago Fernandes, Faustino Gomez, and Jürgen Schmidhuber. So, here’s the idea. Let’s say the audio clip was someone saying, “the quick brown fox”. We’re going to use a new network structured like this with an equal number of input x’s and output y’s, and I have drawn a simple of what uni-directional for the RNN for this, but in practice, this will usually be a bidirectional LSTM and bidirectional GRU and usually, a deeper model. But notice that the number of time steps here is very large and in speech recognition, usually the number of input time steps is much bigger than the number of output time steps. So, for example, if you have 10 seconds of audio and your features come at a 100 hertz so 100 samples per second, then a 10 second audio clip would end up with a thousand inputs. Right, so it’s 100 hertz times 10 seconds, and so with a thousand inputs. But your output might not have a thousand alphabets, might not have a thousand characters. So, what do you do? The CTC cost function allows the RNN to generate an output like this ttt, there’s a special character called the blank character, which we’re going to write as an underscore here, h_eee___, and then maybe a space, we’re going to write like this, so that a space and then _ qqq. And, this is considered a correct output for the first parts of the space, quick with the Q, and the basic rule for the CTC cost function is to collapse repeated characters not separated by “blank”. So, to be clear, I’m using this underscore to denote a special blank character and that’s different than the space character. So, there is a space here between the and quick, so I should output a space. But, by collapsing repeated characters, not separated by blank, it actually collapse the sequence into t, h, e, and then space, and q, and this allows your network to have a thousand outputs by repeating characters allow the times. So, inserting a bunch of blank characters and still ends up with a much shorter output text transcript. So, this phrase here “the quick brown fox” including spaces actually has 19 characters, and if somehow, the newer network is forced upwards of a thousand characters by allowing the network to insert blanks and repeated characters and can still represent this 19 character upwards with this 1000 outputs of values of Y. So, this paper by Alex Grace, as well as by those deep speech recognition system, which I was involved in, used this idea to build effective Speech recognition systems. So, I hope that gives you a rough sense of how speech recognition models work. Attention like models work and CTC models work and present two different options of how to go about building these systems. Now, today, building effective where production skills speech recognition system is a pretty significant effort and requires a very large data set. But, what I like to do in the next video is share you, how you can build a trigger word detection system, where keyword detection system which is actually much easier and can be done with even a smaller or more reasonable amount of data. So, let’s talk about that in the next video. 02_trigger-word-detectionyou’ve now learned so much about deep learning and sequence models that we can actually describe a trigger word system quite simply just on one slide as you see in this video but when the rise of speech recognition have been more and more devices you can wake up with your voice and those are sometimes called trigger word detection systems so let’s see how you can build a trigger word system. Examples of triggering systems include Amazon echo which is broken out with that word Alexa. The Baidu DuerOs part devices woken up with face xiaodunihao. Apple Siri working out with hey Siri and Google home woken up with Ok Google. So stands the trigger word detection that if you have say an Amazon echo in your living room, you can walk the living room and just say: “Alexa what time is it” and have it wake up. It’ll be triggered by the words of Alexa and answer your voice query. So if you can build a trigger word detection system maybe you can make your computer do something by telling it computer activate. One of my friends also works on turning on an offer particular lamp using a trigger word kind of as a fun project but what I want to show you is how you can build a trigger word detection system. Now the trigger word detection literature is still evolving so there actually isn’t a single universally agreed on algorithm for trigger word detection yet the literature on trigger word detection algorithm is still evolving so there isn’t wide consensus yet on what’s the best algorithm for trigger word detection so I’m just going to show you one example of an algorithm you can use. now you’ve seen our ends like this and what we really do is take an audio clip maybe compute spectrogram features and that generates features $x^{} x^{} x^{}$ or audio features $x^{} x^{} x^{}$ that you pass through an RNN and so all that remains to be done is to define the target labels Y so if this point in the audio clip is when someone just finished saying the trigger word such as “Alexa”, “nihaobaidu” or “hey Siri” or “Okay Google” then in the training sets you can set the target labels to be zero for everything before that point and right after that to set the target label of one and then if a little bit later on you know the trigger word was set again and the trigger word said at this point then you can again set the target label to be one right after that now this type of labeling scheme for an RNN you know could work actually this won’t actually work reasonably well. One slight disadvantage of this is it creates a very imbalanced training set so if a lot more zeros than ones. So one other thing you could do that it’s getting a little bit of a hack but could make them all the little bit easy to train is instead of setting only a single time step to output one you can actually make an output a few ones for several times or for a fixed period of time before reverting back to zero so and that slightly evens out the ratio of ones to zeros but this is a little bit of a hack. But if this is when in the audio clipper trigger where the set then right after that you can set the target label to one and if this is the trigger words said again, then right after that just when you want the RNN to output one so you get to play more of this as well in the programming exercise but so I think you should feel quite proud of yourself we’ve learned enough about the learning that it just takes one picture at one slide to this to describe something as complicated as trigger word detection and based on this I hope you’d be able to implement something that works and allows you to detect trigger words but you see more of this in the program exercise. So that’s it for trigger words and I hope you feel quite proud of yourself for how much you’ve learned about deep learning that you can now describe trigger words in just one slide in a few minutes and that you’ve been hopeful II implemented and get it to work maybe even make it do something fun in your house that I’m like turn on or turn off um you could do something like a computer when you’re when someone else says they trigger words on this is the last technical video of this course and to wrap up in this course on sequence models you learned about rnns including both gr use and LS TMS and then in the second week you learned a lot about word embeddings and how they learn representations of words and then in this week you learned about the attention model as well as how to use it to process audio data and I hope you have fun implementing all of these ideas in this beast program sighs let’s go on to the last video. conclusion-and-thank-youcongratulations on making it this far I just wanna wrap up and leave you with a few final thoughts we’ve been on quite a journey together but if you’ve taken the whole specialization then you’ve learned about new networks and deep learning how to improve deep neural networks of the structure machine learning projects convolutional neural networks and then in this most recent course sequence models and I know you work really hard and I also hope you feel very proud of yourself for your hard work and for how much you’ve done.so I want to leave you one maybe important thought which is that I think deep learning is a superpower with deep learning algorithms you can make a computer see you can have a computer synthesize novel art or synthesized music or you can have a computer translate from one language to another maybe have it locally radiology image and render a medical diagnosis or build pieces of a car that can drive itself and if that isn’t a superpower I don’t know what is and as we wrap up this sequence of courses as we wrap up this specialization I hope that you will find ways to use these ideas to further your career to pursue your dreams but perhaps most important to do whatever you think is the best work you can do our humanity the world today has challenges but with the power of a on power of deep learning I think we can make it a much better place and now that you have this superpower I hope you will use it to go out there and make life better for yourself but also for other people and of course I also hope you feel very proud of your accomplishments in the power far you’ve come and of all that you’ve learned and when you complete this sequence of causes you should also share it on social media like Twitter or Facebook and let your friends know. and finally the very last thing I want to say to you is congratulations on Nikolas I hope you feel great about your accomplishments but also I want to thank you very much I know that you have a busy life but despite that spends a lot of time watching these videos and maybe spent a long time also working on the quizzes and the programming exercises I hope you enjoyed it and you got a lot out of the process but I’m also very grateful for all your time you spend and for all your hard work you put into learning these materials so thank you very much.]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Operations on word vectors]]></title>
    <url>%2F2018%2F06%2F03%2FOperations%2Bon%2Bword%2Bvectors%2B-%2Bv2%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 2nd week and the copyright belongs to deeplearning.ai. Operations on word vectorsWelcome to your first assignment of this week! Because word embeddings are very computionally expensive to train, most ML practitioners will load a pre-trained set of embeddings. After this assignment you will be able to: Load pre-trained word vectors, and measure similarity using cosine similarity Use word embeddings to solve word analogy problems such as Man is to Woman as King is to __. Modify word embeddings to reduce their gender bias Let’s get started! Run the following cell to load the packages you will need. 12import numpy as npfrom w2v_utils import * Using TensorFlow backend. Next, lets load the word vectors. For this assignment, we will use 50-dimensional GloVe vectors to represent words. Run the following cell to load the word_to_vec_map. 1words, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt') You’ve loaded: words: set of words in the vocabulary. word_to_vec_map: dictionary mapping words to their GloVe vector representation. You’ve seen that one-hot vectors do not do a good job cpaturing what words are similar. GloVe vectors provide much more useful information about the meaning of individual words. Lets now see how you can use GloVe vectors to decide how similar two words are. 1 - Cosine similarityTo measure how similar two words are, we need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows: $$\text{CosineSimilarity(u, v)} = \frac {u . v} {||u||_2 ||v||_2} = cos(\theta) \tag{1}$$ where $u.v$ is the dot product (or inner product) of two vectors, $||u||_2$ is the norm (or length) of the vector $u$, and $\theta$ is the angle between $u$ and $v$. This similarity depends on the angle between $u$ and $v$. If $u$ and $v$ are very similar, their cosine similarity will be close to 1; if they are dissimilar, the cosine similarity will take a smaller value. Figure 1: The cosine of the angle between two vectors is a measure of how similar they are Exercise: Implement the function cosine_similarity() to evaluate similarity between word vectors. Reminder: The norm of $u$ is defined as $ ||u||_2 = \sqrt{\sum_{i=1}^{n} u_i^2}$ 1234567891011121314151617181920212223242526272829# GRADED FUNCTION: cosine_similaritydef cosine_similarity(u, v): """ Cosine similarity reflects the degree of similariy between u and v Arguments: u -- a word vector of shape (n,) v -- a word vector of shape (n,) Returns: cosine_similarity -- the cosine similarity between u and v defined by the formula above. """ distance = 0.0 ### START CODE HERE ### # Compute the dot product between u and v (≈1 line) dot = np.dot(u, v); # Compute the L2 norm of u (≈1 line) norm_u = np.linalg.norm(u); # Compute the L2 norm of v (≈1 line) norm_v = np.linalg.norm(v); # Compute the cosine similarity defined by formula (1) (≈1 line) cosine_similarity = dot / norm_u / norm_v; ### END CODE HERE ### return cosine_similarity 123456789101112father = word_to_vec_map["father"]mother = word_to_vec_map["mother"]ball = word_to_vec_map["ball"]crocodile = word_to_vec_map["crocodile"]france = word_to_vec_map["france"]italy = word_to_vec_map["italy"]paris = word_to_vec_map["paris"]rome = word_to_vec_map["rome"]print("cosine_similarity(father, mother) = ", cosine_similarity(father, mother))print("cosine_similarity(ball, crocodile) = ",cosine_similarity(ball, crocodile))print("cosine_similarity(france - paris, rome - italy) = ",cosine_similarity(france - paris, rome - italy)) cosine_similarity(father, mother) = 0.890903844289 cosine_similarity(ball, crocodile) = 0.274392462614 cosine_similarity(france - paris, rome - italy) = -0.675147930817 Expected Output: cosine_similarity(father, mother) = 0.890903844289 cosine_similarity(ball, crocodile) = 0.274392462614 cosine_similarity(france - paris, rome - italy) = -0.675147930817 After you get the correct expected output, please feel free to modify the inputs and measure the cosine similarity between other pairs of words! Playing around the cosine similarity of other inputs will give you a better sense of how word vectors behave. 2 - Word analogy taskIn the word analogy task, we complete the sentence “a is to b as c is to ____“. An example is ‘man is to woman as king is to queen‘ . In detail, we are trying to find a word d, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner: $e_b - e_a \approx e_d - e_c$. We will measure the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity. Exercise: Complete the code below to be able to perform word analogies! 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# GRADED FUNCTION: complete_analogydef complete_analogy(word_a, word_b, word_c, word_to_vec_map): """ Performs the word analogy task as explained above: a is to b as c is to ____. Arguments: word_a -- a word, string word_b -- a word, string word_c -- a word, string word_to_vec_map -- dictionary that maps words to their corresponding vectors. Returns: best_word -- the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity """ # convert words to lower case word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower() ### START CODE HERE ### # Get the word embeddings v_a, v_b and v_c (≈1-3 lines) e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]; ### END CODE HERE ### words = word_to_vec_map.keys() max_cosine_sim = -100 # Initialize max_cosine_sim to a large negative number best_word = None # Initialize best_word with None, it will help keep track of the word to output # loop over the whole word vector set for w in words: # to avoid best_word being one of the input words, pass on them. if w in [word_a, word_b, word_c] : continue ### START CODE HERE ### # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c) (≈1 line) cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c); # If the cosine_sim is more than the max_cosine_sim seen so far, # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines) if cosine_sim &gt; max_cosine_sim: max_cosine_sim = cosine_sim; best_word = w; ### END CODE HERE ### return best_word Run the cell below to test your code, this may take 1-2 minutes. 123triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]for triad in triads_to_try: print ('&#123;&#125; -&gt; &#123;&#125; :: &#123;&#125; -&gt; &#123;&#125;'.format( *triad, complete_analogy(*triad,word_to_vec_map))) italy -&gt; italian :: spain -&gt; spanish india -&gt; delhi :: japan -&gt; tokyo man -&gt; woman :: boy -&gt; girl small -&gt; smaller :: large -&gt; larger Expected Output: italy -&gt; italian :: spain -&gt; spanish india -&gt; delhi :: japan -&gt; tokyo man -&gt; woman :: boy -&gt; girl small -&gt; smaller :: large -&gt; larger Once you get the correct expected output, please feel free to modify the input cells above to test your own analogies. Try to find some other analogy pairs that do work, but also find some where the algorithm doesn’t give the right answer: For example, you can try small-&gt;smaller as big-&gt;?. Congratulations!You’ve come to the end of this assignment. Here are the main points you should remember: Cosine similarity a good way to compare similarity between pairs of word vectors. (Though L2 distance works too.) For NLP applications, using a pre-trained set of word vectors from the internet is often a good way to get started. Even though you have finished the graded portions, we recommend you take a look too at the rest of this notebook. Congratulations on finishing the graded portions of this notebook! 3 - Debiasing word vectors (OPTIONAL/UNGRADED)In the following exercise, you will examine gender biases that can be reflected in a word embedding, and explore algorithms for reducing the bias. In addition to learning about the topic of debiasing, this exercise will also help hone your intuition about what word vectors are doing. This section involves a bit of linear algebra, though you can probably complete it even without being expert in linear algebra, and we encourage you to give it a shot. This portion of the notebook is optional and is not graded. Lets first see how the GloVe word embeddings relate to gender. You will first compute a vector $g = e_{woman}-e_{man}$, where $e_{woman}$ represents the word vector corresponding to the word woman, and $e_{man}$ corresponds to the word vector corresponding to the word man. The resulting vector $g$ roughly encodes the concept of “gender”. (You might get a more accurate representation if you compute $g_1 = e_{mother}-e_{father}$, $g_2 = e_{girl}-e_{boy}$, etc. and average over them. But just using $e_{woman}-e_{man}$ will give good enough results for now.) 12g = word_to_vec_map['woman'] - word_to_vec_map['man']print(g) [-0.087144 0.2182 -0.40986 -0.03922 -0.1032 0.94165 -0.06042 0.32988 0.46144 -0.35962 0.31102 -0.86824 0.96006 0.01073 0.24337 0.08193 -1.02722 -0.21122 0.695044 -0.00222 0.29106 0.5053 -0.099454 0.40445 0.30181 0.1355 -0.0606 -0.07131 -0.19245 -0.06115 -0.3204 0.07165 -0.13337 -0.25068714 -0.14293 -0.224957 -0.149 0.048882 0.12191 -0.27362 -0.165476 -0.20426 0.54376 -0.271425 -0.10245 -0.32108 0.2516 -0.33455 -0.04371 0.01258 ] Now, you will consider the cosine similarity of different words with $g$. Consider what a positive value of similarity means vs a negative cosine similarity. 1234567print ('List of names and their similarities with constructed vector:')# girls and boys namename_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']for w in name_list: print (w, cosine_similarity(word_to_vec_map[w], g)) List of names and their similarities with constructed vector: john -0.23163356146 marie 0.315597935396 sophie 0.318687898594 ronaldo -0.312447968503 priya 0.17632041839 rahul -0.169154710392 danielle 0.243932992163 reza -0.079304296722 katy 0.283106865957 yasmin 0.233138577679 As you can see, female first names tend to have a positive cosine similarity with our constructed vector $g$, while male first names tend to have a negative cosine similarity. This is not suprising, and the result seems acceptable. But let’s try with some other words. 12345print('Other words and their similarities:')word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', 'technology', 'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']for w in word_list: print (w, cosine_similarity(word_to_vec_map[w], g)) Other words and their similarities: lipstick 0.276919162564 guns -0.18884855679 science -0.0608290654093 arts 0.00818931238588 literature 0.0647250443346 warrior -0.209201646411 doctor 0.118952894109 tree -0.0708939917548 receptionist 0.330779417506 technology -0.131937324476 fashion 0.0356389462577 teacher 0.179209234318 engineer -0.0803928049452 pilot 0.00107644989919 computer -0.103303588739 singer 0.185005181365 Do you notice anything surprising? It is astonishing how these results reflect certain unhealthy gender stereotypes. For example, “computer” is closer to “man” while “literature” is closer to “woman”. Ouch! We’ll see below how to reduce the bias of these vectors, using an algorithm due to Boliukbasi et al., 2016. Note that some word pairs such as “actor”/“actress” or “grandmother”/“grandfather” should remain gender specific, while other words such as “receptionist” or “technology” should be neutralized, i.e. not be gender-related. You will have to treat these two type of words differently when debiasing. 3.1 - Neutralize bias for non-gender specific wordsThe figure below should help you visualize what neutralizing does. If you’re using a 50-dimensional word embedding, the 50 dimensional space can be split into two parts: The bias-direction $g$, and the remaining 49 dimensions, which we’ll call $g_{\perp}$. In linear algebra, we say that the 49 dimensional $g_{\perp}$ is perpendicular (or “othogonal”) to $g$, meaning it is at 90 degrees to $g$. The neutralization step takes a vector such as $e_{receptionist}$ and zeros out the component in the direction of $g$, giving us $e_{receptionist}^{debiased}$. Even though $g_{\perp}$ is 49 dimensional, given the limitations of what we can draw on a screen, we illustrate it using a 1 dimensional axis below. Figure 2: The word vector for “receptionist” represented before and after applying the neutralize operation. Exercise: Implement neutralize() to remove the bias of words such as “receptionist” or “scientist”. Given an input embedding $e$, you can use the following formulas to compute $e^{debiased}$: $$e^{bias_component} = \frac{e \cdot g}{||g||_2^2} * g\tag{2}$$$$e^{debiased} = e - e^{bias_component}\tag{3}$$ If you are an expert in linear algebra, you may recognize $e^{bias_component}$ as the projection of $e$ onto the direction $g$. If you’re not an expert in linear algebra, don’t worry about this. 123456789101112131415161718192021222324252627def neutralize(word, g, word_to_vec_map): """ Removes the bias of "word" by projecting it on the space orthogonal to the bias axis. This function ensures that gender neutral words are zero in the gender subspace. Arguments: word -- string indicating the word to debias g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender) word_to_vec_map -- dictionary mapping words to their corresponding vectors. Returns: e_debiased -- neutralized word vector representation of the input "word" """ ### START CODE HERE ### # Select word vector representation of "word". Use word_to_vec_map. (≈ 1 line) e = word_to_vec_map[word]; # Compute e_biascomponent using the formula give above. (≈ 1 line) e_biascomponent = np.dot(e, g) / np.dot(g, g) * g; # Neutralize e by substracting e_biascomponent from it # e_debiased should be equal to its orthogonal projection. (≈ 1 line) e_debiased = e - e_biascomponent; ### END CODE HERE ### return e_debiased 12345e = "receptionist"print("cosine similarity between " + e + " and g, before neutralizing: ", cosine_similarity(word_to_vec_map["receptionist"], g))e_debiased = neutralize("receptionist", g, word_to_vec_map)print("cosine similarity between " + e + " and g, after neutralizing: ", cosine_similarity(e_debiased, g)) cosine similarity between receptionist and g, before neutralizing: 0.330779417506 cosine similarity between receptionist and g, after neutralizing: -5.60374039375e-17 Expected Output: The second result is essentially 0, up to numerical roundof (on the order of $10^{-17}$). cosine similarity between receptionist and g, before neutralizing: : 0.330779417506 cosine similarity between receptionist and g, after neutralizing: : -3.26732746085e-17 3.2 - Equalization algorithm for gender-specific wordsNext, lets see how debiasing can also be applied to word pairs such as “actress” and “actor.” Equalization is applied to pairs of words that you might want to have differ only through the gender property. As a concrete example, suppose that “actress” is closer to “babysit” than “actor.” By applying neutralizing to “babysit” we can reduce the gender-stereotype associated with babysitting. But this still does not guarantee that “actor” and “actress” are equidistant from “babysit.” The equalization algorithm takes care of this. The key idea behind equalization is to make sure that a particular pair of words are equi-distant from the 49-dimensional $g_\perp$. The equalization step also ensures that the two equalized steps are now the same distance from $e_{receptionist}^{debiased}$, or from any other work that has been neutralized. In pictures, this is how equalization works: The derivation of the linear algebra to do this is a bit more complex. (See Bolukbasi et al., 2016 for details.) But the key equations are: $$ \mu = \frac{e_{w1} + e_{w2}}{2}\tag{4}$$ $$ \mu_{B} = \frac {\mu \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis}\tag{5}$$ $$\mu_{\perp} = \mu - \mu_{B} \tag{6}$$ $$ e_{w1B} = \frac {e_{w1} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} \text{bias_axis}\tag{7}$$$$ e_{w2B} = \frac {e_{w2} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} \text{bias_axis}\tag{8}$$ $$e_{w1B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w1B}} - \mu_B} {|(e_{w1} - \mu_{\perp}) - \mu_B)|} \tag{9}$$ $$e_{w2B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w2B}} - \mu_B} {|(e_{w2} - \mu_{\perp}) - \mu_B)|} \tag{10}$$ $$e_1 = e_{w1B}^{corrected} + \mu_{\perp} \tag{11}$$$$e_2 = e_{w2B}^{corrected} + \mu_{\perp} \tag{12}$$ Exercise: Implement the function below. Use the equations above to get the final equalized version of the pair of words. Good luck! 1234567891011121314151617181920212223242526272829303132333435363738394041def equalize(pair, bias_axis, word_to_vec_map): """ Debias gender specific words by following the equalize method described in the figure above. Arguments: pair -- pair of strings of gender specific words to debias, e.g. ("actress", "actor") bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender word_to_vec_map -- dictionary mapping words to their corresponding vectors Returns e_1 -- word vector corresponding to the first word e_2 -- word vector corresponding to the second word """ ### START CODE HERE ### # Step 1: Select word vector representation of "word". Use word_to_vec_map. (≈ 2 lines) w1, w2 = pair; e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]; # Step 2: Compute the mean of e_w1 and e_w2 (≈ 1 line) mu = (e_w1 + e_w2) / 2; # Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines) mu_B = np.dot(mu, bias_axis) / np.dot(bias_axis, bias_axis) * bias_axis; mu_orth = mu - mu_B; # Step 4: Use equations (7) and (8) to compute e_w1B and e_w2B (≈2 lines) e_w1B = np.dot(e_w1, bias_axis) / np.dot(bias_axis, bias_axis) * bias_axis; e_w2B = np.dot(e_w2, bias_axis) / np.dot(bias_axis, bias_axis) * bias_axis; # Step 5: Adjust the Bias part of e_w1B and e_w2B using the formulas (9) and (10) given above (≈2 lines) corrected_e_w1B = np.sqrt(np.absolute(1 - np.linalg.norm(mu_orth) ** 2)) * (e_w1B - mu_B) / np.linalg.norm(e_w1 - mu_orth - mu_B); corrected_e_w2B = np.sqrt(np.absolute(1 - np.linalg.norm(mu_orth) ** 2)) * (e_w2B - mu_B) / np.linalg.norm(e_w2 - mu_orth - mu_B); # Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines) e1 = corrected_e_w1B + mu_orth; e2 = corrected_e_w2B + mu_orth; ### END CODE HERE ### return e1, e2 12345678print("cosine similarities before equalizing:")print("cosine_similarity(word_to_vec_map[\"man\"], gender) = ", cosine_similarity(word_to_vec_map["man"], g))print("cosine_similarity(word_to_vec_map[\"woman\"], gender) = ", cosine_similarity(word_to_vec_map["woman"], g))print()e1, e2 = equalize(("man", "woman"), g, word_to_vec_map)print("cosine similarities after equalizing:")print("cosine_similarity(e1, gender) = ", cosine_similarity(e1, g))print("cosine_similarity(e2, gender) = ", cosine_similarity(e2, g)) cosine similarities before equalizing: cosine_similarity(word_to_vec_map[&quot;man&quot;], gender) = -0.117110957653 cosine_similarity(word_to_vec_map[&quot;woman&quot;], gender) = 0.356666188463 cosine similarities after equalizing: cosine_similarity(e1, gender) = -0.700436428931 cosine_similarity(e2, gender) = 0.700436428931 Expected Output: cosine similarities before equalizing: cosine_similarity(word_to_vec_map[“man”], gender) = -0.117110957653 cosine_similarity(word_to_vec_map[“woman”], gender) = 0.356666188463 cosine similarities after equalizing: cosine_similarity(u1, gender) = -0.700436428931 cosine_similarity(u2, gender) = 0.700436428931 Please feel free to play with the input words in the cell above, to apply equalization to other pairs of words. These debiasing algorithms are very helpful for reducing bias, but are not perfect and do not eliminate all traces of bias. For example, one weakness of this implementation was that the bias direction $g$ was defined using only the pair of words _woman_ and _man_. As discussed earlier, if $g$ were defined by computing $g_1 = e_{woman} - e_{man}$; $g_2 = e_{mother} - e_{father}$; $g_3 = e_{girl} - e_{boy}$; and so on and averaging over them, you would obtain a better estimate of the “gender” dimension in the 50 dimensional word embedding space. Feel free to play with such variants as well. CongratulationsYou have come to the end of this notebook, and have seen a lot of the ways that word vectors can be used as well as modified. Congratulations on finishing this notebook! References: The debiasing algorithm is from Bolukbasi et al., 2016, Man is to Computer Programmer as Woman is toHomemaker? Debiasing Word Embeddings The GloVe word embeddings were due to Jeffrey Pennington, Richard Socher, and Christopher D. Manning. (https://nlp.stanford.edu/projects/glove/)]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[natural language processing word embeddings]]></title>
    <url>%2F2018%2F06%2F02%2F02_natural-language-processing-word-embeddings%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal lecture note after studying the course nlp sequence models at the 2nd week and the copyright belongs to deeplearning.ai. 01_introduction-to-word-embeddings01_word-representationHello, and welcome back. Last week, we learned about RNNs, GRUs, and LSTMs. In this week, you see how many of these ideas can be applied to NLP, to Natural Language Processing, which is one of the features of AI because it’s really being revolutionized by deep learning. One of the key ideas you learn about is word embeddings, which is a way of representing words. That let your algorithms automatically understand analogies like that, man is to woman, as king is to queen, and many other examples. And through these ideas of word embeddings, you’ll be able to build NPL applications, even with models the size of, usually of relatively small label training sets. Finally towards the end of the week, you’ll see how to debias word embeddings. That’s to reduce undesirable gender or ethnicity or other types of bias that learning algorithms can sometimes pick up. So with that, let’s get started with a discussion on word representation. So far, we’ve been representing words using a vocabulary of words, and a vocabulary from the previous week might be say, 10,000 words. And we’ve been representing words using a one-hot vector. So for example, if man is word number 5391 in this dictionary, then you represent him with a vector with one in position 5391. And I’m also going to use O subscript 5391 to represent this factor, where O here stands for one-hot. And then, if woman is word number 9853, then you represent it with O subscript 9853 which just has a one in position 9853 and zeros elsewhere. And then other words king, queen, apple, orange will be similarly represented with one-hot vector. One of the weaknesses of this representation is that it treats each word as a thing unto itself, and it doesn’t allow an algorithm to easily generalize the cross words. For example, let’s say you have a language model that has learned that when you see “I want a glass of orange “. Well, what do you think the next word will be? Very likely, it’ll be “juice”. But even if the learning algorithm has learned that “I want a glass of orange juice” is a likely sentence, if it sees “I want a glass of apple _“. As far as it knows the relationship between apple and orange is not any closer as the relationship between any of the other words man, woman, king, queen, and orange. And so, it’s not easy for the learning algorithm to generalize from knowing that orange juice is a popular thing, to recognizing that apple juice might also be a popular thing or a popular phrase. And this is because the any product between any two different one-hot vector is zero. If you take any two vectors say, queen and king and any product of them, the end product is zero. If you take apple and orange and any product of them, the end product is zero. And you couldn’t get distance between any pair of these vectors, which is also the same. So it just doesn’t know that somehow apple and orange are much more similar than king and orange or queen and orange. So, won’t it be nice if instead of a one-hot presentation we can instead learn a featurized representation with each of these words, a man, woman, king, queen, apple, orange or really for every word in the dictionary, we could learn a set of features and values for each of them. So for example, each of these words, we want to know what is the gender associated with each of these things. So, if gender goes from minus one for male to plus one for female, then the gender associated with man might be minus one, for woman might be plus one. And then eventually, learning these things maybe for king you get minus 0.95, for queen plus 0.97, and for apple and orange sort of genderless. Another feature might be, well how royal are these things. And so the terms, man and woman are not really royal, so they might have feature values close to zero. Whereas king and queen are highly royal. And apple and orange are not really loyal. How about age? Well, man and woman doesn’t connotes much about age. Maybe men and woman implies that they’re adults, but maybe neither necessarily young nor old. So maybe values close to zero. Whereas kings and queens are always almost always adults. And apple and orange might be more neutral with respect to age. And then, another feature for here, is this is a food? Well, man is not a food, woman is not a food, neither are kings and queens, but apples and oranges are foods. And they can be many other features as well ranging from, what is the size of this? What is the cost? Is this something that is a live? Is this an action, or is this a noun, or is this a verb, or is it something else? And so on. So you can imagine coming up with many features. And for the sake of the illustration let’s say, 300 different features, and what that does is, it allows you to take this list of numbers, I’ve only written four here, but this could be a list of 300 numbers, that then becomes a 300 dimensional vector for representing the word man. And I’m going to use the notation e subscript 5391 to denote a representation like this. And similarly, this vector, this 300 dimensional vector or 300 dimensional vector like this, I would denote e9853 to denote a 300 dimensional vector we could use to represent the word woman. And similarly, for the other examples here. Now, if you use this representation to represent the words orange and apple, then notice that the representations for orange and apple are now quite similar. Some of the features will differ because of the color of an orange, the color an apple, the taste, or some of the features would differ. But by a large, a lot of the features of apple and orange are actually the same, or take on very similar values. And so, this increases the odds of the learning algorithm that has figured out that orange juice is a thing, to also quickly figure out that apple juice is a thing. So this allows it to generalize better across different words. So over the next few videos, we’ll find a way to learn words embeddings. We just need you to learn high dimensional feature vectors like these, that gives a better representation than one-hot vectors for representing different words. And the features we’ll end up learning, won’t have a easy to interpret interpretation like that component one is gender, component two is royal, component three is age and so on. Exactly what they’re representing will be a bit harder to figure out. But nonetheless, the featurized representations we will learn, will allow an algorithm to quickly figure out that apple and orange are more similar than say, king and orange or queen and orange. If we’re able to learn a 300 dimensional feature vector or 300 dimensional embedding for each words, one of the popular things to do is also to take this 300 dimensional data and embed it say, in a two dimensional space so that you can visualize them. And so, one common algorithm for doing this is the t-SNE algorithm due to Laurens van der Maaten and Geoff Hinton. And if you look at one of these embeddings, one of these representations, you find that words like man and woman tend to get grouped together, king and queen tend to get grouped together, and these are the people which tends to get grouped together. Those are animals who can get grouped together. Fruits will tend to be close to each other. Numbers like one, two, three, four, will be close to each other. And then, maybe the animate objects as whole will also tend to be grouped together. But you see plots like these sometimes on the internet to visualize some of these 300 or higher dimensional embeddings. And maybe this gives you a sense that, word embeddings algorithms like this can learn similar features for concepts that feel like they should be more related, as visualized by that concept that seem to you and me like they should be more similar, end up getting mapped to a more similar feature vectors. And these representations will use these sort of featurized representations in maybe a 300 dimensional space, these are called embeddings. And the reason we call them embeddings is, you can think of a 300 dimensional space. And again, they can’t draw out here in two dimensional space because it’s a 3D one. And what you do is you take every words like orange, and have a three dimensional feature vector so that word orange gets embedded to a point in this 300 dimensional space. And the word apple, gets embedded to a different point in this 300 dimensional space. And of course to visualize it, algorithms like t-SNE, map this to a much lower dimensional space, you can actually plot the 2D data and look at it. But that’s what the term embedding comes from. Word embeddings has been one of the most important ideas in NLP, in Natural Language Processing. In this video, you saw why you might want to learn or use word embeddings. In the next video, let’s take a deeper look at how you’ll be able to use these algorithms, to build NLP algorithims. 02_using-word-embeddingsIn the last video, you saw what it might mean to learn a featurized representations of different words. In this video, you see how we can take these representations and plug them into NLP applications. Let’s start with an example. Continuing with the named entity recognition example, if you’re trying to detect people’s names. Given a sentence like Sally Johnson is an orange farmer, hopefully, you’ll figure out that Sally Johnson is a person’s name, hence, the outputs 1 like that. And one way to be sure that Sally Johnson has to be a person, rather than say the name of the corporation is that you know orange farmer is a person. So previously, we had talked about one hot representations to represent these words, x(1), x(2), and so on. But if you can now use the featurized representations, the embedding vectors that we talked about in the last video. Then after having trained a model that uses word embeddings as the inputs, if you now see a new input, Robert Lin is an apple farmer. Knowing that orange and apple are very similar will make it easier for your learning algorithm to generalize to figure out that Robert Lin is also a human, is also a person’s name. One of the most interesting cases will be, what if in your test set you see not Robert Lin is an apple farmer, but you see much less common words? What if you see Robert Lin is a durian cultivator? A durian is a rare type of fruit, popular in Singapore and a few other countries. But if you have a small label training set for the named entity recognition task, you might not even have seen the word durian or seen the word cultivator in your training set. I guess technically, this should be a durian cultivator. But if you have learned a word embedding that tells you that durian is a fruit, so it’s like an orange, and a cultivator, someone that cultivates is like a farmer, then you might still be generalize from having seen an orange farmer in your training set to knowing that a durian cultivator is also probably a person. So one of the reasons that word embeddings will be able to do this is the algorithms to learning word embeddings can examine very large text corpuses, maybe found off the Internet. So you can examine very large data sets, maybe a billion words, maybe even up to 100 billion words would be quite reasonable. So very large training sets of just unlabeled text. And by examining tons of unlabeled text, which you can download more or less for free, you can figure out that orange and durian are similar. And farmer and cultivator are similar, and therefore, learn embeddings, that groups them together. Now having discovered that orange and durian are both fruits by reading massive amounts of Internet text, what you can do is then take this word embedding and apply it to your named entity recognition task, for which you might have a much smaller training set, maybe just 100,000 words in your training set, or even much smaller. And so this allows you to carry out transfer learning, where you take information you’ve learned from huge amounts of unlabeled text that you can suck down essentially for free off the Internet to figure out that orange, apple, and durian are fruits. And then transfer that knowledge to a task, such as named entity recognition, for which you may have a relatively small labeled training set. And, of course, for simplicity, l drew this for it only as a unidirectional RNN. If you actually want to carry out the named entity recognition task, you should, of course, use a bidirectional RNN rather than a simpler one I’ve drawn here. But to summarize, this is how you can carry out transfer learning using word embeddings. Step 1 is to learn word embeddings from a large text corpus, a very large text corpus or you can also download pre-trained word embeddings online. There are several word embeddings that you can find online under very permissive licenses. And you can then take these word embeddings and transfer the embedding to new task, where you have a much smaller labeled training sets. And use this, let’s say, 300 dimensional embedding, to represent your words. One nice thing also about this is you can now use relatively lower dimensional feature vectors. So rather than using a 10,000 dimensional one-hot vector, you can now instead use maybe a 300 dimensional dense vector. Although the one-hot vector is fast and the 300 dimensional vector that you might learn for your embedding will be a dense vector. And then, finally, as you train your model on your new task, on your named entity recognition task with a smaller label data set, one thing you can optionally do is to continue to fine tune, continue to adjust the word embeddings with the new data. In practice, you would do this only if this task 2 has a pretty big data set. If your label data set for step 2 is quite small, then usually, I would not bother to continue to fine tune the word embeddings. So word embeddings tend to make the biggest difference when the task you’re trying to carry out has a relatively smaller training set. So it has been useful for many NLP tasks. And I’ll just name a few. Don’t worry if you don’t know these terms. It has been useful for named entity recognition, for text summarization, for co-reference resolution, for parsing. These are all maybe pretty standard NLP tasks. It has been less useful for language modeling, machine translation, especially if you’re accessing a language modeling or machine translation task for which you have a lot of data just dedicated to that task. So as seen in other transfer learning settings, if you’re trying to transfer from some task A to some task B, the process of transfer learning is just most useful when you happen to have a ton of data for A and a relatively smaller data set for B. And so that’s true for a lot of NLP tasks, and just less true for some language modeling and machine translation settings. Finally, word embeddings has a interesting relationship to the face encoding ideas that you learned about in the previous course, if you took the convolutional neural networks course. So you will remember that for face recognition, we train this Siamese network architecture that would learn, say, a 128 dimensional representation for different faces. And then you can compare these encodings in order to figure out if these two pictures are of the same face. The words encoding and embedding mean fairly similar things. So in the face recognition literature, people also use the term encoding to refer to these vectors, f(x(i)) and f(x(j)). One difference between the face recognition literature and what we do in word embeddings is that, for face recognition, you wanted to train a neural network that can take as input any face picture, even a picture you’ve never seen before, and have a neural network compute an encoding for that new picture. Whereas what we’ll do, and you’ll understand this better when we go through the next few videos, whereas what we’ll do for learning word embeddings is that we’ll have a fixed vocabulary of, say, 10,000 words. And we’ll learn a vector e1 through, say, e10,000 that just learns a fixed encoding or learns a fixed embedding for each of the words in our vocabulary. So that’s one difference between the set of ideas you saw for face recognition versus what the algorithms we’ll discuss in the next few videos. But the terms encoding and embedding are used somewhat interchangeably. So the difference I just described is not represented by the difference in terminologies. It’s just a difference in how we need to use these algorithms in face recognition, where there’s unlimited sea of pictures you could see in the future. Versus natural language processing, where there might be just a fixed vocabulary, and everything else like that we’ll just declare as an unknown word. So in this video, you saw how using word embeddings allows you to implement this type of transfer learning. And how, by replacing the one-hot vectors we’re using previously with the embedding vectors, you can allow your algorithms to generalize much better, or you can learn from much less label data. Next, I want to show you just a few more properties of these word embeddings. And then after that, we will talk about algorithms for actually learning these word embeddings. Let’s go on to the next video, where you’ll see how word embeddings can help with reasoning about analogies. 03_properties-of-word-embeddingsBy now, you should have a sense of how word embeddings can help you build NLP applications. One of the most fascinating properties of word embeddings is that they can also help with analogy reasoning. And while reasonable analogies may not be by itself the most important NLP application, they might also help convey a sense of what these word embeddings are doing, what these word embeddings can do. Let me show you what I mean here are the featurized representations of a set of words that you might hope a word embedding could capture. Let’s say I pose a question, man is to woman as king is to what? Many of you will say, man is to woman as king is to queen. But is it possible to have an algorithm figure this out automatically? Well, here’s how you could do it, let’s say that you’re using this four dimensional vector to represent man. So this will be your E5391, although just for this video, let me call this e subscript man. And let’s say that’s the embedding vector for woman, so I’m going to call that e subscript woman, and similarly for king and queen. And for this example, I’m just going to assume you’re using four dimensional embeddings, rather than anywhere from 50 to 1,000 dimensional, which would be more typical. One interesting property of these vectors is that if you take the vector, e man, and subtract the vector e woman, then, You end up with approximately -1, negative another 1 is -2, decimal 0- 0, 0- 0, close to 0- 0, so you get roughly -2 0 0 0. And similarly if you take e king minus e queen, then that’s approximately the same thing. That’s about -1- 0.97, it’s about -2. This is about 1- 1, since kings and queens are both about equally royal. So that’s 0, and then age difference, food difference, 0. And so what this is capturing is that the main difference between man and woman is the gender. And the main difference between king and queen, as represented by these vectors, is also the gender. Which is why the difference e man- e woman, and the difference e king- e queen, are about the same. So one way to carry out this analogy reasoning is, if the algorithm is asked, man is to woman as king is to what? What it can do is compute e man- e woman, and try to find a vector, try to find a word so that e man- e woman is close to e king- e of that new word. And it turns out that when queen is the word plugged in here, then the left hand side is close to the the right hand side. So these ideas were first pointed out by Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. And it’s been one of the most remarkable and surprisingly influential results about word embeddings. And I think has helped the whole community get better intuitions about what word embeddings are doing. So let’s formalize how you can turn this into an algorithm. In pictures, the word embeddings live in maybe a 300 dimensional space. And so the word man is represented as a point in the space, and the word woman is represented as a point in the space. And the word king is represented as another point, and the word queen is represented as another point. And what we pointed out really on the last slide is that the vector difference between man and woman is very similar to the vector difference between king and queen. And this arrow I just drew is really the vector that represents a difference in gender. And remember, these are points we’re plotting in a 300 dimensional space. So in order to carry out this kind of analogical reasoning to figure out, man is to woman is king is to what, what you can do is try to find the word w, So that, This equation holds true, so you want there to be, A high degree of a similarity, between I’m going to use s, And so what you want is to find the word w that maximizes the similarity between, e w compared to e king- e man + e woman Right, so what I did is, I took this e question mark, and replaced that with ew, and then brought ew to just one side of the equation. And then the other three terms to the right hand side of this equation. So we have some appropriate similarity function for measuring how similar is the embedding of some word w to this quantity of the right. Then finding the word that maximizes the similarity should hopefully let you pick out the word queen. And the remarkable thing is, this actually works. If you learn a set of word embeddings and find a word w that maximizes this type of similarity, you can actually get the exact right answer. Depending on the details of the task, but if you look at research papers, it’s not uncommon for research papers to report anywhere from, say, 30% to 75% accuracy on analogy using tasks like these. Where you count an anology attempt as correct only if it guesses the exact word right. So only if, in this case, it picks out the word queen. Before moving on, I just want to clarify what this plot on the left is. Previously, we talked about using algorithms like t-SNE to visualize words. What t-SNE does is, it takes 300-D data, and it maps it in a very non-linear way to a 2D space. And so the mapping that t-SNE learns, this is a very complicated and very non-linear mapping. So after the t-SNE mapping, you should not expect these types of parallelogram relationships, like the one we saw on the left, to hold true. And it’s really in this original 300 dimensional space that you can more reliably count on these types of parallelogram relationships in analogy pairs to hold true. And it may hold true after a mapping through t-SNE, but in most cases, because of t-SNE’s non-linear mapping, you should not count on that. And many of the parallelogram analogy relationships will be broken by t-SNE. Now, before moving on, let me just quickly describe the similarity function that is most commonly used. So the most commonly used similarity function is called cosine similarity. So this is the equation we had from the previous slide. So in cosine similarity, you define the similarity between two vectors u and v as u transpose v divided by the lengths by the Euclidean lengths. So ignoring the denominator for now, this is basically the inner product between u and v. And so if u and v are very similar, their inner product will tend to be large. And this is called cosine similarity because this is actually the cosine of the angle between the two vectors, u and v. So that’s the angle phi, so this formula is actually the cosine between them. And so you remember from calculus that if this phi, then the cosine of phi looks like this. So if the angle between them is 0, then the cosine similarity is equal to 1. And if their angle is 90 degrees, the cosine similarity is 0. And then if they’re 180 degrees, or pointing in completely opposite directions, it ends up being -1. So that’s where the term cosine similarity comes from, and it works quite well for these analogy reasoning tasks. If you want, you can also use square distance or Euclidian distance, u-v squared. Technically, this would be a measure of dissimilarity rather than a measure of similarity. So we need to take the negative of this, and this will work okay as well. Although I see cosine similarity being used a bit more often. And the main difference between these is how it normalizes the lengths of the vectors u and v. So one of the remarkable results about word embeddings is the generality of analogy relationships they can learn. So for example, it can learn that man is to woman as boy is to girl, because the vector difference between man and woman, similar to king and queen and boy and girl, is primarily just the gender. It can learn that Ottawa, which is the capital of Canada, that Ottawa is to Canada as Nairobi is to Kenya. So that’s the city capital is to the name of the country. It can learn that big is to bigger as tall is to taller, and it can learn things like that. Yen is to Japan, since yen is the currency of Japan, as ruble is to Russia. And all of these things can be learned just by running a word embedding learning algorithm on the large text corpus. It can spot all of these patterns by itself, just by running from very large bodies of text. So in this video, you saw how word embeddings can be used for analogy reasoning. And while you might not be trying to build an analogy reasoning system yourself as an application, this I hope conveys some intuition about the types of feature-like representations that these representations can learn. And you also saw how cosine similarity can be a way to measure the similarity between two different word embeddings. Now, we talked a lot about properties of these embeddings and how you can use them. Next, let’s talk about how you’d actually learn these word embeddings, let’s go on to the next video. 04_embedding-matrixLet’s start to formalize the problem of learning a good word embedding. When you implement an algorithm to learn a word embedding, what you end up learning is an embedding matrix. Let’s take a look at what I means. Let’s say, as usual we’re using our 10,000-word vocabulary. So, the vocabulary has A, Aaron, Orange, Zulu, maybe also unknown word as a token. What we’re going to do is learn embedding matrix E, which is going to be a 300 dimensional by 10,000 dimensional matrix, if you have 10,000 words vocabulary or maybe 10,001 is our word token, there’s one extra token. And the columns of this matrix would be the different embeddings for the 10,000 different words you have in your vocabulary. So, Orange was word number 6257 in our vocabulary of 10,000 words. So, one piece of notation we’ll use is that 06257 was the one-hot vector with zeros everywhere and a one in position 6257. And so, this will be a 10,000-dimensional vector with a one in just one position. So, this isn’t quite a drawn scale. Yes, this should be as tall as the embedding matrix on the left is wide. And if the embedding matrix is called capital E then notice that if you take E and multiply it by just one-hot vector by 0 of 6257, then this will be a 300-dimensional vector. So, E is 300 by 10,000 and 0 is 10,000 by 1. So, the product will be 300 by 1, so with 300-dimensional vector and notice that to compute the first element of this vector, of this 300-dimensional vector, what you do is you will multiply the first row of the matrix E with this. But all of these elements are zero except for element 6257 and so you end up with zero times this, zero times this, zero times this, and so on. And then, 1 times whatever this is, and zero times this, zero times this, zero times and so on. And so, you end up with the first element as whatever is that elements up there, under the Orange column. And then, for the second element of this 300-dimensional vector we’re computing, you would take the vector 0657 and multiply it by the second row with the matrix E. So again, you have zero times this, plus zero times this, plus zero times all of these are the elements and then one times this, and then zero times everything else and add that together. So you end up with this and so on as you go down the rest of this column. So, that’s why the embedding matrix E times this one-hot vector here winds up selecting out this 300-dimensional column corresponding to the word Orange. So, this is going to be equal to E 6257 which is the notation we’re going to use to represent the embedding vector that 300 by one dimensional vector for the word Orange. And more generally, E for a specific word W, this is going to be embedding for a word W. And more generally, E times O substitute J, one-hot vector with one that position J, this is going to be E_J and that’s going to be the embedding for word J in the vocabulary. So, the thing to remember from this slide is that our goal will be to learn an embedding matrix E and what you see in the next video is you initialize E randomly and you’re straight in the sense to learn all the parameters of this 300 by 10,000 dimensional matrix and E times this one-hot vector gives you the embedding vector. Now just one note, when we’re writing the equation, it’ll be convenient to write this type of notation where you take the matrix E and multiply it by the one-hot vector O. But if when you’re implementing this, it is not efficient to actually implement this as a mass matrix vector multiplication because the one-hot vectors, now this is a relatively high dimensional vector and most of these elements are zero. So, it’s actually not efficient to use a matrix vector multiplication to implement this because if we multiply a whole bunch of things by zeros and so the practice, you would actually use a specialized function to just look up a column of the Matrix E rather than do this with the matrix multiplication. But writing of the map, it is just convenient to write it out this way. So, in Keras’s for example there is a embedding layer and we use the embedding layer then it more efficiently just pulls out the column you want from the embedding matrix rather than does it with a much slower matrix vector multiplication. So, in this video you saw the notations were used to describe algorithms to learning these embeddings and the key terminology is this matrix capital E which contain all the embeddings for the words of the vocabulary. In the next video, we’ll start to talk about specific algorithms for learning this matrix E. Let’s go onto the next video. 02_learning-word-embeddings-word2vec-glove01_learning-word-embeddingsIn this video, you’ll start to learn some concrete algorithms for learning word embeddings. In the history of deep learning as applied to learning word embeddings, people actually started off with relatively complex algorithms. And then over time, researchers discovered they can use simpler and simpler and simpler algorithms and still get very good results especially for a large dataset. But what happened is, some of the algorithms that are most popular today, they are so simple that if I present them first, it might seem almost a little bit magical, how can something this simple work? So, what I’m going to do is start off with some of the slightly more complex algorithms because I think it’s actually easier to develop intuition about why they should work, and then we’ll move on to simplify these algorithms and show you some of the simple algorithms that also give very good results. So, let’s get started. Let’s say you’re building a language model and you do it with a neural network. So, during training, you might want your neural network to do something like input, I want a glass of orange, and then predict the next word in the sequence. And below each of these words, I have also written down the index in the vocabulary of the different words. So it turns out that building a neural language model is the small way to learn a set of embeddings. And the ideas I present on this slide were due to Yoshua Bengio, Rejean Ducharme, Pascals Vincent, and Christian Jauvin. So, here’s how you can build a neural network to predict the next word in the sequence. Let me take the list of words, I want a glass of orange, and let’s start with the first word I. So I’m going to construct one add vector corresponding to the word I. So there’s a one add vector with a one in position, 4343. So this is going to be 10,000 dimensional vector. And what we’re going to do is then have a matrix of parameters E, and take E times O to get an embedding vector e4343, and this step really means that e4343 is obtained by the matrix E times the one add vector 43. And then we’ll do the same for all of the other words. So the word want, is where 9665 one add vector, multiply by E to get the embedding vector. And similarly, for all the other words. A, is a first word in dictionary, alphabetic comes first, so there is O one, gets this E one. And similarly, for the other words in this phrase. So now you have a bunch of three dimensional embedding, so each of this is a 300 dimensional embedding vector. And what we can do, is fill all of them into a neural network. So here is the neural network layer. And then this neural network feeds to a softmax, which has it’s own parameters as well. And a softmax classifies among the 10,000 possible outputs in the vocab for those final word we’re trying to predict. And so, if in the training slide we saw the word juice then, the target for the softmax in training repeat that it should predict the other word juice was what came after this. So this hidden name here will have his own parameters. So have some, I’m going to call this W1 and there’s also B1. The softmax there was this own parameters W2, B2, and they’re using 300 dimensional word embeddings, then here we have six words. So, this would be six times 300. So this layer or this input will be a 1,800 dimensional vector obtained by taking your six embedding vectors and stacking them together. Well, what’s actually more commonly done is to have a fixed historical window. So for example, you might decide that you always want to predict the next word given say the previous four words, where four here is a hyperparameter of the algorithm. So this is how you adjust to either very long or very short sentences or you decide to always just look at the previous four words, so you say, I will still use those four words. And so, let’s just get rid of these. And so, if you’re always using a four word history, this means that your neural network will input a 1,200 dimensional feature vector, go into this layer, then have a softmax and try to predict the output. And again, variety of choices. And using a fixed history, just means that you can deal with even arbitrarily long sentences because the input sizes are always fixed. So, the parameters of this model will be this matrix E, and use the same matrix E for all the words. So you don’t have different matrices for different positions in the proceedings four words, is the same matrix E. And then, these weights are also parameters of the algorithm and you can use backprop to perform gradient descent to maximize the likelihood of your training set to just repeatedly predict given four words in a sequence, what is the next word in your text corpus? And it turns out that this algorithm we’ll learn pretty decent word embeddings. And the reason is, if you remember our orange juice, apple juice example, is in the algorithm’s incentive to learn pretty similar word embeddings for orange and apple because doing so allows it to fit the training set better because it’s going to see orange juice sometimes, or see apple juice sometimes, and so, if you have only a 300 dimensional feature vector to represent all of these words, the algorithm will find that it fits the training set fast. If apples, oranges, and grapes, and pears, and so on and maybe also durians which is a very rare fruit and that with similar feature vectors. So, this is one of the earlier and pretty successful algorithms for learning word embeddings, for learning this matrix E. But now let’s generalize this algorithm and see how we can derive even simpler algorithms. So, I want to illustrate the other algorithms using a more complex sentence as our example. Let’s say that in your training set, you have this longer sentence, I want a glass of orange juice to go along with my cereal. So, what we saw on the last slide was that the job of the algorithm was to predict some word juice, which we are going to call the target words, and it was given some context which was the last four words. And so, if your goal is to learn a embedding of researchers I’ve experimented with many different types of context. If it goes to build a language model then is natural for the context to be a few words right before the target word. But if your goal is into learn the language model per se, then you can choose other contexts. For example, you can pose a learning problem where the context is the four words on the left and right. So, you can take the four words on the left and right as the context, and what that means is that we’re posing a learning problem where the algorithm is given four words on the left. So, a glass of orange, and four words on the right, to go along with, and this has to predict the word in the middle. And posing a learning problem like this where you have the embeddings of the left four words and the right four words feed into a neural network, similar to what you saw in the previous slide, to try to predict the word in the middle, try to put it target word in the middle, this can also be used to learn word embeddings. Or if you want to use a simpler context, maybe you’ll just use the last one word. So given just the word orange, what comes after orange? So this will be different learning problem where you tell it one word, orange, and will say well, what do you think is the next word. And you can construct a neural network that just fits in the word, the one previous word or the embedding of the one previous word to a neural network as you try to predict the next word. Or, one thing that works surprisingly well is to take a nearby one word. Some might tell you that, well, take the word glass, is somewhere close by. Some might say, I saw the word glass and then there’s another words somewhere close to glass, what do you think that word is? So, that’ll be using nearby one word as the context. And we’ll formalize this in the next video but this is the idea of a Skip-Gram model, and just an example of a simpler algorithm where the context is now much simpler, is just one word rather than four words, but this works remarkably well. So what researchers found was that if you really want to build a language model, it’s natural to use the last few words as a context. But if your main goal is really to learn a word embedding, then you can use all of these other contexts and they will result in very meaningful work embeddings as well. I will formalize the details of this in the next video where we talk about the Word2Vec model. To summarize, in this video you saw how the language modeling problem which causes the pose of machines learning problem where you input the context like the last four words and predicts some target words, how posing that problem allows you to learn input word embedding. In the next video, you’ll see how using even simpler context and even simpler learning algorithms to mark from context to target word, can also allow you to learn a good word embedding. Let’s go on to the next video where we’ll discuss the Walter VEC. 02_word2vecIn the last video, you saw how you can learn a neural language model in order to get good word embeddings. In this video, you see the Word2Vec algorithm which is simple and comfortably more efficient way to learn this types of embeddings. Lets take a look. Most of the ideas I’ll present in this video are due to Tomas Mikolov, Kai Chen, Greg Corrado, and Jeff Dean. Let’s say you’re given this sentence in your training set. In the skip-gram model, what we’re going to do is come up with a few context to target pairs to create our supervised learning problem. So rather than having the context be always the last four words or the last end words immediately before the target word, what I’m going to do is, say, randomly pick a word to be the context word. And let’s say we chose the word orange. And what we’re going to do is randomly pick another word within some window. Say plus minus five words or plus minus ten words of the context word and we choose that to be target word. So maybe just by chance you might pick juice to be a target word, that’s just one word later. Or you might choose two words before. So you have another pair where the target could be glass or, Maybe just by chance you choose the word my as the target. And so we’ll set up a supervised learning problem where given the context word, you’re asked to predict what is a randomly chosen word within say, a plus minus ten word window, or plus minus five or ten word window of that input context word. And obviously, this is not a very easy learning problem, because within plus minus 10 words of the word orange, it could be a lot of different words. But a goal of setting up this supervised learning problem, isn’t to do well on the supervised learning problem per se, it is that we want to use this learning problem to learn good word embeddings. So, here are the details of the model. Let’s say that we’ll continue to our vocab of 10,000 words. And some have been on vocab sizes that exceeds a million words. But the basic supervised learning problem we’re going to solve is that we want to learn the mapping from some Context c, such as the word orange to some target, which we will call t, which might be the word juice or the word glass or the word my, if we use the example from the previous slide. So in our vocabulary, orange is word 6257, and the word juice is the word 4834 in our vocab of 10,000 words. And so that’s the input x that you want to learn to map to that open y. So to represent the input such as the word orange, you can start out with some one hot vector which is going to be write as $o_c$, so there’s a one hot vector for the context words. And then similar to what you saw on the last video you can take the embedding matrix E, multiply E by the vector $o_c$, and this gives you your embedding vector for the input context word, so here $e_c$ is equal to capital E times that one hot vector. Then in this new network that we formed we’re going to take this vector $e_c$ and feed it to a softmax unit. So I’ve been drawing softmax unit as a node in a neural network. That’s not an o, that’s a softmax unit. And then there’s a drop in the softmax unit to output $\hat{y}$. So to write out this model in detail. This is the model, the softmax model, probability of different tanka words given the input context word as e to the e, theta t transpose,$e_c$. Divided by some over all words, so we’re going to say, sum from J equals one to all 10,000 words of e to the theta j transposed $e_c$. So here theta T is the parameter associated with, I’ll put t, but really there’s a chance of a particular word, t, being the label. So I’ve left off the biased term to solve mass but we could include that too if we wish. And then finally the loss function for softmax will be the usual. So we use y to represent the target word. And we use a one-hot representation for y hat and y here. Then the lost would be The negative log liklihood, so sum from i equals 1 to 10,000 of $y_ilog(\hat{y}_i)$. So that’s a usual loss for softmax where we’re representing the target y as a one hot vector. So this would be a one hot vector with just 1 1 and the rest zeros. And if the target word is juice, then it’d be element 4834 from up here. That is equal to 1 and the rest will be equal to 0. And similarly Y hat will be a 10,000 dimensional vector output by the softmax unit with probabilities for all 10,000 possible targets words. So to summarize, this is the overall little model, little neural network with basically looking up the embedding and then just a soft max unit. And the matrix E will have a lot of parameters, so the matrix E has parameters corresponding to all of these embedding vectors, $e_c$. And then the softmax unit also has parameters that gives the theta T parameters but if you optimize this loss function with respect to the all of these parameters, you actually get a pretty good set of embedding vectors. So this is called the skip-gram model because is taking as input one word like orange and then tr$y_i$ng to predict some words skipping a few words from the left or the right side. To predict what comes little bit before little bit after the context words. Now, it turns out there are a couple problems with using this algorithm. And the primary problem is computational speed. In particular, for the softmax model, every time you want to evaluate this probability, you need to carry out a sum over all 10,000 words in your vocabulary. And maybe 10,000 isn’t too bad, but if you’re using a vocabulary of size 100,000 or a 1,000,000, it gets really slow to sum up over this denominator every single time. And, in fact, 10,000 is actually already that will be quite slow, but it makes even harder to scale to larger vocabularies. So there are a few solutions to this, one which you see in the literature is to use a hierarchical softmax classifier. And what that means is, instead of trying to categorize something into all 10,000 carries on one go. Imagine if you have one classifier, it tells you is the target word in the first 5,000 words in the vocabulary? Or is in the second 5,000 words in the vocabulary? And lets say this binary cost that it tells you this is in the first 5,000 words, think of second class to tell you that this in the first 2,500 words of vocab or in the second 2,500 words vocab and so on. Until eventually you get down to classify exactly what word it is, so that the leaf of this tree, and so having a tree of classifiers like this, means that each of the retriever nodes of the tree can be just a binding classifier. And so you don’t need to sum over all 10,000 words or else it will capsize in order to make a single classification. In fact, the computational classifying tree like this scales like log of the vocab size rather than linear in vocab size. So this is called a hierarchical softmax classifier. I should mention in practice, the hierarchical softmax classifier doesn’t use a perfectly balanced tree or this perfectly symmetric tree, with equal numbers of words on the left and right sides of each branch. In practice, the hierarchical softmax classifier can be developed so that the common words tend to be on top, whereas the less common words like durian can be buried much deeper in the tree. Because you see the more common words more often, and so you might need only a few traversals to get to common words like the and of. Whereas you see less frequent words like durian much less often, so it says okay that are buried deep in the tree because you don’t need to go that deep. So there are various heuristics for building the tree how you used to build the hierarchical software spire. So this is one idea you see in the literature, the speeding up the softmax classification. But I won’t spend too much more time. And you can read more details of this on the paper that I referenced by Thomas and others, on the first slide. But I won’t spend too much more time on this. Because in the next video, where she talk about a different method, called nectar sampling, which I think is even simpler. And also works really well for speeding up the softmax classifier and the problem of needing the sum over the entire cap size in the denominator. So you see more of that in the next video. But before moving on, one quick Topic I want you to understand is how to sample the context C. So once you sample the context C, the target T can be sampled within, say, a plus minus ten word window of the context C, but how do you choose the context C? One thing you could do is just sample uniformly, at random, from your training corpus. When we do that, you find that there are some words like the, of, a, and, to and so on that appear extremely frequently. And so, if you do that, you find that in your context to target mapping pairs just get these these types of words extremely frequently, whereas there are other words like orange, apple, and also durian that don’t appear that often. And maybe you don’t want your training site to be dominated by these extremely frequently or current words, because then you spend almost all the effort updating $e_c$, for those frequently occurring words. But you want to make sure that you spend some time updating the embedding, even for these less common words like e durian. So in practice the distribution of words $P(c)$ isn’t taken just entirely uniformly at random for the training set purpose, but instead there are different heuristics that you could use in order to balance out something from the common words together with the less common words. So that’s it for the Word2Vec skip-gram model. If you read the original paper by that I referenced earlier, you find that that paper actually had two versions of this Word2Vec model, the skip gram was one. And the other one is called the CBow, the continuous backwards model, which takes the surrounding contexts from middle word, and uses the surrounding words to try to predict the middle word, and that algorithm also works, it has some advantages and disadvantages. But the key problem with this algorithm with the skip-gram model as presented so far is that the softmax step is very expensive to calculate because needing to sum over your entire vocabulary size into the denominator of the soft packs. In the next video I show you an algorithm that modifies the training objective that makes it run much more efficiently therefore lets you apply this in a much bigger fitting set as well and therefore learn much better word embeddings. Lets go onto the next video. 03_negative-samplingIn the last video, you saw how the Skip-Gram model allows you to construct a supervised learning task. So we map from context to target and how that allows you to learn a useful word embedding. But the downside of that was the Softmax objective was slow to compute. In this video, you’ll see a modified learning problem called negative sampling that allows you to do something similar to the Skip-Gram model you saw just now, but with a much more efficient learning algorithm. Let’s see how you can do this. Most of the ideas presented in this video are due to Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeff Dean. So what we’re going to do in this algorithm is create a new supervised learning problem. And the problem is, given a pair of words like orange and juice, we’re going to predict, is this a context-target pair? So in this example, orange juice was a positive example. And how about orange and king? Well, that’s a negative example, so I’m going to write 0 for the target. So what we’re going to do is we’re actually going to sample a context and a target word. So in this case, we have orange and juice and we’ll associate that with a label of 1, so just put words in the middle. And then having generated a positive example, so the positive example is generated exactly how we generated it in the previous videos. Sample a context word, look around a window of say, plus-minus ten words and pick a target word. So that’s how you generate the first row of this table with orange, juice, 1. And then to generate a negative example, you’re going to take the same context word and then just pick a word at random from the dictionary. So in this case, I chose the word king at random and we will label that as 0. And then let’s take orange and let’s pick another random word from the dictionary. Under the assumption that if we pick a random word, it probably won’t be associated with the word orange, so orange, book, 0. And let’s pick a few others, orange, maybe just by chance, we’ll pick the 0 and then orange. And then orange, and maybe just by chance, we’ll pick the word of and we’ll put a 0 there. And notice that all of these are labeled as 0 even though the word of actually appears next to orange as well. So to summarize, the way we generated this data set is, we’ll pick a context word and then pick a target word and that is the first row of this table. That gives us a positive example. So context, target, and then give that a label of 1. And then what we’ll do is for some number of times say, k times, we’re going to take the same context word and then pick random words from the dictionary, king, book, the, of, whatever comes out at random from the dictionary and label all those 0, and those will be our negative examples. And it’s okay if just by chance, one of those words we picked at random from the dictionary happens to appear in the window, in a plus-minus ten word window say, next to the context word, orange. Then we’re going to create a supervised learning problem where the learning algorithm inputs x, inputs this pair of words, and it has to predict the target label to predict the output y. So the problem is really given a pair of words like orange and juice, do you think they appear together? Do you think I got these two words by sampling two words close to each other? Or do you think I got them as one word from the text and one word chosen at random from the dictionary? It’s really to try to distinguish between these two types of distributions from which you might sample a pair of words. So this is how you generate the training set. How do you choose k, Mikolov et al, recommend that maybe k is 5 to 20 for smaller data sets. And if you have a very large data set, then chose k to be smaller. So k equals 2 to 5 for larger data sets, and large values of k for smaller data sets. Okay, and in this example, I’ll just use k = 4. Next, let’s describe the supervised learning model for learning a mapping from x to y. So here was the Softmax model you saw from the previous video. And here is the training set we got from the previous slide where again, this is going to be the new input x and this is going to be the value of y you’re trying to predict. So to define the model, I’m going to use this to denote, this was c for the context word, this to denote the possible target word, t, and this, I’ll use y to denote 0, 1, this is a context target pair. So what we’re going to do is define a logistic regression model. Say, that the chance of y = 1, given the input c, t pair, we’re going to model this as basically a regression model, but the specific formula we’ll use s sigma applied to theta transpose, theta t transpose, e c. So the parameters are similar as before, you have one parameter vector theta for each possible target word. And a separate parameter vector, really the embedding vector, for each possible context word. And we’re going to use this formula to estimate the probability that y is equal to 1. So if you have k examples here, then you can think of this as having a k to 1 ratio of negative to positive examples. So for every positive examples, you have k negative examples with which to train this logistic regression-like model. And so to draw this as a neural network, if the input word is orange, Which is word 6257, then what you do is, you input the one hop vector passing through e, do the multiplication to get the embedding vector 6257. And then what you have is really 10,000 possible logistic regression classification problems. Where one of these will be the classifier corresponding to, well, is the target word juice or not? And then there will be other words, for example, there might be ones somewhere down here which is predicting, is the word king or not and so on, for these possible words in your vocabulary. So think of this as having 10,000 binary logistic regression classifiers, but instead of training all 10,000 of them on every iteration, we’re only going to train five of them. We’re going to train the one responding to the actual target word we got and then train four randomly chosen negative examples. And this is for the case where k is equal to 4. So instead of having one giant 10,000 way Softmax, which is very expensive to compute, we’ve instead turned it into 10,000 binary classification problems, each of which is quite cheap to compute. And on every iteration, we’re only going to train five of them or more generally, k + 1 of them, of k negative examples and one positive examples. And this is why the computation cost of this algorithm is much lower because you’re updating k + 1, let’s just say units, k + 1 binary classification problems. Which is relatively cheap to do on every iteration rather than updating a 10,000 way Softmax classifier. So you get to play with this algorithm in the problem exercise for this week as well. So this technique is called negative sampling because what you’re doing is, you have a positive example, the orange and then juice. And then you will go and deliberately generate a bunch of negative examples, negative samplings, hence, the name negative sampling, with which to train four more of these binary classifiers. And on every iteration, you choose four different random negative words with which to train your algorithm on. Now, before wrapping up, one more important detail with this algorithm is, how do you choose the negative examples? So after having chosen the context word orange, how do you sample these words to generate the negative examples? So one thing you could do is sample the words in the middle, the candidate target words. One thing you could do is sample it according to the empirical frequency of words in your corpus. So just sample it according to how often different words appears. But the problem with that is that you end up with a very high representation of words like the, of, and, and so on. One other extreme would be to say, you use 1 over the vocab size, sample the negative examples uniformly at random, but that’s also very non-representative of the distribution of English words. So the authors, Mikolov et al, reported that empirically, what they found to work best was to take this heuristic value, which is a little bit in between the two extremes of sampling from the empirical frequencies, meaning from whatever’s the observed distribution in English text to the uniform distribution. And what they did was they sampled proportional to their frequency of a word to the power of three-fourths. So if f of wi is the observed frequency of a particular word in the English language or in your training set corpus, then by taking it to the power of three-fourths, this is somewhere in-between the extreme of taking uniform distribution. And the other extreme of just taking whatever was the observed distribution in your training set. And so I’m not sure this is very theoretically justified, but multiple researchers are now using this heuristic, and it seems to work decently well. So to summarize, you’ve seen how you can learn word vectors in a Softmax classier, but it’s very computationally expensive. And in this video, you saw how by changing that to a bunch of binary classification problems, you can very efficiently learn words vectors. And if you run this algorithm, you will be able to learn pretty good word vectors. Now of course, as is the case in other areas of deep learning as well, there are open source implementations. And there are also pre-trained word vectors that others have trained and released online under permissive licenses. And so if you want to get going quickly on a NLP problem, it’d be reasonable to download someone else’s word vectors and use that as a starting point. So that’s it for the Skip-Gram model. In the next video, I want to share with you yet another version of a word embedding learning algorithm that is maybe even simpler than what you’ve seen so far. So in the next video, let’s learn about the Glove algorithm. 04_glove-word-vectorsYou learn about several algorithms for computing words embeddings. Another algorithm that has some momentum in the NLP community is the GloVe algorithm. This is not used as much as the Word2Vec or the skip-gram models, but it has some enthusiasts. Because I think, in part of its simplicity. Let’s take a look. The GloVe algorithm was created by Jeffrey Pennington, Richard Socher, and Chris Manning. And GloVe stands for global vectors for word representation. So, previously, we were sampling pairs of words, context and target words, by picking two words that appear in close proximity to each other in our text corpus. So, what the GloVe algorithm does is, it starts off just by making that explicit. So, let’s say $X_{ij}$ be the number of times that a word i appears in the context of j. And so, here i and j play the role of t and c, so you can think of $X_{ij}$ as being x subscript tc. But, you can go through your training corpus and just count up how many words does a word i appear in the context of a different word j. How many times does the word t appear in context of different words c. And depending on the definition of context and target words, you might have that $X_{ij}$ equals $X_{ji}$. And in fact, if you’re defining context and target in terms of whether or not they appear within plus minus 10 words of each other, then it would be a symmetric relationship. Although, if your choice of context was that, the context is always the word immediately before the target word, then $X_{ij}$ and $X_{ji}$ may not be symmetric like this. But for the purposes of the GloVe algorithm, we can define context and target as whether or not the two words appear in close proximity, say within plus or minus 10 words of each other. So, $X_{ij}$ is a count that captures how often do words i and j appear with each other, or close to each other. So what the GloVe model does is, it optimizes the following. We’re going to minimize the difference between theta i transpose e_j minus log of $X_{ij}$ squared. I’m going to fill in some of the parts of this equation. But again, think of i and j as playing the role of t and c. So this is a bit like what you saw previously with theta t transpose e_c. And what you want is, for this to tell you how related are those two words? How related are words t and c? How related are words i and j as measured by how often they occur with each other? Which is affected by this $X_{ij}$. And so, what we’re going to do is, solve for parameters theta and e using gradient descent to minimize the sum over i equals one to 10,000 sum over j from one to 10,000 of this difference. So you just want to learn vectors, so that their end product is a good predictor for how often the two words occur together. Now, just some additional details, if $X_{ij}$ is equal to zero, then log of 0 is undefined, is negative infinity. And so, what we do is, we want sum over the terms where $X_{ij}$ is equal to zero. And so, what we’re going to do is, add an extra weighting term. So this is going to be a weighting term, and this will be equal to zero if $X_{ij}$ is equal to zero. And we’re going to use a convention that zero log zero is equal to zero. So what this means is, that if $X_{ij}$ is equal to zero, just don’t bother to sum over that $X_{ij}$ pair. So then this log of zero term is not relevant. So this means the sum is sum only over the pairs of words that have co-occurred at least once in that context-target relationship. The other thing that $F(X_{ij})$ does is that, there are some words they just appear very often in the English language like, this, is, of, a, and so on. Sometimes we used to call them stop words but there’s really a continuum between frequent and infrequent words. And then there are also some infrequent words like durion, which you actually still want to take into account, but not as frequently as the more common words. And so, the weighting factor can be a function that gives a meaningful amount of computation, even to the less frequent words like durion, and gives more weight but not an unduly large amount of weight to words like, this, is, of, a, which just appear lost in language. And so, there are various heuristics for choosing this weighting function F that need or gives these words too much weight nor gives the infrequent words too little weight. You can take a look at the GloVe paper, they are referenced in the previous slide, if you want the details of how F can be chosen to be a heuristic to accomplish this. And then, finally, one funny thing about this algorithm is that the roles of theta and e are now completely symmetric. So, theta i and e_j are symmetric in that, if you look at the math, they play pretty much the same role and you could reverse them or sort them around, and they actually end up with the same optimization objective. One way to train the algorithm is to initialize theta and e both uniformly around gradient descent to minimize its objective, and then when you’re done for every word, to then take the average. For a given words w, you can have e final to be equal to the embedding that was trained through this gradient descent procedure, plus theta trained through this gradient descent procedure divided by two, because theta and e in this particular formulation play symmetric roles unlike the earlier models we saw in the previous videos, where theta and e actually play different roles and couldn’t just be averaged like that. That’s it for the GloVe algorithm. I think one confusing part of this algorithm is, if you look at this equation, it seems almost too simple. How could it be that just minimizing a square cost function like this allows you to learn meaningful word embeddings? But it turns out that this works. And the way that the inventors end up with this algorithm was, they were building on the history of much more complicated algorithms like the newer language model, and then later, there came the Word2Vec skip-gram model, and then this came later. And we really hope to simplify all of the earlier algorithms. Before concluding our discussion of algorithms concerning word embeddings, there’s one more property of them that we should discuss briefly. Which is that? We started off with this featurization view as the motivation for learning word vectors. We said, “Well, maybe the first component of the embedding vector to represent gender, the second component to represent how royal it is, then the age and then whether it’s a food, and so on.” But when you learn a word embedding using one of the algorithms that we’ve seen, such as the GloVe algorithm that we just saw on the previous slide, what happens is, you cannot guarantee that the individual components of the embeddings are interpretable. Why is that? Well, let’s say that there is some space where the first axis is gender and the second axis is royal. What you can do is guarantee that the first axis of the embedding vector is aligned with this axis of meaning, of gender, royal, age and food. And in particular, the learning algorithm might choose this to be axis of the first dimension. So, given maybe a context of words, so the first dimension might be this axis and the second dimension might be this. Or it might not even be orthogonal, maybe it’ll be a second non-orthogonal axis, could be the second component of the word embeddings you actually learn. And when we see this, if you have a subsequent understanding of linear algebra is that, if there was some invertible matrix A, then this could just as easily be replaced with A times theta i transpose A inverse transpose e_j. Because we expand this out, this is equal to theta i transpose A transpose A inverse transpose times e_j. And so, the middle term cancels out and we’re left with theta i transpose e_j, same as before. Don’t worry if you didn’t follow the linear algebra, but that’s a brief proof that shows that with an algorithm like this, you can’t guarantee that the axis used to represent the features will be well-aligned with what might be easily humanly interpretable axis. In particular, the first feature might be a combination of gender, and royal, and age, and food, and cost, and size, is it a noun or an action verb, and all the other features. It’s very difficult to look at individual components, individual rows of the embedding matrix and assign the human interpretation to that. But despite this type of linear transformation, the parallelogram map that we worked out when we were describing analogies, that still works. And so, despite this potentially arbitrary linear transformation of the features, you end up learning the parallelogram map for figure analogies still works. So, that’s it for learning word embeddings. You’ve now seen a variety of algorithms for learning these word embeddings and you get to play them more in this week’s programming exercise as well. Next, I’d like to show you how you can use these algorithms to carry out sentiment classification. Let’s go onto the next video. 03_applications-using-word-embeddings01_sentiment-classificationSentiment classification is the task of looking at a piece of text and telling if someone likes or dislikes the thing they’re talking about. It is one of the most important building blocks in NLP and is used in many applications. One of the challenges of sentiment classification is you might not have a huge label training set for it. But with word embeddings, you’re able to build good sentiment classifiers even with only modest-size label training sets. Let’s see how you can do that. So here’s an example of a sentiment classification problem. The input X is a piece of text and the output Y that you want to predict is what is the sentiment, such as the star rating of, let’s say, a restaurant review. So if someone says, “The dessert is excellent” and they give it a four-star review, “Service was quite slow” two-star review, “Good for a quick meal but nothing special” three-star review. And this is a pretty harsh review, “Completely lacking in good taste, good service, and good ambiance.” That’s a one-star review. So if you can train a system to map from X or Y based on a label data set like this, then you could use it to monitor comments that people are saying about maybe a restaurant that you run. So people might also post messages about your restaurant on social media, on Twitter, or Facebook, or Instagram, or other forms of social media. And if you have a sentiment classifier, they can look just a piece of text and figure out how positive or negative is the sentiment of the poster toward your restaurant. Then you can also be able to keep track of whether or not there are any problems or if your restaurant is getting better or worse over time. So one of the challenges of sentiment classification is you might not have a huge label data set. So for sentimental classification task, training sets with maybe anywhere from 10,000 to maybe 100,000 words would not be uncommon. Sometimes even smaller than 10,000 words and word embeddings that you can take can help you to much better understand especially when you have a small training set. So here’s what you can do. We’ll go for a couple different algorithms in this video. Here’s a simple sentiment classification model. You can take a sentence like “dessert is excellent” and look up those words in your dictionary. We use a 10,000-word dictionary as usual. And let’s build a classifier to map it to the output Y that this was four stars. So given these four words, as usual, we can take these four words and look up the one-hot vector. So there’s 0 8 9 2 8 which is a one-hot vector multiplied by the embedding matrix E, which can learn from a much larger text corpus. It can learn in embedding from, say, a billion words or a hundred billion words, and use that to extract out the embedding vector for the word “the”, and then do the same for “dessert”, do the same for “is” and do the same for “excellent”. And if this was trained on a very large data set, like a hundred billion words, then this allows you to take a lot of knowledge even from infrequent words and apply them to your problem, even words that weren’t in your labeled training set. Now here’s one way to build a classifier, which is that you can take these vectors, let’s say these are 300-dimensional vectors, and you could then just sum or average them. And I’m just going to put a bigger average operator here and you could use sum or average. And this gives you a 300-dimensional feature vector that you then pass to a soft-max classifier which then outputs Y-hat. And so the softmax can output what are the probabilities of the five possible outcomes from one-star up to five-star. So this will be assortment of the five possible outcomes to predict what is Y. So notice that by using the average operation here, this particular algorithm works for reviews that are short or long because even if a review that is 100 words long, you can just sum or average all the feature vectors for all hundred words and so that gives you a representation, a 300-dimensional feature representation, that you can then pass into your sentiment classifier. So this average will work decently well. And what it does is it really averages the meanings of all the words or sums the meaning of all the words in your example. And this will work to [inaudible]. So one of the problems with this algorithm is it ignores word order. In particular, this is a very negative review, “Completely lacking in good taste, good service, and good ambiance”. But the word good appears a lot. This is a lot. Good, good, good. So if you use an algorithm like this that ignores word order and just sums or averages all of the embeddings for the different words, then you end up having a lot of the representation of good in your final feature vector and your classifier will probably think this is a good review even though this is actually very harsh. This is a one-star review. So here’s a more sophisticated model which is that, instead of just summing all of your word embeddings, you can instead use a RNN for sentiment classification. So here’s what you can do. You can take that review, “Completely lacking in good taste, good service, and good ambiance”, and find for each of them, the one-hot vector. And so I’m going to just skip the one-hot vector representation but take the one-hot vectors, multiply it by the embedding matrix E as usual, then this gives you the embedding vectors and then you can feed these into an RNN. And the job of the RNN is to then compute the representation at the last time step that allows you to predict Y-hat. So this is an example of a many-to-one RNN architecture which we saw in the previous week. And with an algorithm like this, it will be much better at taking word sequence into account and realize that “things are lacking in good taste” is a negative review and “not good” a negative review unlike the previous algorithm, which just sums everything together into a big-word vector mush and doesn’t realize that “not good” has a very different meaning than the words “good” or “lacking in good taste” and so on. And so if you train this algorithm, you end up with a pretty decent sentiment classification algorithm and because your word embeddings can be trained from a much larger data set, this will do a better job generalizing to maybe even new words now that you’ll see in your training set, such as if someone else says, “Completely absent of good taste, good service, and good ambiance” or something, then even if the word “absent” is not in your label training set, if it was in your 1 billion or 100 billion word corpus used to train the word embeddings, it might still get this right and generalize much better even to words that were in the training set used to train the word embeddings but not necessarily in the label training set that you had for specifically the sentiment classification problem. So that’s it for sentiment classification, and I hope this gives you a sense of how once you’ve learned or downloaded from online a word embedding, this allows you to quite quickly build pretty effective NLP systems. 02_debiasing-word-embeddingsMachine learning and AI algorithms are increasingly trusted to help with, or to make, extremely important decisions. And so we like to make sure that as much as possible that they’re free of undesirable forms of bias, such as gender bias, ethnicity bias and so on. What I want to do in this video is show you some of the ideas for diminishing or eliminating these forms of bias in word embeddings. When I use the term bias in this video, I don’t mean the bias variants or sense of the bias, instead I mean gender, ethnicity, sexual orientation bias. That’s a different sense of bias then is typically used in the technical discussion on machine learning. But mostly the problem, we talked about how word embeddings can learn analogies like man is to woman as king is to queen. But what if you ask it, man is to computer programmer as woman is to what? And so the authors of this paper Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai found a somewhat horrifying result where a learned word embedding might output Man:Computer_Programmer as Woman:Homemaker. And that just seems wrong and it enforces a very unhealthy gender stereotype. It’d be much more preferable to have algorithm output man is to computer programmer as a woman is to computer programmer. And they found also, Father:Doctor as Mother is to what? And the really unfortunate result is that some learned word embeddings would output Mother:Nurse. So word embeddings can reflect the gender, ethnicity, age, sexual orientation, and other biases of the text used to train the model. One that I’m especially passionate about is bias relating to socioeconomic status. I think that every person, whether you come from a wealthy family, or a low income family, or anywhere in between, I think everyone should have great opportunities. And because machine learning algorithms are being used to make very important decisions. They’re influencing everything ranging from college admissions, to the way people find jobs, to loan applications, whether your application for a loan gets approved, to in the criminal justice system, even sentencing guidelines. Learning algorithms are making very important decisions and so I think it’s important that we try to change learning algorithms to diminish as much as is possible, or, ideally, eliminate these types of undesirable biases. Now in the case of word embeddings, they can pick up the biases of the text used to train the model and so the biases they pick up or tend to reflect the biases in text as is written by people. Over many decades, over many centuries, I think humanity has made progress in reducing these types of bias. And I think maybe fortunately for AI, I think we actually have better ideas for quickly reducing the bias in AI than for quickly reducing the bias in the human race. Although I think we’re by no means done for AI as well and there’s still a lot of research and hard work to be done to reduce these types of biases in our learning algorithms. But what I want to do in this video is share with you one example of a set of ideas due to the paper referenced at the bottom by Bolukbasi and others on reducing the bias in word embeddings. So here’s the idea. Let’s say that we’ve already learned a word embedding, so the word babysitter is here, the word doctor is here. We have grandmother here, and grandfather here. Maybe the word girl is embedded there, the word boy is embedded there. And maybe she is embedded here, and he is embedded there. So the first thing we’re going to do it is identify the direction corresponding to a particular bias we want to reduce or eliminate. And, for illustration, I’m going to focus on gender bias but these ideas are applicable to all of the other types of bias that I mention on the previous slide as well. And so how do you identify the direction corresponding to the bias? For the case of gender, what we can do is take the embedding vector for he and subtract the embedding vector for she, because that differs by gender. And take e male, subtract e female, and take a few of these and average them, right? And take a few of these differences and basically average them. And this will allow you to figure out in this case that what looks like this direction(the horizontal direction in the slide) is the gender direction, or the bias direction. Whereas this direction(the vertical direction in the slide) is unrelated to the particular bias we’re trying to address. So this is the non-bias direction. And in this case, the bias direction, think of this as a 1D subspace whereas a non-bias direction, this will be 299-dimensional subspace. Okay, and I’ve simplified the description a little bit in the original paper. The bias direction can be higher than 1-dimensional, and rather than take an average, as I’m describing it here, it’s actually found using a more complicated algorithm called a SVD, a singular value decomposition. Which is closely related to, if you’re familiar with principle component analysis, it uses ideas similar to the pca or the principle component analysis algorithm. After that, the next step is a neutralization step. So for every word that’s not definitional, project it to get rid of bias. So there are some words that intrinsically capture gender. So words like grandmother, grandfather, girl, boy, she, he, a gender is intrinsic in the definition. Whereas there are other word like doctor and babysitter that we want to be gender neutral. And really, in the more general case, you might want words like doctor or babysitter to be ethnicity neutral or sexual orientation neutral, and so on, but we’ll just use gender as the illustrating example here. But so for every word that is not definitional, this basically means not words like grandmother and grandfather, which really have a very legitimate gender component, because, by definition, grandmothers are female, and grandfathers are male. So for words like doctor and babysitter, let’s just project them onto this axis to reduce their components, or to eliminate their component, in the bias direction. So reduce their component in this horizontal direction. So that’s the second neutralize step. And then the final step is called equalization in which you might have pairs of words such as grandmother and grandfather, or girl and boy, where you want the only difference in their embedding to be the gender. And so, why do you want that? Well in this example, the distance, or the similarity, between babysitter and grandmother is actually smaller than the distance between babysitter and grandfather. And so this maybe reinforces an unhealthy, or maybe undesirable, bias that grandmothers end up babysitting more than grandfathers. So in the final equalization step, what we’d like to do is to make sure that words like grandmother and grandfather are both exactly the same similarity, or exactly the same distance, from words that should be gender neutral, such as babysitter or such as doctor. So there are a few linear algebra steps for that. But what it will basically do is move grandmother and grandfather to a pair of points that are equidistant from this axis in the middle. And so the effect of that is that now the distance between babysitter, compared to these two words, will be exactly the same. And so, in general, there are many pairs of words like this grandmother-grandfather, boy-girl, sorority-fraternity, girlhood-boyhood, sister-brother, niece-nephew, daughter-son, that you might want to carry out through this equalization step. So the final detail is, how do you decide what word to neutralize? So for example, the word doctor seems like a word you should neutralize to make it non-gender-specific or non-ethnicity-specific. Whereas the words grandmother and grandmother should not be made non-gender-specific. And there are also words like beard, right, that it’s just a statistical fact that men are much more likely to have beards than women, so maybe beards should be closer to male than female. And so what the authors did is train a classifier to try to figure out what words are definitional, what words should be gender-specific and what words should not be. And it turns out that most words in the English language are not definitional, meaning that gender is not part of the definition. And it’s such a relatively small subset of words like this, grandmother-grandfather, girl-boy, sorority-fraternity, and so on that should not be neutralized. And so a linear classifier can tell you what words to pass through the neutralization step to project out this bias direction, to project it on to this essentially 299-dimensional subspace. And then, finally, the number of pairs you want to equalize, that’s actually also relatively small, and is, at least for the gender example, it is quite feasible to hand-pick most of the pairs you want to equalize. So the full algorithm is a bit more complicated than I present it here, you can take a look at the paper for the full details. And you also get to play with a few of these ideas in the programming exercises as well. So to summarize, I think that reducing or eliminating bias of our learning algorithms is a very important problem because these algorithms are being asked to help with or to make more and more important decisions in society. In this video I shared just one set of ideas for how to go about trying to address this problem, but this is still a very much an ongoing area of active research by many researchers. So that’s it for this week’s videos. Best of luck with this week’s programming exercises and I look forward to seeing you next week.]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
</search>
