<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[《Spark权威指南》的翻译综述]]></title>
    <url>%2F2020%2F02%2F10%2Fsummary_of_Translation(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[前言本系列文章将对《Spark - The Definitive Guide - Big data processing made simple》进行翻译，参照其他译本，取名为：《Spark权威指南》，翻译工作全程由我个人独自翻译，属于对照式翻译，有助于读者理解，如有不当或错误之处，欢迎不吝指出，方便你我他。 本书英文版出版信息2018年2月第一版 翻译进度Part I. Gentle Overview of Big Data and Spark 翻译：《Spark权威指南》第3章：Spark工具一览 Part II. Structured APIs—DataFrames, SQL, and Datasets 翻译：《Spark权威指南》第4章：结构化API概览 翻译：《Spark权威指南》第5章：基本结构化的操作 翻译：《Spark权威指南》第6章：处理不同的数据类型 翻译：《Spark权威指南》第7章：聚合 翻译：《Spark权威指南》第8章：连接 翻译：《Spark权威指南》第9章：数据源 翻译：《Spark权威指南》第10章：Spark SQL 翻译：《Spark权威指南》第11章：Dataset Part III. Low-Level APIs 翻译：《Spark权威指南》第12章：RDD 翻译：《Spark权威指南》第13章：高级RDD 翻译：《Spark权威指南》第14章：分布式共享变量 Part IV. Production Application 翻译：《Spark权威指南》第15章：Spark如何在集群上的运行 翻译：《Spark权威指南》第16章：开发Spark应用程序 翻译：《Spark权威指南》第17章：部署Spark应用程序 翻译：《Spark权威指南》第18章：监控和调试 翻译：《Spark权威指南》第19章：性能调优 Part V. Streaming 翻译：《Spark权威指南》第20章：流处理基础 翻译：《Spark权威指南》 第21章 结构化流基础 翻译：《Spark权威指南》第22章：事件时间和状态处理 翻译：《Spark权威指南》第23章：生产环境中的结构化流 Part VI. Advanced Analytics and Machine Learning 翻译：《Spark权威指南》第24章：高级分析和机器学习概述 翻译：《Spark权威指南》第25章：预处理与特征工程 翻译：《Spark权威指南》第26章：分类 翻译：《Spark权威指南》第27章：回归 翻译：《Spark权威指南》第28章：推荐 翻译：《Spark权威指南》第29章：无监督学习 还有第1,2,30章未翻译，近期太忙，择日翻译。 本书的勘误Errata | O’Reilly Mediawww.oreilly.com Last but not least如果你觉得本系列文章对你有帮助亦或愿意对我的开源付出进行支持，可以对我的本系列文章打赏，毕竟开源不易，由衷感谢你的关注与支持！！！]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Paper Google Bigtable 翻译与总结]]></title>
    <url>%2F2019%2F12%2F31%2FPaper-Google-Bigtable_translation-and-summary_online%2F</url>
    <content type="text"><![CDATA[前言 第一部分主要是论文的翻译与旁注：按照论文原文结构一步步翻译 第二部分主要是BigTable思想总结：BigTable论文相比GFS、MapReduce两篇复杂，行文并不流畅（可能本渣渣太弱），文中甚至没有总体结构说明和一些难点解释（例如：BigTable首创的并且在后来众多出名的开源组件（例如：LevelDB, RocksDB）中常用的SSTable文件索引格式：LSM 都没有详细说明），因此在总结处弥补这方面的说明。 论文翻译与旁注AbstractBigtable is a distributed storage system for managing structured data, which is designed to scale to a very large scale: petabytes of data in thousands of commercial servers. Many Google projects store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different requirements on Bigtable in terms of data size (from URL to web page to satellite imagery) and latency requirements (from back-end batch processing to real-time data services). Despite the varied requirements, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this article, we describe the simple data model provided by Bigtable, which provides customers with dynamic control over data layout and format, and describes the design and implementation of Bigtable. Bigtable是用于管理结构化数据的分布式存储系统，该系统旨在扩展到非常大的规模：数千个商用服务器中的PB级数据。 Google的许多项目都将数据存储在Bigtable中，包括网络索引，Google Earth和Google Finance。 这些应用程序在数据大小（从URL到网页到卫星图像）和延迟要求（从后端批量处理到实时数据服务）方面都对Bigtable提出了截然不同的要求。尽管需求千差万别，Bigtable已成功为所有这些Google产品提供了一种灵活的高性能解决方案。 在本文中，我们描述了Bigtable提供的简单数据模型，该模型为客户提供了对数据布局和格式的动态控制，并描述了Bigtable的设计和实现。 1 IntroductionIntroduction Over the last two and a half years we have designed, implemented, and deployed a distributed storage system for managing structured data at Google called Bigtable. Bigtable is designed to reliably scale to petabytes of data and thousands of machines. Bigtable has achieved several goals: wide applicability, scalability, high performance, and high availability. Bigtable is used by more than sixty Google products and projects, including Google Analytics, Google Finance, Orkut, Personalized Search, Writely, and Google Earth. These products use Bigtable for a variety of demanding workloads, which range from throughput-oriented batch-processing jobs to latency-sensitive serving of data to end users. The Bigtable clusters used by these products span a wide range of configurations, from a handful to thousands of servers, and store up to several hundred terabytes of data. 简介在过去的两年半中，我们在Google上设计，实施和部署了一个分布式存储系统来管理结构化数据，称为Bigtable。 Bigtable旨在可靠地扩展到PB级数据和数千台计算机。Bigtable实现了多个目标：广泛的适用性，可伸缩性，高性能和高可用性。 Bigtable被60多个Google产品和项目所使用，包括Google Analytics（分析），Google Finance，Orkut，个性化搜索，Writely和Google Earth。 这些产品将Bigtable用于各种要求高的工作负载，从面向吞吐量的批处理作业到对延迟敏感的终端用户所享受的数据服务。 这些产品使用的Bigtable集群涵盖了多种配置，从少量服务器到数千个服务器，最多可存储数百TB的数据。 In many ways, Bigtable resembles a database: it shares many implementation strategies with databases. Parallel databases[14] and main-memory databases[13] have achieved scalability and high performance, but Bigtable provides a different interface than such systems. Bigtable does not support a full relational data model; instead, it provides clients with a simple data model that supports dynamic control over data layout and format, and allows clients to reason about the locality properties of the data represented in the underlying storage. Data is indexed using row and column names that can be arbitrary strings. Bigtable also treats data as uninterpreted strings, although clients often serialize various forms of structured and semi-structured data into these strings. Clients can control the locality of their data through careful choices in their schemas. Finally, Bigtable schema parameters let clients dynamically control whether to serve data out of memory or from disk. 在许多方面，Bigtable类似于数据库：它与数据库共享许多实现策略。 并行数据库[14]和主内存数据库[13]已经实现了可伸缩性和高性能，但是Bigtable提供了与此类系统不同的接口。 Bigtable不支持完整的关系数据模型； 相反，它为客户端提供了一个简单的数据模型，该模型支持对数据布局和格式的动态控制，并允许客户端推理存储1在底层的数据的位置属性16。 可以使用任意字符串的行和列名称为数据建立索引。 尽管客户端经常将各种形式的结构化和半结构化数据序列化为这些字符串，但Bigtable还将数据视为未解析2的字符串。客户端可以通过在模式中进行仔细选择来控制其数据的位置。 最后，Bigtable模式参数可让客户端动态控制是从磁盘还是从内存获得数据3。 Section 2 describes the data model in more detail, and Section 3 provides an overview of the client API. Section 4 briefly describes the underlying Google infrastructure on which Bigtable depends. Section 5 describes the fundamentals of the Bigtable implementation, and Section 6 describes some of the refinements that we made to improve Bigtable’s performance. Section 7 provides measurements of Bigtable’s performance. We describe several examples of how Bigtable is used at Google in Section 8, and discuss some lessons we learned in designing and supporting Bigtable in Section 9. Finally, Section 10 describes related work, and Section 11 presents our conclusions. 第2节将更详细地描述数据模型，第3节将概述客户端API，第4节简要介绍了Bigtable所依赖的基础Google架构。 第5节介绍了Bigtable实现的基础知识，第6节介绍了我们为提高Bigtable的性能所做的一些改进。 第7节提供了Bigtable性能的衡量标准。 在第8节中，我们描述了如何在Google中使用Bigtable的几个示例，并在第9节中，讨论了我们在设计和支持Bigtable方面学到的一些教训。最后，第10节描述了相关工作，第11节介绍了我们的结论。 2 Data ModelA Bigtable is a sparse, distributed, persistent multidimensional sorted map. The map is indexed by a row key, column key, and a timestamp; each value in the map is an uninterpreted array of bytes. 一个BigTable是一个稀疏的、分布的、永久的多维有序的映射表（map）。我们采用行键（row key）、列键（column key）和时间戳（timestamp）对映射表（map）进行索引。映射表（map）中的每个值都是未经解析的字节数组。 (row:string, column string, time:int64)→string We settled on this data model after examining a variety of potential uses of a Bigtable-like system. As one concrete example that drove some of our design decisions, suppose we want to keep a copy of a large collection of web pages and related information that could be used by many different projects; let us call this particular table the Webtable. In Webtable, we would use URLs as row keys, various aspects of web pages as column names, and store the contents of the web pages in the contents: column under the timestamps when they were fetched, as illustrated in Figure 1. 在研究了类似Bigtable的系统的各种潜在用途之后，我们选择了此数据模型。 作为推动我们某些设计决策的一个具体示例，假设我们想要保留大量网页和相关信息的副本，这些副本可以由许多不同的项目使用。 让我们将此特定表称为Webtable。 在Webtable中，我们将使用URL作为行键，将网页的各个方面用作列名，并将网页的内容存储在 contents: 列，这些列在时间戳（获取时的时间）底下17，如图1所示。 图1 存储了网页数据的Webtable的一个片段行名称是反转的URL，contents列家族包含了网页内容，anchor列家族包含了任何引用这个页面的anchor文本。CNN的主页被Sports Illustrated和MY-look主页同时引用，因此，我们的行包含了名称为 ”anchor:cnnsi.com” 和 ”anchor:my.look.ca” 的列。每个anchor单元格都只有一个版本，contents列有三个版本，分别对应于时间戳t3，t5和t6。 RowsThe row keys in a table are arbitrary strings (currently up to 64KB in size, although 10-100 bytes is a typical size for most of our users). Every read or write of data under a single row key is atomic (regardless of the number of different columns being read or written in the row), a design decision that makes it easier for clients to reason about the system’s behavior in the presence of concurrent updates to the same row. 表中的行键是任意字符串（当前大小最大为64KB，尽管对于大多数用户而言，典型大小是10-100字节）。 单个行键下的每次数据读取或写入都是原子性的（无论该行中读取或写入的不同列的数量如何），该设计决策使客户端在出现并发更新到同一行时更容易推断系统行为。 Bigtable maintains data in lexicographic order by row key. The row range for a table is dynamically partitioned. Each row range is called a tablet, which is the unit of distribution and load balancing. As a result, reads of short row ranges are efficient and typically require communication with only a small number of machines. Clients can exploit this property by selecting their row keys so that they get good locality for their data accesses. For example, in Webtable, pages in the same domain are grouped together into contiguous rows by reversing the hostname components of the URLs. For example, we store data for maps.google.com/index.html under the key com.google.maps/index.html. Storing pages from the same domain near each other makes some host and domain analyses more efficient. Bigtable按行键的字典顺序维护数据。表的行区间是动态分区的。每个行区间称为一个Tablet，它是分配和负载平衡的单位。结果，对行的小范围读取（reads of short row ranges，这里short修饰的名词是 ranges 还是 row ，最终根据下文的例子进行反推的）是高效的并且通常仅需要与少量机器通信。客户端可以通过选择行键来利用此属性，以便他们可以很好地进行数据访问。例如，在Webtable中，通过反转URL的主机名部分，可以将同一域中的页面分组为连续的行。例如我们将数据maps.google.com/index.html存储在键com.google.maps/index.html下。 将同一域中的页面彼此靠近存储可以使某些主机和域分析更加高效。 Column FamiliesColumn keys are grouped into sets called column families, which form the basic unit of access control. All data stored in a column family is usually of the same type (we compress data in the same column family together). A column family must be created before data can be stored under any column key in that family; after a family has been created, any column key within the family can be used. It is our intent that the number of distinct column families in a table be small (in the hundreds at most), and that families rarely change during operation. In contrast, a table may have an unbounded number of columns. 列键被分组成称为列族的集合，这些集合构成访问控制的基本单元。 列族中存储的所有数据通常都是同一类型（我们将同一列族中的数据压缩在一起）。 必须先创建一个列族，然后才能将数据存储在该族中的任何列键下。 创建族后，可以使用族中的任何列键。 我们的目的是使表中不同的列族的数量少（最多数百个），并且在操作过程中族很少改变。 相反，表可能具有无限数量的列。 A column key is named using the following syntax: family:qualifier. Column family names must be printable, but qualifiers may be arbitrary strings. An example column family for the Webtable is language, which stores the language in which a web page was written. We use only one column key in the language family, and it stores each web page’s language ID. Another useful column family for this table is anchor; each column key in this family represents a single anchor, as shown in Figure 1. The qualifier is the name of the referring site; the cell contents is the link text. 列键使用以下语法命名：family:qualifier。列族（ column family）名称必须是可打印的，但限定词（qualifier）可以是任意字符串。 Webtable的一个示例列族是language，它存储编写网页所用的语言。我们在语言族中仅使用一个列键，并且它存储每个网页的语言ID。此表的另一个有用的列族是锚； 该族中的每个列键都代表一个锚，如图1所示。限定符是引用站点的名称。单元格内容是链接文本。 Access control and both disk and memory accounting are performed at the column-family level. In our Webtable example, these controls allow us to manage several different types of applications: some that add new base data, some that read the base data and create derived column families, and some that are only allowed to view existing data (and possibly not even to view all of the existing families for privacy reasons). 访问控制以及磁盘和内存统计4 均在列族层次执行。 在我们的Webtable示例中，这些控制（权限）使我们能够管理几种不同类型的应用程序：一些应用程序是添加新的基本数据，一些应用程序是读取基本数据并创建派生的列族，某些应用程序仅被许可查看现有数据（出于隐私原因，甚至可能不允许查看所有现有列族）。 TimestampsEach cell in a Bigtable can contain multiple versions of the same data; these versions are indexed by timestamp. Bigtable timestamps are 64-bit integers. They can be assigned by Bigtable, in which case they represent “real time” in microseconds, or be explicitly assigned by client applications. Applications that need to avoid collisions must generate unique timestamps themselves. Different versions of a cell are stored in decreasing timestamp order, so that the most recent versions can be read first. Bigtable中的每个单元格可以包含同一数据的多个版本；这些版本通过时间戳索引。 Bigtable时间戳是64位整数。它们可以由Bigtable分配，在这种情况下，它们以微秒为单位表示“真实时间”，也可以由客户端应用程序明确分配。需要避免冲突的应用程序必须自己生成唯一的时间戳。单元格的不同版本以时间戳的降序5存储，因此可以首先读取最新版本。 To make the management of versioned data less onerous, we support two per-column-family settings that tell Bigtable to garbage-collect cell versions automatically. The client can specify either that only the last n versions of a cell be kept, or that only new-enough versions be kept (e.g., only keep values that were written in the last seven days). 为了减少版本化数据的管理工作，我们支持每个列族的两个设置，这些设置告诉Bigtable自动垃圾回收单元格版本。 客户端可以指定仅保留单元格的最后n个版本，或者仅保留足够新的版本（例如，仅保留最近7天写入的值）。 In our Webtable example, we set the timestamps of the crawled pages stored in the contents: column to the times at which these page versions were actually crawled. The garbage-collection mechanism described above lets us keep only the most recent three versions of every page. 在我们的Webtable示例中，我们将 content: 列中存储的爬虫网页的时间戳设置为实际爬虫这些页面版本的时间。 上述的垃圾收集机制使我们仅保留每个页面的最新三个版本。 3 APIThe Bigtable API provides functions for creating and deleting tables and column families. It also provides functions for changing cluster, table, and column family metadata, such as access control rights. Bigtable API提供了用于创建和删除表和列族的功能。它还提供了用于更改集群，表和列族元数据的功能，例如访问控制权限。 12345678// Open the tableTable *T = OpenOrDie("/bigtable/web/webtable");// Write a new anchor and delete an old anchorRowMutation r1(T, "com.cnn.www");r1.Set("anchor:www.c-span.org", "CNN");r1.Delete("anchor:www.abc.com");Operation op;Apply(&amp;op, &amp;r1); 图 2 写入到BigtableClient applications can write or delete values in Bigtable, look up values from individual rows, or iterate over a subset of the data in a table. Figure 2 shows C++ code that uses a RowMutation abstraction to perform a series of updates. (Irrelevant details were elided to keep the example short.) The call to Apply performs an atomic mutation to the Webtable: it adds one anchor to www.cnn.com and deletes a different anchor. 客户端应用程序可以在Bigtable中写入或删除值，可以从各个行中查找值，也可以遍历表中的数据子集。图2显示了使用RowMutation抽象（对象）6来执行一系列更新的 C++ 代码。（省略了详细信息，以使示例简短）对Apply的调用对Webtable进行了原子修改：它将一个锚点添加到 www.cnn.com 并删除另一个锚点。 123456789101112Scanner scanner(T);ScanStream *stream;stream = scanner.FetchColumnFamily("anchor");stream-&gt;SetReturnAllVersions();scanner.Lookup("com.cnn.www");for (; !stream-&gt;Done(); stream-&gt;Next()) &#123; printf("%s %s %lld %s\n", scanner.RowName(), stream-&gt;ColumnName(), stream-&gt;MicroTimestamp(), stream-&gt;Value());&#125; 图3: 从Bigtable读取数据Figure 3 shows C++ code that uses a Scanner abstraction to iterate over all anchors in a particular row. Clients can iterate over multiple column families, and there are several mechanisms for limiting the rows, columns, and timestamps produced by a scan. For example, we could restrict the scan above to only produce anchors whose columns match the regular expression anchor:*.cnn.com, or to only produce anchors whose timestamps fall within ten days of the current time. 图3显示了使用Scanner抽象（对象） 7 对特定行中的所有锚点进行迭代的C ++代码。客户端可以迭代多个列族，并且有几种机制可以限制扫描产生的行，列和时间戳。例如，我们可以将上面的扫描限制为仅生成其列与正则表达式 anchor:*.cnn.com 匹配的锚，或者仅生成其时间戳在当前时间的十天内之内的锚。 Bigtable supports several other features that allow the user to manipulate data in more complex ways. First, Bigtable supports single-row transactions, which can be used to perform atomic read-modify-write sequences on data stored under a single row key. Bigtable does not currently support general transactions across row keys, although it provides an interface for batching writes across row keys at the clients. Second, Bigtable allows cells to be used as integer counters. Finally, Bigtable supports the execution of client-supplied scripts in the address spaces of the servers. The scripts are written in a language developed at Google for processing data called Sawzall[28]. At the moment, our Sawzall-based API does not allow client scripts to write back into Bigtable, but it does allow various forms of data transformation, filtering based on arbitrary expressions, and summarization via a variety of operators. Bigtable支持其他几种功能，这些功能允许用户以更复杂的方式操作数据。 首先，Bigtable支持单行事务（single-row transaction），该事务可用于对存储在单个行键下的数据执行原子的 “读-修改-写”（read-modify-write） 序列。 Bigtable目前不支持跨行键的常规事务，尽管它提供了用于在客户端跨行键批处理写入的接口。 其次，Bigtable允许将单元格18用作整数计数器。 最后，Bigtable支持在服务器的地址空间中执行客户端提供的脚本。 这些脚本是用Google开发的一种用于处理数据的语言（称为Sawzall[28]）编写的。 目前，我们基于Sawzall的API不允许客户端脚本写回到Bigtable，但允许多种形式的数据转换，基于任意表达式的过滤以及通过各种运算符的汇总。 Bigtable can be used with MapReduce[12], a framework for running large-scale parallel computations developed at Google. We have written a set of wrappers that allow a Bigtable to be used both as an input source and as an output target for MapReduce jobs. Bigtable可与MapReduce[12]结合使用，MapReduce是一种由Google开发的用于运行大规模并行计算的框架。 我们编写了一组包装器（wrappers），这些包装器允许Bigtable用作MapReduce作业的输入源和输出目标。 4 Building BlocksBigtable is built on several other pieces of Google infrastructure. Bigtable uses the distributed Google File System (GFS) [17] to store log and data files. A Bigtable cluster typically operates in a shared pool of machines that run a wide variety of other distributed applications, and Bigtable processes often share the same machines with processes from other applications. Bigtable depends on a cluster management system for scheduling jobs, managing resources on shared machines, dealing with machine failures, and monitoring machine status. Bigtable建立在Google其他几个基础架构之上。 Bigtable使用分布式Google文件系统（GFS）[17]存储日志和数据文件。 Bigtable集群通常运行在与多种其他分布式应用程序共享的服务器池10中，并且Bigtable进程通常与其他应用程序的进程共享同一台计算机。 Bigtable依靠集群管理系统来调度作业、来管理共享计算机上的资源、来处理计算机故障以及监视计算机状态。 The Google SSTable file format is used internally to store Bigtable data. An SSTable provides a persistent, ordered immutable map from keys to values, where both keys and values are arbitrary byte strings. Operations are provided to look up the value associated with a specified key, and to iterate over all key/value pairs in a specified key range. Internally, each SSTable contains a sequence of blocks (typically each block is 64KB in size, but this is configurable). A block index (stored at the end of the SSTable) is used to locate blocks; the index is loaded into memory when the SSTable is opened. A lookup can be performed with a single disk seek: we first find the appropriate block by performing a binary search in the in-memory index, and then reading the appropriate block from disk. Optionally, an SSTable can be completely mapped into memory, which allows us to perform lookups and scans without touching disk. Google SSTable文件格式在内部用于存储Bigtable数据。 SSTable提供了从键到值都可以持久化、有序的、不可变的映射表（map），其中键和值都是任意字节字符串。提供操作以查找与指定键相关联的值，并遍历指定键范围内的所有键/值对。在内部，每个SSTable包含一系列块（通常每个块的大小为64KB，但这是可配置的）。块的索引（存储在SSTable的末尾）用于定位块。当打开SSTable时，索引将加载到内存中。可以使用单次磁盘寻址（ disk seek）执行一次查找：我们首先对内存中的索引执行二分搜索来找到对应的块索引11，然后从磁盘读取相应8的块。可选项是可以将一个SSTable全部映射到内存中，这使我们无需与磁盘进行io12即可执行查找和扫描。 Bigtable relies on a highly-available and persistent distributed lock service called Chubby [8]. A Chubby service consists of five active replicas, one of which is elected to be the master and actively serve requests. The service is live when a majority of the replicas are running and can communicate with each other. Chubby uses the Paxos algorithm [9][23] to keep its replicas consistent in the face of failure. Chubby provides a namespace that consists of directories and small files. Each directory or file can be used as a lock, and reads and writes to a file are atomic. The Chubby client library provides consistent caching of Chubby files. Each Chubby client maintains a session with a Chubby service. A client’s session expires if it is unable to renew its session lease within the lease expiration time. When a client’s session expires, it loses any locks and open handles. Chubby clients can also register callbacks on Chubby files and directories for notification of changes or session expiration. Bigtable依赖一个高可用且持久的分布式锁定服务，称为Chubby[8]。Chubby服务由五个活动副本组成，其中一个活动副本被选为主副本，并积极响应请求13。当大部分副本处于运行状态并且能够彼此通信时，这个服务是可用的9。Chubby使用Paxos算法 [9][23] 应对失败时如何保持其副本的一致性。 Chubby提供了一个由目录和小文件组成的命名空间。每个目录或文件都可以用作锁，并且对文件的读写是原子的。 Chubby客户端函数库提供一致的Chubby文件缓存。每个Chubby客户端都维护一个Chubby服务会话（session）。如果客户端的会话（session）无法在租约（lease）到期时间内续签（renew）其会话租约（session lease），则该会话将过期。客户端会话（session）期满后，它将丢失所有锁以及已打开的文件句柄（handle）。Chubby客户端也可以在Chubby文件和目录上注册回调函数（callback），以通知（出现）变化或会话（session）到期。 Bigtable uses Chubby for a variety of tasks: to ensure that there is at most one active master at any time; to store the bootstrap location of Bigtable data (see Section 5.1); to discover tablet servers and finalize tablet server deaths (see Section 5.2); to store Bigtable schema information (the column family information for each table); and to store access control lists. If Chubby becomes unavailable for an extended period of time, Bigtable becomes unavailable. We recently measured this effect in 14 Bigtable clusters spanning 11 Chubby instances. The average percentage of Bigtable server hours during which some data stored in Bigtable was not available due to Chubby unavailability (caused by either Chubby outages or network issues) was 0.0047%. The percentage for the single cluster that was most affected by Chubby unavailability was 0.0326%. Bigtable使用Chubby来完成各种任务： 确保任何时候最多一个活跃的master（active master）； 存储Bigtable数据的引导位置（bootstrap location）（请参阅第5.1节）； 发现 Tablet 服务器并确定 Tablet 服务器的死机（请参阅第5.2节）； 存储Bigtable模式（schema）信息（每个表的列族信息）； 存储用于访问控制的信息而组成的列表； 如果Chubby长时间不可用，则Bigtable将不可用。我们最近在跨越11个Chubby实例的14个Bigtable集群中测量了这种影响。由于Chubby不可用（由于Chubby中断或网络问题所致）导致存储在Bigtable服务器上的一些数据无法访问的时间平均占比为0.0047％。受Chubby不可用性影响最大的单个集群上面的数据无法访问的时间占比为0.0326％。 5 ImplementationThe Bigtable implementation has three major components: a library that is linked into every client, one master server, and many tablet servers. Tablet servers can be dynamically added (or removed) from a cluster to accomodate changes in workloads. Bigtable实现具有三个主要组件：一个链接到每个客户端的函数库，一个主服务器（master server）和许多Tablet服务器。可以从集群中动态添加（或删除）Tablet服务器，以适应工作负载的变化。 The master is responsible for assigning tablets to tablet servers, detecting the addition and expiration of tablet servers, balancing tablet-server load, and garbage collection of files in GFS. In addition, it handles schema changes such as table and column family creations. 主服务器（master）负责将Tablet分配给Tablet服务器，检测Tablet服务器的添加和到期，平衡Tablet服务器的负载以及GFS中文件的垃圾回收。此外，它还处理模式（schema）的变化，例如创建表和列族。 Each tablet server manages a set of tablets (typically we have somewhere between ten to a thousand tablets per tablet server). The tablet server handles read and write requests to the tablets that it has loaded, and also splits tablets that have grown too large. 每个Tablet服务器管理一组Tablet（通常每个Tablet服务器有十到一千个Tablet）。Tablet服务器处理对已加载的Tablet的读写请求，并且还会切分太大的Tablet。 As with many single-master distributed storage systems [17][21], client data does not move through the master: clients communicate directly with tablet servers for reads and writes. Because Bigtable clients do not rely on the master for tablet location information, most clients never communicate with the master. As a result, the master is lightly loaded in practice. 与许多单个主服务器（single-master）的分布式存储系统[17][21]一样，客户端数据不会传输到主服务器（master）：客户端直接与Tablet服务器通信以进行读取和写入数据。由于Bigtable客户端不依赖主服务器（master）获取Tablet的位置信息，所以大多数客户端从不与主服务器（master）通信。结果，在实践中主服务器（master）是低负载的。 A Bigtable cluster stores a number of tables. Each table consists of a set of tablets, and each tablet contains all data associated with a row range. Initially, each table consists of just one tablet. As a table grows, it is automatically split into multiple tablets, each approximately 100-200 MB in size by default. Bigtable集群存储许多表。每个表由一组Tablet组成，并且每个Tablet包含了关联一个行区间的所有数据。最初，每个表格仅包含一个Tablet。随着表的增长，它会自动切分成多个Tablet，默认情况下每个Tablet的大小约为100-200 MB。 5.1 Tablet LocationWe use a three-level hierarchy analogous to that of a B+ tree [10] to store tablet location information (Figure 4). 我们使用类似于B+树[10]的三级层次结构来存储Tablet位置信息（图4）。 The first level is a file stored in Chubby that contains the location of the root tablet. The root tablet contains the location of all tablets in a special METADATA table. Each METADATA tablet contains the location of a set of user tablets. The root tablet is just the first tablet in the METADATA table, but is treated specially—it is never split—to ensure that the tablet location hierarchy has no more than three levels. 第一级是存储在Chubby中的文件，它包含Root Tablet的位置。 Root Tablet 包含特殊的 METADATA table 中所有Tablet的位置。 每个METADATA Tablet都包含一组 User Tablets 的位置。 Root Tablet只是METADATA table中的第一个Tablet，但经过特殊处理（从不切分），以确保Tablet位置层次结构不超过三个级别。 The METADATA table stores the location of a tablet under a row key that is an encoding of the tablet’s table identifier and its end row. Each METADATA row stores approximately 1KB of data in memory. With a modest limit of 128 MB METADATA tablets, our three-level location scheme is sufficient to address $2^{34}$ tablets (or $2^{61}$ bytes in 128 MB tablets). METADATA table 存储了某个行键下的Tablet的位置信息，该行键是Tablet表标识符及其最后一行的编码。 每个METADATA行在内存中存储大约1KB的数据。 由于 METADATA Tablet 的 128 MB 这个不大的限制，我们的三级定位方案足以处理 $2^{34}$ 个Tablet（或128 MB Tablet中的 $2^{61}$ 字节）。 译者附 第一级：Chubby 中的一个文件 第二级：METADATA tables（第一个 METADATA table 比较特殊，所以在图中单独画出，但它其实和其他 METADATA table 都属于第二级，即 METADATA tables = 图示中的1st METADATA Tablet (Root Tablet) + Other METADATA Tablets） 第三级：User Tables METADATA 是一个特殊的 Tablet，其中的第一个 Tablet 称为 Root Tablet 。Root Tablet 和 METADATA 内其他 Tablet 不同之处在于：它永远不会分裂，这样就可以保证 Tablet location 层级不会超过三层。 三级间的关系： Chubby 中的文件保存了 Root Tablet 的位置 Root Tablet 保存了 METADATA Tablet 内所有其他 Tablet 的位置 每个 METADATA Tablet（Root Tablet 除外）保存了一组 User Tables 的位置 METADATA 的每行数据在内存中大约占 1KB。而 METADATA Tablet 的大小限制在 128MB，这种三级位置方案就可以存储高达 128MB = $2^{17} * $ 1KB，即每个 METADATA Tablet 可以指向 $2^{17}$ 个 User Table，每个 User Table 同样是 128MB 的大小话，就有 $2^{17} * 2^{17} = 2^{34}$ 个 Tablet 。 如果每个 Tablet 128 MB 大小，那总数据量就高达 128MB = $2^{27}$ Byte， $2^{34} * 2^{27} = 2^{61}$ Byte，即2000PB The client library caches tablet locations. If the client does not know the location of a tablet, or if it discovers that cached location information is incorrect, then it recursively moves up the tablet location hierarchy. If the client’s cache is empty, the location algorithm requires three network round-trips, including one read from Chubby. If the client’s cache is stale, the location algorithm could take up to six round-trips, because stale cache entries are only discovered upon misses (assuming that METADATA tablets do not move very frequently). Although tablet locations are stored in memory, so no GFS accesses are required, we further reduce this cost in the common case by having the client library prefetch tablet locations: it reads the metadata for more than one tablet whenever it reads the METADATA table. 客户端库缓存Tablet的位置信息。 如果客户端不知道Tablet的位置，或者发现缓存的位置信息不正确，则它将在Tablet位置层级中向上递归14（查找想要的位置信息）。 如果客户的缓存为空，则定位算法需要进行三次网络往返，包括从Chubby中读取一次。 如果客户的缓存过时，则定位算法最多可能需要进行六次往返，因为过时的缓存项仅在未命中时才被发现（假设METADATA Tablet的移动频率不高）。 尽管Tablet位置存储在内存中，所以不需要GFS访问，但在常见情况下，我们通过让客户端库预取Tablet位置来进一步降低了此成本：每当读取METADATA表时，它都会读取一个以上Tablet的元数据。 We also store secondary information in the METADATA table, including a log of all events pertaining to each tablet (such as when a server begins serving it). This information is helpful for debugging and performance analysis. 我们还将辅助信息存储在METADATA表中，包括与每个Tablet有关的所有事件的日志（例如服务器何时开始为其服务）。 此信息有助于调试和性能分析。 5.2 Tablet AssignmentEach tablet is assigned to one tablet server at a time. The master keeps track of the set of live tablet servers, and the current assignment of tablets to tablet servers, including which tablets are unassigned. When a tablet is unassigned, and a tablet server with sufficient room for the tablet is available, the master assigns the tablet by sending a tablet load request to the tablet server. 每个Tablet每次分配到一个Tablet服务器。主服务器跟踪有效的Tablet服务器的集合15以及Tablet到Tablet服务器的当前分配关系，包括未分配的Tablet。当Tablet未分配并且可用的Tablet服务器有足够的空间来容纳Tablet时，主服务器通过向Tablet服务器发送Tablet加载请求来分配Tablet。 Bigtable uses Chubby to keep track of tablet servers. When a tablet server starts, it creates, and acquires an exclusive lock on, a uniquely-named file in a specific Chubby directory. The master monitors this directory (the servers directory) to discover tablet servers. A tablet server stops serving its tablets if it loses its exclusive lock: e.g., due to a network partition that caused the server to lose its Chubby session. (Chubby provides an efficient mechanism that allows a tablet server to check whether it still holds its lock without incurring network traffic.) A tablet server will attempt to reacquire an exclusive lock on its file as long as the file still exists. If the file no longer exists, then the tablet server will never be able to serve again, so it kills itself. Whenever a tablet server terminates (e.g., because the cluster management system is removing the tablet server’s machine from the cluster), it attempts to release its lock so that the master will reassign its tablets more quickly. Bigtable使用Chubby来跟踪Tablet服务器。Tablet服务器启动后，将在特定的Chubby目录中创建一个命名唯一的文件并获这个文件的独占锁。主服务器监控此目录（服务器目录）以发现Tablet服务器。Tablet服务器如果丢失文件的独占锁，则会停止为其Tablet提供服务：例如，由于网络分区导致服务器丢失了Chubby会话。（Chubby提供了一种高效的机制，可让Tablet服务器检查其是否仍然持有独占锁而不会引起网络通信）只要该文件仍然存在，Tablet服务器将尝试重新获取对其文件的独占锁。如果该文件不再存在，则Tablet服务器将永远无法再次提供服务，因此它将自行终止。Tablet服务器终止时（例如，由于集群管理系统正在从集群中删除Tablet服务器的计算机），它将尝试释放它持有的锁，以便主机可以更快地重新分配这个Tablet服务器被分配到的Tablet。 The master is responsible for detecting when a tablet server is no longer serving its tablets, and for reassigning those tablets as soon as possible. To detect when a tablet server is no longer serving its tablets, the master periodically asks each tablet server for the status of its lock. If a tablet server reports that it has lost its lock, or if the master was unable to reach a server during its last several attempts, the master attempts to acquire an exclusive lock on the server’s file. If the master is able to acquire the lock, then Chubby is live and the tablet server is either dead or having trouble reaching Chubby, so the master ensures that the tablet server can never serve again by deleting its server file. Once a server’s file has been deleted, the master can move all the tablets that were previously assigned to that server into the set of unassigned tablets. To ensure that a Bigtable cluster is not vulnerable to networking issues between the master and Chubby, the master kills itself if its Chubby session expires. However, as described above, master failures do not change the assignment of tablets to tablet servers. 主服务器（master ）负责检测Tablet服务器何时不再为其Tablet提供服务，并负责尽快重新分配这些Tablet。为了检测Tablet服务器何时不再为其Tablet提供服务，主服务器（master ）会定期向每个Tablet服务器询问其锁的状态。如果Tablet服务器报告其锁已丢失，或者主服务器（master ）在最后几次尝试期间都无法访问服务器，则主服务器（master ）将尝试获取Chubby所在的服务器的Chubby目录下的文件独占锁。如果主服务器（master ）能够获取锁，则Chubby处于存活的状态，以及如果Tablet服务器死机或者无法访问Chubby，那么主服务器（master ）通过删除Chubby所在的服务器的Chubby目录下的文件来确保Tablet服务器永远不会再次服务。删除Chubby所在的服务器的Chubby目录下的文件后，主服务器（master ）可以将以前分配给处于无效状态的Tablet服务器的所有Tablet移至未分配的Tablet集合中。为了确保Bigtable集群不会受到主服务器（master ）和Chubby之间的网络问题的影响，如果主服务器的Chubby会话到期，则主服务器会自行杀死。但是，如上所述，主服务器（master）设备故障不会更改Tablet到Tablet服务器的分配关系。 When a master is started by the cluster management system, it needs to discover the current tablet assignments before it can change them. The master executes the following steps at startup. (1) The master grabs a unique master lock in Chubby, which prevents concurrent master instantiations. (2) The master scans the servers directory in Chubby to find the live servers. (3) The master communicates with every live tablet server to discover what tablets are already assigned to each server. (4) The master scans the METADATA table to learn the set of tablets. Whenever this scan encounters a tablet that is not already assigned, the master adds the tablet to the set of unassigned tablets, which makesthe tablet eligible for tablet assignment. 当主服务器由集群管理系统启动时，它需要先发现当前的Tablet分配关系，然后才能更改它们。主服务器在启动时执行以下步骤。 （1）主服务器在Chubby中获取唯一的主服务器锁，这可以防止并发的主服务器实例化。（2）主服务器扫描Chubby中的服务器目录以找到有效的Tablet服务器。（3）主服务器与每个有效的Tablet服务器通信，以发现已分配给每个服务器的Tablet。（4）主服务器扫描METADATA table获知Tablet集合。每当此扫描遇到尚未分配的Tablet时，主服务器就会将该Tablet添加到未分配的Tablet集合中，这使该Tablet有资格进行Tablet分配。 One complication is that the scan of the METADATA table cannot happen until the METADATA tablets have been assigned. Therefore, before starting this scan (step 4), the master adds the root tablet to the set of unassigned tablets if an assignment for the root tablet was not discovered during step 3. This addition ensures that the root tablet will be assigned. Because the root tablet contains the names of all METADATA tablets, the master knows about all of them after it has scanned the root tablet. 一种复杂的情况是，在分配 METADATA Tablet 之前，无法进行 METADATA table 的扫描。因此，在开始此扫描（步骤4）之前，如果在步骤3中未找到针对Root Tablet的分配，则主服务器会将Root Tablet添加到未分配Tablet的集合中。此添加操作确保了将对Root Tablet进行分配。由于Root Tablet包含所有METADATA Tablet的名称，因此主服务器在扫描了Root Tablet之后便知道了所有这些名称。 译者附在扫描 METADATA Tablet 之前，必须保证 METADATA table 自己已经被分配出去了。因此，如果在步骤 3 中发现 Root Tablet 还没有被分配出去，那主服务器就要先将它放到 未分配 Tablet 集合，然后去执行步骤 4。 这样就保证了 Root Tablet 将会被分配出去。 The set of existing tablets only changes when a table is created or deleted, two existing tablets are merged to form one larger tablet, or an existing tablet is split into two smaller tablets. The master is able to keep track of these changes because it initiates all but the last. Tablet splits are treated specially since they are initiated by a tablet server. The tablet server commits the split by recording information for the new tablet in the METADATA table. When the split has committed, it notifies the master. In case the split notification is lost (either because the tablet server or the master died), the master detects the new tablet when it asks a tablet server to load the tablet that has now split. The tablet server will notify the master of the split, because the tablet entry it finds in the METADATA table will specify only a portion of the tablet that the master asked it to load. 现有的Tablet集合，只有在以下情形才会发生改变： （1）当一个Tablet被创建或删除； （2）对两个现有的Tablet进行合并得到一个更大的Tablet； （3）一个现有的tablet被切分成两个较小的Tablet。 主服务器能够跟踪这些变化，因为它负责启动除最后一次以外的所有操作。Tablet切分操作是由Tablet服务器启动的，因此受到特殊对待。Tablet服务器通过在 METADATA table 中记录新Tablet的信息来提交切分操作。提交切分操作后，它将通知主服务器。万一切分事件通知丢失（由于Tablet服务器或主服务器死机），则主服务器在要求Tablet服务器加载现在已切分的Tablet时，会检测到新的Tablet。Tablet服务器会把切分操作通知主服务器，因为它在 METADATA table 中查到的Tablet条目将仅指定一部分的Tablet，而Tablet是主服务器要求Tablet服务器加载的。 译者附如果通知丢失（由于Tablet服务器或主服务器挂掉），主服务器会在它下次要求一个Tablet server 加载 Tablet 时发现。这个 Tablet 服务器会将这次切分事件通知给主服务器，因为“Tablet服务器通过在 METADATA table 中记录新Tablet的信息来提交切分操作。提交切分操作后，它将通知主服务器”。所以它在 METADATA table 中发现的 Tablet 项只覆盖主服务器要求它加载的 Tablet 的了一部分。 5.3 Tablet Serving The persistent state of a tablet is stored in GFS, as illustrated in Figure 5. Updates are committed to a commit log that stores redo records. Of these updates, the recently committed ones are stored in memory in a sorted buffer called a memtable; the older updates are stored in a sequence of SSTables. To recover a tablet, a tablet server reads its metadata from the METADATA table. This metadata contains the list of SSTables that comprise a tablet and a set of a redo points, which are pointers into any commit logs that may contain data for the tablet. The server reads the indices of the SSTables into memory and reconstructs the memtable by applying all of the updates that have committed since the redo points. Tablet的持久化状态存储在GFS中，如图5所示。更新被提交（commit）到一个提交日志（commit log），这些日志存储着重做的记录（redo records）。在这些更新当中，最近提交的更新被存储到内存当中的一个被称为memtable的排序缓冲区，比较老的更新被存储在一系列SSTable中。为了恢复Tablet，Tablet服务器从 METADATA table 读取其元数据。该元数据包含SSTables列表，该SSTables包含一个Tablet和一个重做点（redo point）的集合 ，这些重做点（redo point）是指向任何可能包含该Tablet数据的提交日志的指针。服务器将SSTables的索引读入内存，并通过应用自重做点以来已提交的所有更新来重建memtable。 When a write operation arrives at a tablet server, the server checks that it is well-formed, and that the sender is authorized to perform the mutation. Authorization is performed by reading the list of permitted writers from a Chubby file (which is almost always a hit in the Chubby client cache). A valid mutation is written to the commit log. Group commit is used to improve the throughput of lots of small mutations [13][16]. After the write has been committed, its contents are inserted into the memtable. 当写操作到达Tablet服务器时，服务器将检查其格式是否正确，以及发送方是否有权执行这个更改（mutation）。通过从Chubby文件中读取允许的作者列表来执行授权（这在Chubby客户端缓存中几乎总是命中）。有效的更改（mutation）将写入提交日志（commit log）。整组提交（group commit）用于提高许多小更改的吞吐量 [13][16]。提交写入后，其内容将插入到memtable中。 When a read operation arrives at a tablet server, it is similarly checked for well-formedness and proper authorization. A valid read operation is executed on a merged view of the sequence of SSTables and the memtable. Since the SSTables and the memtable are lexicographically sorted data structures, the merged view can be formed efficiently. Incoming read and write operations can continue while tablets are split and merged. 当读操作到达Tablet服务器时，同样会检查其格式是否正确以及是否获得适当的授权。在SSTables和memtable序列的合并视图上执行有效的读取操作。由于SSTables和memtable是按字典顺序排序的数据结构，因此可以有效地形成合并视图。切分和合并Tablet时，传入的读写操作可以继续。 5.4 CompactionsAs write operations execute, the size of the memtable increases. When the memtable size reaches a threshold, the memtable is frozen, a new memtable is created, and the frozen memtable is converted to an SSTable and written to GFS. This minor compaction process has two goals: it shrinks the memory usage of the tablet server, and it reduces the amount of data that has to be read from the commit log during recovery if this server dies. Incoming read and write operations can continue while compactions occur. 随着写操作的执行，memtable的大小增加。 当memtable大小达到阈值时，该memtable被冻结，创建新的memtable，并将冻结的memtable转换为SSTable并写入GFS。 这个次要的压缩过程有两个目标：减少Tablet服务器的内存使用量，并且如果该服务器死机，那么在恢复期间，压缩将减少必须从提交日志中读取的数据量。 发生压缩时，传入的读取和写入操作可以继续。 Every minor compaction creates a new SSTable. If this behavior continued unchecked, read operations might need to merge updates from an arbitrary number of SSTables. Instead, we bound the number of such files by periodically executing a merging compaction in the background. A merging compaction reads the contents of a few SSTables and the memtable, and writes out a new SSTable. The input SSTables and memtable can be discarded as soon as the compaction has finished. A merging compaction that rewrites all SSTables into exactly one SSTable is called a major compaction. 每次minor compaction（小型压缩）都会创建一个新的SSTable。 如果此行为持续未经检查，则读操作可能需要合并任意数量的SSTables中的更新。 相反，我们通过在后台定期执行merging compaction（合并压缩）来限制此类文件的数量。 合并压缩读取一些SSTables和memtable的内容，并输出新的SSTable。 压缩完成后，可以立即丢弃输入的SSTables和memtable。 将所有SSTable重写为一个SSTable的合并压缩称为major compaction（大型压缩）。 SSTables produced by non-major compactions can contain special deletion entries that suppress deleted data in older SSTables that are still live. A major compaction, on the other hand, produces an SSTable that contains no deletion information or deleted data. Bigtable cycles through all of its tablets and regularly applies major compactions to them. These major compactions allow Bigtable to reclaim resources used by deleted data, and also allow it to ensure that deleted data disappears from the system in a timely fashion, which is important for services that store sensitive data. 由 non-major compaction（非大型压缩）产生的SSTable可以包含特殊的删除条目（这里删除条目视为存储着：起到删除功能的指令，然而执行指令在：major compaction阶段），这些条目用于删除掉仍然存在于旧SSTable中逻辑上视为已删除的数据（逻辑上视为已删除的数据：客户端无法读取这些数据，即对客户端不可见，然而磁盘上这些数据还在。逻辑上已经不存在，物理上还存在）。 另一方面，major compaction（大型压缩）会产生一个SSTable，该表不包含删除信息或已删除的数据。 Bigtable会遍历其所有Tablet，并定期对其应用major compaction（大型压缩）。 这些major compaction（大型压缩）使Bigtable可以回收已删除数据所使用的资源，还可以确保Bigtable及时地从系统中删除已删除的数据，这对于存储敏感数据的服务很重要。 6 RefinementsThe implementation described in the previous section required a number of refinements to achieve the high performance, availability, and reliability required by our users. This section describes portions of the implementation in more detail in order to highlight these refinements.上一节中描述的实现需要大量改进，以实现我们的用户所需的高性能，可用性和可靠性。 本节将更详细地描述实现的各个部分，以突出显示这些改进。 Locality groupsClients can group multiple column families together into a locality group. A separate SSTable is generated for each locality group in each tablet. Segregating column families that are not typically accessed together into separate locality groups enables more efficient reads. For example, page metadata in Webtable (such as language and checksums) can be in one locality group, and the contents of the page can be in a different group: an application that wants to read the metadata does not need to read through all of the page contents. 客户端可以将多个列族组合到一个 locality group 中。为每个Tablet中的每个位置组生成一个单独的SSTable。将通常不一起访问的列族隔离到单独的 locality group 中，可以提高读取效率。例如，Webtable中的页面元数据（例如语言以及校验和）可以在一个 locality group 中，而页面的内容可以在另一个组中：想要读取元数据的应用程序不需要通读所有页面内容。 In addition, some useful tuning parameters can be specified on a per-locality group basis. For example, a locality group can be declared to be in-memory. SSTables for in-memory locality groups are loaded lazily into the memory of the tablet server. Once loaded, column families that belong to such locality groups can be read without accessing the disk. This feature is useful for small pieces of data that are accessed frequently: we use it internally for the location column family in the METADATA table. 另外，可以在每个 locality group 的基础上指定一些有用的调整参数。例如，可以将一个 locality group 声明为内存中。内存中 locality group 的SSTable延迟加载到Tablet服务器的内存中。一旦加载后，无需访问磁盘即可读取属于此类 locality group 的列族。此功能对经常访问的小数据很有用：我们在内部将其用于METADATA表中的location列族。 译者附主要是根据数据访问的局部性原理与在操作系统中内存页的缓存算法是同理。 CompressionClients can control whether or not the SSTables for a locality group are compressed, and if so, which compression format is used. The user-specified compression format is applied to each SSTable block (whose size is controllable via a locality group specific tuning parameter). Although we lose some space by compressing each block separately, we benefit in that small portions of an SSTable can be read without decompressing the entire file. Many clients use a two-pass custom compression scheme. The first pass uses Bentley and McIlroy’s scheme [6], which compresses long common strings across a large window. The second pass uses a fast compression algorithm that looks for repetitions in a small 16 KB window of the data. Both compression passes are very fast—they encode at 100–200 MB/s, and decode at 400–1000 MB/s on modern machines. Even though we emphasized speed instead of space reduction when choosing our compression algorithms, this two-pass compression scheme does surprisingly well. 客户端可以控制是否压缩 locality group 的SSTable，以及如果压缩，则使用哪种压缩格式。用户指定的压缩格式将应用于每个SSTable块（其大小可通过 locality group 的特定的调整参数来控制）。尽管我们通过分别压缩每个块而损失了一些空间，但我们的好处是因为：可以读取SSTable的一小部分而无需解压缩整个文件。许多客户端使用两阶段自定义压缩方案。第一阶段使用Bentley和McIlroy的方案[6]，该方案在一个大窗口中压缩长的公共字符串。第二阶段使用快速压缩算法，该算法在一个小的16 KB数据窗口中查找重复项。两种压缩过程都非常快——在现代机器上，它们的编码速度为100-200 MB/s，解码速度为 400-1000 MB/s。尽管在选择压缩算法时我们强调速度而不是减少空间，但这种两阶段压缩方案的效果出奇地好。 For example, in Webtable, we use this compression scheme to store Web page contents. In one experiment,we stored a large number of documents in a compressed locality group. For the purposes of the experiment, we limited ourselves to one version of each document instead of storing all versions available to us. The scheme achieved a 10-to-1 reduction in space. This is much better than typical Gzip reductions of 3-to-1 or 4-to-1 on HTML pages because of the way Webtable rows are laid out: all pages from a single host are stored close to each other. This allows the Bentley-McIlroy algorithm to identify large amounts of shared boilerplate in pages from the same host. Many applications, not just Webtable, choose their row names so that similar data ends up clustered, and therefore achieve very good compression ratios. Compression ratios get even better when we store multiple versions of the same value in Bigtable. 例如，在Webtable中，我们使用这种压缩方案来存储Web页面内容。在一个实验中，我们将大量文档存储在一个压缩的 locality group 中。为了进行实验，我们将自己限制为每个文档的一个版本，而不是存储所有可用的版本。该方案使空间减少了10比1。由于Webtable行的布局方式，这比HTML页面上通常的Gzip压缩（3比1或4比1）要好得多：来自单个主机的所有页面都存储得彼此靠近。这使Bentley-McIlroy算法可以识别来自同一主机的页面中的大量共享样板。许多应用程序（不仅是Webtable）都选择其行名致使相似的数据最终聚集在一起，因此实现了很好的压缩率。当我们在Bigtable中存储相同值的多个版本时，压缩率甚至会更高。 Caching for read performanceTo improve read performance, tablet servers use two levels of caching. The Scan Cache is a higher-level cache that caches the key-value pairs returned by the SSTable interface to the tablet server code. The Block Cache is a lower-level cache that caches SSTables blocks that were read from GFS. The Scan Cache is most useful for applications that tend to read the same data repeatedly. The Block Cache is useful for applications that tend to read data that is close to the data they recently read (e.g., sequential reads, or random reads of different columns in the same locality group within a hot row). 为了提高读取性能，Tablet服务器使用两个级别的缓存。 Scan Cache是一个更高层次的缓存，它将SSTable接口返回的键值对缓存到Tablet服务器代码。 Block Cache是较低层次的缓存，它缓存从GFS读取的SSTables块。 Scan Cache对于倾向于重复读取相同数据的应用程序最有用。 对于倾向于读取与其最近读取的数据接近的数据的应用程序（例如，顺序读取或对热点行中同一个 locality group 中不同列的随机读取），Block Cache非常有用。 Bloom filtersAs described in Section 5.3, a read operation has to read from all SSTables that make up the state of a tablet. If these SSTables are not in memory, we may end up doing many disk accesses. We reduce the number of accesses by allowing clients to specify that Bloom filters [7] should be created for SSTables in a particular locality group. A Bloom filter allows us to ask whether an SSTable might contain any data for a specified row/column pair. For certain applications, a small amount of tablet server memory used for storing Bloom filters drastically reduces the number of disk seeks required for read operations. Our use of Bloom filters also implies that most lookups for non-existent rows or columns do not need to touch disk. 如第5.3节所述，读取操作必须从构成Tablet状态的所有SSTable中读取。如果这些SSTable不在内存中，我们可能最终会进行许多磁盘访问。通过允许客户端指定应为特定 locality group 中的SSTable创建Bloom过滤器[7]，我们减少了访问次数。 布隆过滤器允许我们询问SSTable是否可以包含指定行/列对的任何数据。对于某些应用程序，用于存储布隆过滤器的少量Tablet服务器的内存会大大减少读取操作所需的磁盘搜寻次数。 我们对Bloom过滤器的使用还意味着对于不存在的行或列的大多数查找都不需要接触磁盘。 Commit-log implementationIf we kept the commit log for each tablet in a separate log file, a very large number of files would be written concurrently in GFS. Depending on the underlying file system implementation on each GFS server, these writes could cause a large number of disk seeks to write to the different physical log files. In addition, having separate log files per tablet also reduces the effectiveness of the group commit optimization, since groups would tend to be smaller. To fix these issues, we append mutations to a single commit log per tablet server, co-mingling mutations for different tablets in the same physical log file [18][20]. 如果我们将每个Tablet的提交日志保存在单独的日志文件中，则会在GFS中同时写入大量文件。根据每个GFS服务器上基础文件系统的实现，这些写操作可能导致大量磁盘搜索以写到不同的物理日志文件。此外，每个Tablet使用单独的日志文件还会降低整组提交（ group commit ）优化的效率，因为组的规模往往较小。为了解决这些问题，我们将数据的变化记录（mutation）追加到每个Tablet服务器的单个提交日志中，将不同Tablet的变化记录（mutation）混合在同一物理日志文件中 [18][20]。 Using one log provides significant performance benefits during normal operation, but it complicates recovery. When a tablet server dies, the tablets that it served will be moved to a large number of other tablet servers: each server typically loads a small number of the original server’s tablets. To recover the state for a tablet, the new tablet server needs to reapply the mutations for that tablet from the commit log written by the original tablet server. However, the mutations for these tablets were co-mingled in the same physical log file. One approach would be for each new tablet server to read this full commit log file and apply just the entries needed for the tablets it needs to recover. However, under such a scheme, if 100 machines were each assigned a single tablet from a failed tablet server, then the log file would be read 100 times (once by each server). 在正常操作期间，使用一个日志可以显著提高性能，但是会使恢复变得复杂。当Tablet服务器死亡时，其所服务的Tablet将被移至大量其他Tablet服务器：每个服务器通常会加载少量原始服务器的Tablet。要恢复Tablet的状态，新的Tablet服务器需要从原始Tablet服务器写入的提交日志中重新应用该Tablet的变化日志。但是，这些Tablet的变化日志被混合在同一物理日志文件中。一种方法是让每个新的Tablet服务器读取此完整的提交日志文件，并仅应用其需要恢复的Tablet所需的条目。但是，在这种方案下，如果从故障的Tablet服务器中分别为100台计算机分配了一个Tablet，那么日志文件将被读取100次（每个服务器一次）。 We avoid duplicating log reads by first sorting the commit log entries in order of the keys &lt;htable; row name; log sequence number&gt;. In the sorted output, all mutations for a particular tablet are contiguous and can therefore be read efficiently with one disk seek followed by a sequential read. To parallelize the sorting, we partition the log file into 64 MB segments, and sort each segment in parallel on different tablet servers. This sorting process is coordinated by the master and is initiated when a tablet server indicates that it needs to recover mutations from some commit log file. 我们通过以 (table; row name; log sequence number) 为键对提交日志条目进行排序来避免重复的日志读取。在排序的输出中，特定Tablet的所有mutation（数据的变化）都是连续的，因此可以通过一个磁盘搜索有效读取，然后顺序读取。为了并行化排序，我们将日志文件划分为64 MB的分段，然后在不同的Tablet服务器上并行地对每个分段进行排序。此排序过程由主服务器（master）协调，并在Tablet服务器指示需要从某些提交日志文件中恢复mutation（数据的更改）时启动。 Writing commit logs to GFS sometimes causes performance hiccups for a variety of reasons (e.g., a GFS server machine involved in the write crashes, or the network paths traversed to reach the particular set of three GFS servers is suffering network congestion, or is heavily loaded). To protect mutations from GFS latency spikes, each tablet server actually has two log writing threads, each writing to its own log file; only one of these two threads is actively in use at a time. If writes to the active log file are performing poorly, the log file writing is switched to the other thread, and mutations that are in the commit log queue are written by the newly active log writing thread. Log entries contain sequence numbers to allow the recovery process to elide duplicated entries resulting from this log switching process. 将提交日志写入GFS有时会由于各种原因而导致性能下降（例如，写入时发生崩溃的GFS服务器计算机，或用来穿越以便到达特定的三个GFS服务器集的网络路径正遭受网络拥塞或负载过重） 。为了保护变化免受GFS延迟高峰的影响，每个Tablet服务器实际上都有两个日志写入线程（一个是被激活也就是正在使用的线程，一个是备用线程），每个线程都写入自己的日志文件。一次仅积极使用这两个线程之一。如果对激活的（active 有些人翻译：活跃的）日志文件的写入性能不佳，则日志文件的写入将切换到另一个线程，并且提交日志队列中的数据变化记录将由新激活的日志写线程进行写入。日志条目包含序列号，以允许恢复过程清除此日志切换过程产生的重复条目。 Speeding up tablet recoveryIf the master moves a tablet from one tablet server to another, the source tablet server first does a minor compaction on that tablet. This compaction reduces recovery time by reducing the amount of uncompacted state in the tablet server’s commit log. After finishing this compaction, the tablet server stops serving the tablet. Before it actually unloads the tablet, the tablet server does another (usually very fast) minor compaction to eliminate any remaining uncompacted state in the tablet server’s log that arrived while the first minor compaction was being performed. After this second minor compaction is complete, the tablet can be loaded on another tablet server without requiring any recovery of log entries. 如果主服务器（master）将 Tablet 从一台 Tablet 服务器移动到另一台 Tablet 服务器，则源 Tablet 服务器首先对该 Tablet 进行 minor compaction（小型压缩）。 这种压缩通过减少 Tablet 服务器的提交日志中未压缩状态的数量来减少恢复时间。 完成这次压缩后，Tablet 服务器将停止为 Tablet 提供服务。 在实际卸载 Tablet 之前，Tablet 服务器会进行另一次（通常非常快） minor compaction（小型压缩）来消除执行第一次 minor compaction（小型压缩）时到达 Tablet 服务器的日志当中任何剩余的未压缩状态。 在完成第二次 minor compaction（小型压缩）后，可将 Tablet 加载到另一台 Tablet 服务器上，而无需恢复日志条目。 Exploiting immutabilityBesides the SSTable caches, various other parts of the Bigtable system have been simplified by the fact that all of the SSTables that we generate are immutable. For example, we do not need any synchronization of accesses to the file system when reading from SSTables. As a result, concurrency control over rows can be implemented very efficiently. The only mutable data structure that is accessed by both reads and writes is the memtable. To reduce contention during reads of the memtable, we make each memtable row copy-on-write and allow reads and writes to proceed in parallel. 除了SSTable缓存外，我们生成的所有SSTable都是不可变的，从而简化了Bigtable系统的其他各个部分。例如，当从SSTables读取数据时，我们不需要对文件系统的访问进行任何同步。结果，可以非常有效地实现对行的并发控制。读取和写入均访问的唯一可变数据结构是memtable。为了减少在读取memtable期间的竞争，我们使每个memtable的行使用写时复制的策略，并允许读取和写入并行进行。 Since SSTables are immutable, the problem of permanently removing deleted data is transformed to garbage collecting obsolete SSTables. Each tablet’s SSTables are registered in the METADATA table. The master removes obsolete SSTables as a mark-and-sweep garbage collection [25] over the set of SSTables, where the METADATA table contains the set of roots. Finally, the immutability of SSTables enables us to split tablets quickly. Instead of generating a new set of SSTables for each child tablet, we let the child tablets share the SSTables of the parent tablet. 由于SSTable是不可变的，因此永久删除已删除数据（前面讲过的发出删除指令，但未被执行的数据）的问题被转换为垃圾收集过期的SSTable。每个Tablet的SSTables都注册在 METADATA table 中。主服务器（master）删除过时的SSTables作为SSTables集合上的标记再清除式的垃圾收集[25]，其中 METADATA table 包含根集合（按照前文：METADATA table 记录了这些 SSTable 的对应的 tablet 的 root）。最后，SSTables的不变性使我们能够快速拆分Tablet。我们不必为每个子 Tablet 生成一组新的SSTable，而是让子 Tablet 共享 Tablet 的SSTable。 7 Performance EvaluationWe set up a Bigtable cluster with N tablet servers to measure the performance and scalability of Bigtable as N is varied. The tablet servers were configured to use 1 GB of memory and to write to a GFS cell consisting of 1786 machines with two 400 GB IDE hard drives each. N client machines generated the Bigtable load used for these tests. (We used the same number of clients as tablet servers to ensure that clients were never a bottleneck.) Each machine had two dual-core Opteron 2 GHz chips, enough physical memory to hold the working set of all running processes, and a single gigabit Ethernet link. The machines were arranged in a two-level tree-shaped switched network with approximately 100-200 Gbps of aggregate bandwidth available at the root. All of the machines were in the same hosting facility and therefore the round-trip time between any pair of machines was less than a millisecond. 我们建立了一个由N台Tablet服务器组成的Bigtable集群，以随着 N 的变化来衡量Bigtable的性能和可扩展性。Tablet服务器配置为使用1 GB的内存，并写入由1786台计算机组成的GFS单元，每台计算机具有两个400 GB的IDE硬盘驱动器。 N个客户端计算机生成了用于这些测试的Bigtable负载。（我们使用与Tablet服务器相同数量的客户端，以确保客户端永远不会成为瓶颈）每台机器都具有两个双核Opteron 2 GHz芯片，足够的物理内存来容纳所有正在运行的进程的工作集以及一个 1Gbp/s 以太网链路。这些机器被安排在两级树形交换网络（two-level tree-shaped switched network）中，网络根节点大约有100-200 Gbps的总带宽。所有机器都位于同一托管设施中，因此任何两对机器之间的往返时间均不到一毫秒。 The tablet servers and master, test clients, and GFS servers all ran on the same set of machines. Every machine ran a GFS server. Some of the machines also ran either a tablet server, or a client process, or processes from other jobs that were using the pool at the same time as these experiments. Tablet服务器以及主服务器，测试客户端和GFS服务器都在同一组计算机上运行。每台机器都运行GFS服务器。其中一些机器还运行了Tablet服务器或客户端进程，或者运行了与这些实验同时使用这些机器池（根据本文第四节第一段推测 “the pool” 翻译为：机器池）的其他作业的进程。 R is the distinct number of Bigtable row keys involved in the test. R was chosen so that each benchmark read or wrote approximately 1 GB of data per tablet server. R 是测试中涉及的Bigtable不重复行键的数量。选择R是为了使每个基准测试中每个Tablet服务器读取或写入大约1 GB的数据。 The sequential write benchmark used row keys with names 0 to R - 1. This space of row keys was partitioned into 10N equal-sized ranges. These ranges were assigned to the N clients by a central scheduler that as signed the next available range to a client as soon as the client finished processing the previous range assigned to it. This dynamic assignment helped mitigate the effects of performance variations caused by other processes running on the client machines. We wrote a single string under each row key. Each string was generated randomly and was therefore uncompressible. In addition, strings under different row key were distinct, so no cross-row compression was possible. The random write benchmark was similar except that the row key was hashed modulo R immediately before writing so that the write load was spread roughly uniformly across the entire row space for the entire duration of the benchmark. 顺序写基准测试使用名称为 0 到 R - 1 的行键。此行键空间被划分为 10N 个相等大小的区间。这些区间由中央调度程序分配给N个客户端，该中央调度程序在客户端完成对分配给它的先前区间的处理后立即将下一个可用区间分配给客户端。这种动态分配有助于减轻由客户端计算机上运行的其他进程引起的性能变化的影响。我们在每个行键下写了一个字符串。每个字符串都是随机生成的，因此不可压缩的。另外，不同行键下的字符串是不同的，因此不可能进行跨行压缩。随机写基准测试类似于顺序写基准测试，不同的是在写入之前立即对行密钥进行了模R哈希运算，以便在基准测试的整个期间，写入负载大致均匀地分布在整个行空间中。 The sequential read benchmark generated row keys in exactly the same way as the sequential write benchmark, but instead of writing under the row key, it read the string stored under the row key (which was written by an earlier invocation of the sequential write benchmark). Similarly, the random read benchmark shadowed the operation of the random write benchmark. 顺序读基准测试产生的行密钥与顺序写入基准完全相同，但它不是在行密钥下写入，而是读取存储在行密钥下的字符串（该字符串是由顺序写基准测试的较早时候调用写入的） 。同样，随机读基准测试与随机写基准测试的操作一样。 The scan benchmark is similar to the sequential read benchmark, but uses support provided by the Bigtable API for scanning over all values in a row range. Using a scan reduces the number of RPCs executed by the benchmark since a single RPC fetches a large sequence of values from a tablet server. 扫描基准测试（scan benchmark）类似于顺序读基准测试，但是使用Bigtable API提供的支持来扫描行区间内的所有值。使用扫描减少了基准测试执行的RPC数量，因为单个RPC从Tablet服务器中提取了大量的值。 The random reads (mem) benchmark is similar to the random read benchmark, but the locality group that contains the benchmark data is marked as in-memory, and therefore the reads are satisfied from the tablet server’s memory instead of requiring a GFS read. For just this benchmark, we reduced the amount of data per tablet server from 1 GB to 100 MB so that it would fit comfortably in the memory available to the tablet server. 随机读（mem）基准测试类似于随机读基准测试，但是包含基准数据的 locality group 被标记为内存中，因此可以从Tablet服务器的内存中读取数据，而无需进行GFS读取。对于该基准测试，我们将每个Tablet服务器的数据量从1 GB减少到100 MB，以便可以合适地容纳在Tablet服务器可用的内存中。 Figure 6 shows two views on the performance of our benchmarks when reading and writing 1000-byte values to Bigtable. The table shows the number of operations per second per tablet server; the graph shows the aggregate number of operations per second. 图6显示了在向 Bigtable 读取和写入 1000MB/S 时基准测试性能的两个视图。该表显示了每台Tablet服务器每秒的操作数；该图显示了每秒的总操作数。 Single tablet-server performanceLet us first consider performance with just one tablet server. Random reads are slower than all other operations by an order of magnitude or more. Each random read involves the transfer of a 64 KB SSTable block over the network from GFS to a tablet server, out of which only a single 1000-byte value is used. The tablet server executes approximately 1200 reads per second, which translates into approximately 75 MB/s of data read from GFS. This bandwidth is enough to saturate the tablet server CPUs because of overheads in our networking stack, SSTable parsing, and Bigtable code, and is also almost enough to saturate the network links used in our system. Most Bigtable applications with this type of an access pattern reduce the block size to a smaller value, typically 8KB. 首先让我们考虑一台Tablet服务器的性能。随机读取的速度比所有其他操作慢一个数量级或更多。每次随机读取都涉及通过网络将64 KB SSTable块从GFS传输到Tablet服务器，其中仅使用一个1000字节的值。Tablet服务器每秒执行大约1200次读取，这意味着从GFS读取的数据大约为75 MB/s (1200 * 64 KB / 1024 = 75MB/s)。由于网络堆栈，SSTable解析和Bigtable代码的开销，该带宽足以使 Tablet 服务器的CPU饱和，也几乎足以使系统中使用的网络链路饱和。具有这种访问模式的大多数Bigtable应用程序将块大小减小为一个较小的值，通常为8KB。 Random reads from memory are much faster since each 1000-byte read is satisfied from the tablet server’s local memory without fetching a large 64 KB block from GFS. 从内存中进行随机读取的速度要快得多，因为每次从Tablet服务器的本地内存读取 1000B 即可满足需要，而无需从GFS提取大的 64 KB块。 Random and sequential writes perform better than random reads since each tablet server appends all incoming writes to a single commit log and uses group commit to stream these writes efficiently to GFS. There is no significant difference between the performance of random writes and sequential writes; in both cases, all writes to the tablet server are recorded in the same commit log. 随机和顺序写入的性能要优于随机读取，因为每个Tablet服务器会将所有传入的写入都追加到单个提交日志中，并使用整组提交（group commit）将这些写入高效地流式传输到GFS。随机写入和顺序写入的性能之间没有显着差异。在这两种情况下，对Tablet服务器的所有写入都记录在同一提交日志中。 Sequential reads perform better than random reads since every 64 KB SSTable block that is fetched from GFS is stored into our block cache, where it is used to serve the next 64 read requests. 顺序读取的性能要优于随机读取，因为从GFS提取的每个64 KB SSTable块都存储在我们的块缓存中，用于满足接下来的64个读取请求。 Scans are even faster since the tablet server can return a large number of values in response to a single client RPC, and therefore RPC overhead is amortized over a large number of values. 由于Tablet服务器可以在响应单个客户端RPC时返回大量值，因此 scan 速度甚至更快，因此RPC开销将在大量值上摊销。 ScalingAggregate throughput increases dramatically, by over a factor of a hundred, as we increase the number of tablet servers in the system from 1 to 500. For example, the performance of random reads from memory increases by almost a factor of 300 as the number of tablet server increases by a factor of 500. This behavior occurs because the bottleneck on performance for this benchmark is the individual tablet server CPU. 随着我们将系统中Tablet服务器的数量从1个增加到500个，总的吞吐量急剧增加了一百倍。例如，随着内存数量的增加，从内存中随机读取的性能几乎提高了300倍。Tablet服务器增加了500倍。之所以发生这种现象，是因为该基准测试的性能瓶颈是各个Tablet服务器CPU。 However, performance does not increase linearly. For most benchmarks, there is a significant drop in per-server throughput when going from 1 to 50 tablet servers. This drop is caused by imbalance in load in multiple server configurations, often due to other processes contending for CPU and network. Our load balancing algorithm attempts to deal with this imbalance, but cannot do a perfect job for two main reasons: rebalancing is throttled to reduce the number of tablet movements (a tablet is unavailable for a short time, typically less than one second, when it is moved), and the load generated by our benchmarks shifts around as the benchmark progresses. 但是，性能不会线性增加。对于大多数基准测试，当从1台Tablet服务器增加到50台Tablet服务器时，每台服务器的吞吐量将大幅下降。这种下降是由于多个服务器配置中的负载不平衡而引起的，通常是由于其他争用CPU和网络的进程所致。我们的负载平衡算法试图解决这种不平衡问题，但由于两个主要原因而无法做到完美：限制重新平衡以减少Tablet的移动次数（Tablet在短时间内无法使用，通常少于一秒钟，移动），并且随着基准测试的进行，由基准测试产生的负载也会发生变化。 The random read benchmark shows the worst scaling (an increase in aggregate throughput by only a factor of 100 for a 500-fold increase in number of servers). This behavior occurs because (as explained above) we transfer one large 64KB block over the network for every 1000- byte read. This transfer saturates various shared 1 Gigabit links in our network and as a result, the per-server throughput drops significantly as we increase the number of machines. 随机读基准测试显示最差的扩展性（服务器数量增加500倍时，总吞吐量仅增加100倍）。发生这种现象的原因是（如上所述），每读取1000字节，我们就会通过网络传输一个 64 KB的大块。这种转移使我们网络中的各种共享 1 Gigabit 链路饱和，结果，随着计算机数量的增加，每服务器的吞吐量显着下降。 8 Real Applications As of August 2006, there are 388 non-test Bigtable clusters running in various Google machine clusters, with a combined total of about 24,500 tablet servers. Table 1 shows a rough distribution of tablet servers per cluster. Many of these clusters are used for development purposes and therefore are idle for significant periods. One group of 14 busy clusters with 8069 total tablet servers saw an aggregate volume of more than 1.2 million requests per second, with incoming RPC traffic of about 741 MB/s and outgoing RPC traffic of about 16 GB/s. 截至2006年8月，在各种Google机器集群中运行着388个非测试版Bigtable集群，总共约有24,500台Tablet服务器。表1显示了每个集群的Tablet服务器的大致分布。这些集群中的许多集群都用于开发目的，因此在相当长的一段时间内都处于空闲状态。一组14个繁忙的集群（总共8069个Tablet服务器）每秒总计收到超过120万个请求，其中传入RPC流量约为 741 MB/s，传出RPC流量约为 16 GB/s。 Table 2 provides some data about a few of the tables currently in use. Some tables store data that is served to users, whereas others store data for batch processing; the tables range widely in total size, average cell size, percentage of data served from memory, and complexity of the table schema. In the rest of this section, we briefly describe how three product teams use Bigtable. 表2提供了一些有关当前使用的表的数据。有些表存储提供给用户的数据，而另一些表则存储用于批处理的数据。这些表在总大小，平均单元大小，从内存提供的数据百分比以及表模式的复杂性方面分布的范围很广。在本节的其余部分，我们简要描述三个产品团队如何使用Bigtable。 8.1 Google AnalyticsGoogle Analytics (analytics.google.com) is a service that helps webmasters analyze traffic patterns at their web sites. It provides aggregate statistics, such as the number of unique visitors per day and the page views per URL per day, as well as site-tracking reports, such as the percentage of users that made a purchase, given that they earlier viewed a specific page. Google Analytics（分析）（analytics.google.com）是一项服务，可帮助网站管理员分析其网站上的流量模式。它提供了汇总统计信息，例如每天，身份不重复的访客数量和每个URL每天的页面浏览量，以及网站跟踪报告，例如在先前查看特定页面的情况下进行购买的用户所占的百分比。 To enable the service, webmasters embed a small JavaScript program in their web pages. This program is invoked whenever a page is visited. It records various information about the request in Google Analytics, such as a user identifier and information about the page being fetched. Google Analytics summarizes this data and makes it available to webmasters. 为了启用该服务，网站管理员将一个小的JavaScript程序嵌入其网页中。每当访问页面时都会调用此程序。它在Google Analytics（分析）中记录有关请求的各种信息，例如用户标识符和有关正在获取的页面的信息。 Google Analytics（分析）会汇总这些数据并将其提供给网站管理员。 We briefly describe two of the tables used by Google Analytics. The raw click table (˜200 TB) maintains a row for each end-user session. The row name is a tuple containing the website’s name and the time at which the session was created. This schema ensures that sessions that visit the same web site are contiguous, and that they are sorted chronologically. This table compresses to 14% of its original size. 我们简要介绍了Google Analytics（分析）使用的两个表格。原始点击表（约200 TB）为每个终端用户会话维护一行。行名称是一个元组，其中包含网站的名称和创建会话的时间。此模式可确保访问同一网站的会话是连续的，并且可以按时间顺序对其进行排序。该表压缩到其原始大小的14％。 The summary table (˜20 TB) contains various predefined summaries for each website. This table is generated from the raw click table by periodically scheduled MapReduce jobs. Each MapReduce job extracts recent session data from the raw click table. The overall system’s throughput is limited by the throughput of GFS. This table compresses to 29% of its original size. 摘要表（约20 TB）包含每个网站的各种预定义摘要。该表是通过定期计划的MapReduce作业从原始点击表生成的。每个MapReduce作业都会从原始点击表中提取最近的会话数据。整个系统的吞吐量受GFS吞吐量的限制。该表压缩到其原始大小的29％。 8.2 Google EarthGoogle operates a collection of services that provide users with access to high-resolution satellite imagery of the world’s surface, both through the web-based Google Maps interface (maps.google.com) and through the Google Earth (earth.google.com) custom client software. These products allow users to navigate across the world’s surface: they can pan, view, and annotate satellite imagery at many different levels of resolution. This system uses one table to preprocess data, and a different set of tables for serving client data. Google提供了一系列服务，可通过基于Web的Google Maps界面（maps.google.com）和Google Earth（earth.google.com）自定义客户端软件向用户提供世界地面的高分辨率卫星图像。这些产品使用户可以在整个地球表面导航：他们可以以许多不同的分辨率摇动拍摄，查看和注释卫星图像。该系统使用一个表预处理数据，并使用一组不同的表来提供客户端数据。 The preprocessing pipeline uses one table to store raw imagery. During preprocessing, the imagery is cleaned and consolidated into final serving data. This table contains approximately 70 terabytes of data and therefore is served from disk. The images are efficiently compressed already, so Bigtable compression is disabled. 预处理管道使用一张表存储原始图像。在预处理期间，图像将被清理并合并为最终投放数据。该表包含大约70 TB的数据，因此是从磁盘提供的。图像已被高效压缩，因此已禁用Bigtable压缩。 Each row in the imagery table corresponds to a single geographic segment. Rows are named to ensure that adjacent geographic segments are stored near each other. The table contains a column family to keep track of the sources of data for each segment. This column family has a large number of columns: essentially one for each raw data image. Since each segment is only built from a few images, this column family is very sparse. The preprocessing pipeline relies heavily on MapReduce over Bigtable to transform data. The overall system processes over 1 MB/sec of data per tablet server during some of these MapReduce jobs. 图像表格中的每一行都对应一个地理区域。 对行进行命名以确保相邻的地理段彼此相邻存储。 该表包含一个列族，以跟踪每个段的数据源。 该列族有大量列：基本上每个原始数据图像（raw data image）都有一列。 由于每个段仅由几个图像构成，因此该列族非常稀疏。 预处理管道严重依赖BigTable上的MapReduce来转换数据。 在其中的某些MapReduce作业中，整个系统每台Tablet服务器处理超过 1 MB/s 的数据。 The serving system uses one table to index data stored in GFS. This table is relatively small (˜500 GB), but it must serve tens of thousands of queries per second per datacenter with low latency. As a result, this table is hosted across hundreds of tablet servers and contains in-memory column families. 服务系统使用一张表索引存储在GFS中的数据。该表相对较小（约500 GB），但每个数据中心每秒必须处理数万个查询，且延迟低。结果，该表托管在数百台Tablet服务器中，并包含内存列族。 8.3 Personalized SearchPersonalized Search (www.google.com/psearch) is an opt-in service that records user queries and clicks across a variety of Google properties such as web search, images, and news. Users can browse their search histories to revisit their old queries and clicks, and they can ask for personalized search results based on their historical Google usage patterns. 个性化搜索（www.google.com/psearch）是一项选择性服务，可记录用户对各种Google属性（例如网络搜索，图片和新闻）的查询和点击。用户可以浏览其搜索历史记录以重新访问其以前的查询和点击，还可以根据其Google的历史使用模式（pattern：模型，模式）来请求个性化搜索结果。 Personalized Search stores each user’s data in Bigtable. Each user has a unique userid and is assigned a row named by that userid. All user actions are stored in a table. A separate column family is reserved for each type of action (for example, there is a column family that stores all web queries). Each data element uses as its Bigtable timestamp the time at which the corresponding user action occurred. Personalized Search generates user profiles using a MapReduce over Bigtable. These user profiles are used to personalize live search results. 个性化搜索将每个用户的数据存储在Bigtable中。每个用户都有一个唯一的用户ID，并分配有一个由该用户ID命名的行。所有用户操作（action）都存储在一个表中。每种操作（action）类型都保留一个单独的列族（例如，有一个列系列存储所有Web查询）。每个数据元素都将发生相应用户操作的时间用作其Bigtable时间戳。个性化搜索使用BigTable上的MapReduce生成用户个人资料（user profiles）。这些用户个人资料用于个性化实时搜索结果。 The Personalized Search data is replicated across several Bigtable clusters to increase availability and to reduce latency due to distance from clients. The Personalized Search team originally built a client-side replication mechanism on top of Bigtable that ensured eventual consistency of all replicas. The current system now uses a replication subsystem that is built into the servers. 个性化搜索数据可在多个Bigtable集群之间复制，以提高可用性并减少由于与客户端之间的距离而引起的延迟。个性化搜索团队最初在Bigtable之上构建了一个客户端复制机制，以确保所有副本的最终一致性。现在，当前系统使用服务器内置的复制子系统。 The design of the Personalized Search storage system allows other groups to add new per-user information in their own columns, and the system is now used by many other Google properties that need to store per-user configuration options and settings. Sharing a table amongst many groups resulted in an unusually large number of column families. To help support sharing, we added a simple quota mechanism to Bigtable to limit the storage consumption by any particular client in shared tables; this mechanism provides some isolation between the various product groups using this system for per-user information storage. 个性化搜索存储系统的设计允许其他组在其自己的列中添加新的每个用户信息，并且该系统现在已由许多其他需要存储每个用户配置选项和设置的Google属性所使用。在许多组之间共享一张表导致了异常多的列族。为了帮助支持共享，我们在Bigtable中添加了一个简单的配额机制，以限制共享表中任何特定客户端的存储消耗。这种机制使用此系统为每个用户的信息存储提供了各种产品组之间的隔离。 9 LessonsIn the process of designing, implementing, maintaining, and supporting Bigtable, we gained useful experience and learned several interesting lessons. 在设计，实施，维护和支持Bigtable的过程中，我们获得了有益的经验并吸取了一些有趣的经验教训。 One lesson we learned is that large distributed systems are vulnerable to many types of failures, not just the standard network partitions and fail-stop failures assumed in many distributed protocols. For example, we have seen problems due to all of the following causes: memory and network corruption, large clock skew, hung machines, extended and asymmetric network partitions, bugs in other systems that we are using (Chubby for example), overflow of GFS quotas, and planned and unplanned hardware maintenance. As we have gained more experience with these problems, we have addressed them by changing various protocols. For example, we added check summing to our RPC mechanism. We also handled some problems by removing assumptions made by one part of the system about another part. For example, we stopped assuming a given Chubby operation could return only one of a fixed set of errors. 我们 吸取的教训是，大型分布式系统容易遭受多种类型的故障，而不仅仅是许多分布式协议中假定的标准网络分区和出错后停止服务（fail-stop failures）。例如，由于以下所有原因，我们发现了问题： 内存和网络损坏 很大的时钟偏差（clock skew） 停止响应的机器 扩展 非对称的网络分区 我们正在使用的其他系统中的错误（例如，Chubby） GFS溢出配额 计划和计划外的硬件维护 随着我们在这些问题上获得更多经验，我们已通过更改各种协议来解决这些问题。例如，我们在RPC机制中添加了校验和。我们还通过消除系统某个部分对另一部分所做的假设来处理一些问题。例如，我们停止假设给定的Chubby操作只能返回一组固定错误中的一个。 总结非预期的故障源远比你想象中多 Another lesson we learned is that it is important to delay adding new features until it is clear how the new features will be used. For example, we initially planned to support general-purpose transactions in our API. Because we did not have an immediate use for them, however, we did not implement them. Now that we have many real applications running on Bigtable, we have been able to examine their actual needs, and have discovered that most applications require only single-row transactions. Where people have requested distributed transactions, the most important use is for maintaining secondary indices, and we plan to add a specialized mechanism to satisfy this need. The new mechanism will be less general than distributed transactions, but will be more efficient (especially for updates that span hundreds of rows or more) and will also interact better with our scheme for optimistic cross-data-center replication. 我们吸取的 另一个教训是，重要的是延迟添加新特性直到明确如何使用新特性。例如，我们最初计划在我们的API中支持通用事务（general-purpose transaction）。因为我们没有立即使用它们，所以我们没有实现它们。现在，我们在Bigtable上运行了许多真实的应用程序，我们已经能够检查它们的实际需求，并且发现大多数应用程序仅需要单行事务。当人们要求进行分布式交易时，最重要的用途是维护二级索引，我们计划添加一种专门的机制来满足这一需求。新机制将不如分布式事务通用，但效率更高（特别是对于跨越数百行或更多行的更新），并且还将与我们的乐观跨数据中心复制方案更好地交互。 总结避免过早添加使用场景不明确的新特性 A practical lesson that we learned from supporting Bigtable is the importance of proper system-level monitoring (i.e., monitoring both Bigtable itself, as well as the client processes using Bigtable). For example, we extended our RPC system so that for a sample of the RPCs, it keeps a detailed trace of the important actions done on behalf of that RPC. This feature has allowed us to detect and fix many problems such as lock contention on tablet data structures, slow writes to GFS while committing Bigtable mutations, and stuck accesses to the METADATA table when METADATA tablets are unavailable. Another example of useful monitoring is that every Bigtable cluster is registered in Chubby. This allows us to track down all clusters, discover how big they are, see which versions of our software they are running, how much traffic they are receiving, and whether or not there are any problems such as unexpectedly large latencies. 我们从支持Bigtable中学到的实践经验是正确进行系统级监视的重要性（即监视Bigtable本身以及使用Bigtable的客户端进程）。例如，我们扩展了RPC系统，以便对于RPC的抽样，它可以详细记录代表该RPC进行的重要操作。此功能使我们能够检测并修复许多问题，例如Tablet数据结构上的锁争用，在提交Bigtable更改时缓慢写入GFS以及在 METADATA Tablet 不可用时卡住对 METADATA table 的访问。有用监视的另一个示例是，每个Bigtable集群都在Chubby中注册。这使我们能够跟踪所有集群，发现它们有多大，查看它们正在运行的软件版本，正在接收多少流量，以及是否存在诸如意外的长延迟之类的问题。 总结合理的系统级监控非常重要。 The most important lesson we learned is the value of simple designs. Given both the size of our system (about 100,000 lines of non-test code), as well as the fact that code evolves over time in unexpected ways, we have found that code and design clarity are of immense help in code maintenance and debugging. One example of this is our tablet-server membership protocol. Our first protocol was simple: the master periodically issued leases to tablet servers, and tablet servers killed themselves if their lease expired. Unfortunately, this protocol reduced availability significantly in the presence of network problems, and was also sensitive to master recovery time. We redesigned the protocol several times until we had a protocol that performed well. However, the resulting protocol was too complex and depended on the behavior of Chubby features that were seldom exercised by other applications. We discovered that we were spending an inordinate amount of time debugging obscure corner cases, not only in Bigtable code, but also in Chubby code. Eventually, we scrapped this protocol and moved to a newer simpler protocol that depends solely on widely-used Chubby features. 我们 学到的最重要的一课是简单设计的价值。考虑到系统的大小（大约100,000行非测试代码），以及代码会以意想不到的方式随时间变化的事实，我们 发现代码和设计的清晰性对代码维护和调试有极大的帮助。我们的Tablet服务器成员身份协议就是一个例子。我们的第一个协议很简单：主服务器定期向Tablet服务器发布租约，而Tablet服务器在租约到期时会自杀。不幸的是，该协议在存在网络问题的情况下大大降低了可用性，并且对主服务器的恢复时间也很敏感。我们多次对协议进行了重新设计，直到有了一个性能良好的协议。但是，最终的协议太复杂了，取决于其他应用程序很少使用的Chubby功能的行为。我们发现，不仅在Bigtable代码中，而且在Chubby代码中，我们花费大量时间调试晦涩难解的案例。最终，我们放弃了该协议，转而使用仅依赖于广泛使用的Chubby特性的更新的更简单协议。 总结保持设计的简洁 10 Related WorkThe Boxwood project [24] has components that overlap in some ways with Chubby, GFS, and Bigtable, since it provides for distributed agreement, locking, distributed chunk storage, and distributed B-tree storage. In each case where there is overlap, it appears that the Boxwood’s component is targeted at a somewhat lower level than the corresponding Google service. The Boxwood project’s goal is to provide infrastructure for building higher-level services such as file systems or databases, while the goal of Bigtable is to directly support client applications that wish to store data. Boxwood项目[24]具有与Chubby，GFS和Bigtable在某些方面重叠的组件，因为它提供了分布式协议，锁，分布式块存储和分布式B树存储。在每种情况下，如果出现重叠，则Boxwood的组件似乎定位在比相应Google服务更低的级别上。 Boxwood项目的目标是为构建高级服务（例如文件系统或数据库）提供基础结构，而Bigtable的目标是直接支持希望存储数据的客户端应用程序。 Many recent projects have tackled the problem of providing distributed storage or higher-level services over wide area networks, often at “Internet scale.” This includes work on distributed hash tables that began with projects such as CAN [29], Chord [32], Tapestry [37], and Pastry [30]. These systems address concerns that do not arise for Bigtable, such as highly variable bandwidth, untrusted participants, or frequent reconfiguration; decentralized control and Byzantine fault tolerance are not Bigtable goals. 许多最近的项目解决了通常在“ Internet规模”上通过广域网提供分布式存储或更高级别服务的问题。这包括以CAN [29]，Chord [32]，Tapestry [37] 和 Pastry [30] 等项目开头的分布式哈希表的工作。这些系统解决了Bigtable不会出现的问题，例如带宽可变，参与者不受信任或频繁重新配置。分散控制和拜占庭容错并不是Bigtable的目标。 In terms of the distributed data storage model that one might provide to application developers, we believe the key-value pair model provided by distributed B-trees or distributed hash tables is too limiting. Key-value pairs are a useful building block, but they should not be the only building block one provides to developers. The model we chose is richer than simple key-value pairs, and supports sparse semi-structured data. Nonetheless, it is still simple enough that it lends itself to a very efficient flat-file representation, and it is transparent enough (via locality groups) to allow our users to tune important behaviors of the system. 就可能会提供给应用程序开发人员的分布式数据存储模型而言，我们认为由分布式B树或分布式哈希表提供的键值对模型过于局限。键值对是一个有用的构建块，但它们不应成为一个唯一提供给开发人员的构建块。我们选择的模型比简单的键/值对丰富，并且支持稀疏的半结构化数据。尽管如此，它仍然非常简单，可以使其非常有效地使用 flate file 表示，并且它（通过locality group）足够透明以允许我们的用户调整系统的重要行为。 译者附flat file: n. a file consisting of records of a single record type in which there is no embedded structure information that governs relationships between records. 由单一记录类型的记录组成的文件，其中没有控制记录之间关系的嵌入式结构信息。 —— 《微软计算机词典》 Several database vendors have developed parallel databases that can store large volumes of data. Oracle’s Real Application Cluster database [27] uses shared disks to store data (Bigtable uses GFS) and a distributed lock manager (Bigtable uses Chubby). IBM’s DB2 Parallel Edition [4] is based on a shared-nothing [33] architecture similar to Bigtable. Each DB2 server is responsible for a subset of the rows in a table which it stores in a local relational database. Both products provide a complete relational model with transactions. 几个数据库供应商已经开发了可以存储大量数据的并行数据库。 Oracle 的 Real Application Cluster 数据库[27]使用共享磁盘存储数据（Bigtable使用GFS）和分布式锁管理器（Bigtable使用Chubby）。 IBM的DB2并行版[4]基于类似于Bigtable的无共享[33]架构。每个DB2服务器负责存储在本地关系数据库中的表中行的子集。两种产品都提供了完整的交易关系模型。 Bigtable locality groups realize similar compression and disk read performance benefits observed for other systems that organize data on disk using column-based rather than row-based storage, including C-Store [1][34] and commercial products such as Sybase IQ [15][36], SenSage [31], KDB+ [22], and the ColumnBM storage layer in MonetDB/X100 [38]. Another system that does vertical and horizontal data partioning into flat files and achieves good data compression ratios is AT&amp;T’s Daytona database [19]. Locality groups do not support CPUcache-level optimizations, such as those described by Ailamaki [2]. 对于使用基于列而不是基于行的存储在磁盘上组织数据的其他系统，Bigtable locality group 实现了类似的压缩和磁盘读取性能优势，包括C-Store [1][34] 和Sybase IQ [15][36]，SenSage [31]，KDB + [22] 和MonetDB / X100 [38] 中的ColumnBM存储层。 AT＆T的Daytona数据库[19]是将垂直和水平数据分成 flat file 并实现良好数据压缩率的另一个系统。 locality group 不支持 CPU 缓存级别的优化，例如 Ailamaki [2] 所描述的那些。 The manner in which Bigtable uses memtables and SSTables to store updates to tablets is analogous to the way that the Log-Structured Merge Tree [26] stores updates to index data. In both systems, sorted data is buffered in memory before being written to disk, and reads must merge data from memory and disk. Bigtable 使用 memtable 和 SSTables 将更新存储到Tablet的方式类似于 Log-Structured Merge Tree [26] 存储更新到索引数据的方式。在这两个系统中，已排序的数据在写入磁盘之前都要先在内存中进行缓冲，并且读取操作必须合并内存和磁盘中的数据。 C-Store and Bigtable share many characteristics: both systems use a shared-nothing architecture and have two different data structures, one for recent writes, and one for storing long-lived data, with a mechanism for moving data from one form to the other. The systems differ significantly in their API: C-Store behaves like a relational database, whereas Bigtable provides a lower level read and write interface and is designed to support many thousands of such operations per second per server. C-Store is also a “read-optimized relational DBMS”, whereas Bigtable provides good performance on both read-intensive and write-intensive applications. C-Store和Bigtable具有许多特征：这两个系统都使用无共享架构，并且具有两种不同的数据结构，一种用于最近的写入，一种用于存储长期存在的数据，其机制是将数据从一种形式转移到另一种形式。这些系统的API显着不同：C-Store的行为类似于关系数据库，而Bigtable提供了较低级别的读写接口，并且旨在每服务器每秒支持数千个此类操作。 C-Store也是“读取优化的关系DBMS”，而Bigtable在读取密集型和写入密集型应用程序上均提供了良好的性能。 Bigtable’s load balancer has to solve some of the same kinds of load and memory balancing problems faced by shared-nothing databases (e.g., [11][35]). Our problem is somewhat simpler: (1) we do not consider the possibility of multiple copies of the same data, possibly in alternate forms due to views or indices; (2) we let the user tell us what data belongs in memory and what data should stay on disk, rather than trying to determine this dynamically; (3) we have no complex queries to execute or optimize. Bigtable的负载平衡器必须解决无共享数据库（例如 [11][35]）面临的某些相同类型的负载和内存平衡问题。我们的问题稍微简单一些： （1）我们不考虑同一数据的多个副本的可能性，这些副本可能由于视图或索引而以其他形式出现； （2）让用户告诉我们哪些数据属于内存，哪些数据应保留在磁盘上，而不是试图动态地确定它； （3）我们没有执行或优化的复杂查询； 11 ConclusionsWe have described Bigtable, a distributed system for storing structured data at Google. Bigtable clusters have been in production use since April 2005, and we spent roughly seven person-years on design and implementation before that date. As of August 2006, more than sixty projects are using Bigtable. Our users like the performance and high availability provided by the Bigtable implementation, and that they can scale the capacity of their clusters by simply adding more machines to the system as their resource demands change over time. 我们已经介绍了Bigtable，这是一个用于在Google存储结构化数据的分布式系统。自2005年4月以来，Bigtable集群已投入生产使用，在此日期之前，我们在设计和实施上花费了大约7人年的时间。截至2006年8月，超过60个项目正在使用Bigtable。我们的用户喜欢Bigtable实施提供的性能和高可用性，他们可以通过随资源需求随时间的变化向系统中添加更多计算机，从而扩展集群的容量。 Given the unusual interface to Bigtable, an interesting question is how difficult it has been for our users to adapt to using it. New users are sometimes uncertain of how to best use the Bigtable interface, particularly if they are accustomed to using relational databases that support general-purpose transactions. Nevertheless, the fact that many Google products successfully use Bigtable demonstrates that our design works well in practice. 鉴于Bigtable具有非同寻常的界面，一个有趣的问题是，我们的用户适应使用它有多困难。新用户有时不确定如何最好地使用Bigtable接口，特别是如果他们习惯于使用支持通用事务的关系数据库时。不过，许多Google产品成功使用Bigtable的事实表明我们的设计在实践中效果很好。 We are in the process of implementing several additional Bigtable features, such as support for secondary indices and infrastructure for building cross-data-center replicated Bigtables with multiple master replicas. We have also begun deploying Bigtable as a service to product groups, so that individual groups do not need to maintain their own clusters. As our service clusters scale, we will need to deal with more resource-sharing issues within Bigtable itself [3], [5]. 我们正在实现几个其他Bigtable功能，例如对二级索引的支持以及用于构建具有多个主副本的跨数据中心复制Bigtable的基础结构。我们也已开始将Bigtable作为服务部署到产品组，以便各个组不需要维护自己的集群。随着我们服务集群的扩展，我们将需要在Bigtable自身内部处理更多的资源共享问题 [3], [5]。 Finally, we have found that there are significant advantages to building our own storage solution at Google. We have gotten a substantial amount of flexibility from designing our own data model for Bigtable. In addition, our control over Bigtable’s implementation, and the other Google infrastructure upon which Bigtable depends, means that we can remove bottlenecks and inefficiencies as they arise. 最后，我们发现在Google建立自己的存储解决方案具有明显的优势。通过为Bigtable设计我们自己的数据模型，我们获得了很大的灵活性。此外，我们对Bigtable的实施以及Bigtable依赖的其他Google基础架构的控制权意味着我们可以消除瓶颈和效率低下的情况。 AcknowledgementsWe thank the anonymous reviewers, David Nagle, and our shepherd Brad Calder, for their feedback on this paper. The Bigtable system has benefited greatly from the feedback of our many users within Google. In addition,we thank the following people for their contributions to Bigtable: Dan Aguayo, Sameer Ajmani, Zhifeng Chen, Bill Coughran, Mike Epstein, Healfdene Goguen, Robert Griesemer, Jeremy Hylton, Josh Hyman, Alex Khesin, Joanna Kulik, Alberto Lerner, Sherry Listgarten, Mike Maloney, Eduardo Pinheiro, Kathy Polizzi, Frank Yellin, and Arthur Zwiegincew. 我们感谢匿名审稿人David Nagle和我们的牧羊人Brad Calder对本文的反馈。 Bigtable系统得益于Google众多用户的反馈。 此外，我们感谢以下人员对Bigtable的贡献：Dan Aguayo，Sameer Ajmani，Zhifeng Chen，Bill Coughran，Mike Epstein，Healfdene Goguen，Robert Griesemer，Jeremy Hylton，Josh Hyman，Alex Khesin，Joanna Kulik，Alberto Lerner， Sherry Listgarten，Mike Maloney，Eduardo Pinheiro，Kathy Polizzi，Frank Yellin和Arthur Zwiegincew。 References[1] ABADI, D. J., MADDEN, S. R., AND FERREIRA, M. C. Integrating compression and execution in column oriented database systems. Proc. of SIGMOD (2006). [2] AILAMAKI, A., DEWITT, D. J., HILL, M. D., AND SKOUNAKIS, M. Weaving relations for cache performance. In The VLDB Journal (2001), pp. 169-180. [3] BANGA, G., DRUSCHEL, P., AND MOGUL, J. C. Resource containers: A new facility for resource management in server systems. In Proc. of the 3rd OSDI (Feb. 1999), pp. 45-58. [4] BARU, C. K., FECTEAU, G., GOYAL, A., HSIAO, H., JHINGRAN, A., PADMANABHAN, S., COPELAND,G. P., AND WILSON, W. G. DB2 parallel edition. IBM Systems Journal 34, 2 (1995), 292-322. [5] BAVIER, A., BOWMAN, M., CHUN, B., CULLER, D., KARLIN, S., PETERSON, L., ROSCOE, T., SPALINK, T., AND WAWRZONIAK, M. Operating system support for planetary-scale network services. In Proc. of the 1st NSDI(Mar. 2004), pp. 253-266. [6] BENTLEY, J. L., AND MCILROY, M. D. Data compression using long common strings. In Data Compression Conference (1999), pp. 287-295. [7] BLOOM, B. H. Space/time trade-offs in hash coding with allowable errors. CACM 13, 7 (1970), 422-426. [8] BURROWS, M. The Chubby lock service for loosely coupled distributed systems. In Proc. of the 7th OSDI (Nov. 2006). [9] CHANDRA, T., GRIESEMER, R., AND REDSTONE, J. Paxos made live ? An engineering perspective. In Proc. of PODC (2007).[10] COMER, D. Ubiquitous B-tree. Computing Surveys 11, 2 (June 1979), 121-137. [11] COPELAND, G. P., ALEXANDER, W., BOUGHTER, E. E., AND KELLER, T. W. Data placement in Bubba. In Proc. of SIGMOD (1988), pp. 99-108. [12] DEAN, J., AND GHEMAWAT, S. MapReduce: Simplified data processing on large clusters. In Proc. of the 6th OSDI (Dec. 2004), pp. 137-150. [13] DEWITT, D., KATZ, R., OLKEN, F., SHAPIRO, L., STONEBRAKER, M., AND WOOD, D. Implementation techniques for main memory database systems. In Proc. of SIGMOD (June 1984), pp. 1-8. [14] DEWITT, D. J., AND GRAY, J. Parallel database systems: The future of high performance database systems. CACM 35, 6 (June 1992), 85-98. [15] FRENCH, C. D. One size ts all database architectures do not work for DSS. In Proc. of SIGMOD (May 1995), pp. 449-450. [16] GAWLICK, D., AND KINKADE, D. Varieties of concurrency control in IMS/VS fast path. Database Engineering Bulletin 8, 2 (1985), 3-10. [17] GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google file system. In Proc. of the 19th ACM SOSP (Dec.2003), pp. 29-43. [18] GRAY, J. Notes on database operating systems. In Operating Systems ? An Advanced Course, vol. 60 of Lecture Notes in Computer Science. Springer-Verlag, 1978. [19] GREER, R. Daytona and the fourth-generation language Cymbal. In Proc. of SIGMOD (1999), pp. 525-526. [20] HAGMANN, R. Reimplementing the Cedar file system using logging and group commit. In Proc. of the 11th SOSP (Dec. 1987), pp. 155-162. [21] HARTMAN, J. H., AND OUSTERHOUT, J. K. The Zebra striped network file system. In Proc. of the 14th SOSP(Asheville, NC, 1993), pp. 29-43. [22] KX.COM. kx.com/products/database.php. Product page. [23] LAMPORT, L. The part-time parliament. ACM TOCS 16,2 (1998), 133-169. [24] MACCORMICK, J., MURPHY, N., NAJORK, M., THEKKATH, C. A., AND ZHOU, L. Boxwood: Abstractions as the foundation for storage infrastructure. In Proc. of the 6th OSDI (Dec. 2004), pp. 105-120. [25] MCCARTHY, J. Recursive functions of symbolic expressions and their computation by machine. CACM 3, 4 (Apr. 1960), 184-195. [26] O’NEIL, P., CHENG, E., GAWLICK, D., AND O’NEIL, E. The log-structured merge-tree (LSM-tree). Acta Inf. 33, 4 (1996), 351-385. [27] ORACLE.COM. www.oracle.com/technology/products/database/clustering/index.html. Product page. [28] PIKE, R., DORWARD, S., GRIESEMER, R., AND QUINLAN, S. Interpreting the data: Parallel analysis with Sawzall. Scientific Programming Journal 13, 4 (2005), 227-298. [29] RATNASAMY, S., FRANCIS, P., HANDLEY, M., KARP, R., AND SHENKER, S. A scalable content-addressable network. In Proc. of SIGCOMM (Aug. 2001), pp. 161-172. [30] ROWSTRON, A., AND DRUSCHEL, P. Pastry: Scalable, distributed object location and routing for largescale peer-to-peer systems. In Proc. of Middleware 2001(Nov. 2001), pp. 329-350. [31] SENSAGE.COM. sensage.com/products-sensage.htm. Product page. [32] STOICA, I., MORRIS, R., KARGER, D., KAASHOEK, M. F., AND BALAKRISHNAN, H. Chord: A scalable peer-to-peer lookup service for Internet applications. In Proc. of SIGCOMM (Aug. 2001), pp. 149-160. [33] STONEBRAKER, M. The case for shared nothing. Database Engineering Bulletin 9, 1 (Mar. 1986), 4-9. [34] STONEBRAKER,M., ABADI, D. J., BATKIN, A., CHEN, X., CHERNIACK, M., FERREIRA, M., LAU, E., LIN, A., MADDEN, S., O’NEIL, E., O’NEIL, P., RASIN, A., TRAN, N., AND ZDONIK, S. C-Store: A columnoriented DBMS. In Proc. of VLDB (Aug. 2005), pp. 553-564. [35] STONEBRAKER, M., AOKI, P. M., DEVINE, R., LITWIN, W., AND OLSON, M. A. Mariposa: A new architecture for distributed data. In Proc. of the Tenth ICDE(1994), IEEE Computer Society, pp. 54-65. [36] SYBASE.COM. www.sybase.com/products/databaseservers/sybaseiq. Product page. [37] ZHAO, B. Y., KUBIATOWICZ, J., AND JOSEPH, A. D. Tapestry: An infrastructure for fault-tolerant wide-area location and routing. Tech. Rep. UCB/CSD-01-1141, CS Division, UC Berkeley, Apr. 2001. [38] ZUKOWSKI, M., BONCZ, P. A., NES, N., AND HEMAN, S. MonetDB/X100 ?A DBMS in the CPU cache. IEEE Data Eng. Bull. 28, 2 (2005), 17-22. 翻译参考： Google Bigtable (中文版) BIGTABLE中文版论文 [译] [论文] Bigtable: A Distributed Storage System for Structured Data (OSDI 2006) 深入浅出BigTable BigTable论文阅读 论文总结BigTable 推演过程 BigTable架构 SSTable参考： Log Structured Merge Trees(LSM) 原理 Log Structured Merge-Trees(LSM) 论文翻译 SSTable 原理 Leveled Compaction · facebook/rocksdb Wiki 1.原文 ：reason about the locality properties of the data represented in the underlying storage. ↩2.原文：uninterpreted strings. ↩3.原文：whether to serve data out of memory or from disk. ↩4.原文: and memory accounting. ↩5.原文: in decreasing timestamp order. ↩6.原文: Figure 2 shows C++ code that uses a RowMutation abstraction to perform a series of updates. ↩7.原文: C++ code that uses a Scanner abstraction to iterate over all anchors in a particular row. ↩8.原文: and then reading the appropriate block from disk. ↩9.原文: The service is live. ↩10.原文: A Bigtable cluster typically operates in a shared pool of machines. ↩11.原文: we first find the appropriate block by performing a binary search in the in-memory index. ↩12.原文: without touching disk. ↩13.原文: which is elected to be the master and actively serve requests. ↩14.原文: it recursively moves up the tablet location hierarchy. ↩15.原文: live tablet sever有些人翻译为：存活的Tablet服务器，我觉得不够贴切，因为机器只要还在运行，那么我们认为它是活着的（就像生物的生命一样），但是有木有效是看机器运行过程的自身资源符不符合相关服务的规定，这个是动态的，在特定时刻的一台机器对有些服务而言是有效的，对有其他服务言可能是无效的。当然有些人觉得也可以这样解释：在某个特定时刻，针对有些服务而言，某台机器是存活的，然而对其他服务而言是已经死了（down， death），但是这样解释更像是强行与 live 的意思靠拢。 ↩16.位置属性可以这样理解，比如树状结构，具有相同前缀的数据的存放位置接近。在读取的时候，可以把这些数据一次读取出来，联想到数据的局部性原理。 ↩17.获取该网页的时间戳作为标识：即按照获取时间不同，存储了多个版本的网页数据。 ↩18.原文：cell，这里指的是table中一个cell. ↩]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>distributed system</tag>
        <tag>papers</tag>
        <tag>google</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CAP定理图示与Raft各种场景演示]]></title>
    <url>%2F2019%2F12%2F11%2Fa-brief-introduction-of-Paxos-Raft-ZAB%2F</url>
    <content type="text"><![CDATA[本文主要着重于CAP定理和raft各种场景演示 CAP 定理根据加州大学伯克利分校计算机科学家Eric Brewer说法，该定理于1998年秋季首次出现。该定理于1999年作为CAP原理发表，并由Brewer在2000 年的分布式原理研讨会上提出计算（PODC）。2002年，麻省理工学院的塞斯·吉尔伯特（Seth Gilbert ) 和 南希·林奇（Nancy Lynch）发表了布鲁尔猜想的正式证明，使之成为一个定理。 CAP定理指出分布式计算机系统不可能同时提供以下三个保证（来自 wiki : CAP Theorum）： Consistency: Every read receives the most recent write or an error 一致性：每次读取都会收到最新的写入或错误 Availability: Every request receives a (non-error) response, without the guarantee that it contains the most recent write 可用性：每个请求都会收到一个（非错误）响应，但不能保证它包含最新的写入 Partition tolerance: The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes 分区容忍：尽管节点之间的网络丢弃（或延迟）了任意数量的消息，但系统仍继续运行 CAP定理证明详细查看：Gilbert和Lynch的论文，以下翻译自图解CAP定理： 分布式系统让我们考虑一个非常简单的分布式系统。我们的系统由两个服务器 $G_1$ 和 $G_2$ 组成。这两个服务器都跟踪相同的变量 $v_0$，其初始值为 $v_0$。$G_1$ 和 $G_2$ 可以相互通信，也可以与外部客户端通信。这是我们的系统的布局。 客户端可以请求从任何服务器进行写入和读取。服务器收到请求后，将执行所需的任何计算，然后响应客户端。例如，这是写的样子。 这就是读取的情况。 现在我们已经建立了系统，接下来让我们回顾一下对于系统一致性，可用性和分区容错性意味着什么。 一致性 在写操作完成之后开始的任何读操作必须返回该值，或者以后的写操作的结果 —— Gilbert，Lynch 在一致的系统中，客户端将值写入任何服务器并获得响应后，它希望从其读取的任何服务器取回该值（或更新的值）。 这是一个不一致的系统的示例。 我们的客户端写 $v_1$ 至 $G_1$ 和 $G_1$ 确认，但是当它从 $G_2$ 读取时 $G_2$，它将获取旧的数据：$v_0$。 另一方面，这是一个一致的系统的示例。 在这个系统中，在向客户端发送确认之前 $G_1$ 将其值复制到 $G_2$ 。因此，当客户端从 $G_2$读取 ，它获取 $v$ 的最新值： $v_1$ 。 可用性 系统中非故障节点收到的每个请求都必须得到响应 —— Gilbert，Lynch 在可用的系统中，如果我们的客户端向服务器发送请求并且服务器没有崩溃，则服务器最终必须响应客户端。不允许服务器忽略客户端的请求。 分区容错性 网络将被允许任意丢失从一个节点发送到另一节点的许多消息 —— Gilbert，Lynch 这意味着 $G_1$ 和 $G_2$ 互相发送的任何消息能被删除。如果所有消息都被丢弃，那么我们的系统将如下所示（注：初始值为 $v_0$ ）。 为了达到分区容错性，我们的系统必须能够在任意网络分区下正常运行。 证明现在我们已经了解了一致性，可用性和分区容错性的概念，我们可以证明一个系统不能同时拥有这三个。 对于这个矛盾，假设确实存在一个一致，可用且分区容错的系统。我们要做的第一件事是对系统进行分区。看起来像这样。 接下来，我们有客户端要求 $v_1$ 写入 $G_1$。由于我们的系统可用，因此 $G_1$ 必须响应。由于网络是分区的，因此 $G_1$ 无法将其数据复制到 $G_2$。Gilbert 和 Lynch 将此执行阶段称为 $\alpha_1$。 译者附原文：”Since the network is partitioned, however, $G_1$ cannot replicate its data to $G_2$.” 网络被分区，比如：中美海底电缆火山喷发断掉了，淘宝电缆被施工方不小心挖掉了，这时候，对于前者美国的所有服务器和中国的所有服务器都是对外可用的，国家内的服务器节点都是互通的，但是中美之间的服务器是不通的，虽然开始阶段所有各个服务器节点都是互通的，那么这时候就发生了网络分区。对于后者，国内的情况也同理，各个大区之间也有可能发生网络分区，举例：华东，华北，华南，西北，西南等等。 接下来，我们让客户端向 $G_2$ 发出读取请求。同样，由于我们的系统可用，因此 $G_2$ 必须响应。由于网络是分区的，因此 $G_2$ 无法从 $G_1$ 更新其值。返回 $v_0$。Gilbert 和 Lynch 将此执行阶段称为 $\alpha_2$。 客户端已经将 $v_1$ 写至 $G_1$ 之后，$G_2$ 返回 $v_0$ 给客户端。这是不一致的。 我们假设存在一个一致的，可用的，分区容错的系统，但是我们只是展示了存在任何此类系统执行的情况，其中该系统的行为不一致。因此，不存在这样的系统。 译者附由于发生了网络分区，此时只能在可用性和一致性之间做一个取舍，比如上文提到的海底电缆断掉，那么为了一致性，停掉中国或者美国服务器节点提供的对外服务，这时候可能所有原本请求国内的服务器节点都得转向去请求美国的服务器节点，但是这时候就降低了可用性，比如：请求的延迟。那么另外一种情况，保持中美服务器节点继续对外服务，那么可用性没有变化，但是就破坏了一致性，因为网络分区以后中美内部服务器节点行为不一致，这样给系统留下的影响（数据，及其间接衍生物：比如算法迭代更新等）是不一样的。 什么是一致性 弱一致性 最终一致性（无法实时获取最新更新的数据，但是一段时间过后，数据是一致的） DNS(Domain Name System) Gossip(Cassandra的通信协议) 优先保证AP（Availability, Partition tolerance）的 CouchDB，Cassandra，DynamoDB，Riak 强一致性 同步 Paxos Raft（multi-paxos） ZAB（multi-paxos） 定义问题 数据不能存在单个节点上，防止单点故障。 分布式系统对fault tolerance的一般解决方案是state machine replication。 本文主题是 state machine replication的共识（consensus）算法。paxos其实是一个共识算法。 系统的最终一致性，不仅需要达成共识，还会取决于客户端（client）的行为，后文将详细说明。 一致性模拟强一致性算法主从同步复制 只有主节点（master）接受客户端（client）请求。 由主节点复制日志到多个从节点（slave）。 主节点（master）等待直到所有从节点（slave）返回成功，才能向客户端返回写成功。 缺点： 任意一个节点失败（master或者某一个slave阻塞）都将导致整个集群不可用，虽然保证了强一致性（Strong Consistency），但是却大大降低了可用性（Weak Availability）。 多数派每次写都保证写入大于 N/2 个节点，每次读都保证大于 N/2 个节点中读，总共 N 个节点。 主要是为了解决主从同步复制中“所有”节点都得处于正常运转。缺点：在并发环境下，无法保证系统正确性，顺序非常重要。例如以下场景： PaxosPaxos算法是 Lesile Lamport（Latex发明者）提出的一种基于消息传递的分布式一致性算法，于1998年在《The Part-Time Parliament》论文中首次公开，使其获得2013年图灵奖。 最初的描述使用希腊的一个小岛Paxos作为比喻，描述了Paxos小岛中通过决议的流程，并以此命名这个算法，但是这个描述理解起来比较有挑战性。 为描述Paxos算法，Lamport虚拟了一个叫做Paxos的希腊小岛，这个小岛按照议会民主制的政治模式制定法律，但是没有人愿意将自己的全部时间精力放在这种事上。所以无论是议员，议长或者传递纸条的服务员都不能承诺别人需要时一定会出现，也无法承诺批准决议或者传递消息的时间。 Basic Paxos角色介绍（roles） client：系统外部角色，请求发起者。像民众。 proposer：接受client请求，向集群提出提案（proposal）。并在冲突发生时，起到冲突调节的作用。像议员，替民众提出提案。 acceptor (voter)：提案的投票和接收者，只有在达到法定人数（Quorum，一般即为 majority 多数派）时，提案者提出的提案才会最终被接受。像国会。 learner：提议接收者，backup，备份，对集群一致性没什么影响。像记录员。 2阶段（phases）的步骤在这个部分先大致过一遍流程，细节后文说明，形象化描述先把流程走通。 Phase 1a: Prepare proposer 提出一个提案，编号为N，此 N 大于这个proposer之前提出的提案编号。向所有 acceptor 请求接受。 Phase 1b: Promise 如果 N 大于acceptor之前接受的任何提案编号则接受，否则认为此提案是已经提出过的旧提案，直接拒绝。如果 promise 阶段达到了 quorum（法定人数），proposer 这一步成功否则失败。 Phase 2a: Accept promise 阶段成功以后，proposer 进一步发出accept请求，此请求包含提案编号 （N），以及提案内容（V）。 Phase 2b: Accepted 如果此acceptor在此期间没有收到任何编号大于 N 的提案否则忽略，且接受的acceptor达到法定人数，则接受此提案内容。因此在这个阶段，编号N的提案也可能失效。 所以由以上可知：跟现实生活中不太一样的地方在于acceptor（voter）不在乎提案内容，在乎提案编号。 图示流程Basic Paxos when an Acceptor fails 在下图中，有1个client，1个proposer，3个acceptor（即法定人数为3）和2个 learner（由2条垂直线表示）。该图表示第一轮成功的情况（即网络中没有进程失败）。 123456789Client Proposer Acceptor Learner | | | | | | | X--------&gt;| | | | | | Request | X---------&gt;|-&gt;|-&gt;| | | Prepare(1) | |&lt;---------X--X--X | | Promise(1,&#123;Va,Vb,Vc&#125;) | X---------&gt;|-&gt;|-&gt;| | | Accept!(1,V) | |&lt;---------X--X--X------&gt;|-&gt;| Accepted(1,V) |&lt;---------------------------------X--X Response | | | | | | | 这里 V 是最后的 {Va, Vb, Vc}. Basic Paxos when an Acceptor fails 12345678910Client Proposer Acceptor Learner | | | | | | | X--------&gt;| | | | | | Request | X---------&gt;|-&gt;|-&gt;| | | Prepare(1) | | | | ! | | !! FAIL !! | |&lt;---------X--X | | Promise(1,&#123;Va, Vb, null&#125;) | X---------&gt;|-&gt;| | | Accept!(1,V) | |&lt;---------X--X---------&gt;|-&gt;| Accepted(1,V) |&lt;---------------------------------X--X Response | | | | | | Basic Paxos when a Proposer fails 在这种情况下，proposer 在提出值之后但在达成协议之前失败。具体来说，它在Accept消息的中间失败，因此只有一个Acceptor接收到该值。此时由新的proposer（即图中的 NEW LEADER，选举出来的，怎么选举看后面详细分析）。请注意，在这种情况下有2轮（轮从上到下垂直进行）。 12345678910111213141516Client Proposer Acceptor Learner | | | | | | | X-----&gt;| | | | | | Request | X------------&gt;|-&gt;|-&gt;| | | Prepare(1) | |&lt;------------X--X--X | | Promise(1,&#123;Va, Vb, Vc&#125;) | | | | | | | | | | | | | | !! Leader fails during broadcast !! | X------------&gt;| | | | | Accept!(1,V) | ! | | | | | | | | | | | | !! NEW LEADER !! | X---------&gt;|-&gt;|-&gt;| | | Prepare(2) | |&lt;---------X--X--X | | Promise(2,&#123;V, null, null&#125;) | X---------&gt;|-&gt;|-&gt;| | | Accept!(2,V) | |&lt;---------X--X--X------&gt;|-&gt;| Accepted(2,V) |&lt;---------------------------------X--X Response | | | | | | | 新的提案人重新提出此前失败的提案，但是此时提案编号已经增大。 Basic Paxos when a redundant learner fails 在以下情况下，（冗余的）学习者之一失败，但是Basic Paxos协议仍然成功。 12345678910Client Proposer Acceptor Learner | | | | | | | X--------&gt;| | | | | | Request | X---------&gt;|-&gt;|-&gt;| | | Prepare(1) | |&lt;---------X--X--X | | Promise(1,&#123;Va,Vb,Vc&#125;) | X---------&gt;|-&gt;|-&gt;| | | Accept!(1,V) | |&lt;---------X--X--X------&gt;|-&gt;| Accepted(1,V) | | | | | | ! !! FAIL !! |&lt;---------------------------------X Response | | | | | | Basic Paxos when multiple Proposers conflict 潜在问题：多个proposer竞争地提出各自提案，比如一个proposer，假设叫 Mike，提出提案的时候，正在处理第二个阶段却被另一个叫 Tom 的 proposer，打断（即这个提案失效了），因为 Tom 提出更大提案编号的提案，然后被打断的 Mike 重新提出提案，这时刚好也打断了 Tom 提出的提案，而这时候 Tom 的提案刚好也进行到了第二阶段，然后循环反复。这个现象称为：活锁（liveness）或 dueling（竞争） 1234567891011121314151617181920212223242526Client Proposer Acceptor Learner | | | | | | | X-----&gt;| | | | | | Request | X------------&gt;|-&gt;|-&gt;| | | Prepare(1) | |&lt;------------X--X--X | | Promise(1,&#123;null,null,null&#125;) | ! | | | | | !! LEADER FAILS | | | | | | | !! NEW LEADER (knows last number was 1) | X---------&gt;|-&gt;|-&gt;| | | Prepare(2) | |&lt;---------X--X--X | | Promise(2,&#123;null,null,null&#125;) | | | | | | | | !! OLD LEADER recovers | | | | | | | | !! OLD LEADER tries 2, denied | X------------&gt;|-&gt;|-&gt;| | | Prepare(2) | |&lt;------------X--X--X | | Nack(2) | | | | | | | | !! OLD LEADER tries 3 | X------------&gt;|-&gt;|-&gt;| | | Prepare(3) | |&lt;------------X--X--X | | Promise(3,&#123;null,null,null&#125;) | | | | | | | | !! NEW LEADER proposes, denied | | X---------&gt;|-&gt;|-&gt;| | | Accept!(2,Va) | | |&lt;---------X--X--X | | Nack(3) | | | | | | | | !! NEW LEADER tries 4 | | X---------&gt;|-&gt;|-&gt;| | | Prepare(4) | | |&lt;---------X--X--X | | Promise(4,&#123;null,null,null&#125;) | | | | | | | | !! OLD LEADER proposes, denied | X------------&gt;|-&gt;|-&gt;| | | Accept!(3,Vb) | |&lt;------------X--X--X | | Nack(4) | | | | | | | | ... and so on ... Multi PaxosBasic Paxos 除了活锁（liveness），还有2轮RPC效率低下且难以实现的问题。 Leader这是新的概念，是“唯一”的proposer，所有请求都要经过此 leader。 Multi-Paxos without failures 123456789Client Proposer Acceptor Learner | | | | | | | --- First Request --- X--------&gt;| | | | | | Request | X---------&gt;|-&gt;|-&gt;| | | Prepare(N) | |&lt;---------X--X--X | | Promise(N,I,&#123;Va,Vb,Vc&#125;) | X---------&gt;|-&gt;|-&gt;| | | Accept!(N,I,V) where V = last of (Va, Vb, Vc) | |&lt;---------X--X--X------&gt;|-&gt;| Accepted(N,I,V) |&lt;---------------------------------X--X Response | | | | | | | N 表示竞选出来的第 N 任 leader， I 表示第 I 个提案 在这种情况下，新的提案过来，由于使用相同的且唯一的 leader，因此Basic Paxos中包含“Prepare”和“Promise”子阶段的阶段一都可以被跳过。 1234567Client Proposer Acceptor Learner | | | | | | | --- Following Requests --- X--------&gt;| | | | | | Request | X---------&gt;|-&gt;|-&gt;| | | Accept!(N,I+1,W) | |&lt;---------X--X--X------&gt;|-&gt;| Accepted(N,I+1,W) |&lt;---------------------------------X--X Response | | | | | | | 其实Basic Paxos中 prepare 和 promise 阶段可以认为是多个proposer在申请相应编号的提案权，所以会出现活锁（liveness），而在 Multi-Paxos 中由于只有唯一的被竞选出来的leader有提案权，所以就可以省去了阶段一。 角色精简的 Multi-Paxos Multi-Paxos的常见部署包括将 proposer，acceptor 和 learner 的角色精简为为“server”。因此，最后只有“client”和“server”。 123456789Client Servers | | | | --- First Request --- X--------&gt;| | | Request | X-&gt;|-&gt;| Prepare(N) | |&lt;-X--X Promise(N, I, &#123;Va, Vb&#125;) | X-&gt;|-&gt;| Accept!(N, I, Vn) | X&lt;&gt;X&lt;&gt;X Accepted(N, I) |&lt;--------X | | Response | | | | 角色精简且 leader 稳定的 Multi-Paxos 因此后面来的提案，就可以简化流程了。 123456Client Servers X--------&gt;| | | Request | X-&gt;|-&gt;| Accept!(N,I+1,W) | X&lt;&gt;X&lt;&gt;X Accepted(N,I+1) |&lt;--------X | | Response | | | | 这时候，可以明显看出，只要 leader 稳定，没有经常竞选 leader，那么服务器之间的请求（RPC：远程过程调用）减少了，相应效率也提高了。 Fast PaxosRaft3个子问题 Leader election Log Replication Safety 重新定义角色任意一个节点可以在不同时期扮演一下三个角色中的一个，因此这里的角色理解可以为状态（state）： Leader Follower Candidate 原理的动画解释初始时，集群中所有节点，都是 follower 状态。 如果 follower 没有收到 leader 的来信，那么他们可以成为 candidate，怎么成为candidate后文会说明。 说明： term：表示任期，表示节点处在第几任的leader管辖下） Vote Count : 投票计数。 然后，candidate 从其他节点请求投票，其他节点会用投票进行回复。 如果 candidate 从多数（majority）节点中获得选票，它将成为 leader。这个过程称为领导人选举（Leader election）。下面是这个过程的细节： 在Raft中，有两个超时设置可控制选举。首先是选举超时（election timeout）。选举超时是指 follower 成为 candidate 之前所等待的时间。选举超时被随机分配在150毫秒至300毫秒之间。选举超时后，follower 将成为 candidate，开始新的选举任期（term），对其进行投票（ballot），然后将“请求投票”消息发送给其他节点。如果接收节点在此期限内尚未投票，则它将为 candidate 投票，节点将重置其选举超时。一旦 candidate 获得多数票（majority），便成为 leader。leader 开始向其 follower 发送“添加条目”消息。这些消息将按心跳超时（heartbeat timeout）指定的时间间隔发送，然后 follower 响应每个追加条目消息。此选举任期将持续到 follower 停止接收心跳并成为 candidate 为止。 让我们停止 leader 并观察再次选举，节点B现在是第2届的 leader。需要多数表决才能保证每个任期只能选举一名leader。如果两个节点同时成为 candidate，则“可能”会发生投票表决的分裂。 让我们看一个投票表决的分裂的例子。两个节点都开始以相同的任期进行选举，并且每个节点都已经选举超时，并且先获得一个follower。现在，每个 candidate 都有2票，并且在这个任期中将无法获得更多选票。节点将等待新的选举，然后重试。节点A在第5届中获得了多数选票，因此成为 leader。 系统的所有更改现在都通过领导者。每次更改都将添加为节点日志中的条目。该日志条目当前未提交（uncommitted），因此不会更新节点的值。 要提交条目，节点首先将其复制到 follower 节点上， 然后 leader 等待，直到大多数（majority）节点都写入了条目，在这期间leader不断发送给follower心跳包，一方面确定集群中各节点是否存活，另一方面也可以知道follower是否写入了条目；现在，该条目已提交到 leader 节点上，并且节点状态为“ 5”；leader 节点然后通知 follower 节点该条目已提交（commited）；现在，集群已就系统状态达成共识（consensus），然后最后再响应给客户端表示成功。 此过程称为日志复制（Log Replication）。 面对网络分区，日志甚至可以保持一致。让我们添加一个分区以将A＆B与C，D＆E分开。由于有了这个分区，我们现在拥有两个术语不同的 leader。让我们添加另一个客户端，并尝试更新两个 leader。一个客户端将尝试将节点B的值设置为“ 3”。节点B无法复制为多数，因此其日志条目保持未提交状态。另一个客户端将尝试将节点E的值设置为“ 8”。这将成功，因为它可以复制到大多数。现在，让我们修复网络分区。节点B将看到更高的任期（term）并退出。节点A和B都将回滚其未提交的条目并匹配新 leader 的日志。现在，我们的日志在整个集群中是一致的。 这里验证上文 CAP定理中的保证了强一致性和分区容忍，但是分区时，底下两个节点形成的集群(即一个分区)不是多数派(quorum)不具备可用性，但是整个集群依然可用。但是因为 “SET 3” 被删除了，没有写到集群中，所以并没有完全正确虽然保证了一致性。此处跟前文我注明的”译者附””一致：网络分区下，只能选择C A中的一者，这里实验选择了一致性，牺牲了B节点作为另外一个leader的可用性，即降低了可用性：所有客户端只能通过节点成功完成服务。 场景测试由以上可知保证一致性并不能代表完全正确。接下来又一个例子将会说明，并到 https://raft.github.io/ 网站进行验证。 假设集群共有5个节点，Client 写请求，leader向follower同步日志，此时急集群中有3个节点失败，2个结点存活，对于客户端得到结果有3种情况。 unknown（Timeout） （超时后）成功 client 发送给 S5 的请求刚开始，由于只有2个节点存活，因此S4，S5的第二条日志只是虚线（表示未提交），client 此时并不知道，是否请求成功。随后其他节点相继恢复服务，同步了S5的日志，最终client第二条日志写入成功。 （超时后）失败 client 发送给 S4 的请求刚开始，由于只有2个节点存活，因此S4，S5的索引为2的日志（即第二条内容为2的日志）只是虚线（表示未提交），client 此时并不知道，是否请求成功。此时如果S4，S5停止服务，但是S1，S2，S3恢复服务并且S3为leader，有client向S3发送了另一条请求，而后S4，S5恢复服务，那么此时将抹掉S4，S5的第二条日志。索引为2的日志内容统一为：4，即依然保证集群一致性。 对于client请求响应是unkown（Timeout），客户端可以跟集群配合来增强可用性，比如：重试机制，但同时带来了副作用——可能重复写入（在超时后成功却重试了）。 ZAB基本上与raft一样。不同点在于名词叫法上不同：ZAB将某一个leader的周期称为 epoch 而不是 term，实现上的不同：raft日志是连续的，心跳方向为leader-&gt;follower，而ZAB相反。 相关项目实现 Zookeeper（ZAB的实现） etcd（raft的实现） 巨人的肩膀 wiki CAP theorem an_illustrated_proof_of_the_cap_theorem The CAP Theorem | Learn Cassandra Gilbert and Lynch’s specification and proof of the CAP Theorem Raft : Understandable Distributed Consensus https://raft.github.io/]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>distributed-system</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HyperLogLog估计算法模拟]]></title>
    <url>%2F2019%2F11%2F14%2FHyperLogLog%2F</url>
    <content type="text"><![CDATA[巨人的肩膀 论文《HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm》 钱文品《Redis深度历险：核心原理与应用实践》 探索HyperLogLog算法（含Java实现） HyperLogLog 算法的原理讲解以及 Redis 是如何应用它的 Sketch of the Day: HyperLogLog — Cornerstone of a Big Data Infrastructure Redis new data structure: the HyperLogLog HyperLogLog 算法的原理讲解以及 Redis 是如何应用它的 走近源码：神奇的HyperLogLog 源码模拟试验说明：不考虑论文中的常数无偏估计修正因子，以尽量简单的方式模拟，而且从结果中可以看出分桶数据（数组）大小会影响HyperLogLog算法的精确度，也因此反过来可以理解常数无偏估计因子需要根据分桶数据（数组）大小适时调整。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106import java.util.concurrent.ThreadLocalRandom;public class HyperLogLogTest &#123; static class BitBucket &#123; /** * 桶内所有随机数的二进制形式的低位零的最大个数 */ private byte maxbits; /** * 求解出在这个桶内所有随机数的二进制形式的低位零的最大个数 * * @param value 这个桶内的任意一个随机数 */ public void update(long value) &#123; byte bits = lowZeros(value); //更新这个桶的低位零的最大个数，所以maxbits只能估计这个桶内随机数的数量，注意多个桶 this.maxbits = bits &gt; this.maxbits ? bits : this.maxbits; &#125; /** * 求解某个数它的二进制形式的低位0的个数 * * @param value 随机数 * @return 低位0的个数 */ private byte lowZeros(long value) &#123; byte i = 1; for (; i &lt; 32; i++) &#123; if (value &gt;&gt; i &lt;&lt; i != value) &#123; break; &#125; &#125; return (byte)(i-1); &#125; &#125; static class Experiment &#123; private int trialNum; private short bucketNum; private BitBucket[] bitBuckets; public Experiment(int trialNum, short bucketNum) &#123; this.trialNum = trialNum; this.bucketNum = bucketNum; this.bitBuckets = new BitBucket[bucketNum]; for (int i = 0; i &lt; bucketNum; i++) &#123; this.bitBuckets[i] = new BitBucket(); &#125; &#125; public void work() &#123; for (int i = 0; i &lt; this.trialNum; i++) &#123; //获取一个随机数，业务上面可以假想成：UUID，活动记录等等的hash值 //重要的是满足随机性，这里用随机数模拟才能保证HyperLogLog算法的有效性和精确度 long m = ThreadLocalRandom.current().nextLong(1L &lt;&lt; 32); //获取这个随机数对应的桶 BitBucket bitBucket = bitBuckets[(int) (((m &amp; 0xffff0000) &gt;&gt; 16) % this.bucketNum)]; //更新这个桶的所有随机数的二进制形式的低位0的最大个数 bitBucket.update(m); &#125; &#125; /** * 利用HyperLogLog的算法对实验次数的估计值 * * @return 估计值 */ public double estimate() &#123; double maxbitInverseSum = 0.0; for (BitBucket bitBucket : bitBuckets) &#123; maxbitInverseSum += 1.0 / (float) bitBucket.maxbits; &#125; double harmonicMean = this.bucketNum / maxbitInverseSum; return Math.pow(2, harmonicMean) * this.bucketNum; &#125; &#125; public static void main(String[] args) &#123; short bukectNum; double harmonicMean; double estimateErrorRate; double estimateErrorRateInverse; double estimateErrorRateAvg; double est = 0.0; for (byte j = 14; j &gt;= 6; j--) &#123; bukectNum = (short) Math.pow(2, j); estimateErrorRateAvg = 0.0; estimateErrorRateInverse = 0.0; for (int i = 200000; i &lt;= 2000000; i += 200000) &#123; Experiment exp = new Experiment(i, bukectNum); exp.work(); est = exp.estimate(); estimateErrorRate = Math.abs(est - i) / i; estimateErrorRateAvg += estimateErrorRate; estimateErrorRateInverse += 1 / estimateErrorRate; System.out.printf("%d %.3f %.3f\n", i, est, estimateErrorRate); &#125; harmonicMean = 10 / estimateErrorRateInverse; estimateErrorRateAvg /= 10; System.out.println("桶数：" + bukectNum + ", 平均：" + estimateErrorRateAvg + ", 调和平均：" + harmonicMean + "\n"); &#125; &#125;&#125; 试验结果123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107200000 16384.000 0.918 #注意：桶内随机数太少，不满足随机性和足够的样本容量400000 319525.334 0.201600000 505962.682 0.157800000 698289.180 0.1271000000 900077.739 0.1001200000 1087621.839 0.094 #从这个样本容量开始，满足抽样的随机性，HyperLogLog算法得以发挥1400000 1304437.661 0.0681600000 1469819.103 0.0811800000 1669666.816 0.0722000000 1845353.676 0.077桶数：16384, 平均：0.18960571419843567, 调和平均：0.10562357091733171200000 159352.060 0.203400000 348148.548 0.130600000 541721.720 0.097800000 737661.410 0.0781000000 942261.594 0.0581200000 1141143.750 0.0491400000 1361564.781 0.0271600000 1537696.500 0.0391800000 1692330.506 0.0602000000 1968427.770 0.016桶数：8192, 平均：0.07567032338037119, 调和平均：0.04637322622929935200000 177238.898 0.114400000 364321.245 0.089600000 562224.082 0.063800000 775783.867 0.0301000000 1021759.320 0.0221200000 1154526.053 0.0381400000 1338244.486 0.0441600000 1604504.350 0.0031800000 1778948.301 0.0122000000 1987024.401 0.006桶数：4096, 平均：0.0420996187985746, 调和平均：0.013178971576605327200000 180307.864 0.098400000 390080.489 0.025600000 566246.137 0.056800000 817751.889 0.0221000000 986946.287 0.0131200000 1187760.915 0.0101400000 1430421.433 0.0221600000 1605588.554 0.0031800000 1794603.571 0.0032000000 2112011.625 0.056桶数：2048, 平均：0.03091849770917094, 调和平均：0.01029119723250593200000 194312.848 0.028400000 386648.893 0.033600000 576056.608 0.040800000 828576.160 0.0361000000 1011162.106 0.0111200000 1285425.863 0.0711400000 1440170.191 0.0291600000 1727461.010 0.0801800000 1963140.456 0.0912000000 2077138.318 0.039桶数：1024, 平均：0.04573485746225806, 调和平均：0.03266019810377191200000 205210.301 0.026400000 438146.356 0.095600000 603954.951 0.007800000 753863.379 0.0581000000 967701.597 0.0321200000 1257909.108 0.0481400000 1502659.824 0.0731600000 1619446.014 0.0121800000 1747104.882 0.0292000000 2270369.251 0.135桶数：512, 平均：0.05162887564250197, 调和平均：0.02457325664618657200000 191260.947 0.044400000 438321.676 0.096600000 558098.091 0.070800000 836355.100 0.0451000000 1107291.478 0.1071200000 1248805.971 0.0411400000 1565138.959 0.1181600000 1776091.504 0.1101800000 1943602.042 0.0802000000 2325410.231 0.163桶数：256, 平均：0.08732405839292125, 调和平均：0.07153063739557493200000 201052.706 0.005400000 447490.927 0.119600000 503101.620 0.161800000 799865.427 0.0001000000 944046.078 0.0561200000 1230985.985 0.0261400000 1356999.132 0.0311600000 1694042.415 0.0591800000 1825287.933 0.0142000000 2206206.744 0.103桶数：128, 平均：0.05740755761168429, 调和平均：0.0015781846255803417200000 234332.476 0.172400000 370402.926 0.074600000 786417.984 0.311800000 705595.987 0.1181000000 1236640.933 0.2371200000 1063361.567 0.1141400000 1487164.878 0.0621600000 1936580.851 0.2101800000 1792995.043 0.0042000000 2178572.682 0.089桶数：64, 平均：0.13906646547273657, 调和平均：0.030028482076914373]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Redis</tag>
        <tag>distributed compute</tag>
        <tag>Big Data</tag>
        <tag>Estimate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 3 A Tour of Spark’s Toolset]]></title>
    <url>%2F2019%2F11%2F07%2FChapter3_A-Tour-of-Spark-Toolset(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 3. A Tour of Spark’s Toolset 译者：https://snaildove.github.ioIn Chapter 2, we introduced Spark’s core concepts, like transformations and actions, in the context of Spark’s Structured APIs. These simple conceptual building blocks are the foundation of Apache Spark’s vast ecosystem of tools and libraries (Figure 3-1). Spark is composed of these primitives— the lower-level APIs and the Structured APIs—and then a series of standard libraries for additional functionality. 在第2章中，我们在Spark的结构化API中介绍了Spark的核心概念，例如转换和动作。这些简单的概念构建块是Apache Spark庞大的工具和库生态系统的基础（图3-1）。Spark由这些基础元素（低阶API和结构化API）以及一系列用于附加功能的标准库组成。 Spark’s libraries support a variety of different tasks, from graph analysis and machine learning to streaming and integrations with a host of computing and storage systems. This chapter presents a whirlwind tour of much of what Spark has to offer, including some of the APIs we have not yet covered and a few of the main libraries. For each section, you will find more detailed information in other parts of this book; our purpose here is provide you with an overview of what’s possible. Spark的库支持各种不同的任务，从图形分析和机器学习到流以及与大量计算和存储系统的集成。本章介绍了Spark所提供的许多功能，包括一些我们尚未介绍的API和一些主要的库。对于每一部分，您都可以在本书的其他部分找到更详细的信息。我们的目的是为您提供可能的概览。 This chapter covers the following: 本章内容如下： Running production applications with spark-submit 通过 spark-submit 来运行生产应用程序 Datasets: type-safe APIs for structured data Datasets： 用于结构化数据的类型安全的API Structured Streaming 结构化流 Machine learning and advanced analytics 机器学习和高级分析 Resilient Distributed Datasets (RDD): Spark’s low level APIs 弹性分布式数据集（RDD）：Spark的低阶API SparkR The third-party package ecosystem 第三方软件包生态系统 After you’ve taken the tour, you’ll be able to jump to the corresponding parts of the book to find answers to your questions about particular topics. 游览之后，您可以跳到本书的相应部分，以找到有关特定主题的问题的答案。 Running Production ApplicationSpark makes it easy to develop and create big data programs. Spark also makes it easy to turn your interactive exploration into production applications with spark-submit, a built-in command-line tool. spark-submit does one thing: it lets you send your application code to a cluster and launch it to execute there. Upon submission, the application will run until it exits (completes the task) or encounters an error. You can do this with all of Spark’s support cluster managers including Standalone, Mesos, and YARN. Spark使开发和创建大数据程序变得容易。使用内置的命令行工具 spark-submit，Spark还可以轻松地将交互式探索转变为生产应用程序。spark-submit做一件事：它使您可以将应用程序代码发送到集群并启动它以在其中执行。提交后，应用程序将运行，直到退出（完成任务）或遇到错误。您可以使用Spark的所有支持集群管理器（包括Standalone，Mesos和YARN）来执行此操作。 spark-submit offers several controls with which you can specify the resources your application needs as well as how it should be run and its command-line arguments. spark-submit 提供了几个控制（选项），您可以使用这些控制（选项）指定应用程序所需的资源以及应如何运行该应用程序及其命令行参数。 You can write applications in any of Spark’s supported languages and then submit them for execution. The simplest example is running an application on your local machine. We’ll show this by running a sample Scala application that comes with Spark, using the following command in the directory where you downloaded Spark: 您可以使用Spark支持的任何语言编写应用程序，然后将其提交执行。最简单的示例是在本地计算机上运行应用程序。我们将通过运行Spark随附的示例Scala应用程序（在您下载Spark的目录中使用以下命令）来显示此信息： 1234./bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master local \ ./examples/jars/spark-examples_2.11-2.2.0.jar 10 This sample application calculates the digits of pi to a certain level of estimation. Here, we’ve told spark-submit that we want to run on our local machine, which class and which JAR we would like to run, and some command-line arguments for that class. 该示例应用程序将 $\pi$ 的位数计算为一定程度的估计。在这里，我们告诉 spark-submit，我们要在本地计算机上运行，我们要运行哪个类和哪个JAR，以及该类的一些命令行参数。 We can also run a Python version of the application using the following command: 我们还可以使用以下命令运行该应用程序的Python版本： 123./bin/spark-submit \ --master local \ ./examples/src/main/python/pi.py 10 By changing the master argument of spark-submit, we can also submit the same application to a cluster running Spark’s standalone cluster manager, Mesos or YARN. 通过更改spark-submit的主参数，我们还可以将同一应用程序提交给运行Spark独立的集群管理器Mesos或YARN的集群。 spark-submit will come in handy to run many of the examples we’ve packaged with this book. In the rest of this chapter, we’ll go through examples of some APIs that we haven’t yet seen in our introduction to Spark. 在运行与本书一起打包的许多示例时，spark-submit 将派上用场。在本章的其余部分，我们将介绍在Spark简介中尚未见到的一些API的示例。 Datasets: Type-Safe Structured APIsThe first API we’ll describe is a type-safe version of Spark’s structured API called Datasets, for writing statically typed code in Java and Scala. The Dataset API is not available in Python and R, because those languages are dynamically typed. 我们将介绍的第一个API是Spark结构化API的类型安全版本，称为 Datasets，用于以Java和Scala编写静态类型的代码。Datasets API在Python和R中不可用，因为这些语言是动态类型的。 Recall that DataFrames, which we saw in the previous chapter, are a distributed collection of objects of type Row that can hold various types of tabular data. The Dataset API gives users the ability to assign a Java/Scala class to the records within a DataFrame and manipulate it as a collection of typed objects, similar to a Java ArrayList or Scala Seq. The APIs available on Datasets are type-safe, meaning that you cannot accidentally view the objects in a Dataset as being of another class than the class you put in initially. This makes Datasets especially attractive for writing large applications, with which multiple software engineers must interact through well-defined interfaces. 回想一下，我们在上一章中看到的DataFrames是Row类型的对象的分布式集合，这些对象可以保存各种类型的表格数据。Dataset API使用户能够将 Java/Scala 类分配给DataFrame中的记录，并将其作为类型化对象的集合进行操作，类似于Java ArrayList 或 Scala Seq。Datasets 上可用的API是类型安全的，这意味着您不能意外地将 Dataset 中的对象视为不同于最初放置的类的其他类。这使得 Datasets 对于编写大型应用程序特别有吸引力，多个软件工程师必须通过定义良好的接口与之进行交互。 The Dataset class is parameterized with the type of object contained inside: Dataset in Java and Dataset[T] in Scala. For example, a Dataset[Person] will be guaranteed to contain objects of class Person. As of Spark 2.0, the supported types are classes following the JavaBean pattern in Java and case classes in Scala. These types are restricted because Spark needs to be able to automatically analyze the type T and create an appropriate schema for the tabular data within your Dataset. 使用内部包含对象的类型对Dataset类进行参数化：Java中的 Dataset&lt;T&gt; 和 Scala 中的 Dataset[T]。例如，将保证Dataset[Person] 包含Person类的对象。从Spark 2.0开始，支持的类型是Java中遵循JavaBean模式的类以及Scala中的案例类。这些类型受到限制，因为Spark需要能够自动分析类型T并为数据集中的表格数据创建适当的模式。 One great thing about Datasets is that you can use them only when you need or want to. For instance, in the following example, we’ll define our own data type and manipulate it via arbitrary map and filter functions. After we’ve performed our manipulations, Spark can automatically turn it back into a DataFrame, and we can manipulate it further by using the hundreds of functions that Spark includes. This makes it easy to drop down to lower level, perform type-safe coding when necessary, and move higher up to SQL for more rapid analysis. Here is a small example showing how you can use both type-safe functions and DataFrame-like SQL expressions to quickly write business logic: Datasets 的一大优点是，只有在需要或想要时才可以使用它们。例如，在以下示例中，我们将定义自己的数据类型，并通过任意的map和filter函数对其进行操作。执行完操作后，Spark可以自动将其转换回DataFrame，并且可以使用Spark包含的数百种函数对其进行进一步操作。这样可以轻松地降低到较低阶（的API），在必要时执行类型安全的编码，并可以将其上移至SQL以进行更快的分析。这是一个小示例，展示了如何使用类型安全函数和类似DataFrame的SQL表达式来快速编写业务逻辑： 1234// in Scalacase class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt)val flightsDF = spark.read.parquet("/data/flight-data/parquet/2010-summary.parquet/")val flights = flightsDF.as[Flight] One final advantage is that when you call collect or take on a Dataset, it will collect objects of the proper type in your Dataset, not DataFrame Rows. This makes it easy to get type safety and securely perform manipulation in a distributed and a local manner without code changes: 最后一个优点是，当您调用收集或使用数据集时，它将收集数据集中正确类型的对象，而不是 DataFrame Rows。这使得轻松获得类型安全性并以分布式和本地方式安全地执行操作而无需更改代码： 12345678910// in Scalaflights.filter(flight_row =&gt; flight_row.ORIGIN_COUNTRY_NAME != "Canada").map(flight_row =&gt; flight_row).take(5)flights.take(5).filter(flight_row =&gt; flight_row.ORIGIN_COUNTRY_NAME != "Canada").map(fr =&gt; Flight(fr.DEST_COUNTRY_NAME, fr.ORIGIN_COUNTRY_NAME, fr.count + 5)) We cover Datasets in depth in Chapter 11. 我们将在第11章中深入介绍 Datasets。 Structured StreamingStructured Streaming is a high-level API for stream processing that became production-ready in Spark 2.2. With Structured Streaming, you can take the same operations that you perform in batch mode using Spark’s structured APIs and run them in a streaming fashion. This can reduce latency and allow for incremental processing. The best thing about Structured Streaming is that it allows you to rapidly and quickly extract value out of streaming systems with virtually no code changes. It also makes it easy to conceptualize because you can write your batch job as a way to prototype it and then you can convert it to a streaming job. The way all of this works is by incrementally processing that data. 结构化流是用于流处理的高层API，在 Spark 2.2 中已投入生产。借助结构化流，您可以执行与使用Spark的结构化API以批处理模式执行的相同操作，并以流式方式运行它们。这可以减少等待时间并允许增量处理。关于结构化流技术的最好之处在于，它使您能够快速地从流系统中提取价值，而几乎无需更改代码。这也使概念化变得容易，因为您可以编写批处理作业以将其原型化，然后将其转换为流式作业。所有这些工作的方式是通过逐步处理该数据。 Let’s walk through a simple example of how easy it is to get started with Structured Streaming. For this, we will use a retail dataset, one that has specific dates and times for us to be able to use. We will use the “by-day” set of files, in which one file represents one day of data. 让我们看一个简单的示例，说明开始使用结构化流技术有多么容易。为此，我们将使用零售数据集，其中包含可使用的特定日期和时间。我们将使用“按天”文件集，其中一个文件代表一天的数据。 We put it in this format to simulate data being produced in a consistent and regular manner by a different process. This is retail data so imagine that these are being produced by retail stores and sent to a location where they will be read by our Structured Streaming job. 我们将其以这种格式放置，以模拟通过不同过程以一致且规则的方式生成的数据。这是零售数据，因此可以想象这些是由零售商店生产的，并发送到我们的“结构化流”作业可以读取的位置。 It’s also worth sharing a sample of the data so you can reference what the data looks like: 还值得分享数据样本，以便您可以参考数据的外观： 1234InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country536365, 85123A, WHITE HANGING HEART T-LIGHT HOLDER,6,2010-12-01 08:26:00,2.55,17...536365, 71053, WHITE METAL LANTERN,6,2010-12-01 08:26:00,3.39,17850.0,United Kin...536365, 84406B, CREAM CUPID HEARTS COAT HANGER,8,2010-12-01 08:26:00,2.75,17850... To ground this, let’s first analyze the data as a static dataset and create a DataFrame to do so. We’ll also create a schema from this static dataset (there are ways of using schema inference with streaming that we will touch on in Part V): 为此，我们首先将数据分析为静态数据集，然后创建一个DataFrame进行分析。我们还将从此静态数据集中创建一个模式（在第五部分中将介绍用流进行模式推断的方法）： 1234567// in Scalaval staticDataFrame = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/data/retail-data/by-day/*.csv")staticDataFrame.createOrReplaceTempView("retail_data")val staticSchema = staticDataFrame.schema 1234567# in PythonstaticDataFrame = spark.read.format("csv")\.option("header", "true")\.option("inferSchema", "true")\.load("/data/retail-data/by-day/*.csv")staticDataFrame.createOrReplaceTempView("retail_data")staticSchema = staticDataFrame.schema Because we’re working with time–series data, it’s worth mentioning how we might go along grouping and aggregating our data. In this example we’ll take a look at the sale hours during which a given customer (identified by CustomerId) makes a large purchase. For example, let’s add a total cost column and see on what days a customer spent the most. 由于我们正在处理时间序列数据，因此值得一提的是我们可能如何对数据进行分组和汇总。在此示例中，我们将查看特定客户（由 CustomerId 标识）进行大笔交易的销售时间。例如，让我们添加一个“总费用”列，然后查看客户花费最多的几天。 The window function will include all data from each day in the aggregation. It’s simply a window over the time–series column in our data. This is a helpful tool for manipulating date and timestamps because we can specify our requirements in a more human form (via intervals), and Spark will group all of them together for us : 窗口函数将包括汇总中每天的所有数据。它只是我们数据中基于“时间序列”这一列的一个窗口。这是处理日期和时间戳的有用工具，因为我们可以以更人性化的形式（通过时间间隔）指定需求，Spark会为我们将所有需求分组在一起： 1234567891011// in Scalaimport org.apache.spark.sql.functions.&#123;window, column, desc, col&#125;staticDataFrame.selectExpr("CustomerId","(UnitPrice * Quantity) as total_cost","InvoiceDate").groupBy(col("CustomerId"), window(col("InvoiceDate"), "1 day")).sum("total_cost").show(5) 12345678910# in Pythonfrom pyspark.sql.functions import window, column, desc, colstaticDataFrame\.selectExpr("CustomerId","(UnitPrice * Quantity) as total_cost","InvoiceDate")\.groupBy(col("CustomerId"), window(col("InvoiceDate"), "1 day"))\.sum("total_cost")\.show(5) It’s worth mentioning that you can also run this as SQL code, just as we saw in the previous chapter. Here’s a sample of the output that you’ll see: 值得一提的是，您也可以将其作为SQL代码运行，就像在上一章中看到的那样。这是您将看到的输出示例： The null values represent the fact that we don’t have a customerId for some transactions. 空值表示以下事实：我们没有某些交易的customerId。 That’s the static DataFrame version; there shouldn’t be any big surprises in there if you’re familiar with the syntax. 那是静态的DataFrame版本；如果您熟悉语法，那么应该不会有什么大的惊喜。 Because you’re likely running this in local mode, it’s a good practice to set the number of shuffle partitions to something that’s going to be a better fit for local mode. This configuration specifies the number of partitions that should be created after a shuffle. By default, the value is 200, but because there aren’t many executors on this machine, it’s worth reducing this to 5. We did this same operation in Chapter 2, so if you don’t remember why this is important, feel free to flip back to review. 由于您可能会在本地模式下运行此程序，因此最好将洗牌分区的数量设置为更适合本地模式的数量。此配置指定洗牌后应创建的分区数。默认情况下，该值为200，但是由于这台机器上的执行程序（executor）并不多，因此值得将其减少为5。我们在第2章中进行了相同的操作，因此，如果您不记得为什么这样做很重要，请放心后退以进行查看。 1spark.conf.set("spark.sql.shuffle.partitions", "5") Now that we’ve seen how that works, let’s take a look at the streaming code! You’ll notice that very little actually changes about the code. The biggest change is that we used readStream instead of read, additionally you’ll notice the maxFilesPerTrigger option, which simply specifies the number of files we should read in at once. This is to make our demonstration more “streaming,” and in a production scenario this would probably be omitted. 现在我们已经了解了它的工作原理，下面让我们看一下流代码！您会注意到，实际上对代码所做的更改很少。最大的变化是我们使用readStream代替了read，此外，您还会注意到maxFilesPerTrigger选项，该选项仅指定我们一次应读取的文件数。这是为了使我们的演示更加“流畅”，在生产场景中，可能会省略。 123456val streamingDataFrame = spark.readStream.schema(staticSchema).option("maxFilesPerTrigger", 1).format("csv").option("header", "true").load("/data/retail-data/by-day/*.csv") 1234567# in PythonstreamingDataFrame = spark.readStream\.schema(staticSchema)\.option("maxFilesPerTrigger", 1)\.format("csv")\.option("header", "true")\.load("/data/retail-data/by-day/*.csv") Now we can see whether our DataFrame is streaming: 现在我们可以看看我们的DataFrame是否正在流式传输： 1streamingDataFrame.isStreaming // returns true Let’s set up the same business logic as the previous DataFrame manipulation. We’ll perform a summation in the process: 让我们设置与以前的DataFrame操作相同的业务逻辑。我们将在此过程中进行汇总： 123456789// in Scalaval purchaseByCustomerPerHour = streamingDataFrame.selectExpr("CustomerId","(UnitPrice * Quantity) as total_cost","InvoiceDate").groupBy($"CustomerId", window($"InvoiceDate", "1 day")).sum("total_cost") 123456789# in PythonpurchaseByCustomerPerHour = streamingDataFrame\.selectExpr("CustomerId","(UnitPrice * Quantity) as total_cost","InvoiceDate")\.groupBy(col("CustomerId"), window(col("InvoiceDate"), "1 day"))\.sum("total_cost") This is still a lazy operation, so we will need to call a streaming action to start the execution of this data flow. 这仍然是一个懒惰的操作，因此我们将需要调用流操作来开始执行此数据流。 Streaming actions are a bit different from our conventional static action because we’re going to be populating data somewhere instead of just calling something like count (which doesn’t make any sense on a stream anyways). The action we will use will output to an in-memory table that we will update after each trigger. In this case, each trigger is based on an individual file (the read option that we set). Spark will mutate the data in the in-memory table such that we will always have the highest value as specified in our previous aggregation: 流操作与常规的静态操作有所不同，因为我们将在某个地方填充数据，而不是仅仅调用诸如count之类的东西（无论如何，这对流没有任何意义）。我们将使用的操作将输出到一个内存表中，我们将在每次触发后对其进行更新。在这种情况下，每个触发器都基于一个单独的文件（我们设置的读取选项）。Spark将对内存表中的数据进行突变，以使我们将始终具有先前聚合中指定的最高值： 123456// in ScalapurchaseByCustomerPerHour.writeStream.format("memory") // memory = store in-memory table.queryName("customer_purchases") // the name of the in-memory table.outputMode("complete") // complete = all the counts should be in the table.start() 123456# in PythonpurchaseByCustomerPerHour.writeStream\.format("memory")\.queryName("customer_purchases")\.outputMode("complete")\.start() When we start the stream, we can run queries against it to debug what our result will look like if we were to write this out to a production sink: 启动流时，可以对它运行查询进行调试将结果写到生产环境的接收器后的结果： 1234567// in Scalaspark.sql("""SELECT *FROM customer_purchasesORDER BY `sum(total_cost)` DESC""").show(5) 1234567# in Pythonspark.sql("""SELECT *FROM customer_purchasesORDER BY `sum(total_cost)` DESC""")\.show(5) You’ll notice that the composition of our table changes as we read in more data! With each file, the results might or might not be changing based on the data. Naturally, because we’re grouping customers, we hope to see an increase in the top customer purchase amounts over time (and do for a period of time!). Another option you can use is to write the results out to the console: 您会注意到，随着我们读取更多数据，表的组成也会发生变化！对于每个文件，结果可能会或可能不会根据数据而改变。自然，因为我们将客户分组，所以我们希望随着时间的推移（并持续一段时间！），客户的最大购买量会增加。您可以使用的另一个选项是将结果写到控制台： 12345purchaseByCustomerPerHour.writeStream.format("console").queryName("customer_purchases_2").outputMode("complete").start() You shouldn’t use either of these streaming methods in production, but they do make for convenient demonstration of Structured Streaming’s power. Notice how this window is built on event time, as well, not the time at which Spark processes the data. This was one of the shortcomings of Spark Streaming that Structured Streaming has resolved. We cover Structured Streaming in depth in Part V. 您不应该在生产中使用这两种流传输方法中的任何一种，但是它们确实可以方便地演示结构化流传输的功能。请注意，此窗口也是基于事件时间构建的，而不是基于Spark处理数据的时间。这是结构化流已解决的Spark流的缺点之一。我们将在第五部分深入介绍结构化流。 Machine Learning and Advanced AnalyticsAnother popular aspect of Spark is its ability to perform large-scale machine learning with a built-in library of machine learning algorithms called MLlib. MLlib allows for preprocessing, munging, training of models, and making predictions at scale on data. You can even use models trained in MLlib to make predictions in Strucutred Streaming. Spark provides a sophisticated machine learning API for performing a variety of machine learning tasks, from classification to regression, and clustering to deep learning. To demonstrate this functionality, we will perform some basic clustering on our data using a standard algorithm called -means. Spark的另一个受欢迎的方面是它具有使用称为MLlib的内置机器学习算法库执行大规模机器学习的能力。MLlib允许进行预处理，修改，模型训练以及对数据进行大规模预测。您甚至可以使用在MLlib中训练的模型在Strucutred Streaming中进行预测。Spark提供了完善的机器学习API，可用于执行各种机器学习任务，从分类到回归，再到集群再到深度学习。为了演示此功能，我们将使用称为-means的标准算法对数据执行一些基本的聚类。 WHAT IS K-MEANS?-means is a clustering algorithm in which “” centers are randomly assigned within the data. The points closest to that point are then “assigned” to a class and the center of the assigned points is computed. This center point is called the centroid. We then label the points closest to that centroid, to the centroid’s class, and shift the centroid to the new center of that cluster of points. We repeat this process for a finite set of iterations or until convergence (our center points stop changing). -means是一种聚类算法，其中在数据内随机分配“”中心。然后，将最接近该点的点“分配”给一个类，并计算分配点的中心。该中心点称为质心（centroid）。然后，我们标记最接近该质心（centroid），质心类别的点，然后将质心移动到该点簇的新中心。我们对有限的一组迭代或直到收敛（我们的中心点停止更改）重复此过程。 Spark includes a number of preprocessing methods out of the box. To demonstrate these methods, we will begin with some raw data, build up transformations before getting the data into the right format, at which point we can actually train our model and then serve predictions: Spark包括许多现成的预处理方法。为了演示这些方法，我们将从一些原始数据开始，在将数据转换为正确格式之前先进行转换，然后才能实际训练模型，然后进行预测： 1staticDataFrame.printSchema() 123456789root|-- InvoiceNo: string (nullable = true)|-- StockCode: string (nullable = true)|-- Description: string (nullable = true)|-- Quantity: integer (nullable = true)|-- InvoiceDate: timestamp (nullable = true)|-- UnitPrice: double (nullable = true)|-- CustomerID: double (nullable = true)|-- Country: string (nullable = true) Machine learning algorithms in MLlib require that data is represented as numerical values. Our current data is represented by a variety of different types, including timestamps, integers, and strings. Therefore we need to transform this data into some numerical representation. In this instance, we’ll use several DataFrame transformations to manipulate our date data: MLlib中的机器学习算法要求将数据表示为数值。我们当前的数据由各种不同的类型表示，包括时间戳，整数和字符串。因此，我们需要将此数据转换为某种数字表示形式。在这种情况下，我们将使用几种DataFrame转换来操纵日期数据： 123456// in Scalaimport org.apache.spark.sql.functions.date_formatval preppedDataFrame = staticDataFrame.na.fill(0).withColumn("day_of_week", date_format($"InvoiceDate", "EEEE")).coalesce(5) 123456# in Pythonfrom pyspark.sql.functions import date_format, colpreppedDataFrame = staticDataFrame\.na.fill(0)\.withColumn("day_of_week", date_format(col("InvoiceDate"), "EEEE"))\.coalesce(5) We are also going to need to split the data into training and test sets. In this instance, we are going to do this manually by the date on which a certain purchase occurred; however, we could also use MLlib’s transformation APIs to create a training and test set via train validation splits or cross validation (these topics are covered at length in Part VI): 我们还需要将数据分为训练集和测试集。在这种情况下，我们将在特定购买发生的日期之前手动执行此操作；但是，我们也可以使用MLlib的转换API通过训练验证拆分或交叉验证来创建训练和测试集（这些主题在第VI部分中进行了详细介绍）： 12345// in Scalaval trainDataFrame = preppedDataFrame.where("InvoiceDate &lt; '2011-07-01'")val testDataFrame = preppedDataFrame.where("InvoiceDate &gt;= '2011-07-01'") 12345# in PythontrainDataFrame = preppedDataFrame\.where("InvoiceDate &lt; '2011-07-01'")testDataFrame = preppedDataFrame\.where("InvoiceDate &gt;= '2011-07-01'") Now that we’ve prepared the data, let’s split it into a training and test set. Because this is a time– series set of data, we will split by an arbitrary date in the dataset. Although this might not be the optimal split for our training and test, for the intents and purposes of this example it will work just fine. We’ll see that this splits our dataset roughly in half: 现在我们已经准备好数据，让我们将其分为训练和测试集。因为这是一个时间序列数据集，所以我们将在数据集中按任意日期分割。尽管对于我们的训练和测试而言，这可能不是最佳选择，但出于本示例的目的，它仍然可以正常工作。我们将看到这将数据集大致分为两半： 12trainDataFrame.count()testDataFrame.count() Note that these transformations are DataFrame transformations, which we cover extensively in Part II. Spark’s MLlib also provides a number of transformations with which we can automate some of our general transformations. One such transformer is a StringIndexer: 请注意，这些转换是DataFrame转换，我们将在第二部分中进行广泛讨论。Spark的MLlib还提供了许多转换，通过这些转换我们可以自动化一些常规转换。这样的转换器就是 StringIndexer： 12345// in Scalaimport org.apache.spark.ml.feature.StringIndexerval indexer = new StringIndexer().setInputCol("day_of_week").setOutputCol("day_of_week_index") 12345# in Pythonfrom pyspark.ml.feature import StringIndexerindexer = StringIndexer()\.setInputCol("day_of_week")\.setOutputCol("day_of_week_index") This will turn our days of weeks into corresponding numerical values. For example, Spark might represent Saturday as 6, and Monday as 1. However, with this numbering scheme, we are implicitly stating that Saturday is greater than Monday (by pure numerical values). This is obviously incorrect. To fix this, we therefore need to use a OneHotEncoder to encode each of these values as their own column. These Boolean flags state whether that day of week is the relevant day of the week: 这会将我们的星期几转换为相应的数值。例如，Spark可能将星期六表示为6，将星期一表示为1。但是，使用此编号方案，我们隐式地指出，星期六大于星期一（按纯数值）。这显然是不正确的。为了解决这个问题，因此我们需要使用OneHotEncoder将这些值中的每一个编码为自己的列。这些布尔标志说明星期几是否是星期几的相关日期： 12345// in Scalaimport org.apache.spark.ml.feature.OneHotEncoderval encoder = new OneHotEncoder().setInputCol("day_of_week_index").setOutputCol("day_of_week_encoded") 12345# in Pythonfrom pyspark.ml.feature import OneHotEncoderencoder = OneHotEncoder()\.setInputCol("day_of_week_index")\.setOutputCol("day_of_week_encoded") Each of these will result in a set of columns that we will “assemble” into a vector. All machine learning algorithms in Spark take as input a Vector type, which must be a set of numerical values: 这些中的每一个都会产生一组列，我们将这些列“组合”为向量。Spark中的所有机器学习算法都将Vector类型作为输入，该类型必须是一组数值： 12345// in Scalaimport org.apache.spark.ml.feature.VectorAssemblerval vectorAssembler = new VectorAssembler().setInputCols(Array("UnitPrice", "Quantity", "day_of_week_encoded")).setOutputCol("features") 12345# in Pythonfrom pyspark.ml.feature import VectorAssemblervectorAssembler = VectorAssembler()\.setInputCols(["UnitPrice", "Quantity", "day_of_week_encoded"])\.setOutputCol("features") Here, we have three key features: the price, the quantity, and the day of week. Next, we’ll set this up into a pipeline so that any future data we need to transform can go through the exact same process: 在这里，我们具有三个主要功能：价格，数量和星期几。接下来，我们将其设置为管道，以便将来需要转换的所有数据都可以经过完全相同的过程： 1234// in Scalaimport org.apache.spark.ml.Pipelineval transformationPipeline = new Pipeline().setStages(Array(indexer, encoder, vectorAssembler)) 1234# in Pythonfrom pyspark.ml import PipelinetransformationPipeline = Pipeline()\.setStages([indexer, encoder, vectorAssembler]) Preparing for training is a two-step process. We first need to fit our transformers to this dataset. We cover this in depth in Part VI, but basically our StringIndexer needs to know how many unique values there are to be indexed. After those exist, encoding is easy but Spark must look at all the distinct values in the column to be indexed in order to store those values later on: 准备训练是一个分为两个步骤的过程。我们首先需要使我们的转换器（transformer）拟合该数据集。我们将在第六部分中对此进行深入介绍，但是基本上我们的StringIndexer需要知道要索引多少个唯一值。这些值存在之后，编码就很容易了，但是Spark必须查看要索引的列中的所有不同值，以便以后存储这些值： 12// in Scalaval fittedPipeline = transformationPipeline.fit(trainDataFrame) 12# in PythonfittedPipeline = transformationPipeline.fit(trainDataFrame) After we fit the training data, we are ready to take that fitted pipeline and use it to transform all of our data in a consistent and repeatable way: 拟合训练数据后，我们准备采用已经拟合的管道，并使用这个管道以一致且可重复的方式转换所有数据： 12// in Scalaval transformedTraining = fittedPipeline.transform(trainDataFrame) 12# in PythontransformedTraining = fittedPipeline.transform(trainDataFrame) At this point, it’s worth mentioning that we could have included our model training in our pipeline. We chose not to in order to demonstrate a use case for caching the data. Instead, we’re going to perform some hyperparameter tuning on the model because we do not want to repeat the exact same transformations over and over again; specifically, we’ll use caching, an optimization that we discuss in more detail in Part IV. This will put a copy of the intermediately transformed dataset into memory, allowing us to repeatedly access it at much lower cost than running the entire pipeline again. If you’re curious to see how much of a difference this makes, skip this line and run the training without caching the data. Then try it after caching; you’ll see the results are significant: 在这一点上，值得一提的是，我们可以将我们的模型训练纳入我们的流程中。我们选择不这样做是为了演示用于缓存数据的用例。相反，我们将对模型执行一些超参数调整，因为我们不想一遍又一遍地重复完全相同的转换。具体来说，我们将使用缓存，该优化将在第四部分中详细讨论。这会将中间转换后的数据集的副本放入内存中，从而使我们能够以比再次运行整个管道更低的成本重复访问它。如果您想知道这有什么不同，请跳过此行并进行训练，而无需缓存数据。然后在缓存后尝试；您会看到效果显着： 1transformedTraining.cache() We now have a training set; it’s time to train the model. First we’ll import the relevant model that we’d like to use and instantiate it: 我们现在有一个训练集；是时候训练模型了。首先，我们导入要使用的相关模型并实例化： 12345// in Scalaimport org.apache.spark.ml.clustering.KMeansval kmeans = new KMeans().setK(20).setSeed(1L) 12345# in Pythonfrom pyspark.ml.clustering import KMeanskmeans = KMeans()\.setK(20)\.setSeed(1L) In Spark, training machine learning models is a two-phase process. First, we initialize an untrained model, and then we train it. There are always two types for every algorithm in MLlib’s DataFrame API. They follow the naming pattern of Algorithm, for the untrained version, and AlgorithmModel for the trained version. In our example, this is KMeans and then KMeansModel. 在Spark中，训练机器学习模型是一个分为两个阶段的过程。首先，我们初始化未训练的模型，然后训练它。MLlib的DataFrame API中的每种算法总是有两种类型。对于未训练的版本，它们遵循算法的命名模式，对于训练的版本，它们遵循 AlgorithmModel 的命名模式。在我们的示例中，这是 KMeans，然后是 KMeansModel。 Estimators in MLlib’s DataFrame API share roughly the same interface that we saw earlier with our preprocessing transformers like the StringIndexer. This should come as no surprise because it makes training an entire pipeline (which includes the model) simple. For our purposes here, we want to do things a bit more step by step, so we chose to not do this in this example: MLlib的DataFrame API中的估计器（estimator）与我们之前使用 StringIndexer 等预处理转换器（transformer）看到的接口大致相同。这不足为奇，因为它使训练整个管道（包括模型）变得简单。出于此处的目的，我们希望一步一步地做一些事情，因此在此示例中我们选择不这样做： 12// in Scalaval kmModel = kmeans.fit(transformedTraining) 12# in PythonkmModel = kmeans.fit(transformedTraining) After we train this model, we can compute the cost according to some success merits on our training set. The resulting cost on this dataset is actually quite high, which is likely due to the fact that we did not properly preprocess and scale our input data, which we cover in depth in Chapter 25: 训练此模型后，我们可以根据训练集上的一些成功功绩来计算成本。该数据集上的最终成本实际上是很高的，这很可能是由于我们没有适当地预处理和缩放我们的输入数据这一事实，我们将在第25章中进行深入介绍： 1kmModel.computeCost(transformedTraining) 12// in Scalaval transformedTest = fittedPipeline.transform(testDataFrame) 12# in PythontransformedTest = fittedPipeline.transform(testDataFrame) 1kmModel.computeCost(transformedTest) Naturally, we could continue to improve this model, layering more preprocessing as well as performing hyperparameter tuning to ensure that we’re getting a good model. We leave that discussion for Part VI. 当然，我们可以继续改进此模型，对更多的预处理进行分层，并执行超参数调整，以确保获得好的模型。我们将讨论留给第六部分。 Lower-Level APIsSpark includes a number of lower-level primitives to allow for arbitrary Java and Python object manipulation via Resilient Distributed Datasets (RDDs). Virtually everything in Spark is built on top of RDDs. As we will discuss in Chapter 4, DataFrame operations are built on top of RDDs and compile down to these lower-level tools for convenient and extremely efficient distributed execution. There are some things that you might use RDDs for, especially when you’re reading or manipulating raw data, but for the most part you should stick to the Structured APIs. RDDs are lower level than DataFrames because they reveal physical execution characteristics (like partitions) to end users. Spark包含许多较低层次的原语，以允许通过弹性分布式数据集（RDD）进行任意Java和Python对象操作。实际上，Spark中的所有内容都建立在RDD之上。正如我们将在第4章中讨论的那样，DataFrame操作建立在RDD之上，并向下编译为这些较低层次的工具，以实现便捷而高效的分布式执行。在某些情况下，您可能会使用RDD，尤其是在读取或处理原始数据时，但在大多数情况下，您应该坚持使用结构化API。RDD比DataFrames低，因为它们向最终用户揭示了物理执行特征（如分区）。 One thing that you might use RDDs for is to parallelize raw data that you have stored in memory on the driver machine. For instance, let’s parallelize some simple numbers and create a DataFrame after we do so. We then can convert that to a DataFrame to use it with other DataFrames: 可能使用RDD的一件事是并行化存储在驱动程序计算机内存中的原始数据。例如，让我们并行处理一些简单的数字，然后创建一个DataFrame。然后，我们可以将其转换为DataFrame以与其他DataFrame一起使用： 12// in Scalaspark.sparkContext.parallelize(Seq(1, 2, 3)).toDF() 123# in Pythonfrom pyspark.sql import Rowspark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF() RDDs are available in Scala as well as Python. However, they’re not equivalent. This differs from the DataFrame API (where the execution characteristics are the same) due to some underlying implementation details. We cover lower-level APIs, including RDDs in Part IV. As end users, you shouldn’t need to use RDDs much in order to perform many tasks unless you’re maintaining older Spark code. There are basically no instances in modern Spark, for which you should be using RDDs instead of the structured APIs beyond manipulating some very raw unprocessed and unstructured data. RDD在Scala和Python中均可用。但是，它们并不相同。由于某些基础实现细节，这与DataFrame API（执行特性相同）不同。我们将介绍较低层次的API，包括第IV部分中的RDD。作为最终用户，除非您要维护较旧的Spark代码，否则无需为了执行许多任务而使用太多RDD。在现代Spark中，基本上没有实例，除了处理一些非常原始的未处理和非结构化数据外，您应该使用RDD而不是结构化API。 SparkRSparkR is a tool for running R on Spark. It follows the same principles as all of Spark’s other language bindings. To use SparkR, you simply import it into your environment and run your code. It’s all very similar to the Python API except that it follows R’s syntax instead of Python. For the most part, almost everything available in Python is available in SparkR: SparkR是用于在Spark上运行R的工具。它遵循与Spark所有其他语言绑定相同的原则。要使用SparkR，只需将其导入到环境中并运行代码。除了遵循R的语法而不是Python之外，所有其他方面都与Python API非常相似。在大多数情况下，SparkR提供了Python中几乎所有可用的功能： 12345678# in Rlibrary(SparkR)sparkDF &lt;- read.df("/data/flight-data/csv/2015-summary.csv",source = "csv", header="true", inferSchema = "true")take(sparkDF, 5)# in Rcollect(orderBy(sparkDF, "count"), 20) R users can also use other R libraries like the pipe operator in magrittr to make Spark transformations a bit more R-like. This can make it easy to use with other libraries like ggplot for more sophisticated plotting: R用户还可以使用其他R库，例如magrittr中的管道运算符，使Spark转换更像R。这可以使它易于与ggplot等其他库一起使用，以进行更复杂的绘图： 12345678# in Rlibrary(magrittr)sparkDF %&gt;%orderBy(desc(sparkDF$count)) %&gt;%groupBy("ORIGIN_COUNTRY_NAME") %&gt;%count() %&gt;%limit(10) %&gt;%collect() We will not include R code samples as we do in Python, because almost every concept throughout this book that applies to Python also applies to SparkR. The only difference will by syntax. We cover SparkR and sparklyr in Part VII. 我们不会像在Python中那样包含R代码示例，因为本书中适用于Python的几乎所有概念也都适用于SparkR。唯一的区别在于语法。我们将在第七部分介绍SparkR和sparklyr。 Spark’s Ecosystem and PackagesOne of the best parts about Spark is the ecosystem of packages and tools that the community has created. Some of these tools even move into the core Spark project as they mature and become widely used. As of this writing, the list of packages is rather long, numbering over 300—and more are added frequently. You can find the largest index of Spark Packages at spark-packages.org, where any user can publish to this package repository. There are also various other projects and packages that you can find on the web; for example, on GitHub. 关于Spark的最好的部分之一是社区创建的软件包和工具的生态系统。随着这些工具的成熟和广泛使用，其中一些工具甚至进入了Spark核心项目。在撰写本文时，软件包列表相当长，超过300个，并且经常添加更多。您可以在 spark-packages.org 上找到Spark Packages的最新索引，任何用户都可以在其中将其发布到此软件包存储库。您还可以在网上找到其他各种项目和软件包。例如，在GitHub上。 ConclusionWe hope this chapter showed you the sheer variety of ways in which you can apply Spark to your own business and technical challenges. Spark’s simple, robust programming model makes it easy to apply to a large number of problems, and the vast array of packages that have crept up around it, created by hundreds of different people, are a true testament to Spark’s ability to robustly tackle a number of business problems and challenges. As the ecosystem and community grows, it’s likely that more and more packages will continue to crop up. We look forward to seeing what the community has in store! 我们希望本章向您展示将Spark应用于自己的业务和技术挑战的多种方法。Spark的简单，健壮的编程模型使您可以轻松地将其应用于大量问题，并且由数百个不同的人创建的大量围绕它的软件包，这些都是Spark强大解决大量问题的能力的真实证明。业务问题和挑战。随着生态系统和社区的发展，越来越多的软件包可能会继续出现。 The rest of this book will provide deeper dives into the product areas in Figure 3-1. 我们期待看到社区中拥有的一切！本书的其余部分将更深入地研究图3-1中的产品区域。 You may read the rest of the book any way that you prefer, we find that most people hop from area to area as they hear terminology or want to apply Spark to certain problems they’re facing. 您可以按照自己喜欢的任何方式阅读本书的其余部分，我们发现大多数人在听到术语或希望将Spark应用到他们所面临的某些问题时会跳来跳去。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 14 Distributed Shared Variables]]></title>
    <url>%2F2019%2F11%2F07%2FChapter14_Distributed-Shared-Variables(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 14 Distributed Shared VariablesIn addition to the Resilient Distributed Dataset (RDD) interface, the second kind of low-level API in Spark is two types of “distributed shared variables”: broadcast variables and accumulators. These are variables you can use in your user-defined functions (e.g., in a map function on an RDD or a DataFrame) that have special properties when running on a cluster. Specifically, accumulators let you add together data from all the tasks into a shared result (e.g., to implement a counter so you can see how many of your job’s input records failed to parse), while broadcast variables let you save a large value on all the worker nodes and reuse it across many Spark actions without re-sending it to the cluster. This chapter discusses some of the motivation for each of these variable types as well as how to use them. 除了弹性分布式数据集（RDD）接口外，Spark中的第二种底层API是两种“分布式共享变量”：广播变量（broadcast variable）和累加器（accumulator）。这些是您可以在用户定义的函数（例如，在RDD或DataFrame上的映射函数）中使用的变量，这些变量在集群上运行时具有特殊的属性。具体来说，累加器使您可以将所有任务的数据加到一个共享的结果中（例如，实现一个计数器，以便您可以查看有多少作业的输入记录无法解析），而广播变量使您可以在所有工作节点上保存较大的值，并在许多Spark action中重复使用它，而无需将其重新发送到集群。本章讨论了每种变量类型的一些动机以及如何使用它们。 Broadcast VariablesBroadcast variables are a way you can share an immutable value efficiently around the cluster without encapsulating that variable in a function closure. The normal way to use a variable in your driver node inside your tasks is to simply reference it in your function closures (e.g., in a map operation), but this can be inefficient, especially for large variables such as a lookup table or a machine learning model. The reason for this is that when you use a variable in a closure, it must be deserialized on the worker nodes many times (one per task). Moreover, if you use the same variable in multiple Spark actions and jobs, it will be re-sent to the workers with every job instead of once. 广播变量是一种无需在函数闭包中封装该变量就可以有效地在集群中共享不可变值的方法。在任务的驱动程序节点中使用变量的通常方法是在函数闭包中（例如在映射操作中）简单地引用它，但这可能效率不高，尤其是对于较大的变量，例如查找表或机器学习模型。这样做的原因是，当在闭包中使用变量时，必须在 worker 上多次对它进行反序列化（每个任务一个）。而且，如果您在多个Spark action和作业中使用相同的变量，则它将随每个作业重新发送给 worker，而不是一次。 This is where broadcast variables come in. Broadcast variables are shared, immutable variables that are cached on every machine in the cluster instead of serialized with every single task. The canonical use case is to pass around a large lookup table that fits in memory on the executors and use that in a function, as illustrated in Figure 14-1. 这就是广播变量的用处。广播变量是共享的，不可变的变量，它们缓存在集群中的每台计算机上，而不是与每个任务序列化。规范的用例是传递一个大的查找表，该表的大小适合 executor 的内存，并在函数中使用它，如图14-1所示。 For example, suppose that you have a list of words or values: 例如，假设您有一个单词或值的列表： 1234// in Scalaval myCollection = "Spark The Definitive Guide : Big Data Processing Made Simple".split(" ")val words = spark.sparkContext.parallelize(myCollection, 2) 1234# in Pythonmy_collection = "Spark The Definitive Guide : Big Data Processing Made Simple"\.split(" ")words = spark.sparkContext.parallelize(my_collection, 2) You would like to supplement your list of words with other information that you have, which is many kilobytes, megabytes, or potentially even gigabytes in size. This is technically a right join if we thought about it in terms of SQL: 您想用其他信息来补充单词列表，这些信息的大小可能为千字节，兆字节甚至是千兆字节。如果从SQL角度考虑，从技术上讲这是一个正确的连接： 123// in Scalaval supplementalData = Map("Spark" -&gt; 1000, "Definitive" -&gt; 200,"Big" -&gt; -300, "Simple" -&gt; 100) 123# in PythonsupplementalData = &#123;"Spark":1000, "Definitive":200,"Big":-300, "Simple":100&#125; We can broadcast this structure across Spark and reference it by using suppBroadcast. This value is immutable and is lazily replicated across all nodes in the cluster when we trigger an action: 我们可以在Spark上广播此结构，并使用suppBroadcast引用它。当我们触发一个 action 时，该值是不可变的，并且会在集群中的所有节点之间惰性复制： 12// in Scalaval suppBroadcast = spark.sparkContext.broadcast(supplementalData) 12# in PythonsuppBroadcast = spark.sparkContext.broadcast(supplementalData) We reference this variable via the value method, which returns the exact value that we had earlier. This method is accessible within serialized functions without having to serialize the data. This can save you a great deal of serialization and deserialization costs because Spark transfers data more efficiently around the cluster using broadcasts: 我们通过value方法引用此变量，该方法返回我们之前的确切值。 此方法可在序列化函数内访问，而不必序列化数据。这可以为您节省大量的序列化和反序列化成本，因为Spark使用广播在集群中更高效地传输数据： 12// in ScalasuppBroadcast.value 12# in PythonsuppBroadcast.value Now we could transform our RDD using this value. In this instance, we will create a key–value pair according to the value we might have in the map. If we lack the value, we will simply replace it with 0: 现在，我们可以使用此值转换RDD。在这种情况下，我们将根据映射中可能具有的值创建一个键值对。如果缺少该值，则只需将其替换为0： 1234// in Scalawords.map(word =&gt; (word, suppBroadcast.value.getOrElse(word, 0))).sortBy(wordPair =&gt; wordPair._2).collect() 1234# in Pythonwords.map(lambda word: (word, suppBroadcast.value.get(word, 0)))\.sortBy(lambda wordPair: wordPair[1])\.collect() This returns the following value in Python and the same values in an array type in Scala: 这将在Python中返回以下值，在Scala中返回数组类型中的相同值： 12345[(&apos;Big&apos;, -300),(&apos;The&apos;, 0),...(&apos;Definitive&apos;, 200),(&apos;Spark&apos;, 1000)] The only difference between this and passing it into the closure is that we have done this in a much more efficient manner (Naturally, this depends on the amount of data and the number of executors. For very small data (low KBs) on small clusters, it might not be). Although this small dictionary probably is not too large of a cost, if you have a much larger value, the cost of serializing the data for every task can be quite significant. 此操作与将其传递给闭包之间的唯一区别是，我们以一种更加高效的方式完成了此操作（自然，这取决于数据量和 executor 的数量。对于小型集群中的非常小的数据（低KB）而言），可能不是）。尽管这个小词典的开销可能不会太大，但是如果您拥有更大的价值，则为每个任务序列化数据的开销可能会非常大。 One thing to note is that we used this in the context of an RDD; we can also use this in a UDF or in a Dataset and achieve the same result. 需要注意的一件事是，我们在RDD的上下文中使用了它。我们也可以在 UDF 或 Dataset 中使用它来达到相同的结果。 AccumulatorsAccumulators (Figure 14-2), Spark’s second type of shared variable, are a way of updating a value inside of a variety of transformations and propagating that value to the driver node in an efficient and fault-tolerant way. 累加器（图14-2）是Spark的第二种共享变量，是一种在各种转换中更新值并将该值以有效且容错的方式传播到驱动程序节点的方法。 Accumulators provide a mutable variable that a Spark cluster can safely update on a per-row basis. You can use these for debugging purposes (say to track the values of a certain variable per partition in order to intelligently use it over time) or to create low-level aggregation. Accumulators are variables that are “added” to only through an associative and commutative operation and can therefore be efficiently supported in parallel. You can use them to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types. 累加器提供一个可变变量，Spark集群可以在每行的基础上安全地对其进行更新。您可以将它们用于调试目的（例如跟踪每个分区中某个变量的值，以便随着时间的推移智能地使用它）或创建底层聚合。累加器是仅通过关联和交换操作“添加”的变量，因此可以并行有效地得到支持。您可以使用它们来实现计数器（如MapReduce）或总和。Spark原生支持数字类型的累加器，程序员可以添加对新类型的支持。 For accumulator updates performed inside actions only, Spark guarantees that each task’s update to the accumulator will be applied only once, meaning that restarted tasks will not update the value. In transformations, you should be aware that each task’s update can be applied more than once if tasks or job stages are reexecuted. 对于仅在 action 内部执行的累加器更新，Spark保证每个任务对累加器的更新将仅应用一次，这意味着重新启动的任务将不会更新该值。在转换中，您应该意识到，如果重新执行任务或作业阶段，则可以多次应用每个任务的更新。 Accumulators do not change the lazy evaluation model of Spark. If an accumulator is being updated within an operation on an RDD, its value is updated only once that RDD is actually computed (e.g., when you call an action on that RDD or an RDD that depends on it). Consequently, accumulator updates are not guaranteed to be executed when made within a lazy transformation like map(). 累加器不会更改Spark的惰性求值模型（lazy evaluation model）。如果在RDD上的操作中正在更新累加器，则仅在实际计算RDD之后才更新其值（例如，当您对该RDD或依赖于该RDD的RDD调用操作时）。因此，当在诸如 map() 的惰性转换中进行累加器更新时，不能保证执行更新。 Accumulators can be both named and unnamed. Named accumulators will display their running results in the Spark UI, whereas unnamed ones will not. 累加器可以命名也可以不命名。命名累加器将在Spark UI中显示其运行结果，而未命名累加器则不会。 Basic ExampleLet’s experiment by performing a custom aggregation on the Flight dataset that we created earlier in the book. In this example, we will use the Dataset API as opposed to the RDD API, but the extension is quite similar: 让我们通过对我们在本书前面创建的 Flight 数据集执行自定义汇总来进行实验。在此示例中，我们将使用Dataset API而不是RDD API，但扩展名非常相似： 123456// in Scalacase class Flight(DEST_COUNTRY_NAME: String,ORIGIN_COUNTRY_NAME: String, count: BigInt)val flights = spark.read.parquet("/data/flight-data/parquet/2010-summary.parquet").as[Flight] 123# in Pythonflights = spark.read\.parquet("/data/flight-data/parquet/2010-summary.parquet") Now let’s create an accumulator that will count the number of flights to or from China. Even though we could do this in a fairly straightfoward manner in SQL, many things might not be so straightfoward. Accumulators provide a programmatic way of allowing for us to do these sorts of counts. The following demonstrates creating an unnamed accumulator: 现在，我们创建一个累加器，该累加器将计算往返中国的航班数量。即使我们可以在SQL中以相当直截了当的方式执行此操作，但许多事情可能并不那么直截了当。 累加器提供了一种编程方式，使我们能够进行此类计数。下面演示了如何创建未命名的累加器： 1234// in Scalaimport org.apache.spark.util.LongAccumulatorval accUnnamed = new LongAccumulatorval acc = spark.sparkContext.register(accUnnamed) 12# in PythonaccChina = spark.sparkContext.accumulator(0) Our use case fits a named accumulator a bit better. There are two ways to do this: a short-hand method and a long-hand one. The simplest is to use the SparkContext. Alternatively, we can instantiate the accumulator and register it with a name: 我们的用例更适合命名的累加器。 有两种方法可以做到这一点：一种简便方法和一种常规方法。最简单的是使用SparkContext。另外，我们可以实例化累加器并使用名称注册它： 1234// in Scalaval accChina = new LongAccumulatorval accChina2 = spark.sparkContext.longAccumulator("China")spark.sparkContext.register(accChina, "China") We specify the name of the accumulator in the string value that we pass into the function, or as the second parameter into the register function. Named accumulators will display in the Spark UI, whereas unnamed ones will not. 我们在传递给函数的字符串值中指定累加器的名称，或者将其指定为寄存器函数的第二个参数。已命名的累加器将显示在Spark UI中，而未命名的累加器则不会显示。 The next step is to define the way we add to our accumulator. This is a fairly straightforward function: 下一步是定义添加到累加器中的方式。这是一个相当简单的功能： 1234567891011// in Scaladef accChinaFunc(flight_row: Flight) = &#123; val destination = flight_row.DEST_COUNTRY_NAME val origin = flight_row.ORIGIN_COUNTRY_NAME if (destination == "China") &#123; accChina.add(flight_row.count.toLong) &#125; if (origin == "China") &#123; accChina.add(flight_row.count.toLong) &#125;&#125; 12345678# in Pythondef accChinaFunc(flight_row): destination = flight_row["DEST_COUNTRY_NAME"] origin = flight_row["ORIGIN_COUNTRY_NAME"] if destination == "China": accChina.add(flight_row["count"]) if origin == "China": accChina.add(flight_row["count"]) Now, let’s iterate over every row in our flights dataset via the foreach method. The reason for this is because foreach is an action, and Spark can provide guarantees that perform only inside of actions. 现在，让我们通过foreach方法遍历 flight 数据集中的每一行。这样做的原因是因为foreach是一个动作，Spark可以提供仅在 action 内部执行的保证。 The foreach method will run once for each row in the input DataFrame (assuming that we did not filter it) and will run our function against each row, incrementing the accumulator accordingly: foreach方法将对输入DataFrame中的每一行运行一次（假设我们没有对其进行过滤），并将针对每一行运行我们的函数，从而相应地增加累加器： 12// in Scalaflights.foreach(flight_row =&gt; accChinaFunc(flight_row)) 12# in Pythonflights.foreach(lambda flight_row: accChinaFunc(flight_row)) This will complete fairly quickly, but if you navigate to the Spark UI, you can see the relevant value, on a per-Executor level, even before querying it programmatically, as demonstrated in Figure 14-3. 这将很快完成，但是 如果导航到Spark UI，则即使在以编程方式查询它之前，也可以在每个执行器级别上看到相关值，如图14-3所示。 Of course, we can query it programmatically, as well. To do this, we use the value property: 当然，我们也可以通过编程方式查询它。为此，我们使用value属性： 12// in ScalaaccChina.value // 953 12# in PythonaccChina.value # 953 Custom AccumulatorsAlthough Spark does provide some default accumulator types, sometimes you might want to build your own custom accumulator. In order to do this you need to subclass the AccumulatorV2 class. There are several abstract methods that you need to implement, as you can see in the example that follows. In this example, you we will add only values that are even to the accumulator. Although this is again simplistic, it should show you how easy it is to build up your own accumulators: 尽管Spark确实提供了一些默认的累加器类型，但有时您可能想要构建自己的自定义累加器。为此，您需要对AccumulatorV2类进行子类化。您需要实现多种抽象方法，如以下示例所示。在此示例中，您将甚至仅将值加到累加器。尽管这再简单不过了，但它应该告诉您建立自己的累加器有多么容易： 123456789101112131415161718192021222324252627282930313233343536// in Scalaimport scala.collection.mutable.ArrayBufferimport org.apache.spark.util.AccumulatorV2val arr = ArrayBuffer[BigInt]()class EvenAccumulator extends AccumulatorV2[BigInt, BigInt] &#123; private var num:BigInt = 0 def reset(): Unit = &#123; this.num = 0 &#125; def add(intValue: BigInt): Unit = &#123; if (intValue % 2 == 0) &#123; this.num += intValue &#125; &#125; def merge(other: AccumulatorV2[BigInt,BigInt]): Unit = &#123; this.num += other.value &#125; def value():BigInt = &#123; this.num &#125; def copy(): AccumulatorV2[BigInt,BigInt] = &#123; new EvenAccumulator &#125; def isZero():Boolean = &#123; this.num == 0 &#125;&#125; val acc = new EvenAccumulatorval newAcc = sc.register(acc, "evenAcc") 1234// in Scalaacc.value // 0flights.foreach(flight_row =&gt; acc.add(flight_row.count))acc.value // 31390 If you are predominantly a Python user, you can also create your own custom accumulators by subclassing AccumulatorParam and using it as we saw in the previous example. 如果您主要是Python用户，则也可以通过将 AccumulatorParam 子类化并使用它来创建自己的自定义累加器，如上例所示。 ConclusionIn this chapter, we covered distributed variables. These can be helpful tools for optimizations or for debugging. In Chapter 15, we define how Spark runs on a cluster to better understand when these can be helpful. 在本章中，我们介绍了分布式变量。这些对于优化或调试可能是有用的工具。在第15章中，我们定义了Spark如何在集群上运行，以更好地了解何时可以提供帮助。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 13 Advanced RDDs]]></title>
    <url>%2F2019%2F11%2F07%2FChapter13_Advanced-RDDs(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 13. Advanced RDDsChapter 12 explored the basics of single RDD manipulation. You learned how to create RDDs and why you might want to use them. In addition, we discussed map, filter, reduce, and how to create functions to transform single RDD data. This chapter covers the advanced RDD operations and focuses on key–value RDDs, a powerful abstraction for manipulating data. We also touch on some more advanced topics like custom partitioning, a reason you might want to use RDDs in the first place. With a custom partitioning function, you can control exactly how data is laid out on the cluster and manipulate that individual partition accordingly. Before we get there, let’s summarize the key topics we will cover: 第12章探讨了单个RDD操作的基础。您学习了如何创建RDD，以及为什么要使用它们。此外，我们讨论了 map，filter，reduce以及如何创建函数来转换单个RDD数据。本章介绍高级RDD操作，并重点介绍键值RDD，这是用于处理数据的强大抽象。我们还涉及一些更高级的主题，例如自定义分区，这是您可能首先要使用RDD的原因。使用自定义分区功能，您可以精确控制数据在群集上的布局方式，并相应地操作该单个分区。在到达那里之前，让我们总结一下我们将涉及的关键主题： Aggregations and key–value RDDs 聚合以及键值对RDD Custom partitioning 定制分区 RDD joins RDD连接 NOTE 注意This set of APIs has been around since, essentially, the beginning of Spark, and there are a ton of examples all across the web on this set of APIs. This makes it trivial to search and find examples that will show you how to use these operations. 从本质上讲，这是从Spark诞生以来就存在的这组API，并且在这组API上的网络上都有大量示例。这使搜索和查找示例向您展示如何使用这些操作变得很简单。 Let’s use the same dataset we used in the last chapter: 我们使用与上一章相同的数据集： 1234// in Scalaval myCollection = "Spark The Definitive Guide : Big Data Processing Made Simple".split(" ")val words = spark.sparkContext.parallelize(myCollection, 2) 1234# in PythonmyCollection = "Spark The Definitive Guide : Big Data Processing Made Simple"\.split(" ")words = spark.sparkContext.parallelize(myCollection, 2) Key-Value Basics (Key-Value RDDs)There are many methods on RDDs that require you to put your data in a key–value format. A hint that this is required is that the method will include &lt;some-operation&gt;ByKey . Whenever you see ByKey in a method name, it means that you can perform this only on a PairRDD type. The easiest way is to just map over your current RDD to a basic key–value structure. This means having two values in each record of your RDD: RDD上有许多方法要求您将数据以键值格式存储。需要的提示是该方法将包含 &lt;some-operation&gt;ByKey。只要在方法名称中看到ByKey，就意味着您只能在PairRDD类型上执行此操作。最简单的方法是将当前的RDD映射到基本键值结构。这意味着在RDD的每个记录中都有两个值： 12// in Scalawords.map(word =&gt; (word.toLowerCase, 1)) 12# in Pythonwords.map(lambda word: (word.lower(), 1)) keyByThe preceding example demonstrated a simple way to create a key. However, you can also use the keyBy function to achieve the same result by specifying a function that creates the key from your current value. In this case, you are keying by the first letter in the word. Spark then keeps the record as the value for the keyed RDD: 前面的示例演示了一种创建 key 的简单方法。但是，您还可以通过指定从当前值创建 key 的函数，使用 keyBy 函数获得相同的结果。在这种情况下，您将按单词中的第一个字母作为 key。然后，Spark将记录保留为键控RDD的值： 12// in Scalaval keyword = words.keyBy(word =&gt; word.toLowerCase.toSeq(0).toString) 12# in Pythonkeyword = words.keyBy(lambda word: word.lower()[0]) Mapping over ValuesAfter you have a set of key–value pairs, you can begin manipulating them as such. If we have a tuple, Spark will assume that the first element is the key, and the second is the value. When in this format, you can explicitly choose to map-over the values (and ignore the individual keys). Of course, you could do this manually, but this can help prevent errors when you know that you are just going to modify the values: 在拥有一组键值对之后，您就可以像这样操作它们。如果我们有一个元组，Spark将假定第一个元素是键，第二个是值。采用这种格式时，您可以明确选择映射值（并忽略各个键）。当然，您可以手动执行此操作，但是当您知道将要修改值时，这可以帮助防止错误： 12// in Scalakeyword.mapValues(word =&gt; word.toUpperCase).collect() 12# in Pythonkeyword.mapValues(lambda word: word.upper()).collect() Here’s the output in Python: 这是Python的输出： 12345678910[(&apos;s&apos;, &apos;SPARK&apos;),(&apos;t&apos;, &apos;THE&apos;),(&apos;d&apos;, &apos;DEFINITIVE&apos;),(&apos;g&apos;, &apos;GUIDE&apos;),(&apos;:&apos;, &apos;:&apos;),(&apos;b&apos;, &apos;BIG&apos;),(&apos;d&apos;, &apos;DATA&apos;),(&apos;p&apos;, &apos;PROCESSING&apos;),(&apos;m&apos;, &apos;MADE&apos;),(&apos;s&apos;, &apos;SIMPLE&apos;)] (The values in Scala are the same but omitted for brevity.) Scala中的值相同，但为简洁起见，省略了它们。 You can flatMap over the rows, as we saw in Chapter 12, to expand the number of rows that you have to make it so that each row represents a character. In the following example, we will omit the output, but it would simply be each character as we converted them into arrays: 您可以对行进行 flatMap，如我们在第12章中看到的那样，以扩展必须包含的行数，以便每行代表一个字符。在下面的示例中，我们将省略输出，但是在将它们转换为数组时，将只是每个字符： 12// in Scalakeyword.flatMapValues(word =&gt; word.toUpperCase).collect() 12# in Pythonkeyword.flatMapValues(lambda word: word.upper()).collect() Extracting Keys and ValuesWhen we are in the key–value pair format, we can also extract the specific keys or values by using the following methods: 当采用键值对格式时，我们还可以使用以下方法提取特定的键或值： 123// in Scalakeyword.keys.collect()keyword.values.collect() 123# in Pythonkeyword.keys().collect()keyword.values().collect() lookupOne interesting task you might want to do with an RDD is look up the result for a particular key. Note that there is no enforcement mechanism with respect to there being only one key for each input, so if we lookup “s”, we are going to get both values associated with that—“Spark” and “Simple”: 您可能想对RDD进行的一项有趣的任务是查找特定 key 的结果。请注意，没有针对每个输入只有一个键的强制机制，因此，如果我们查找“ s”，我们将获得与此相关的两个值：“ Spark”和“ Simple”： 1keyword.lookup("s") 1Seq[String] = WrappedArray(Spark, Simple) sampleByKeyThere are two ways to sample an RDD by a set of keys. We can do it via an approximation or exactly. Both operations can do so with or without replacement as well as sampling by a fraction by a given key. This is done via simple random sampling with one pass over the RDD, which produces a sample of size that’s approximately equal to the sum of math.ceil(numItems * samplingRate) over all key values: 有两种方法可以通过一组 key 对RDD进行采样。我们可以通过近似或精确地做到这一点。两种操作都可以进行替换，也可以不进行替换，也可以通过给定的 key 按比例采样。这是通过对RDD进行一遍的简单随机抽样完成的，该抽样会产生一个样本，其大小近似等于所有关键值的 math.ceil(numItems * samplingRate) 之和： 1234567// in Scalaval distinctChars = words.flatMap(word =&gt; word.toLowerCase.toSeq).distinct.collect()import scala.util.Randomval sampleMap = distinctChars.map(c =&gt; (c, new Random().nextDouble())).toMapwords.map(word =&gt; (word.toLowerCase.toSeq(0), word)).sampleByKey(true, sampleMap, 6L).collect() 123456# in Pythonimport randomdistinctChars = words.flatMap(lambda word: list(word.lower())).distinct().collect()sampleMap = dict(map(lambda c: (c, random.random()), distinctChars))words.map(lambda word: (word.lower()[0], word))\.sampleByKey(True, sampleMap, 6).collect() This method differs from sampleByKey in that you make additional passes over the RDD to create a sample size that’s exactly equal to the sum of math.ceil(numItems * samplingRate) over all key values with a 99.99% confidence. When sampling without replacement, you need one additional pass over the RDD to guarantee sample size; when sampling with replacement, you need two additional passes: 此方法 sampleByKeyExact 与 sampleByKey 的不同之处在于，您在RDD上进行了额外的遍历，以创建一个样本大小，该样本大小在99.99％的置信度下完全等于所有键值的 math.ceil(numItems * samplingRate) 之和。在不更换样本的情况下，您需要在RDD上再进行一次传递以确保样本量。在进行替换采样时，您需要另外两次传递： 123// in Scalawords.map(word =&gt; (word.toLowerCase.toSeq(0), word)).sampleByKeyExact(true, sampleMap, 6L).collect() 译者附这一块内容本书并没有讲清楚，属于分层抽样，详询：https://spark.apache.org/docs/latest/mllib-statistics.html#stratified-sampling AggregationsYou can perform aggregations on plain RDDs or on PairRDDs, depending on the method that you are using. Let’s use some of our datasets to demonstrate this: 您可以在纯RDD或PairRDD上执行聚合，具体取决于您使用的方法。让我们使用一些数据集来证明这一点： 123456// in Scalaval chars = words.flatMap(word =&gt; word.toLowerCase.toSeq)val KVcharacters = chars.map(letter =&gt; (letter, 1))def maxFunc(left:Int, right:Int) = math.max(left, right)def addFunc(left:Int, right:Int) = left + rightval nums = sc.parallelize(1 to 30, 5) 12345678# in Pythonchars = words.flatMap(lambda word: word.lower())KVcharacters = chars.map(lambda letter: (letter, 1))def maxFunc(left, right): return max(left, right)def addFunc(left, right): return left + rightnums = sc.parallelize(range(1,31), 5) After you have this, you can do something like countByKey, which counts the items per each key. 完成此操作后，您可以执行诸如countByKey之类的操作，该操作对每个键的项目进行计数。 countByKeyYou can count the number of elements for each key, collecting the results to a local Map. You can also do this with an approximation, which makes it possible for you to specify a timeout and confidence when using Scala or Java: 您可以计算每个键的元素数量，然后将结果收集到本地Map中。您也可以使用近似值来执行此操作，这使您可以在使用Scala或Java时指定超时和置信度： 1234// in Scalaval timeout = 1000L //millisecondsval confidence = 0.95KVcharacters.countByKey()KVcharacters.countByKeyApprox(timeout, confidence) 12# in PythonKVcharacters.countByKey() Understanding Aggregation ImplementationsThere are several ways to create your key–value PairRDDs; however, the implementation is actually quite important for job stability. Let’s compare the two fundamental choices, groupBy and reduce. We’ll do these in the context of a key, but the same basic principles apply to the groupBy and reduce methods. 有多种方法可以创建键值PairRDD。但是，实施对于提高工作稳定性实际上非常重要。让我们比较两个基本选择groupBy和reduce。我们将在键的上下文中进行这些操作，但是相同的基本原理也适用于groupBy和reduce方法。 groupByKeyLooking at the API documentation, you might think groupByKey with a map over each grouping is the best way to sum up the counts for each key: 查看API文档，您可能会认为groupByKey和每个分组的映射是求和每个 key 计数的最佳方法： 12// in ScalaKVcharacters.groupByKey().map(row =&gt; (row._1, row._2.reduce(addFunc))).collect() 1234# in PythonKVcharacters.groupByKey().map(lambda row: (row[0], reduce(addFunc, row[1])))\.collect()# note this is Python 2, reduce must be imported from functools in Python 3 However, this is, for the majority of cases, the wrong way to approach the problem. The fundamental issue here is that each executor must hold all values for a given key in memory before applying the function to them. Why is this problematic? If you have massive key skew, some partitions might be completely overloaded with a ton of values for a given key, and you will get OutOfMemoryErrors. This obviously doesn’t cause an issue with our current dataset, but it can cause serious problems at scale. This is not guaranteed to happen, but it can happen. 但是，在大多数情况下，这是解决问题的错误方法。此处的根本问题是，每个执行程序应用于函数之前，必须在内存中保存给定键的所有值。为什么这有问题？如果您有大量的键偏斜，则某些分区可能会因给定键的大量值而完全过载，并且您将获得OutOfMemoryErrors。显然，这不会导致我们当前的数据集出现问题，但可能会导致严重的大规模问题。不能保证会发生这种情况，但是有可能发生。 There are use cases when groupByKey does make sense. If you have consistent value sizes for each key and know that they will fit in the memory of a given executor, you’re going to be just fine. It’s just good to know exactly what you’re getting yourself into when you do this. There is a preferred approach for additive use cases: reduceByKey. 在某些情况下，groupByKey确实有意义。如果每个键的值大小都一致，并且知道它们的大小将适合给定执行器的内存，那么就可以了。最好能确切地知道自己在进行此操作时会遇到什么。对于用例，有一种首选方法：reduceByKey。 reduceByKeyBecause we are performing a simple count, a much more stable approach is to perform the same flatMap and then just perform a map to map each letter instance to the number one, and then perform a reduceByKey with a summation function in order to collect back the array. This implementation is much more stable because the reduce happens within each partition and doesn’t need to put everything in memory. Additionally, there is no incurred shuffle during this operation; everything happens at each worker individually before performing a final reduce. This greatly enhances the speed at which you can perform the operation as well as the stability of the operation: 因为我们执行的是简单计数，所以一种更稳定的方法是执行相同的 flatMap，然后执行映射以将每个字母实例映射到数字，然后执行带有 sum 函数的 reduceByKey 以便回收数组。这种实现更加稳定，因为减少操作发生在每个分区内，不需要将所有内容都放在内存中。此外，在此操作过程中不会发生洗牌；在执行最终归约（reduce）之前，每件事都会在每个 worker 上发生。这极大地提高了您执行操作的速度以及操作的稳定性： 1KVcharacters.reduceByKey(addFunc).collect() Here’s the result of the operation: 操作结果如下： 123Array((d,4), (p,3), (t,3), (b,1), (h,1), (n,2),...(a,4), (i,7), (k,1), (u,1), (o,1), (g,3), (m,2), (c,1)) The reduceByKey method returns an RDD of a group (the key) and sequence of elements that are not guaranteed to have an ordering. Therefore this method is completely appropriate when our workload is associative but inappropriate when the order matters. reduceByKey方法返回一组（键）的RDD以及不保证具有顺序的元素序列。因此，当我们的工作量具有关联性时，此方法是完全合适的，但是当订单重要时，此方法是不合适的。 Other Aggregation MethodsThere exist a number of advanced aggregation methods. For the most part these are largely implementation details depending on your specific workload. We find it very rare that users come across this sort of workload (or need to perform this kind of operation) in modern-day Spark. There just aren’t that many reasons for using these extremely low-level tools when you can perform much simpler aggregations using the Structured APIs. These functions largely allow you very specific, very low-level control on exactly how a given aggregation is performed on the cluster of machines. 存在许多高级聚合方法。在大多数情况下，这些主要是实现细节，具体取决于您的特定工作负载。我们发现用户很少在现代Spark中遇到这种工作负载（或需要执行这种操作）。当您可以使用结构化API执行更简单的汇总时，使用这些极底层工具的原因并不多。这些功能很大程度上允许您非常精确，非常底层地控制如何在计算机群集上执行给定聚合。 aggregateAnother function is aggregate. This function requires a null and start value and then requires you to specify two different functions. The first aggregates within partitions, the second aggregates across partitions. The start value will be used at both aggregation levels: 另一个功能是聚合。该函数需要一个null和起始值，然后需要您指定两个不同的函数。第一个聚集（aggregate）在分区内，第二个聚集（aggregates ）跨分区之间。起始值将在两个聚合级别上使用： 12// in Scalanums.aggregate(0)(maxFunc, addFunc) 12# in Pythonnums.aggregate(0, maxFunc, addFunc) aggregate does have some performance implications because it performs the final aggregation on the driver. If the results from the executors are too large, they can take down the driver with an OutOfMemoryError. There is another method, treeAggregate that does the same thing as aggregate (at the user level) but does so in a different way. It basically “pushes down” some of the subaggregations (creating a tree from executor to executor) before performing the final aggregation on the driver. Having multiple levels can help you to ensure that the driver does not run out of memory in the process of the aggregation. These tree-based implementations are often to try to improve stability in certain operations: 聚合确实会影响性能，因为它在驱动程序上执行最终聚合。如果执行程序的结果太大，则它们可能会因OutOfMemoryError而关闭驱动程序。还有另一种方法，treeAggregate，它与聚合（在用户级别）执行相同的操作，但是执行方式不同。在对驱动程序执行最终聚合之前，它基本上是“下推”某些子聚合（在 executor 之间创建树）。具有多个级别可以帮助您确保驱动程序在聚合过程中不会耗尽内存。这些基于树的实现通常是为了尝试提高某些操作的稳定性： 123// in Scalaval depth = 3nums.treeAggregate(0)(maxFunc, addFunc, depth) 123# in Pythondepth = 3nums.treeAggregate(0, maxFunc, addFunc, depth) aggregateByKeyThis function does the same as aggregate but instead of doing it partition by partition, it does it by key. The start value and functions follow the same properties: 此功能与聚合功能相同，但不是一个分区一个分区地进行，而是按键进行。起始值和函数具有相同的属性： 12// in ScalaKVcharacters.aggregateByKey(0)(addFunc, maxFunc).collect() 12# in PythonKVcharacters.aggregateByKey(0, addFunc, maxFunc).collect() combineByKeyInstead of specifying an aggregation function, you can specify a combiner. This combiner operates on a given key and merges the values according to some function. It then goes to merge the different outputs of the combiners to give us our result. We can specify the number of output partitions as a custom output partitioner as well: 您可以指定组合器，而不是指定聚合函数。该组合器对给定的键进行操作，并根据某些函数合并值。然后将合并器的不同输出进行合并，以得到我们的结果。我们还可以将输出分区的数量指定为自定义输出分区程序： 123456789101112// in Scalaval valToCombiner = (value:Int) =&gt; List(value)val mergeValuesFunc = (vals:List[Int], valToAppend:Int) =&gt; valToAppend :: valsval mergeCombinerFunc = (vals1:List[Int], vals2:List[Int]) =&gt; vals1 ::: vals2// now we define these as function variablesval outputPartitions = 6KVcharacters.combineByKey( valToCombiner, mergeValuesFunc, mergeCombinerFunc, outputPartitions).collect() 123456789101112131415# in Pythondef valToCombiner(value): return [value]def mergeValuesFunc(vals, valToAppend): vals.append(valToAppend) return valsdef mergeCombinerFunc(vals1, vals2): return vals1 + vals2outputPartitions = 6KVcharacters.combineByKey( valToCombiner, mergeValuesFunc, mergeCombinerFunc, outputPartitions).collect() foldByKeyfoldByKey merges the values for each key using an associative function and a neutral “zero value,” which can be added to the result an arbitrary number of times, and must not change the result (e.g., 0 for addition, or 1 for multiplication): foldByKey 使用关联函数和中性的“零值”合并每个键的值，该值可以任意多次添加到结果中，并且不得更改结果（例如，0表示加法，1表示乘法）： 12// in ScalaKVcharacters.foldByKey(0)(addFunc).collect() 12# in PythonKVcharacters.foldByKey(0, addFunc).collect() CoGroupsCoGroups give you the ability to group together up to three key–value RDDs together in Scala and two in Python. This joins the given values by key. This is effectively just a group-based join on an RDD. When doing this, you can also specify a number of output partitions or a custom partitioning function to control exactly how this data is distributed across the cluster (we talk about partitioning functions later on in this chapter): 借助 CoGroup，您可以在Scala中将最多三个键值RDD组合在一起，而在Python中将它们组合在一起。这通过键将给定值连接在一起。实际上，这只是RDD上基于组的连接。在执行此操作时，您还可以指定多个输出分区或自定义分区函数，以精确控制该数据在整个集群中的分布方式（我们将在本章稍后讨论分区功能）： 1234567// in Scalaimport scala.util.Randomval distinctChars = words.flatMap(word =&gt; word.toLowerCase.toSeq).distinctval charRDD = distinctChars.map(c =&gt; (c, new Random().nextDouble()))val charRDD2 = distinctChars.map(c =&gt; (c, new Random().nextDouble()))val charRDD3 = distinctChars.map(c =&gt; (c, new Random().nextDouble()))charRDD.cogroup(charRDD2, charRDD3).take(5) 123456# in Pythonimport randomdistinctChars = words.flatMap(lambda word: word.lower()).distinct()charRDD = distinctChars.map(lambda c: (c, random.random()))charRDD2 = distinctChars.map(lambda c: (c, random.random()))charRDD.cogroup(charRDD2).take(5) The result is a group with our key on one side, and all of the relevant values on the other side. 结果是一组，我们的 key 在一侧，而所有相关值在另一侧。 1Array[(Char, (Iterable[Double], Iterable[Double], Iterable[Double]))] = Array((d,(CompactBuffer(0.12833684521321143),CompactBuffer(0.5229399319461184),CompactBuffer(0.39412761081641534))), (p,(CompactBuffer(0.5563787512207469),CompactBuffer(0.8482281764275303),CompactBuffer(0.05654936603265115))), (t,(CompactBuffer(0.8063968912600572),CompactBuffer(0.8059552537188721),CompactBuffer(0.4538221779361298))), (b,(CompactBuffer(0.19635385859609022),CompactBuffer(0.15376521889330752),CompactBuffer(0.07330965327320438))), (h,(CompactBuffer(0.1639926173875862),CompactBuffer(0.139685392942837),CompactBuffer(0.10124445377925972)))) JoinsRDDs have much the same joins as we saw in the Structured API, although RDDs are much more involved for you. They all follow the same basic format: the two RDDs we would like to join, and, optionally, either the number of output partitions or the customer partition function to which they should output. We’ll talk about partitioning functions later on in this chapter. 尽管RDD对您来说涉及更多，但RDD的连接与结构化API中的连接几乎相同。它们都遵循相同的基本格式：我们要加入的两个RDD，以及可选的输出分区的数量或它们应输出到的自定义分区函数。我们将在本章稍后讨论分区函数。 Inner JoinWe’ll demonstrate an inner join now. Notice how we are setting the number of output partitions we would like to see: 现在，我们将演示内部连接。注意我们如何设置我们希望看到的输出分区数： 12345// in Scalaval keyedChars = distinctChars.map(c =&gt; (c, new Random().nextDouble()))val outputPartitions = 10KVcharacters.join(keyedChars).count()KVcharacters.join(keyedChars, outputPartitions).count() 12345# in PythonkeyedChars = distinctChars.map(lambda c: (c, random.random()))outputPartitions = 10KVcharacters.join(keyedChars).count()KVcharacters.join(keyedChars, outputPartitions).count() We won’t provide an example for the other joins, but they all follow the same basic format. You can learn about the following join types at the conceptual level in Chapter 8: 我们不会提供其他连接的示例，但是它们都遵循相同的基本格式。您可以在第8章的概念层次上了解以下连接类型： fullOuterJoin leftOuterJoin rightOuterJoin cartesian (This, again, is very dangerous! It does not accept a join key and can have a massive output.) 笛卡尔（再次，这非常危险！它不接受连接键，并且可以产生大量输出。） zipsThe final type of join isn’t really a join at all, but it does combine two RDDs, so it’s worth labeling it as a join. zip allows you to “zip” together two RDDs, assuming that they have the same length. This creates a PairRDD. The two RDDs must have the same number of partitions as well as the same number of elements: 最终的连接类型根本不是连接，但它确实结合了两个RDD，因此值得将其标记为连接。zip允许您将两个RDD“压缩”在一起，前提是它们具有相同的长度。这将创建一个PairRDD。两个RDD必须具有相同数量的分区和相同数量的元素： 123// in Scalaval numRange = sc.parallelize(0 to 9, 2)words.zip(numRange).collect() 123# in PythonnumRange = sc.parallelize(range(10), 2)words.zip(numRange).collect() This gives us the following result, an array of keys zipped to the values: 这为我们提供了以下结果，将键数组压缩为值： 12345678910[(&apos;Spark&apos;, 0),(&apos;The&apos;, 1),(&apos;Definitive&apos;, 2),(&apos;Guide&apos;, 3),(&apos;:&apos;, 4),(&apos;Big&apos;, 5),(&apos;Data&apos;, 6),(&apos;Processing&apos;, 7),(&apos;Made&apos;, 8),(&apos;Simple&apos;, 9)] Controlling PartitionsWith RDDs, you have control over how data is exactly physically distributed across the cluster. Some of these methods are basically the same from what we have in the Structured APIs but the key addition (that does not exist in the Structured APIs) is the ability to specify a partitioning function (formally a custom Partitioner, which we discuss later when we look at basic methods). 使用RDD，您可以控制如何在整个群集中准确地物理分布数据。其中一些方法与结构化API中的方法基本相同，但关键的附加功能（结构化API中不存在）具有指定分区函数（通常是自定义分区程序）的能力，稍后我们将在后面讨论看看基本方法）。 coalescecoalesce effectively collapses partitions on the same worker in order to avoid a shuffle of the data when repartitioning. For instance, our words RDD is currently two partitions, we can collapse that to one partition by using coalesce without bringing about a shuffle of the data: coalesce 有效地折叠了同一 worker 上的分区，以避免在重新分区时数据洗牌。例如，我们的词RDD当前是两个分区，我们可以使用 coalesce 将其折叠为一个分区，而不会造成数据洗牌： 12// in Scalawords.coalesce(1).getNumPartitions // 1 12# in Pythonwords.coalesce(1).getNumPartitions() # 1 repartitionThe repartition operation allows you to repartition your data up or down but performs a shuffle across nodes in the process. Increasing the number of partitions can increase the level of parallelism when operating in map- and filter-type operations: 使用重新分区操作，您可以向上或向下重新分区数据，但可以在进程中的各个节点之间执行随机洗牌。在map和filter类型的操作中进行操作时，增加分区数量可以提高并行度： 1words.repartition(10) // gives us 10 partitions repartitionAndSortWithinPartitionsThis operation gives you the ability to repartition as well as specify the ordering of each one of those output partitions. We’ll omit the example because the documentation for it is good, but both the partitioning and the key comparisons can be specified by the user. 此操作使您能够重新分区以及指定这些输出分区中每个分区的顺序。我们将省略该示例，因为该示例的文档不错，但是分区和键比较都可以由用户指定。 Custom PartitioningThis ability is one of the primary reasons you’d want to use RDDs. Custom partitioners are not available in the Structured APIs because they don’t really have a logical counterpart. They’re a lowlevel, implementation detail that can have a significant effect on whether your jobs run successfully. The canonical example to motivate custom partition for this operation is PageRank whereby we seek to control the layout of the data on the cluster and avoid shuffles. In our shopping dataset, this might mean partitioning by each customer ID (we’ll get to this example in a moment). 此功能是您要使用RDD的主要原因之一。自定义分区程序在结构化API中不可用，因为它们实际上并没有逻辑上的对应关系。它们是底层的实施细节，可能会对您的作业是否成功运行产生重大影响。鼓励对此操作进行自定义分区的典型示例是PageRank，据此，我们试图控制集群上数据的布局并避免乱序。在我们的购物数据集中，这可能意味着按每个客户ID进行分区（我们稍后将转到此示例）。 In short, the sole goal of custom partitioning is to even out the distribution of your data across the cluster so that you can work around problems like data skew. 简而言之，自定义分区的唯一目标是使数据在集群中的分布均匀，以便您可以解决数据倾斜等问题。 If you’re going to use custom partitioners, you should drop down to RDDs from the Structured APIs, apply your custom partitioner, and then convert it back to a DataFrame or Dataset. This way, you get the best of both worlds, only dropping down to custom partitioning when you need to. 如果要使用自定义分区程序，则应从结构化API降到RDD，应用自定义分区程序，然后将其转换回DataFrame或Dataset。这样，您可以兼得两全其美，只在需要时才使用自定义分区。 To perform custom partitioning you need to implement your own class that extends Partitioner. You need to do this only when you have lots of domain knowledge about your problem space—if you’re just looking to partition on a value or even a set of values (columns), it’s worth just doing it in the DataFrame API. 要执行自定义分区，您需要实现自己的扩展Partitioner的类。只有在对问题空间有很多领域知识的情况下，才需要执行此操作。如果您只是想对一个值甚至一组值（列）进行分区，那么只需在DataFrame API中进行操作即可。 Let’s dive into an example: 让我们投入到一个例子： 1234// in Scalaval df = spark.read.option("header", "true").option("inferSchema", "true").csv("/spark/The-Definitive-Guide/data/retail-data/all/")val rdd = df.coalesce(10).rdd 1234# in Pythondf = spark.read.option("header", "true").option("inferSchema", "true")\.csv("/data/retail-data/all/")rdd = df.coalesce(10).rdd 1df.printSchema() Spark has two built-in Partitioners that you can leverage off in the RDD API, a HashPartitioner for discrete values and a RangePartitioner. These two work for discrete values and continuous values, respectively. Spark’s Structured APIs will already use these, although we can use the same thing in RDDs: Spark具有两个可在RDD API中使用的内置分区程序，一个用于离散值的 HashPartitioner 和一个 RangePartitioner。这两个分别适用于离散值和连续值。尽管我们可以在RDD中使用相同的东西，但Spark的结构化API已经使用了它们： 12345// in Scalaimport org.apache.spark.HashPartitionerrdd.map(r =&gt; r(6)).take(5).foreach(println)val keyedRDD = rdd.keyBy(row =&gt; row(6).asInstanceOf[Int].toDouble)keyedRDD.partitionBy(new HashPartitioner(10)).take(10) Although the hash and range partitioners are useful, they’re fairly rudimentary. At times, you will need to perform some very low-level partitioning because you’re working with very large data and large key skew. Key skew simply means that some keys have many, many more values than other keys. You want to break these keys as much as possible to improve parallelism and prevent OutOfMemoryErrors during the course of execution. 尽管哈希和范围分区器很有用，但还很初级。有时，由于要处理非常大的数据和较大的键倾斜，因此您将需要执行一些非常底层的分区。键倾斜只是意味着某些键比其他键具有更多很多的值。您希望尽可能地拆分这些键，以提高并行度并在执行过程中防止 OutOfMemoryErrors。 One instance might be that you need to partition more keys if and only if the key matches a certain format. For instance, we might know that there are two customers in your dataset that always crash your analysis and we need to break them up further than other customer IDs. In fact, these two are so skewed that they need to be operated on alone, whereas all of the others can be lumped into large groups. This is obviously a bit of a caricatured example, but you might see similar situations in your data, as well: 一个实例可能是，当且仅当键与某种格式匹配时，才需要对更多键进行分区。例如，我们可能知道您的数据集中有两个客户总是使您的分析崩溃，因此我们需要比其他客户ID进一步细分它们。实际上，这两个倾斜了，需要单独操作，而其他所有都可以分成大组。这显然是一个讽刺的例子，但是您可能还会在数据中看到类似的情况： 1234567891011121314151617// in Scalaimport org.apache.spark.Partitionerclass DomainPartitioner extends Partitioner &#123; def numPartitions = 3 def getPartition(key: Any): Int = &#123; val customerId = key.asInstanceOf[Double].toInt if (customerId == 17850.0 || customerId == 12583.0) &#123; return 0 &#125; else &#123; return new java.util.Random().nextInt(2) + 1 &#125; &#125;&#125; keyedRDD.partitionBy(new DomainPartitioner).map(_._1).glom().map(_.toSet.toSeq.length).take(5) After you run this, you will see the count of results in each partition. The second two numbers will vary, because we’re distributing them randomly (as you will see when we do the same in Python) but the same principles apply: 运行此命令后，您将看到每个分区中的结果计数。后两个数字会有所不同，因为我们是随机分配它们（如您在Python中进行相同操作时所见），但是适用相同的原理： 1234567# in Pythondef partitionFunc(key): import random if key == 17850 or key == 12583: return 0 else: return random.randint(1,2) 1234567keyedRDD = rdd.keyBy(lambda row: row[6])keyedRDD\.partitionBy(3, partitionFunc)\.map(lambda x: x[0])\.glom()\.map(lambda x: len(set(x)))\.take(5) This custom key distribution logic is available only at the RDD level. Of course, this is a simple example, but it does show the power of using arbitrary logic to distribute the data around the cluster in a physical manner. 此自定义 key 分发逻辑仅在RDD级别可用。当然，这是一个简单的示例，但是它确实显示了使用任意逻辑以物理方式在集群中分布数据的强大能力。 Custom SerializationThe last advanced topic that is worth talking about is the issue of Kryo serialization. Any object that you hope to parallelize (or function) must be serializable: 值得讨论的最后一个高级主题是Kryo序列化问题。您希望并行化（或函数化）的任何对象都必须可序列化： 123456789// in Scalaclass SomeClass extends Serializable &#123;var someValue = 0 def setSomeValue(i:Int) = &#123; someValue = i this &#125;&#125; sc.parallelize(1 to 10).map(num =&gt; new SomeClass().setSomeValue(num)) The default serialization can be quite slow. Spark can use the Kryo library (version 2) to serialize objects more quickly. Kryo is significantly faster and more compact than Java serialization (often as much as 10x), but does not support all serializable types and requires you to register the classes you’ll use in the program in advance for best performance. 默认序列化可能会很慢。Spark可以使用Kryo库（版本2）更快地序列化对象。与Java序列化（通常多达10倍）相比，Kryo显着更快，更紧凑，但是它不支持所有可序列化的类型，并且需要您预先注册要在程序中使用的类才能获得最佳性能。 You can use Kryo by initializing your job with a SparkConf and setting the value of “spark.serializer” to “org.apache.spark.serializer.KryoSerializer” (we discuss this in the next part of the book). This setting configures the serializer used for shuffling data between worker nodes and serializing RDDs to disk. The only reason Kryo is not the default is because of the custom registration requirement, but we recommend trying it in any . Since Spark 2.0.0, we internally use Kryo serializer when shuffling RDDs with simple types, arrays of simple types, or string type. 您可以通过使用SparkConf初始化作业并将 “spark.serializer” 的值设置为 “org.apache.spark.serializer.KryoSerializer”来使用 Kryo（我们将在本书的下一部分中讨论）。此设置配置序列化器用于在 worker 节点之间对数据进行洗牌以及序列化RDD到磁盘。Kryo 不是默认值的唯一原因是由于自定义注册要求，但是我们建议在任何网络密集型应用程序中尝试使用它。从Spark 2.0.0开始，在对具有简单类型，简单类型数组或字符串类型的RDD进行改组时，我们在内部使用Kryo序列化程序。 Spark automatically includes Kryo serializers for the many commonly used core Scala classes covered in the AllScalaRegistrar from the Twitter chill library. 对于许多来自twitter chill库中的属于AllScalaRegistrar的常用核心Scala类，Spark自动为它们包括Kryo序列化器。 To register your own custom classes with Kryo, use the registerKryoClasses method: 要向 Kryo 注册您自己的自定义类，请使用registerKryoClasses方法： 1234// in Scalaval conf = new SparkConf().setMaster(...).setAppName(...)conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))val sc = new SparkContext(conf) Conclusion 小结In this chapter we discussed many of the more advanced topics regarding RDDs. Of particular note was the section on custom partitioning, which allows you very specific functions to layout your data.In Chapter 14, we discuss another of Spark’s low-level tools: distributed variables. 在本章中，我们讨论了有关RDD的许多更高级的主题。特别值得注意的是有关自定义分区的部分，该节允许您使用非常特定的功能来布局数据。在第14章中，我们讨论了Spark的另一个底层工具：分布式变量。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 12 Resilient Distributed Datasets (RDDs)]]></title>
    <url>%2F2019%2F11%2F07%2FChapter12_Resilient-Distributed-Datasets-(RDDs)(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 12. Resilient Distributed Datasets (RDDs)The previous part of the book covered Spark’s Structured APIs. You should heavily favor these APIs in almost all scenarios. That being said, there are times when higher-level manipulation will not meet the business or engineering problem you are trying to solve. For those cases, you might need to use Spark’s lower-level APIs, specifically the Resilient Distributed Dataset (RDD), the SparkContext, and distributed shared variables like accumulators and broadcast variables. The chapters that follow in this part cover these APIs and how to use them. 本书的上一部分介绍了Spark的结构化API。在几乎所有情况下，您都应该大力支持这些API。话虽这么说，有时更高层别的操作无法满足您要解决的业务或工程问题。在这种情况下，您可能需要使用Spark的较底层API，特别是弹性分布式数据集（RDD），SparkContext和分布式共享变量（distributed shared variable），例如累加器（accumulator）和广播变量（broadcast variable）。本部分后面的章节介绍了这些API以及如何使用它们。 WARNING 警告If you are brand new to Spark, this is not the place to start. Start with the Structured APIs, you’ll be more productive more quickly! 如果您是Spark的新手，那么这不是一个开始的地方。 从结构化API开始，您将更快地提高生产力！ What Are the Low-Level APIs?There are two sets of low-level APIs: there is one for manipulating distributed data (RDDs), and another for distributing and manipulating distributed shared variables (broadcast variables and accumulators). 有两组底层API：一组用于处理分布式数据（RDD），另一组用于分发和处理分布式共享变量（广播变量和累加器）。 When to Use the Low-Level APIs? You should generally use the lower-level APIs in three situations: 通常，您应在以下三种情况下使用较底层的API： You need some functionality that you cannot find in the higher-level APIs; for example, if you need very tight control over physical data placement across the cluster. 您需要一些在高层API中找不到的功能。 例如，如果您需要非常严格地控制整个集群中的物理数据放置。 You need to maintain some legacy codebase written using RDDs. 您需要维护一些使用RDD编写的旧代码库。 You need to do some custom shared variable manipulation. We will discuss shared variables more in Chapter 14. 您需要执行一些自定义共享变量操作。我们将在第14章中讨论共享变量。 Those are the reasons why you should use these lower-level tools, buts it’s still helpful to understand these tools because all Spark workloads compile down to these fundamental primitives. When you’re calling a DataFrame transformation, it actually just becomes a set of RDD transformations. This understanding can make your task easier as you begin debugging more and more complex workloads. 这就是为什么您应该使用这些底层工具的原因，但是了解这些工具仍然有帮助，因为所有Spark工作负载均会编译为这些基本原语（fundamental primitives）。当您调用DataFrame转换时，它实际上只是一组RDD转换。当您开始调试越来越复杂的工作负载时，这种理解可以使您的任务更轻松。 Even if you are an advanced developer hoping to get the most out of Spark, we still recommend focusing on the Structured APIs. However, there are times when you might want to “drop down” to some of the lower-level tools to complete your task. You might need to drop down to these APIs to use some legacy code, implement some custom partitioner, or update and track the value of a variable over the course of a data pipeline’s execution. These tools give you more fine-grained control at the expense of safeguarding you from shooting yourself in the foot. 即使您是希望充分利用Spark的高级开发人员，我们仍然建议您专注于结构化API。但是，有时您可能需要“下拉”一些较底层的工具来完成任务。您可能需要使用这些API来使用一些旧代码，实现一些自定义分区程序，或者在数据管道执行过程中更新和跟踪变量的值。这些工具可为您提供更细粒度的控制，但以保护您伤害到自己为代价。 How to Use the Low-Level APIs?A SparkContext is the entry point for low-level API functionality. You access it through the SparkSession, which is the tool you use to perform computation across a Spark cluster. We discuss this further in Chapter 15 but for now, you simply need to know that you can access a SparkContext via the following call: SparkContext是底层API功能的入口点。您可以通过 SparkSession 访问它，SparkSession 是用于跨 Spark 集群执行计算的工具。我们将在第15章中对此进行进一步讨论，但是现在，您只需要知道可以通过以下调用访问SparkContext ： 1spark.sparkContext About RDDsRDDs were the primary API in the Spark 1.X series and are still available in 2.X, but they are not as commonly used. However, as we’ve pointed out earlier in this book, virtually all Spark code you run, whether DataFrames or Datasets, compiles down to an RDD. The Spark UI, covered in the next part of the book, also describes job execution in terms of RDDs. Therefore, it will behoove you to have at least a basic understanding of what an RDD is and how to use it. RDD是Spark 1.X系列中的主要API，并且在2.X中仍然可用，但是并不常用。但是，正如我们在本书前面所指出的那样，您运行的几乎所有Spark代码（无论是DataFrames还是Datasets）都可以编译为RDD。本书下一部分介绍的Spark UI还以RDD来描述作业执行。因此，您应该至少对RDD是什么以及如何使用它有基本的了解。 In short, an RDD represents an immutable, partitioned collection of records that can be operated on in parallel. Unlike DataFrames though, where each record is a structured row containing fields with a known schema, in RDDs the records are just Java, Scala, or Python objects of the programmer’s choosing. 简而言之，RDD表示一个不变的，分区的记录集合，可以并行操作。不过，与DataFrames不同的是，每个记录都是一个结构化的行，其中包含具有已知模式的字段，而在RDD中，记录只是程序员选择的Java，Scala或Python对象。 RDDs give you complete control because every record in an RDD is a just a Java or Python object. You can store anything you want in these objects, in any format you want. This gives you great power, but not without potential issues. Every manipulation and interaction between values must be defined by hand, meaning that you must “reinvent the wheel” for whatever task you are trying to carry out. Also, optimizations are going to require much more manual work, because Spark does not understand the inner structure of your records as it does with the Structured APIs. For instance, Spark’s Structured APIs automatically store data in an optimized, compressed binary format, so to achieve the same space-efficiency and performance, you’d also need to implement this type of format inside your objects and all the low-level operations to compute over it. Likewise, optimizations like reordering filters and aggregations that occur automatically in Spark SQL need to be implemented by hand. For this reason and others, we highly recommend using the Spark Structured APIs when possible. RDD提供了完全的控制权，因为RDD中的每条记录都只是一个Java或Python对象。您可以以任何所需的格式将所需的任何内容存储在这些对象中。这将为您提供强大的功能，但并非没有潜在的问题。值之间的每个操作和交互都必须手动定义，这意味着您必须“重新发明轮子”才能完成您要执行的任何任务。而且，优化将需要更多的人工工作，因为Spark无法像使用结构化API那样理解记录的内部结构。例如，Spark的结构化API自动以优化的压缩二进制格式存储数据，因此，要实现相同的空间效率和性能，还需要在对象内部以及所有底层操作中实现这种格式计算它。同样，需要手动执行在Spark SQL中自动进行的优化（例如重新排序过滤器和聚合）。因此，我们强烈建议您尽可能使用Spark结构化API。 The RDD API is similar to the Dataset, which we saw in the previous part of the book, except that RDDs are not stored in, or manipulated with, the structured data engine. However, it is trivial to convert back and forth between RDDs and Datasets, so you can use both APIs to take advantage of each API’s strengths and weaknesses. We’ll show how to do this throughout this part of the book. RDD API 与 Dataset 类似，我们在本书的上半部分中看到了，除了 RDD 不存储在结构化数据引擎中或不使用结构化数据引擎操纵之外。但是，在 RDD 和 Datasets 之间来回转换很简单，因此您可以使用这两个API来利用每个API的优点和缺点。在本书的这一部分中，我们将展示如何执行此操作。 Types of RDDsIf you look through Spark’s API documentation, you will notice that there are lots of subclasses of RDD. For the most part, these are internal representations that the DataFrame API uses to create optimized physical execution plans. As a user, however, you will likely only be creating two types of RDDs: the “generic” RDD type or a key-value RDD that provides additional functions, such as aggregating by key. For your purposes, these will be the only two types of RDDs that matter. Both just represent a collection of objects, but key-value RDDs have special operations as well as a concept of custom partitioning by key. 查看Spark的API文档时，您会发现RDD有很多子类。在大多数情况下，这些是DataFrame API用于创建优化的物理执行计划的内部表示。但是，作为用户，您可能只会创建两种类型的RDD：“通用” RDD类型或提供附加功能（例如，按键聚合）的键值RDD。就您的目的而言，这将是仅有的两种重要的RDD类型。两者都仅表示对象的集合，但是键值RDD具有特殊的操作以及按键自定义分区的概念。 Let’s formally define RDDs. Internally, each RDD is characterized by five main properties : 让我们正式定义RDD。在内部，每个RDD具有五个主要属性： A list of partitions 分区列表 A function for computing each split 用于计算每个拆分的函数 A list of dependencies on other RDDs 对其他RDD的依赖关系列表 Optionally, a Partitioner for key-value RDDs (e.g., to say that the RDD is hash-partitioned) （可选）一个键值RDD的分区程序（例如，说RDD是哈希分区的） Optionally, a list of preferred locations on which to compute each split (e.g., block locations for a Hadoop Distributed File System [HDFS] file) （可选）在其上计算每个拆分的首选位置的列表（例如，Hadoop分布式文件系统[HDFS]文件的块位置） NOTE 注意The Partitioner is probably one of the core reasons why you might want to use RDDs in your code. Specifying your own custom Partitioner can give you significant performance and stability improvements if you use it correctly. This is discussed in more depth in Chapter 13 when we introduce Key–Value Pair RDDs. 分区程序可能是您可能想在代码中使用RDD的核心原因之一。如果正确使用自定义分区程序，则可以显著提高性能和稳定性。当我们介绍键值对RDD时，将在第13章中对此进行更深入的讨论。 These properties determine all of Spark’s ability to schedule and execute the user program. Different kinds of RDDs implement their own versions of each of the aforementioned properties, allowing you to define new data sources. 这些属性决定了Spark安排和执行用户程序的全部能力。不同种类的RDD会为每个上述属性实现各自的版本，从而允许您定义新的数据源。 RDDs follow the exact same Spark programming paradigms that we saw in earlier chapters. They provide transformations, which evaluate lazily, and actions, which evaluate eagerly, to manipulate data in a distributed fashion. These work the same way as transformations and actions on DataFrames and Datasets. However, there is no concept of “rows” in RDDs; individual records are just raw Java/Scala/Python objects, and you manipulate those manually instead of tapping into the repository of functions that you have in the structured APIs. RDD遵循我们在前几章中看到的完全相同的Spark编程范例。它们提供了延迟求值（evaluate lazily）的转换和迫切求值（evaluate eagerly）的动作，以分布式方式处理数据。这些工作方式与对DataFrame和Dataset进行转换和操作相同。但是，RDD中没有“行”的概念；单个记录只是原始的 Java/Scala/Python 对象，您可以手动操作它们，而不必进入结构化API中具有的函数存储库。 The RDD APIs are available in Python as well as Scala and Java. For Scala and Java, the performance is for the most part the same, the large costs incurred in manipulating the raw objects. Python, however, can lose a substantial amount of performance when using RDDs. Running Python RDDs equates to running Python user-defined functions (UDFs) row by row. Just as we saw inChapter 6. We serialize the data to the Python process, operate on it in Python, and then serialize it back to the Java Virtual Machine (JVM). This causes a high overhead for Python RDD manipulations. Even though many people ran production code with them in the past, we recommend building on the Structured APIs in Python and only dropping down to RDDs if absolutely necessary. RDD API在Python以及Scala和Java中均可用。对于Scala和Java，性能在很大程度上是相同的，这是操作原始对象所产生的巨大成本。但是，使用RDD时，Python可能会损失大量性能。运行Python RDD等同于逐行运行Python用户定义函数（UDF）。就像在第6章中看到的那样。我们将数据序列化到Python进程，在Python中对其进行操作，然后将其序列化回Java虚拟机（JVM）。这会导致Python RDD操作的开销很大。即使过去有很多人使用生产代码来运行它们，我们还是建议在Python中基于结构化API进行构建，并且仅在绝对必要时才使用RDD。 When to Use RDDs?In general, you should not manually create RDDs unless you have a very, very specific reason for doing so. They are a much lower-level API that provides a lot of power but also lacks a lot of the optimizations that are available in the Structured APIs. For the vast majority of use cases, DataFrames will be more efficient, more stable, and more expressive than RDDs. 通常，除非有非常特殊的原因，否则不应手动创建RDD。它们是一个底层的API，它提供了很多功能，但缺乏结构化API中可用的许多优化。在绝大多数用例中，DataFrames将比RDDs更高效，更稳定和更具表现力。 The most likely reason for why you’ll want to use RDDs is because you need fine-grained control over the physical distribution of data (custom partitioning of data). 之所以要使用RDD，最可能的原因是因为您需要对数据的物理分布（数据的自定义分区）进行细粒度的控制。 Datasets and RDDs of Case ClassesWe noticed this question on the web and found it to be an interesting one: what is the difference between RDDs of Case Classes and Datasets? The difference is that Datasets can still take advantage of the wealth of functions and optimizations that the Structured APIs have to offer. With Datasets, you do not need to choose between only operating on JVM types or on Spark types, you can choose whatever is either easiest to do or most flexible. You get the both of best worlds. 我们在网上注意到了这个问题，发现这是一个有趣的问题：案例类和 Datasets 的RDD有什么区别？区别在于，Datasets 仍然可以利用结构化API必须提供的丰富功能和优化。使用 Datasets ，您不需要是仅在JVM类型上的操作或是仅在Spark类型上的操作进行选择，可以选择最容易执行或最灵活的操作。你们两全其美。 Creating RDDsNow that we discussed some key RDD properties, let’s begin applying them so that you can better understand how to use them. 现在，我们讨论了一些RDD关键属性，让我们开始应用它们，以便您可以更好地了解如何使用它们。 Interoperating Between DataFrames, Datasets, and RDDsOne of the easiest ways to get RDDs is from an existing DataFrame or Dataset. Converting these to an RDD is simple: just use the rdd method on any of these data types. You’ll notice that if you do a conversion from a Dataset[T] to an RDD, you’ll get the appropriate native type T back (remember this applies only to Scala and Java): 获取RDD的最简单方法之一是从现有的DataFrame或Dataset中获取。将它们转换为RDD很简单：只需对任何这些数据类型使用 rdd 方法。您会注意到，如果您进行了从 Dataset[T] 到 RDD 的转换，则会获得适当的本地类型T（请记住，这仅适用于 Scala 和 Java）： 12// in Scala: converts a Dataset[Long] to RDD[Long]spark.range(500).rdd Because Python doesn’t have Datasets—it has only DataFrames—you will get an RDD of type Row: 由于 Python 没有 Datasets——它只有DataFrames——您将获得Row类型的RDD： 12# in Pythonspark.range(10).rdd To operate on this data, you will need to convert this Row object to the correct data type or extract values out of it, as shown in the example that follows. This is now an RDD of type Row: 要对该数据进行操作，您将需要将该Row对象转换为正确的数据类型或从中提取值，如以下示例所示。现在是Row类型的RDD： 12// in Scalaspark.range(10).toDF().rdd.map(rowObject =&gt; rowObject.getLong(0)) 12# in Pythonspark.range(10).toDF("id").rdd.map(lambda row: row[0]) You can use the same methodology to create a DataFrame or Dataset from an RDD. All you need to do is call the toDF method on the RDD: 您可以使用相同的方法从RDD创建DataFrame或Dataset。您需要做的就是在RDD上调用toDF方法： 12// in Scalaspark.range(10).rdd.toDF() 12# in Pythonspark.range(10).rdd.toDF() This command creates an RDD of type Row. This row is the internal Catalyst format that Spark uses to represent data in the Structured APIs. This functionality makes it possible for you to jump between the Structured and low-level APIs as it suits your use case. (We talk about this in Chapter 13.) 该命令创建Row类型的RDD。此行是Spark用来表示Structured API中的数据的内部Catalyst格式。此功能使您可以在适合您的用例的情况下在结构化API和底层API之间进行切换。（我们将在第13章中讨论这一点。） The RDD API will feel quite similar to the Dataset API in Chapter 11 because they are extremely similar to each other (RDDs being a lower-level representation of Datasets) that do not have a lot of the convenient functionality and interfaces that the Structured APIs do. RDD API与第11章中的Dataset API非常相似，因为它们彼此非常相似（RDD 是 Datasets 的底层表示），并且没有结构化API所具有的许多便利功能和接口。 From a Local CollectionTo create an RDD from a collection, you will need to use the parallelize method on a SparkContext (within a SparkSession). This turns a single node collection into a parallel collection. 要从集合创建RDD，您将需要在 SparkContext 上（在 SparkSession 中）使用 parallelize 方法。这会将单个节点集合变成并行集合。 When creating this parallel collection, you can also explicitly state the number of partitions into which you would like to distribute this array. In this case, we are creating two partitions: 创建此并行集合时，您还可以明确声明要将此数组分发到的分区数。在这种情况下，我们将创建两个分区： 123// in Scalaval myCollection = "Spark The Definitive Guide : Big Data Processing Made Simple".split(" ")val words = spark.sparkContext.parallelize(myCollection, 2) 1234# in PythonmyCollection = "Spark The Definitive Guide : Big Data Processing Made Simple"\.split(" ")words = spark.sparkContext.parallelize(myCollection, 2) An additional feature is that you can then name this RDD to show up in the Spark UI according to a given name: 另一个功能是，您可以根据给定的名称将该RDD命名为显示在Spark UI中： 123// in Scalawords.setName("myWords")words.name // myWords 123# in Pythonwords.setName("myWords")words.name() # myWords From Data SourcesAlthough you can create RDDs from data sources or text files, it’s often preferable to use the Data Source APIs. RDDs do not have a notion of “Data Source APIs” like DataFrames do; they primarily define their dependency structures and lists of partitions. The Data Source API that we saw in Chapter 9 is almost always a better way to read in data. That being said, you can also read data as RDDs using sparkContext. For example, let’s read a text file line by line: 尽管您可以从数据源或文本文件创建RDD，但通常最好使用数据源API。RDD不像DataFrames那样具有“数据源API”的概念。它们主要定义其依赖关系结构和分区列表。我们在第9章中看到的数据源API几乎总是一种读取数据的更好方法。话虽如此，您也可以使用 sparkContext 将数据读取为RDD。例如，让我们逐行阅读一个文本文件： 1spark.sparkContext.textFile("/some/path/withTextFiles") This creates an RDD for which each record in the RDD represents a line in that text file or files. Alternatively, you can read in data for which each text file should become a single record. The use case here would be where each file is a file that consists of a large JSON object or some document that you will operate on as an individual: 这将创建一个RDD，RDD中的每个记录都代表该文本文件中的一行。或者，您可以读取每个文本文件应成为单个记录的数据。这里的用例是，每个文件都是一个由大型JSON对象或您将单独处理的文档组成的文件： 1spark.sparkContext.wholeTextFiles("/some/path/withTextFiles") In this RDD, the name of the file is the first object and the value of the text file is the second string object. 在此RDD中，文件名是第一个对象，文本文件的值是第二个字符串对象。 Manipulating RDDsYou manipulate RDDs in much the same way that you manipulate DataFrames. As mentioned, the core difference being that you manipulate raw Java or Scala objects instead of Spark types. There is also a dearth of “helper” methods or functions that you can draw upon to simplify calculations. Rather, you must define each filter, map functions, aggregation, and any other manipulation that you want as a function. 处理RDD的方式与处理DataFrames的方式几乎相同。如前所述，核心区别在于您可以操纵原始Java或Scala对象而不是Spark类型。缺少用于简化计算的“辅助”方法或函数，您必须定义每个过滤器，映射函数，聚合以及要作为函数进行的任何其他操作。 To demonstrate some data manipulation, let’s use the simple RDD (words) we created previously to define some more details. 为了演示一些数据操作，让我们使用之前创建的简单RDD（单词）来定义更多细节。 TransformationsFor the most part, many transformations mirror the functionality that you find in the Structured APIs. Just as you do with DataFrames and Datasets, you specify transformations on one RDD to create another. In doing so, we define an RDD as a dependency to another along with some manipulation of the data contained in that RDD. 在大多数情况下，许多转换都反映了您在结构化API中找到的功能。就像使用DataFrames和Datasets一样，您可以在一个RDD上指定转换以创建另一个。为此，我们将RDD定义为对另一个的依赖，并对该RDD中包含的数据进行一些操作。 distinctA distinct method call on an RDD removes duplicates from the RDD: 在RDD上进行不同的方法调用可从RDD中删除重复项： 1words.distinct().count() This gives a result of 10. 结果为10。 filterFiltering is equivalent to creating a SQL-like where clause. You can look through our records in the RDD and see which ones match some predicate function. This function just needs to return a Boolean type to be used as a filter function. The input should be whatever your given row is. In this next example, we filter the RDD to keep only the words that begin with the letter “S”: 过滤等效于创建类似SQL的where子句。您可以在RDD中浏览我们的记录，看看哪些与某些谓词函数匹配。该函数只需要返回一个布尔类型即可用作过滤器函数。输入应为您给定的行。在下一个示例中，我们对RDD进行过滤，以仅保留以字母“ S”开头的单词： 1234// in Scaladef startsWithS(individual:String) = &#123; individual.startsWith("S")&#125; 123# in Pythondef startsWithS(individual):return individual.startswith("S") Now that we defined the function, let’s filter the data. This should feel quite familiar if you read Chapter 11 because we simply use a function that operates record by record in the RDD. The function is defined to work on each record in the RDD individually: 现在我们定义了函数，让我们过滤数据。如果您阅读第11章，应该会感到非常熟悉，因为我们仅使用了一个函数来操作RDD中的记录。该函数被定义为分别在RDD中的每个记录上工作： 12// in Scalawords.filter(word =&gt; startsWithS(word)).collect() 12# in Pythonwords.filter(lambda word: startsWithS(word)).collect() This gives a result of Spark and Simple. We can see, like the Dataset API, that this returns native types. That is because we never coerce our data into type Row, nor do we need to convert the data after collecting it. 这给出了Spark和Simple的结果。我们可以看到，就像Dataset API一样，这将返回本地类型。那是因为我们从不将数据强制转换为Row类型，也不需要在收集数据后转换数据。 mapMapping is again the same operation that you can read about in Chapter 11. You specify a function that returns the value that you want, given the correct input. You then apply that, record by record. Let’s perform something similar to what we just did. In this example, we’ll map the current word to the word, its starting letter, and whether the word begins with “S.” 映射同样是您在第11章中可以了解的相同操作。给定正确的输入，您可以指定一个函数，该函数返回所需的值。然后，您将其应用，逐条记录。让我们执行与我们刚做的类似的事情。在此示例中，我们将当前单词映射到该单词，其起始字母以及该单词是否以 “S” 开头。 Notice in this instance that we define our functions completely inline using the relevant lambda syntax: 注意在这种情况下，我们使用相关的lambda语法完全内联定义了我们的函数： 12// in Scalaval words2 = words.map(word =&gt; (word, word(0), word.startsWith("S"))) 12# in Pythonwords2 = words.map(lambda word: (word, word[0], word.startswith("S"))) You can subsequently filter on this by selecting the relevant Boolean value in a new function: 随后，您可以通过在新函数中选择相关的布尔值来对此进行过滤： 12// in Scalawords2.filter(record =&gt; record._3).take(5) 12# in Pythonwords2.filter(lambda record: record[2]).take(5) This returns a tuple of “Spark,” “S,” and “true,” as well as “Simple,” “S,” and “True.” 这将返回“ Spark”，“ S”和“ true”以及“ Simple”，“ S”和“ True”的元组。 flatMapflatMap provides a simple extension of the map function we just looked at. Sometimes, each current row should return multiple rows, instead. For example, you might want to take your set of words and flatMap it into a set of characters. Because each word has multiple characters, you should use flatMap to expand it. flatMap requires that the ouput of the map function be an iterable that can be expanded: flatMap提供了我们刚刚看过的map函数的简单扩展。有时，每个当前行应该返回多个行。例如，您可能想将一组单词和 flatMap 转换成一组字符。由于每个单词都有多个字符，因此应使用 flatMap 对其进行扩展。flatMap 要求map函数的输出是可迭代的，可以扩展： 12// in Scalawords.flatMap(word =&gt; word.toSeq).take(5) 12# in Pythonwords.flatMap(lambda word: list(word)).take(5) This yields S, P, A, R, K. 这产生S，P，A，R，K。 sortTo sort an RDD you must use the sortBy method, and just like any other RDD operation, you do this by specifying a function to extract a value from the objects in your RDDs and then sort based on that. For instance, the following example sorts by word length from longest to shortest: 要对RDD进行排序，必须使用 sortBy 方法，就像其他任何RDD操作一样，您可以通过指定一个函数来从RDD中的对象中提取值，然后基于该函数进行排序。例如，以下示例按单词长度从最长到最短排序： 12// in Scalawords.sortBy(word =&gt; word.length() * -1).take(2) 12# in Pythonwords.sortBy(lambda word: len(word) * -1).take(2) Random SplitsWe can also randomly split an RDD into an Array of RDDs by using the randomSplit method, which accepts an Array of weights and a random seed: 我们还可以使用 randomSplit 方法将RDD随机分为RDD数组，该方法接受权重数组和随机种子： 12// in Scalaval fiftyFiftySplit = words.randomSplit(Array[Double](0.5, 0.5)) 12# in PythonfiftyFiftySplit = words.randomSplit([0.5, 0.5]) This returns an array of RDDs that you can manipulate individually. 这将返回可以单独操作的RDD数组。 ActionsJust as we do with DataFrames and Datasets, we specify actions to kick off our specified transformations. Actions either collect data to the driver or write to an external data source. 就像处理DataFrames和Datasets一样，我们指定action（动作/算子）来启动我们指定的转换。action（动作/算子）要么将数据收集到驱动程序，要么写入外部数据源。 reduceYou can use the reduce method to specify a function to “reduce” an RDD of any kind of value to one value. For instance, given a set of numbers, you can reduce this to its sum by specifying a function that takes as input two values and reduces them into one. If you have experience in functional programming, this should not be a new concept: 您可以使用reduce方法来指定一个函数，以将任何类型的RDD“reduce”为一个值。例如，给定一组数字，您可以通过指定一个将两个值作为输入并减小为一个的函数来将其减少为总和。如果您具有函数式编程的经验，那么这不是一个新概念： 12// in Scalaspark.sparkContext.parallelize(1 to 20).reduce(_ + _) // 210 12# in Pythonspark.sparkContext.parallelize(range(1, 21)).reduce(lambda x, y: x + y) # 210 You can also use this to get something like the longest word in our set of words that we defined a moment ago. The key is just to define the correct function: 您也可以使用它来获得类似我们刚才定义的单词集中最长的单词。关键只是定义正确的功能： 123456789// in Scaladef wordLengthReducer(leftWord:String, rightWord:String): String = &#123; if (leftWord.length &gt; rightWord.length) return leftWord else return rightWord&#125; words.reduce(wordLengthReducer) 12345678# in Pythondef wordLengthReducer(leftWord, rightWord): if len(leftWord) &gt; len(rightWord): return leftWord else: return rightWordwords.reduce(wordLengthReducer) This reducer is a good example because you can get one of two outputs. Because the reduce operation on the partitions is not deterministic, you can have either “definitive” or “processing” (both of length 10) as the “left” word. This means that sometimes you can end up with one, whereas other times you end up with the other. 这个reducer是一个很好的例子，因为您可以获得两个输出之一。由于对分区的reduce操作不是确定性的，因此可以将“definitive”或“processing”（长度均为10）作为“左”字。这意味着有时候您可以以一个结局，而其他时候则以另一个结局。 countThis method is fairly self-explanatory. Using it, you could, for example, count the number of rows in the RDD: 这种方法是不言自明的。使用它，例如，您可以计算RDD中的行数： 1words.count() countApproxEven though the return signature for this type is a bit strange, it’s quite sophisticated. This is an approximation of the count method we just looked at, but it must execute within a timeout (and can return incomplete results if it exceeds the timeout). 即使此类型的返回签名有些奇怪，也相当复杂。这是我们刚刚看过的count方法的近似值，但是它必须在超时内执行（如果超过超时，则可能返回不完整的结果）。 The confidence is the probability that the error bounds of the result will contain the true value. That is, if countApprox were called repeatedly with confidence 0.9, we would expect 90% of the results to contain the true count. The confidence must be in the range [0,1], or an exception will be thrown: 置信度是结果的误差范围包含真实值的概率。也就是说，如果以0.9的置信度重复调用 countApprox，则我们期望90％的结果包含真实计数。置信度必须在[0,1]范围内，否则将引发异常： 123val confidence = 0.95val timeoutMilliseconds = 400words.countApprox(timeoutMilliseconds, confidence) countApproxDistinctThere are two implementations of this, both based on streamlib’s implementation of “HyperLogLog in Practice: Algorithmic Engineering of a State-of-the-Art Cardinality Estimation Algorithm.” 此方法有两种实现，均基于 streamlib 的“HyperLogLog inPractice: Algorithmic Engineering of a State-of-the-Art Cardinality Estimation Algorithm” 的实现。 In the first implementation, the argument we pass into the function is the relative accuracy. Smaller values create counters that require more space. The value must be greater than 0.000017: 在第一种实现中，我们传递给函数的参数是相对精度。较小的值会创建需要更多空间的计数器。该值必须大于0.000017： 1words.countApproxDistinct(0.05) With the other implementation you have a bit more control; you specify the relative accuracy based on two parameters: one for “regular” data and another for a sparse representation. 使用其他实现，您可以控制更多。您可以根据两个参数指定相对精度：一个用于“常规”数据，另一个用于稀疏表示。 The two arguments are p and sp where p is precision and sp is sparse precision. The relative accuracy is approximately 1.054 / sqrt(2 ). Setting a nonzero (sp &gt; p) can reduce the memory consumption and increase accuracy when the cardinality is small. Both values are integers: 两个参数是p和sp，其中p是精度，而sp是稀疏精度。相对精度约为 1.054/sqrt(2) 。当基数较小时，将非零值设置为 (sp&gt; p)可以减少内存消耗并提高准确性。这两个值都是整数： 1words.countApproxDistinct(4, 10) countByValueThis method counts the number of values in a given RDD. However, it does so by finally loading the result set into the memory of the driver. You should use this method only if the resulting map is expected to be small because the entire thing is loaded into the driver’s memory. Thus, this method makes sense only in a scenario in which either the total number of rows is low or the number of distinct items is low: 此方法计算给定RDD中值的数量。但是，它是通过将结果集最终加载到驱动程序的内存中来实现的。仅在预期生成的 map较小的情况下才应使用此方法，因为整个 map 都已加载到驱动程序的内存中。因此，此方法仅在行总数少或不同项目数少的情况下才有意义： 1words.countByValue() countByValueApproxThis does the same thing as the previous function, but it does so as an approximation. This must execute within the specified timeout (first parameter) (and can return incomplete results if it exceeds the timeout). 该功能与先前的功能相同，但仅作为近似值。此操作必须在指定的超时（第一个参数）内执行（如果超过超时，则可能返回不完整的结果）。 The confidence is the probability that the error bounds of the result will contain the true value. That is, if countApprox were called repeatedly with confidence 0.9, we would expect 90% of the results to contain the true count. The confidence must be in the range [0,1], or an exception will be thrown: 置信度是结果的误差范围包含真实值的概率。也就是说，如果以0.9的置信度重复调用countApprox，则我们期望90％的结果包含真实计数。置信度必须在[0,1]范围内，否则将引发异常： 1words.countByValueApprox(1000, 0.95) firstThe first method returns the first value in the dataset: 第一个方法返回数据集中的第一个值： 1words.first() max and minmax and min return the maximum values and minimum values, respectively: max和min分别返回最大值和最小值： 12spark.sparkContext.parallelize(1 to 20).max()spark.sparkContext.parallelize(1 to 20).min() taketake and its derivative methods take a number of values from your RDD. This works by first scanning one partition and then using the results from that partition to estimate the number of additional partitions needed to satisfy the limit. take及其派生方法从RDD中获取许多值。该方法是这样工作的：通过首先扫描一个分区，然后使用该分区的结果来估计满足该限制（“限制”指的是方法参数指定的值）所需的其他分区的数量。 There are many variations on this function, such as takeOrdered, takeSample, and top. You can use takeSample to specify a fixed-size random sample from your RDD. You can specify whether this should be done by using withReplacement, the number of values, as well as the random seed. top is effectively the opposite of takeOrdered in that it selects the top values according to the implicit ordering: 此函数有很多变体，例如takeOrdered，takeSample和top。您可以使用takeSample从RDD中指定一个固定大小的随机样本。您可以使用withReplacement，值的数量以及随机种子来指定是否应该这样做。top 实际上与takeOrdered相反，它根据隐式顺序选择顶部值： 123456789words.take(5)words.takeOrdered(5)words.top(5)val withReplacement = trueval numberToTake = 6val randomSeed = 100Lwords.takeSample(withReplacement, numberToTake, randomSeed) Saving FilesSaving files means writing to plain-text files. With RDDs, you cannot actually “save” to a data source in the conventional sense. You must iterate over the partitions in order to save the contents of each partition to some external database. This is a low-level approach that reveals the underlying operation that is being performed in the higher-level APIs. Spark will take each partition, and write that out to the destination. 保存文件意味着写入纯文本文件。使用RDD，您实际上无法按照传统意义上的“保存”到数据源。您必须遍历分区才能将每个分区的内容保存到某个外部数据库。这是一种低层方法，它揭示了高层API中正在执行的基础操作。Spark将获取每个分区，并将其写出到目标位置。 saveAsTextFileTo save to a text file, you just specify a path and optionally a compression codec: 要保存到文本文件，只需指定路径和压缩编解码器即可： 1words.saveAsTextFile("file:/tmp/bookTitle") To set a compression codec, we must import the proper codec from Hadoop. You can find these in the org.apache.hadoop.io.compress library: 要设置压缩编解码器，我们必须从Hadoop导入正确的编解码器。您可以在 org.apache.hadoop.io.compress 库中找到这些： 123// in Scalaimport org.apache.hadoop.io.compress.BZip2Codecwords.saveAsTextFile("file:/tmp/bookTitleCompressed", classOf[BZip2Codec]) SequenceFilesSpark originally grew out of the Hadoop ecosystem, so it has a fairly tight integration with a variety of Hadoop tools. A sequenceFile is a flat file consisting of binary key–value pairs. It is extensively used in MapReduce as input/output formats. Spark最初起源于Hadoop生态系统，因此与各种Hadoop工具紧密集成。sequenceFile 是一个扁平结构的文件（flat file），由二进制键值对组成。它在MapReduce中广泛用作输入/输出格式。 译者附a flat file : A file consisting of records of a single record type in which there is no embedded structure information that governs relationships between records. 扁平结构的文件：由单一记录类型的记录组成的文件，其中没有控制记录之间关系的嵌入式结构信息。 Spark can write to sequenceFiles using the saveAsObjectFile method or by explicitly writing key–value pairs, as described in Chapter 13: 如第13章所述，Spark可以使用 saveAsObjectFile 方法或通过显式编写键值对来写入 sequenceFiles： 1words.saveAsObjectFile("/tmp/my/sequenceFilePath") Hadoop FilesThere are a variety of different Hadoop file formats to which you can save. These allow you to specify classes, output formats, Hadoop configurations, and compression schemes. (For information on these formats, read Hadoop: The Definitive Guide [O’Reilly, 2015].) These formats are largely irrelevant except if you’re working deeply in the Hadoop ecosystem or with some legacy mapReduce jobs. 您可以保存多种不同的Hadoop文件格式。这些允许您指定类，输出格式，Hadoop配置和压缩方案。（有关这些格式的信息，请阅读 O’Reilly 2015年出版的《Hadoop权威指南》这些格式在很大程度上无关紧要，除非您正在Hadoop生态系统中深入工作或使用一些旧的 mapReduce 作业。 CachingThe same principles apply for caching RDDs as for DataFrames and Datasets. You can either cache or persist an RDD. By default, cache and persist only handle data in memory. We can name it if we use the setName function that we referenced previously in this chapter: 缓存RDD的原理与DataFrame和Dataset的原理相同。您可以缓存或保留RDD。默认情况下，缓存和持久性仅处理内存中的数据。如果使用本章前面引用的setName函数，则可以为它命名： 1words.cache() We can specify a storage level as any of the storage levels in the singleton object: org.apache.spark.storage.StorageLevel, which are combinations of memory only; disk only; and separately, off heap. 我们可以将存储级别指定为单例对象中的任何存储级别：org.apache.spark.storage.StorageLevel，它们是仅在内存，仅在磁盘以及内存和磁盘的组合存储。 We can subsequently query for this storage level (we talk about storage levels when we discuss persistence in Chapter 20): 随后，我们可以查询该存储级别（在第20章中讨论持久性时，我们将讨论存储级别）： 12// in Scalawords.getStorageLevel 12# in Pythonwords.getStorageLevel() CheckpointingOne feature not available in the DataFrame API is the concept of checkpointing. Checkpointing is the act of saving an RDD to disk so that future references to this RDD point to those intermediate partitions on disk rather than recomputing the RDD from its original source. This is similar to caching except that it’s not stored in memory, only disk. This can be helpful when performing iterative computation, similar to the use cases for caching: DataFrame API中不可用的一项功能是检查点的概念。检查点是将RDD保存到磁盘的行为，以便将来对该RDD的引用指向磁盘上的那些中间分区，而不是从其原始源重新计算RDD。除了不存储在内存中，仅存储在磁盘中，这与缓存相似。这在执行迭代计算时可能会有所帮助，类似于缓存的用例： 12spark.sparkContext.setCheckpointDir("/some/path/for/checkpointing")words.checkpoint() Now, when we reference this RDD, it will derive from the checkpoint instead of the source data. This can be a helpful optimization. 现在，当我们引用此RDD时，它将从检查点而不是源数据派生。这可能是有用的优化。 Pipe RDDs to System CommandsThe pipe method is probably one of Spark’s more interesting methods. With pipe, you can return an RDD created by piping elements to a forked external process. The resulting RDD is computed by executing the given process once per partition. All elements of each input partition are written to a process’s stdin as lines of input separated by a newline. The resulting partition consists of the process’s stdout output, with each line of stdout resulting in one element of the output partition. A process is invoked even for empty partitions. 管道方法可能是Spark更有趣的方法之一。使用管道，可以将通过将元素传递到分叉的外部过程来创建的RDD。通过对每个分区执行一次给定的过程来计算得出的RDD。每个输入分区的所有元素都以换行符分隔的形式输入到进程的stdin中。结果分区由进程的 stdout 输出组成，每行 stdout产生输出分区的一个元素。甚至为空分区调用一个进程。 The print behavior can be customized by providing two functions. 可以通过提供两个函数来自定义打印行为。 We can use a simple example and pipe each partition to the command wc. Each row will be passed in as a new line, so if we perform a line count, we will get the number of lines, one per partition: 我们可以使用一个简单的示例，并将每个分区通过管道传递给命令wc。每行将作为新行传递，因此，如果执行行计数，我们将获得行数，每个分区一个： 1words.pipe("wc -l").collect() In this case, we got five lines per partition. 在这种情况下，每个分区有五行。 mapPartitionsThe previous command revealed that Spark operates on a per-partition basis when it comes to actually executing code. You also might have noticed earlier that the return signature of a map function on an RDD is actually MapPartitionsRDD. This is because map is just a row-wise alias for mapPartitions, which makes it possible for you to map an individual partition (represented as an iterator). That’s because physically on the cluster we operate on each partition individually (and not a specific row). A simple example creates the value “1” for every partition in our data, and the sum of the following expression will count the number of partitions we have: 上一条命令显示，Spark在实际执行代码时会按分区运行。您之前可能还已经注意到，RDD上的映射函数的返回签名实际上是 MapPartitionsRDD。这是因为map只是 mapPartitions 的行别名，这使您可以映射单个分区（表示为迭代器）。这是因为从物理上讲，我们在集群上分别对每个分区（而不是特定的行）进行操作。一个简单的示例：为数据中的每个分区创建值“ 1”，以下表达式的总和将计算我们拥有的分区数： 12// in Scalawords.mapPartitions(part =&gt; Iterator[Int](1)).sum() // 2 12# in Pythonwords.mapPartitions(lambda part: [1]).sum() # 2 Naturally, this means that we operate on a per-partition basis and allows us to perform an operation on that entire partition. This is valuable for performing something on an entire subdataset of your RDD. You can gather all values of a partition class or group into one partition and then operate on that entire group using arbitrary functions and controls. An example use case of this would be that you could pipe this through some custom machine learning algorithm and train an individual model for that company’s portion of the dataset. A Facebook engineer has an interesting demonstration of their particular implementation of the pipe operator with a similar use case demonstrated at Spark Summit East 2017. 自然地，这意味着我们在每个分区的基础上进行操作，并允许我们在整个分区上执行操作。这对于在RDD的整个子数据集上执行某些操作非常有用。您可以将一个分区类或组的所有值收集到一个分区中，然后使用任意函数和控制（动作和转换）对该整个组进行操作。一个示例是，您可以通过一些自定义的机器学习算法对此进行处理，并为该公司的数据集部分训练一个单独的模型。一位Facebook工程师通过在Spark Spark East 2017上展示了一个类似的用例，有趣地演示了他们对管道算子的特定实现。 Other functions similar to mapPartitions include mapPartitionsWithIndex. With this you specify a function that accepts an index (within the partition) and an iterator that goes through all items within the partition. The partition index is the partition number in your RDD, which identifies where each record in our dataset sits (and potentially allows you to debug). You might use this to test whether your map functions are behaving correctly: 其他类似于 mapPartitions 的功能包括 mapPartitionsWithIndex。使用此功能，您可以指定一个接受索引（在分区内）和一个迭代器的函数，该迭代器遍历该分区内的所有项。分区索引是RDD中的分区号，它标识数据集中每个记录的位置（并可能允许您调试）。您可以使用它来测试您的 map 函数是否行为正确： 123456// in Scaladef indexedFunc(partitionIndex:Int, withinPartIterator: Iterator[String]) = &#123; withinPartIterator.toList.map(value =&gt; s"Partition: $partitionIndex =&gt; $value").iterator&#125; words.mapPartitionsWithIndex(indexedFunc).collect() 1234# in Pythondef indexedFunc(partitionIndex, withinPartIterator): return ["partition: &#123;&#125; =&gt; &#123;&#125;".format(partitionIndex, x) for x in withinPartIterator]words.mapPartitionsWithIndex(indexedFunc).collect() 1Array[String] = Array(Partition: 0 =&gt; Spark, Partition: 0 =&gt; The, Partition: 0 =&gt; Definitive, Partition: 0 =&gt; Guide, Partition: 0 =&gt; :, Partition: 1 =&gt; Big, Partition: 1 =&gt; Data, Partition: 1 =&gt; Processing, Partition: 1 =&gt; Made, Partition: 1 =&gt; Simple) foreachPartitionAlthough mapPartitions needs a return value to work properly, this next function does not. foreachPartition simply iterates over all the partitions of the data. The difference is that the function has no return value. This makes it great for doing something with each partition like writing it out to a database. In fact, this is how many data source connectors are written. You can create our own text file source if you want by specifying outputs to the temp directory with a random ID: 尽管 mapPartitions 需要一个返回值才能正常工作，但是下一个函数不需要。foreachPartition 只是简单地遍历数据的所有分区。区别在于该函数没有返回值。这非常适合对每个分区执行操作，例如将其写到数据库中。实际上，这就是写入的数据源连接器数量。如果需要，可以通过使用随机ID将输出指定到temp目录来创建自己的文本文件源： 12345678910words.foreachPartition &#123; iter =&gt; import java.io._ import scala.util.Random val randomFileName = new Random().nextInt() val pw = new PrintWriter(new File(s"/tmp/random-file-$&#123;randomFileName&#125;.txt")) while (iter.hasNext) &#123; pw.write(iter.next()) &#125; pw.close()&#125; You’ll find these two files if you scan your /tmp directory. 如果您扫描 /tmp 目录，则会找到这两个文件。 glomglom is an interesting function that takes every partition in your dataset and converts them to arrays. This can be useful if you’re going to collect the data to the driver and want to have an array for each partition. However, this can cause serious stability issues because if you have large partitions or a large number of partitions, it’s simple to crash the driver. glom是一个有趣的函数，它获取数据集中的每个分区并将其转换为数组。如果您要将数据收集到驱动程序，并希望每个分区都有一个数组，这将很有用。但是，这可能会导致严重的稳定性问题，因为如果您具有较大的分区或大量的分区，则很容易使驱动程序崩溃。 In the following example, you can see that we get two partitions and each word falls into one partition each: 在下面的示例中，您可以看到我们得到两个分区，每个单词都落入一个分区： 123# in Scalaspark.sparkContext.parallelize(Seq("Hello", "World"), 2).glom().collect()// Array(Array(Hello), Array(World)) 123# in Pythonspark.sparkContext.parallelize(["Hello", "World"], 2).glom().collect()# [['Hello'], ['World']] ConclusionIn this chapter, you saw the basics of the RDD APIs, including single RDD manipulation. Chapter 13 touches on more advanced RDD concepts, such as joins and key-value RDDs. 在本章中，您了解了RDD API的基础知识，包括单个RDD操作。第13章介绍了更高层的RDD概念，例如连接接和键值RDD。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 11 Datasets]]></title>
    <url>%2F2019%2F11%2F07%2FChapter11_DataSets(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 11 Datasets 译者：https://snaildove.github.io Datasets are the foundational type of the Structured APIs. We already worked with DataFrames, which are Datasets of type Row, and are available across Spark’s different languages. Datasets are a strictly Java Virtual Machine (JVM) language feature that work only with Scala and Java. Using Datasets, you can define the object that each row in your Dataset will consist of. In Scala, this will be a case class object that essentially defines a schema that you can use, and in Java, you will define a Java Bean. Experienced users often refer to Datasets as the “typed set of APIs” in Spark. For more information, see Chapter 4. Dataset 是结构化 API 的基本类型。我们已经使用了 DataFrames，它们是 Row 类型的 Dataset，可在Spark的不同语言中使用。Dataset 是严格的 Java 虚拟机（JVM）语言特性（feature），只能使用 Scala 和 Java。使用 Dataset，您可以定义 Dataset 中每一行将组成的对象。在 Scala 中，这将是一个 case 类对象，该对象本质上定义了可以使用的模式，而在Java中，您将定义 Java Bean。有经验的用户通常将 Dataset 称为Spark中的 “API的类型集”。有关更多信息，请参见第4章。 In Chapter 4, we discussed that Spark has types like StringType, BigIntType, StructType, and so on. Those Spark-specific types map to types available in each of Spark’s languages like String, Integer, and Double. When you use the DataFrame API, you do not create strings or integers, but Spark manipulates the data for you by manipulating the Row object. In fact, if you use Scala or Java, all “DataFrames” are actually Datasets of type Row. To efficiently support domain-specific objects, a special concept called an “Encoder” is required. The encoder maps the domain-specific type T to Spark’s internal type system. 在第4章中，我们讨论了Spark具有StringType，BigIntType，StructType等类型。这些特定于Spark的类型映射到每种Spark语言中可用的类型，例如String，Integer和Double。使用DataFrame API时，您不会创建字符串或整数，但是Spark通过操纵Row对象为您操纵数据。实际上，如果使用Scala或Java，则所有 “DataFrame” 实际上都是Row类型的Dataset。为了有效地支持特定于域的对象，需要一个称为“编码器”的特殊概念。编码器将特定于域的类型T映射到Spark的内部类型系统。 For example, given a class Person with two fields, name (string) and age (int), an encoder directs Spark to generate code at runtime to serialize the Person object into a binary structure. When using DataFrames or the “standard” Structured APIs, this binary structure will be a Row. When we want to create our own domain-specific objects, we specify a case class in Scala or a JavaBean in Java. Spark will allow us to manipulate this object (in place of a Row) in a distributed manner. 例如，给定Person类具有 name (string) 和 age (int) 两个字段，编码器指示Spark在运行时生成代码以将Person对象序列化为二进制结构。当使用 DataFrame 或“标准”结构化API时，此二进制结构将为行。当我们要创建自己的特定于域的对象时，我们在Scala中指定一个案例类，在Java中指定一个JavaBean。Spark将允许我们以分布式方式操纵该对象（代替Row）。 When you use the Dataset API, for every row it touches, this domain specifies type, Spark converts the Spark Row format to the object you specified (a case class or Java class). This conversion slows down your operations but can provide more flexibility. You will notice a hit in performance but this is a far different order of magnitude from what you might see from something like a user-defined function (UDF) in Python, because the performance costs are not as extreme as switching programming languages, but it is an important thing to keep in mind. 使用 Dataset API时，该域为它遇到的每一行指定类型，Spark将Spark Row格式转换为您指定的对象（案例类或Java类）。这种转换会减慢您的操作速度，但可以提供更大的灵活性。您会注意到性能受到了影响，但这与您在Python中的用户定义函数（UDF）之类的看到的结果数量级相差很大，因为性能成本并不像切换编程语言那样极端，但是是一件重要的事情要牢记。 When to Use DatasetsYou might ponder, if I am going to pay a performance penalty when I use Datasets, why should I use them at all? If we had to condense this down into a canonical list, here are a couple of reasons: 您可能会思考，如果我在使用 Dataset 时要付出性能损失，那为什么还要使用它们呢？如果我们必须将其简化为规范列表，则有以下两个原因： When the operation(s) you would like to perform cannot be expressed using DataFrame manipulations. 当您要执行的操作无法使用DataFrame操作表示时。 When you want or need type-safety, and you’re willing to accept the cost of performance to achieve it。 当您想要或需要类型安全性时，您愿意接受性能成本来实现它。 Let’s explore these in more detail. There are some operations that cannot be expressed using the Structured APIs we have seen in the previous chapters. Although these are not particularly common, you might have a large set of business logic that you’d like to encode in one specific function instead of in SQL or DataFrames. This is an appropriate use for Datasets. Additionally, the Dataset API is type-safe. Operations that are not valid for their types, say subtracting two string types, will fail at compilation time not at runtime. If correctness and bulletproof code is your highest priority, at the cost of some performance, this can be a great choice for you. This does not protect you from malformed data but can allow you to more elegantly handle and organize it. 让我们更详细地探讨这些。有些操作无法使用我们在前几章中看到的结构化API来表达。尽管这些并不是特别常见，但是您可能想使用一个特定的功能而不是SQL或DataFrames进行编码的大量业务逻辑。这是 Datasets 的适当用法。此外，Dataset API是类型安全的。对于其类型无效的操作（例如减去两个字符串类型）将在编译时而不是在运行时失败。如果正确性和安全代码是您的最高优先级，而以牺牲性能为代价，那么这对于您来说是个不错的选择。这不能保护您免受格式错误的数据的侵害，但可以使您更优雅地处理和组织数据。 Another potential time for which you might want to use Datasets is when you would like to reuse a variety of transformations of entire rows between single-node workloads and Spark workloads. If you have some experience with Scala, you might notice that Spark’s APIs reflect those of Scala Sequence Types, but they operate in a distributed fashion. In fact, Martin Odersky, the inventor of Scala, said just that in 2015 at Spark Summit Europe. Due to this, one advantage of using Datasets is that if you define all of your data and transformations as accepting case classes it is trivial to reuse them for both distributed and local workloads. Additionally, when you collect your DataFrames to local disk, they will be of the correct class and type, sometimes making further manipulation easier. 您可能希望使用 Dataset 的另一个潜在时间是，您想在单节点工作负载和Spark工作负载之间重用整个行的各种转换时。如果您有使用Scala的经验，您可能会注意到Spark的API反映了Scala序列类型的API，但是它们以分布式方式运行。实际上，Scala的发明者马丁·奥德斯基（Martin Odersky）在2015年欧洲Spark峰会上就这样说过。因此，使用 Dataset 的一个优势是，如果您将所有数据和转换定义为接受案例类，那么将它们重用于分布式和本地工作负载就都很简单。另外，当您将DataFrame收集到本地磁盘时，它们将具有正确的类和类型，有时使进一步的操作变得容易。 Probably the most popular use case is to use DataFrames and Datasets in tandem, manually trading off between performance and type safety when it is most relevant for your workload. This might be at the end of a large, DataFrame-based extract, transform, and load (ETL) transformation when you’d like to collect data to the driver and manipulate it by using single-node libraries, or it might be at the beginning of a transformation when you need to perform per-row parsing before performing filtering and further manipulation in Spark SQL. 可能最流行的用例是串联使用DataFrame和Dataset，在与您的工作负载最相关的性能和类型安全之间进行手动权衡。当您想将数据收集到驱动程序（driver）并使用单节点库对其进行操作时，这可能是在大型的，基于DataFrame的提取，转换和加载（ETL）转换的结尾，或者可能在需要在Spark SQL中执行过滤和进一步处理之前需要执行每行解析的转换的开始。 Creating DatasetsCreating Datasets is somewhat of a manual operation, requiring you to know and define the schemas ahead of time. 创建数据集有些是手动操作，需要您提前了解和定义模式。 In Java: EncodersJava Encoders are fairly simple, you simply specify your class and then you’ll encode it when you come upon your DataFrame (which is of type Dataset&lt;Row&gt;): Java编码器非常简单，只需指定您的类，然后在遇到DataFrame（类型为 Dataset&lt;Row&gt; ）时对它进行编码： 1234567891011import org.apache.spark.sql.Encoders;public class Flight implements Serializable&#123; String DEST_COUNTRY_NAME; String ORIGIN_COUNTRY_NAME; Long DEST_COUNTRY_NAME;&#125;Dataset&lt;Flight&gt; flights = spark.read.parquet("/data/flight-data/parquet/2010-summary.parquet/").as(Encoders.bean(Flight.class)); In Scala: Case ClassesTo create Datasets in Scala, you define a Scala case class. A case class is a regular class that has the following characteristics: 要在Scala中创建数据集，您需要定义一个Scala案例类。案例类是具有以下特征的常规类： Immutable 不可更改 Decomposable through pattern matching 通过模式匹配可分解 Allows for comparison based on structure instead of reference 允许基于结构而不是引用进行比较 Easy to use and manipulate 易于使用和操作 These traits make it rather valuable for data analysis because it is quite easy to reason about a case class. Probably the most important feature is that case classes are immutable and allow for comparison by structure instead of value. 这些特征使其对于数据分析非常有价值，因为对于案例类进行推理非常容易。可能最重要的特征是案例类是不可变的，并允许按结构而不是值进行比较。 Here’s how the Scala documentation describes it: 以下是Scala文档的描述方式： immutability frees you from needing to keep track of where and when things are mutated. 不可变性使您无需跟踪发生突变的位置和时间 Comparison-by-value allows you to compare instances as if they were primitive values—no more uncertainty regarding whether instances of a class are compared by value or reference. 按值比较允许您将实例视为原始值进行比较——不再不确定是否通过值或引用比较类的实例。 Pattern matching simplifies branching logic, which leads to less bugs and more readable code. 模式匹配可简化分支逻辑，从而减少错误并提高可读性。 These advantages carry over to their usage within Spark, as well. 这些优势也可以延续到Spark中。 To begin creating a Dataset, let’s define a case class for one of our datasets: 要开始创建数据集，请为我们的一个数据集定义一个案例类： 12case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt) Now that we defined a case class, this will represent a single record in our dataset. More succinctly, we now have a Dataset of Flights. This doesn’t define any methods for us, simply the schema. When we read in our data, we’ll get a DataFrame. However, we simply use the as method to cast it to our specified row type: 现在我们定义了一个案例类，它将代表我们 dataset 中的一条记录。更简洁地说，我们现在有了一个航班数据集。这并没有为我们定义任何方法，仅是模式。读取数据后，我们将获得一个DataFrame。但是，我们仅使用as方法将其强制转换为指定的行类型： 12val flightsDF = spark.read.parquet("/data/flight-data/parquet/2010-summary.parquet/")val flights = flightsDF.as[Flight] ActionsEven though we can see the power of Datasets, what’s important to understand is that actions like collect, take, and count apply to whether we are using Datasets or DataFrames: 即使我们可以看到 Datasets 的强大能力，但重要的是要了解，诸如 collect, take, 和 count 的 action（动作，算子） 适用于我们使用的不管是 Datasets 还是 DataFrame： 1flights.show(2) 123456+-----------------+-------------------+-----+|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|+-----------------+-------------------+-----+| United States | Romania | 1 || United States | Ireland | 264 |+-----------------+-------------------+-----+ You’ll also notice that when we actually go to access one of the case classes, we don’t need to do any type coercion, we simply specify the named attribute of the case class and get back, not just the expected value but the expected type, as well: 您还会注意到，当我们实际上要访问其中一个案例类时，我们不需要执行任何类型强制转换，我们只需指定案例类已经命名的属性并获取，不仅返回期望值，还返回预期类型： 1flights.first.DEST_COUNTRY_NAME // United States TransformationsTransformations on Datasets are the same as those that we saw on DataFrames. Any transformation that you read about in this section is valid on a Dataset, and we encourage you to look through the specific sections on relevant aggregations or joins. Datasets 的转换与我们在 DataFrame 上看到的转换相同。您在本节中了解的任何转换都对 Dataset 有效，我们建议您仔细阅读有关聚合或连接的特定部分。 In addition to those transformations, Datasets allow us to specify more complex and strongly typed transformations than we could perform on DataFrames alone because we manipulate raw Java Virtual Machine (JVM) types. To illustrate this raw object manipulation, let’s filter the Dataset that you just created. 除了这些转换之外，数据集还允许我们指定比单独在 DataFrames 上执行的更复杂，类型更强的转换，因为我们可以处理原始的Java虚拟机（JVM）类型。为了说明这种原始对象的操作，让我们过滤刚刚创建的数据集。 Filtering Let’s look at a simple example by creating a simple function that accepts a Flight and returns a Boolean value that describes whether the origin and destination are the same. This is not a UDF (at least, in the way that Spark SQL defines UDF) but a generic function. 让我们看一个简单的例子，创建一个简单的函数，该函数接受一个Flight并返回一个布尔值，该值描述起点和终点是否相同。这不是UDF（至少以Spark SQL定义UDF的方式），而是通用函数。 TIP 提示You’ll notice in the following example that we’re going to create a function to define this filter. This is an important difference from what we have done thus far in the book. By specifying a function, we are forcing Spark to evaluate this function on every row in our Dataset. This can be very resource intensive. For simple filters it is always preferred to write SQL expressions. This will greatly reduce the cost of filtering out the data while still allowing you to manipulate it as a Dataset later on: 在以下示例中，您会注意到我们将创建一个函数来定义此过滤器。这与到目前为止我们在书中所做的是一个重要的区别。通过指定一个函数，我们迫使Spark在数据集中的每一行上计算这个函数。这可能会占用大量资源。对于简单的过滤器，总是首选编写SQL表达式。这将大大降低过滤数据的成本，同时仍允许您稍后将其作为 Dataset 进行操作： 123def originIsDestination(flight_row: Flight): Boolean = &#123; return flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME&#125; We can now pass this function into the filter method specifying that for each row it should verify that this function returns true and in the process will filter our Dataset down accordingly: 现在，我们可以将此函数传递到filter方法中，指定它应针对每一行验证该函数返回true，并在此过程中相应地过滤掉我们的数据集： 1flights.filter(flight_row =&gt; originIsDestination(flight_row)).first() The result is: 结果是： 1Flight = Flight(United States,United States,348113) As we saw earlier, this function does not need to execute in Spark code at all. Similar to our UDFs, we can use it and test it on data on our local machines before using it within Spark. 如我们先前所见，此功能根本不需要在Spark代码中执行。与我们的UDF类似，在Spark中使用它之前，我们可以使用它并在本地计算机上的数据上对其进行测试。 For example, this dataset is small enough for us to collect to the driver (as an Array of Flights) on which we can operate and perform the exact same filtering operation: 例如，此数据集足够小，我们可以收集给驱动程序（作为航班数组），在该驱动程序上我们可以进行操作并执行完全相同的过滤操作： 1flights.collect().filter(flight_row =&gt; originIsDestination(flight_row)) The result is: 结果是： 1Array[Flight] = Array(Flight(United States,United States,348113)) We can see that we get the exact same answer as before. 我们可以看到我们得到了与以前完全相同的答案。 MappingFiltering is a simple transformation, but sometimes you need to map one value to another value. We did this with our function in the previous example: it accepts a flight and returns a Boolean, but other times we might actually need to perform something more sophisticated like extract a value, compare a set of values, or something similar. 过滤是一个简单的转换，但是有时您需要将一个值映射到另一个值。我们在上一个示例中使用函数进行了此操作：它接受一个 flight 并返回一个布尔值，但是有时我们实际上可能需要执行更复杂的操作，例如提取值，比较一组值或类似操作。 The simplest example is manipulating our Dataset such that we extract one value from each row. This is effectively performing a DataFrame like select on our Dataset. Let’s extract the destination: 最简单的示例是处理 Dataset ，以便从每一行提取一个值。这实际上是在我们的 Dataset 上执行类似于select的DataFrame。让我们提取目的地： 1val destinations = flights.map(f =&gt; f.DEST_COUNTRY_NAME) Notice that we end up with a Dataset of type String. That is because Spark already knows the JVM type that this result should return and allows us to benefit from compile-time checking if, for some reason, it is invalid. 注意，我们最终得到的是String类型的 Dataset。这是因为Spark已经知道该结果应返回的JVM类型，并允许我们从编译时检查中受益（如果出于某种原而无效）。 We can collect this and get back an array of strings on the driver: 我们可以收集这些并获取驱动程序上的字符串数组： 1val localDestinations = destinations.take(5) This might feel trivial and unnecessary; we can do the majority of this right on DataFrames. We in fact recommend that you do this because you gain so many benefits from doing so. You will gain advantages like code generation that are simply not possible with arbitrary user-defined functions. However, this can come in handy with much more sophisticated row-by-row manipulation. 这可能是琐碎且不必要的。我们可以在DataFrames上行使大部分权利。实际上，我们建议您这样做，因为这样做会带来很多好处。您将获得诸如代码生成之类的优势，而这些优势是任意用户定义函数根本无法实现的。但是，这可以通过更复杂的逐行操作来派上用场。 Joins Joins, as we covered earlier, apply just the same as they did for DataFrames. However Datasets also provide a more sophisticated method, the joinWith method. joinWith is roughly equal to a co-group (in RDD terminology) and you basically end up with two nested Datasets inside of one. Each column represents one Dataset and these can be manipulated accordingly. This can be useful when you need to maintain more information in the join or perform some more sophisticated manipulation on the entire result, like an advanced map or filter. 如前所述，连接的应用方式与对 DataFrame 的应用方式相同。但是，数据集还提供了更复杂的方法 joinWith 方法。joinWith 大致等于一个 co-group（在RDD术语中），您基本上在一个内部拥有两个嵌套的数据集。每一列代表一个数据集，可以相应地对其进行操作。当您需要在连接中维护更多信息或对整个结果执行一些更复杂的操作（例如高级映射或过滤器）时，这将很有用。 Let’s create a fake flight metadata dataset to demonstrate joinWith: 我们创建一个假的航班元数据数据集来演示 joinWith： 12345678case class FlightMetadata(count: BigInt, randomData: BigInt)val flightsMeta = spark.range(500).map(x =&gt; (x, scala.util.Random.nextLong)).withColumnRenamed("_1", "count").withColumnRenamed("_2", "randomData").as[FlightMetadata]val flights2 = flights.joinWith(flightsMeta, flights.col("count") === flightsMeta.col("count")) Notice that we end up with a Dataset of a sort of key-value pair, in which each row represents a Flight and the Flight Metadata. We can, of course, query these as a Dataset or a DataFrame with complex types: 请注意，我们最后得到的是一种键值对的数据集，其中每一行代表一个Flight和Flight Metadata。当然，我们可以将它们查询为具有复杂类型的 Dataset 或 DataFrame： 1flights2.selectExpr("_1.DEST_COUNTRY_NAME") We can collect them just as we did before: 我们可以像以前一样收集它们： 123flights2.take(2)Array[(Flight, FlightMetadata)] = Array((Flight(United States,Romania,1),... 1val flights2 = flights.join(flightsMeta, Seq("count")) We can always define another Dataset to gain this back. It’s also important to note that there are no problems joining a DataFrame and a Dataset—we end up with the same result: 我们总是可以定义另一个数据集来获得回报。同样重要的是要注意，将DataFrame和Dataset连接起来没有问题——我们最终得到了相同的结果： 1val flights2 = flights.join(flightsMeta.toDF(), Seq("count")) Grouping and AggregationsGrouping and aggregations follow the same fundamental standards that we saw in the previous aggregation chapter, so groupBy rollup and cube still apply, but these return DataFrames instead of Datasets (you lose type information): 分组和聚合遵循在上一聚合章中看到的相同基本标准，因此 groupBy, rollup和 cube 仍然适用，但是它们返回DataFrames而不是Datasets（您会丢失类型信息）： 1flights.groupBy("DEST_COUNTRY_NAME").count() This often is not too big of a deal, but if you want to keep type information around there are other groupings and aggregations that you can perform. An excellent example is the groupByKey method. This allows you to group by a specific key in the Dataset and get a typed Dataset in return. This function, however, doesn’t accept a specific column name but rather a function. This makes it possible for you to specify more sophisticated grouping functions that are much more akin to something like this: 这通常没什么大不了的，但是如果您想保留类型信息，则可以执行其他分组和聚合。一个很好的例子是groupByKey方法。这使您可以按数据集中的特定键进行分组，并获取返回的类型化数据集。但是，此函数不接受特定的列名，而是接受一个函数。这使您可以指定更复杂的分组功能，这些功能类似于以下内容： 1flights.groupByKey(x =&gt; x.DEST_COUNTRY_NAME).count() Although this provides flexibility, it’s a trade-off because now we are introducing JVM types as well as functions that cannot be optimized by Spark. This means that you will see a performance difference and we can see this when we inspect the explain plan. In the following, you can see that we are effectively appending a new column to the DataFrame (the result of our function) an d then performing the grouping on that: 尽管这提供了灵活性，但是这是一个折衷，因为现在我们引入了JVM类型以及Spark无法优化的功能。这意味着您将看到性能差异，并且在检查解释计划时可以看到此差异。在下面的内容中，您可以看到我们正在有效地向DataFrame追加新列（我们函数的结果），然后对该分组执行分组： 1flights.groupByKey(x =&gt; x.DEST_COUNTRY_NAME).count().explain 12345678== Physical Plan ==*HashAggregate(keys=[value#1396], functions=[count(1)]) +- Exchange hashpartitioning(value#1396, 200) +- *HashAggregate(keys=[value#1396], functions=[partial_count(1)]) +- *Project [value#1396] +- AppendColumns &lt;function1&gt;, newInstance(class ... [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, ... +- *FileScan parquet [D... After we perform a grouping with a key on a Dataset, we can operate on the Key Value Dataset with functions that will manipulate the groupings as raw objects: 在对 Dataset 上的键执行分组之后，我们可以对键值数据集进行操作，该函数具有将分组作为原始对象进行操作的功能： 12345def grpSum(countryName:String, values: Iterator[Flight]) = &#123; values.dropWhile(_.count &lt; 5).map(x =&gt; (countryName, x))&#125; flights.groupByKey(x =&gt; x.DEST_COUNTRY_NAME).flatMapGroups(grpSum).show(5) 123456789+--------+--------------------+| _1 | _2 |+--------+--------------------+|Anguilla|[Anguilla,United ...||Paraguay|[Paraguay,United ...|| Russia |[Russia,United St...|| Senegal|[Senegal,United S...|| Sweden |[Sweden,United St...|+--------+--------------------+ 12345def grpSum2(f:Flight):Integer = &#123; 1&#125;flights.groupByKey(x =&gt; x.DEST_COUNTRY_NAME).mapValues(grpSum2).count().take(5) We can even create new manipulations and define how groups should be reduced: 我们甚至可以创建新的操作并定义应如何减少组： 123456def sum2(left:Flight, right:Flight) = &#123; Flight(left.DEST_COUNTRY_NAME, null, left.count + right.count)&#125; flights.groupByKey(x =&gt; x.DEST_COUNTRY_NAME).reduceGroups((l, r) =&gt; sum2(l, r)).take(5) It should be straightfoward enough to understand that this is a more expensive process than aggregating immediately after scanning, especially because it ends up in the same end result: 应该足够直观地了解到，与扫描后立即进行聚合相比，这是一个更昂贵的过程，尤其是因为它最终会达到相同的最终结果： 123456flights.groupBy(&quot;DEST_COUNTRY_NAME&quot;).count().explain== Physical Plan ==*HashAggregate(keys=[DEST_COUNTRY_NAME#1308], functions=[count(1)])+- Exchange hashpartitioning(DEST_COUNTRY_NAME#1308, 200) +- *HashAggregate(keys=[DEST_COUNTRY_NAME#1308], functions=[partial_count(1)]) +- *FileScan parquet [DEST_COUNTRY_NAME#1308] Batched: tru... This should motivate using Datasets only with user-defined encoding surgically and only where it makes sense. This might be at the beginning of a big data pipeline or at the end of one. 这应该仅通过外科手术并且仅在有意义的地方激发使用 Datasets 的动机。这可能是在大数据管道的开始或结束时。 ConclusionIn this chapter, we covered the basics of Datasets and provided some motivating examples. Although short, this chapter actually teaches you basically all that you need to know about Datasets and how to use them. It can be helpful to think of them as a blend between the higher-level Structured APIs and the low-level RDD APIs, which is the topic of Chapter 12. 在本章中，我们介绍了 Datasets 的基础知识，并提供了一些激励性的示例。尽管简短，但本章实际上教会了您基本上需要了解的有关数据集以及如何使用它们的所有知识。将它们视为高级结构化API和低级RDD API之间的混合会很有帮助，这是第12章的主题。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 9 Data Sources]]></title>
    <url>%2F2019%2F10%2F20%2FChapter9_DataSources(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 9 Data Sources 数据源 译者：https://snaildove.github.io This chapter formally introduces the variety of other data sources that you can use with Spark out of the box as well as the countless other sources built by the greater community. Spark has six “core” data sources and hundreds of external data sources written by the community. The ability to read and write from all different kinds of data sources and for the community to create its own contributions is arguably one of Spark’s greatest strengths. Following are Spark’s core data sources: 本章正式介绍了可与Spark一起开箱即用的各种其他数据源，以及更大的社区构建的无数其他数据源。Spark有六个“核心”数据源和社区编写的数百个外部数据源。从所有不同类型的数据源读取和写入数据以及使社区自行做出贡献的能力可以说是Spark的最大优势之一。以下是Spark的核心数据源： CSV JSON Parquet ORC JDBC/ODBC connections Plain-text files 纯文本文件 As mentioned, Spark has numerous community-created data sources. Here’s just a small sample: 如前所述，Spark具有大量社区创建的数据源。这只是一个小样本： Cassandra HBase MongoDB AWS Redshift XML And many, many others The goal of this chapter is to give you the ability to read and write from Spark’s core data sources and know enough to understand what you should look for when integrating with third-party data sources. To achieve this, we will focus on the core concepts that you need to be able to recognize and understand. 本章的目的是使您能够从Spark的核心数据源进行读写，并且足够了解与第三方数据源集成时应寻找的内容。 为此，我们将重点关注您需要能够识别和理解的核心概念。 The Structure of the Data Sources API 数据源API的结构Before proceeding with how to read and write from certain formats, let’s visit the overall organizational structure of the data source APIs. 在继续进行某些格式的读取和写入之前，让我们先访问数据源API的总体组织结构。 Read API Structure 读取数据的API的结构The core structure for reading data is as follows: 1DataFrameReader.format(...).option("key", "value").schema(...).load() We will use this format to read from all of our data sources. format is optional because by default Spark will use the Parquet format. option allows you to set key-value configurations to parameterize how you will read data. Lastly, schema is optional if the data source provides a schema or if you intend to use schema inference. Naturally, there are some required options for each format, which we will discuss when we look at each format. 我们将使用这种格式来读取所有数据源。格式是可选的，因为默认情况下，Spark将使用Parquet格式。选项允许您设置键值配置，以参数化如何读取数据。最后，如果数据源提供了模式，或者您打算使用模式推断，则模式是可选的。自然，每种格式都有一些必需的选项，我们将在讨论每种格式时进行讨论。 NOTE 注意There is a lot of shorthand notation in the Spark community, and the data source read API is no exception. We try to beconsistent throughout the book while still revealing some of the shorthand notation along the way. Spark社区中有很多速记符号，并且数据源读取API也不例外。我们试图在整本书中保持一致，同时仍然沿途揭示一些速记符号。 Basics of Reading Data 读取数据的基础要素The foundation for reading data in Spark is the DataFrameReader. We access this through the SparkSession via the read attribute: 在Spark中读取数据的基础是 DataFrameReader。我们通过 SparkSession 的 read 属性来使用这个： 1spark.read After we have a DataFrame reader, we specify several values: 有了DataFrame读取器后，我们指定几个值： The format The schema The read mode A series of options The format, options, and schema each return a DataFrameReader that can undergo further transformations and are all optional, except for one option. Each data source has a specific set of options that determine how the data is read into Spark (we cover these options shortly). At a minimum, you must supply the DataFrameReader a path to from which to read. 格式，选项和模式每个都返回一个DataFrameReader，该对象可以进行进一步的转换，并且都是可选的，除了一个选项。每个数据源都有一组特定的选项，这些选项决定了如何将数据读入Spark（稍后将介绍这些选项）。至少必须为DataFrameReader提供读取的路径。 Here’s an example of the overall layout: 这是整体布局的示例： 123456spark.read.format("csv").option("mode", "FAILFAST").option("inferSchema", "true").option("path", "path/to/file(s)").schema(someSchema).load() There are a variety of ways in which you can set options; for example, you can build a map and pass in your configurations. For now, we’ll stick to the simple and explicit way that you just saw. 您可以通过多种方式设置选项。例如，您可以构建映射并传递配置。目前，我们将继续使用您刚才看到的简单明了的方式。 Read modes 读取模式Reading data from an external source naturally entails encountering malformed data, especially when working with only semi-structured data sources. Read modes specify what will happen when Spark does come across malformed records. Table 9-1 lists the read modes. 从外部源读取数据自然会遇到格式错误的数据，尤其是在仅使用半结构化数据源时。读取方式指定当Spark遇到格式错误的记录时将发生的情况。表9-1列出了读取方式。 Table 9-1. Spark’s read modes Read mode Description permissive Sets all fields to null when it encounters a corrupted record and places all corrupted records in a string column called _corrupt_record遇到损坏的记录并将所有损坏的记录放在 called _corrupt_record 的字符串列中时，将所有字段设置为 null 。 dropMalformed Drops the row that contains malformed records删除包含格式错误的记录的行。 failFast Fails immediately upon encountering malformed records遇到格式错误的记录后立即失败。 The default is permissive. 默认是 permissive。 Write API Structure 写入数据的API的结构The core structure for writing data is as follows: 写入数据的核心结构如下： 12DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save() We will use this format to write to all of our data sources. format is optional because by default, Spark will use the Parquet format. option, again, allows us to configure how to write out our given data. PartitionBy, bucketBy, and sortBywork only for file-based data sources; you can use them to control the specific layout of files at the destination. Basics of Writing Data 写入数据的基础要素The foundation for writing data is quite similar to that of reading data. Instead of the DataFrameReader, we have the DataFrameWriter. Because we always need to write out some given data source, we access the DataFrameWriteron a per-DataFrame basis via the write attribute: 写入数据的基础要素与读取数据的基础非常相似。代替了DataFrameReader，我们有了DataFrameWriter。因为我们总是需要写出某些给定的数据源，所以我们通过write属性在每个DataFrame的基础上访问DataFrameWriter： 12// in ScaladataFrame.write After we have a DataFrameWriter, we specify three values: the format, a series of options, and the save mode. At a minimum, you must supply a path. We will cover the potential for options, which vary from data source to data source, shortly. 有了 DataFrameWriter 之后，我们指定三个值：格式，一系列选项和保存方式。至少必须提供一条路径。不久之后，我们将介绍各种选项的潜力，这些选项因数据源而异。 123456// in Scaladataframe.write.format("csv").option("mode", "OVERWRITE").option("dateFormat", "yyyy-MM-dd").option("path", "path/to/file(s)").save() Save modes 保存方式Save modes specify what will happen if Spark finds data at the specified location (assuming all else equal). Table 9-2 lists the save modes. 保存方式指定如果Spark在指定位置找到数据（假设所有其他条件相等）将发生的情况。表9-2列出了保存方式。 Table 9-2. Spark’s save modes Save mode Description append Appends the output files to the list of files that already exist at that location将输出文件追加到该位置已存在的文件列表中 overwrite Will completely overwrite any data that already exists there将完全覆盖那里已经存在的任何数据 errorIfExists Throws an error and fails the write if data or files already exist at the specified location如果指定位置已经存在数据或文件，则会引发错误并导致写入失败 ignore If data or files exist at the location, do nothing with the current DataFrame如果该位置存在数据或文件，请对当前DataFrame不执行任何操作 The default is errorIfExists. This means that if Spark finds data at the location to which you’re writing, it will fail the write immediately. 默认值为errorIfExists。这意味着，如果Spark在您要写入的位置找到数据，它将立即导致写入失败。 We’ve largely covered the core concepts that you’re going to need when using data sources, so now let’s dive into each of Spark’s native data sources. 我们已经在很大程度上涵盖了使用数据源时需要的核心概念，因此现在让我们深入研究Spark的每个本地数据源。 CSV Files CSV stands for commma-separated values. This is a common text file format in which each line represents a single record, and commas separate each field within a record. CSV files, while seeming well structured, are actually one of the trickiest file formats you will encounter because not many assumptions can be made in production scenarios about what they contain or how they are structured. For this reason, the CSV reader has a large number of options. These options give you the ability to work around issues like certain characters needing to be escaped—for example, commas inside of columns when the file is also comma-delimited or null values labeled in an unconventional way. CSV代表以逗号分隔的值。这是一种常见的文本文件格式，其中每一行代表一个记录，并用逗号分隔记录中的每个字段。CSV文件虽然看起来结构良好，但实际上是您将遇到的最棘手的文件格式之一，因为在生产方案中无法对其包含的内容或结构进行很多假设。因此，CSV读取器具有大量选项。这些选项使您能够解决某些需要转义的字符等问题，例如，文件也是逗号分隔时的列内逗号，或者以非常规方式标记的空值。 CSV OptionsTable 9-3 presents the options available in the CSV reader. 表9-3列出了CSV阅读器中可用的选项。 Table 9-3. CSV data source options CSV数据源选项 Read/write Key Potential values Default Description Both sep Any single string character , The single character that is used as separator for each field and value.用作每个字段和值的分隔符的单个字符。 Both header true, false false A Boolean flag that declares whether the first line in the file(s) are the names of the columns.布尔值标志，用于声明文件中的第一行是否为列名。 Read escape Any string character \ The character Spark should use to escape other characters in the file.字符Spark应该用于转义文件中的其他字符。 Read inferSchema true, false false Specifies whether Spark should infer column types when reading the file.指定在读取文件时Spark是否应推断列类型。 Read IgnoreLeadingWhiteSpace true, false false Declares whether leading spaces from values being read should be skipped.声明是否应跳过读取值的前导空格。 Read IgnoreTrailingWhiteSpace true, false false Declares whether trailing spaces from values being read should be skipped.声明是否应跳过读取值的尾随空格。 Both nullValue Any string character “” Declares what character represents a null value in the file.声明什么字符代表文件中的空值。 Both nanValue Any string character NaN Declares what character represents a NaN or missing character in the CSV file.在CSV文件中声明代表NaN或缺少字符的字符。 Both positivelnf Any string or character Inf Declares what character(s) represent a positive infinite value.声明哪些字符表示正无穷大。 Both negativelnf Any string or character -Inf Declares what character(s) represent a negative infinite value.声明哪些字符表示负无穷大。 Both compression or codec None, uncompressed. bzip2, deflate, gzip, Iz4, or snappy none Declares what compression codec Spark should use to read or write the file.声明Spark应当使用哪种压缩编解码器读取或写入文件。 Both dateFormat Any string or character that conforms to java’s SimpleDataFormat. yyyy-MM-dd Declares the date format for any columns that are date type.声明任何日期类型列的日期格式。 Both timestampFormat Any string or character that conforms to java’s SimpleDataFormat. yyyy-MM-dd’T’HH:mm :ss.SSSZZ Declares the timestamp format for any columns that are timestamp type.声明所有属于时间戳类型的列的时间戳格式。 Read maxColumns Any integer 20480 Declares the maximum number of columns in the file.声明文件中的最大列数。 Read maxCharsPerColunn Any integer 1000000 Declares the maximum number of characters in a column.声明一列中的最大字符数。 Read escapeQuotes true, false true Declares whether Spark should escape quotes that are found in lines.声明Spark是否应该转义在行中找到的引号。 Read maxMalformedLogPerPartition Any integer 10 Sets the maximum number of malformed rows Spark will log for each partition. Malformed records beyond this number will be ignored.设置Spark将为每个分区记录的格式错误的最大行数。超出此数字的格式错误的记录将被忽略。 Write quoteAll true, false false Specifies whether all values should be enclosed in quotes, as opposed to just escaping values that have a quote character.指定是否所有值都应该用引号引起来，而不是仅转义具有引号字符的值。 Read multiLine true, false false This option allows you to read multiline CSV files where each logical row in the CSV file might span multiple rows in the file itself.此选项使您可以读取多行CSV文件，其中CSV文件中的每个逻辑行都可能跨越文件本身中的多个行。 Reading CSV FilesTo read a CSV file, like any other format, we must first create a DataFrameReader for that specific format. Here, we specify the format to be CSV: 要读取CSV文件，就像其他任何格式一样，我们必须首先为该特定格式创建一个DataFrameReader。在这里，我们将格式指定为CSV： 1spark.read.format("csv") After this, we have the option of specifying a schema as well as modes as options. Let’s set a couple of options, some that we saw from the beginning of the book and others that we haven’t seen yet. 此后，我们可以选择指定模式（schema）以及方式（mode）作为选项。让我们设置几个选项，其中一些是我们从本书开始就看到的，还有一些我们还没有看到的。 We’ll set the header to true for our CSV file, the mode to be FAILFAST, and inferSchema to true: 我们将CSV文件的标头设置为true，将方式（mode）设置为 FAILFAST，将 inferSchema 设置为true： 123456// in Scalaspark.read.format("csv").option("header", "true").option("mode", "FAILFAST").option("inferSchema", "true").load("some/path/to/file.csv") As mentioned, we can use the mode to specify how much tolerance we have for malformed data. For example, we can use these modes and the schema that we created in Chapter 5 to ensure that our file(s) conform to the data that we expected: 如前所述，我们可以使用该方式（mode）来指定对畸形数据的容忍度。例如，我们可以使用这些方式（mode）和我们在第5章中创建的模式（schema）来确保我们的文件符合我们期望的数据： 1234567891011121314// in Scalaimport org.apache.spark.sql.types.&#123;StructField, StructType, StringType, LongType&#125;val myManualSchema = new StructType(Array(new StructField("DEST_COUNTRY_NAME", StringType, true),new StructField("ORIGIN_COUNTRY_NAME", StringType, true),new StructField("count", LongType, false)))spark.read.format("csv").option("header", "true").option("mode", "FAILFAST").schema(myManualSchema).load("/data/flight-data/csv/2010-summary.csv").show(5) Things get tricky when we don’t expect our data to be in a certain format, but it comes in that way, anyhow. For example, let’s take our current schema and change all column types to LongType. This does not match the actual schema, but Spark has no problem with us doing this. The problem will only manifest itself when Spark actually reads the data. As soon as we start our Spark job, it will immediately fail (after we execute a job) due to the data not conforming to the specified schema: 当我们不希望数据采用某种特定格式时，事情就会变得棘手，但无论如何都是这样。例如，让我们采用当前的模式（schema）并将所有列类型更改为LongType。这与实际的模式（schema）不匹配，但是Spark对此没有问题。仅当Spark实际读取数据时，问题才会显现出来。一旦开始执行Spark作业，由于数据不符合指定的模式，它将立即失败（在执行作业之后）： 1234567891011// in Scalaval myManualSchema = new StructType(Array(new StructField("DEST_COUNTRY_NAME", StringType, true),new StructField("ORIGIN_COUNTRY_NAME", StringType, true),new StructField("count", LongType, false) ))spark.read.format("csv").option("header", "true").option("mode", "FAILFAST").schema(myManualSchema).load("/data/flight-data/csv/2010-summary.csv").take(5) In general, Spark will fail only at job execution time rather than DataFrame definition time—even if, for example, we point to a file that does not exist. This is due to lazy evaluation, a concept we learned about in Chapter 2. 通常，Spark仅在作业执行时失败，而不是在DataFrame定义时失败，即使例如，我们指向的文件不存在。这是由于我们在第2章中学到了惰性求值（lazy evaluation）。 Writing CSV FilesJust as with reading data, there are a variety of options (listed in Table 9-3) for writing data when we write CSV files. This is a subset of the reading options because many do not apply when writing data (like maxColumns and inferSchema). Here’s an example: 就像读取数据一样，当我们编写CSV文件时，有多种选项（表9-3中列出）用于写数据。这是读取选项的子集，因为在写入数据时，许多选项均不适用（例如maxColumns和inferSchema）。这是一个例子： 12345678910// in Scalaval csvFile = spark.read.format("csv").option("header", "true").option("mode", "FAILFAST").schema(myManualSchema).load("/data/flight-data/csv/2010-summary.csv")# in PythoncsvFile = spark.read.format("csv")\.option("header", "true")\.option("mode", "FAILFAST")\.option("inferSchema", "true")\.load("/data/flight-data/csv/2010-summary.csv") For instance, we can take our CSV file and write it out as a TSV file quite easily: 例如，我们可以轻松提取CSV文件并将其作为TSV文件写出： 12// in ScalacsvFile.write.format("csv").mode("overwrite").option("sep", "\t").save("/tmp/my-tsv-file.tsv") 123# in PythoncsvFile.write.format("csv").mode("overwrite").option("sep", "\t")\.save("/tmp/my-tsv-file.tsv") When you list the destination directory, you can see that my-tsv-file is actually a folder with numerous files within it: 当您列出目标目录时，您可以看到my-tsv-file实际上是一个文件夹，其中包含许多文件： 12$ ls /tmp/my-tsv-file.tsv//tmp/my-tsv-file.tsv/part-00000-35cf9453-1943-4a8c-9c82-9f6ea9742b29.csv This actually reflects the number of partitions in our DataFrame at the time we write it out. If we were to repartition our data before then, we would end up with a different number of files. We discuss this trade-off at the end of this chapter. 实际上，这反映了我们在写出DataFrame时分区的数量。如果要在此之前对数据进行重新分区，最终将获得不同数量的文件。我们将在本章末尾讨论这种权衡。 JSON FilesThose coming from the world of JavaScript are likely familiar with JavaScript Object Notation, or JSON, as it’s commonly called. There are some catches when working with this kind of data that are worth considering before we jump in. In Spark, when we refer to JSON files, we refer to line-delimited JSON files. This contrasts with files that have a large JSON object or array per file. 那些来自JavaScript世界的人可能熟悉JavaScript Object Notation，即JSON（通常称为JSON）。使用此类数据时，有一些陷阱值得我们跳入之前考虑。在Spark中，当我们引用JSON文件时，我们引用的是行分隔JSON文件。这与每个文件具有较大JSON对象或数组的文件形成对比。 The line-delimited versus multiline trade-off is controlled by a single option: multiLine. When you set this option to true, you can read an entire file as one json object and Spark will go through the work of parsing that into a DataFrame. Line-delimited JSON is actually a much more stable format because it allows you to append to a file with a new record (rather than having to read in an entire file and then write it out), which is what we recommend that you use. Another key reason for the popularity of line-delimited JSON is because JSON objects have structure, and JavaScript (on which JSON is based) has at least basic types. This makes it easier to work with because Spark can make more assumptions on our behalf about the data. You’ll notice that there are significantly less options than we saw for CSV because of the objects. 行定界与多行权衡由一个选项控制：multiLine。当将此选项设置为true时，您可以将整个文件作为一个json对象读取，Spark将完成将其解析为DataFrame的工作。行分隔的JSON实际上是一种更加稳定的格式，因为它允许您将具有新记录的文件追加到文件中（而不是必须读取整个文件然后将其写出），这是我们建议您使用的格式。行分隔JSON流行的另一个关键原因是因为JSON对象具有结构，而JavaScript（基于JSON的JavaScript）至少具有基本类型。这使使用起来更容易，因为Spark可以代表我们对数据做出更多假设。您会注意到，由于对象的原因，选项比我们看到的要少得多。 JSON OptionsTable 9-4 lists the options available for the JSON object, along with their descriptions. 表9-4列出了可用于JSON对象的选项及其说明。 Table 9-4. JSON data source options JSON数据源选项 Read/write Key Potential values Default Description Both compression or codec None,uncompressed,bzip2, deflate,gzip, lz4, orsnappy none Declares what compression codec Spark should use to read or write the file.声明当Spark读取或写入文件的压缩编解码器。 Both dateFormat Any string orcharacter thatconforms to Java’sSimpleDataFormat. yyyy-MM-dd Declares the date format for any columns that are date type.声明任何日期类型列的日期格式。 Both timestampFormat Any string orcharacter thatconforms to Java’sSimpleDataFormat. yyyy-MM-dd’T’HH:mm:ss.SSSZZ Declares the timestamp format for any columns that are timestamp type.声明任何日期类型列的日期格式。 Read primitiveAsString true, false false Infers all primitive values as string type.将所有原始值推断为字符串类型。 Read allowComments true, false false Ignores Java/C++ style comment in JSON records.忽略JSON记录中的Java / C ++样式注释。 Read allowUnquotedFieldNames true, false false Allows unquoted JSON field names.允许不带引号的JSON字段名称 Read allowSingleQuotes true, false true Allows single quotes in addition to double quotes.除双引号外，还允许单引号。 Read allowNumericLeadingZeros true, false false Allows leading zeroes in numbers (e.g., 00012).允许数字前导零（例如00012）。 Read allowBackslashEscapingAnyCharacter true, false false Allows accepting quoting of all characters using backslash quoting mechanism.允许使用反斜杠引用机制接受所有字符的引用。 Read columnNameOfCorruptRecord Any string Value ofspark.sql.column&amp;NameOfCorruptRecord new field having a malformed string created by permissive mode. This will override the configuration value.由 permissive 方式（mode）创建的字符串格式错误的新字段。这将覆盖配置值。 Read multiLine true, false false Allows for reading in non-line-delimited JSON files.允许读取非行分隔的JSON文件。 Now, reading a line-delimited JSON file varies only in the format and the options that we specify: 现在，读取以行分隔的JSON文件仅在格式和我们指定的选项上有所不同： 1spark.read.format("json") Reading JSON FilesLet’s look at an example of reading a JSON file and compare the options that we’re seeing: 让我们看一个读取JSON文件并比较我们看到的选项的示例： 123// in Scalaspark.read.format("json").option("mode", "FAILFAST").schema(myManualSchema).load("/data/flight-data/json/2010-summary.json").show(5) 1234# in Pythonspark.read.format("json").option("mode", "FAILFAST")\.option("inferSchema", "true")\.load("/data/flight-data/json/2010-summary.json").show(5) Writing JSON Files Writing JSON files is just as simple as reading them, and, as you might expect, the data source does not matter. Therefore, we can reuse the CSV DataFrame that we created earlier to be the source for our JSON file. This, too, follows the rules that we specified before: one file per partition will be written out, and the entire DataFrame will be written out as a folder. It will also have one JSON object per line: 编写JSON文件就像读取它们一样简单，而且，正如您可能期望的那样，数据源无关紧要。因此，我们可以重用我们先前创建的CSV DataFrame作为JSON文件的源。这也遵循我们之前指定的规则：每个分区将写入一个文件，而整个DataFrame将作为一个文件夹写入。每行还将有一个JSON对象： 12// in ScalacsvFile.write.format("json").mode("overwrite").save("/tmp/my-json-file.json") 12# in PythoncsvFile.write.format("json").mode("overwrite").save("/tmp/my-json-file.json") 1$ ls /tmp/my-json-file.json//tmp/my-json-file.json/part-00000-tid-543....json Parquet FilesParquet is an open source column-oriented data store that provides a variety of storage optimizations, especially for analytics workloads. It provides columnar compression, which saves storage space and allows for reading individual columns instead of entire files. It is a file format that works exceptionally well with Apache Spark and is in fact the default file format. We recommend writing data out to Parquet for long-term storage because reading from a Parquet file will always be more efficient than JSON or CSV. Another advantage of Parquet is that it supports complex types. This means that if your column is an array (which would fail with a CSV file, for example), map, or struct, you’ll still be able to read and write that file without issue. Here’s how to specify Parquet as the read format: Parquet是面向列的开源数据存储，可提供各种存储优化，尤其是针对分析工作负载。它提供了列压缩，从而节省了存储空间，并允许读取单个列而不是整个文件。它是一种文件格式，可与Apache Spark配合使用，并且实际上是默认文件格式。我们建议将数据写到Parquet中进行长期存储，因为从Parquet文件中读取数据总是比JSON或CSV更有效。Parquet的另一个优点是它支持复杂类型。这意味着，如果您的列是数组（例如，CSV文件会失效），映射或结构，那么您仍然可以毫无问题地读写该文件。以下是将Parquet指定为读取格式的方法： 1spark.read.format("parquet") Reading Parquet FilesParquet has very few options because it enforces its own schema when storing data. Thus, all you need to set is the format and you are good to go. We can set the schema if we have strict requirements for what our DataFrame should look like. Oftentimes this is not necessary because we can use schema on read, which is similar to the inferSchema with CSV files. However, with Parquet files, this method is more powerful because the schema is built into the file itself (so no inference needed). Parquet具有很少的选项，因为它在存储数据时会强制执行自己的模式。因此，您只需要设置格式就可以了。如果我们对DataFrame有严格的要求，则可以设置模式。通常，这不是必需的，因为我们可以在读取时使用模式，这与带有CSV文件的 inferSchema 相似。但是，对于Parquet文件，此方法功能更强大，因为该模式内置在文件本身中（因此无需进行推断）。 Here are some simple examples reading from parquet : 以下是从 parquet 上读取的一些简单示例： 1spark.read.format("parquet") 123// in Scalaspark.read.format("parquet").load("/data/flight-data/parquet/2010-summary.parquet").show(5) 123# in Pythonspark.read.format("parquet")\.load("/data/flight-data/parquet/2010-summary.parquet").show(5) Parquet optionsAs we just mentioned, there are very few Parquet options—precisely two, in fact—because it has a well-defined specification that aligns closely with the concepts in Spark. Table 9-5 presents the options. 正如我们刚才提到的，Parquet选项很少，实际上只有两个，因为它具有定义明确的规范，可以与Spark中的概念紧密结合。表9-5列出了这些选项。 WARNING 警告Even though there are only two options, you can still encounter problems if you’re working with incompatible Parquet files. Be careful when you write out Parquet files with different versions of Spark (especially older ones) because this can cause significant headache. 即使只有两个选项，但如果使用不兼容的Parquet文件，仍然会遇到问题。用不同版本的Spark（尤其是较旧的Spark）写出Parquet文件时要小心，因为这会引起严重的问题。 Table 9-5. Parquet data source options Writing Parquet FilesWriting Parquet is as easy as reading it. We simply specify the location for the file. The same partitioning rules apply: 编写 Parquet 就像阅读它一样容易。 我们只需指定文件的位置。 相同的分区规则适用： 123// in ScalacsvFile.write.format("parquet").mode("overwrite").save("/tmp/my-parquet-file.parquet") 123# in PythoncsvFile.write.format("parquet").mode("overwrite")\.save("/tmp/my-parquet-file.parquet") ORC FilesORC is a self-describing, type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads, but with integrated support for finding required rows quickly. ORC actually has no options for reading in data because Spark understands the file format quite well. An often-asked question is: What is the difference between ORC and Parquet? For the most part, they’re quite similar; the fundamental difference is that Parquet is further optimized for use with Spark, whereas ORC is further optimized for Hive. ORC是一种专为Hadoop工作负载设计的自我描述、注意类型的列式文件格式。它针对大型流读取进行了优化，但是集成了对快速查找所需行的支持。ORC实际上没有读取数据的选项，因为Spark非常了解文件格式。一个经常问到的问题是：ORC和Parquet有什么区别？在大多数情况下，它们非常相似； 根本的区别在于Parquet进一步优化了与Spark一起使用，而ORC进一步优化了针对Hive。 Reading Orc FilesHere’s how to read an ORC file into Spark: 以下是将ORC文件读入Spark的方法： 12// in Scalaspark.read.format("orc").load("/data/flight-data/orc/2010-summary.orc").show(5) 12# in Pythonspark.read.format("orc").load("/data/flight-data/orc/2010-summary.orc").show(5) Writing Orc FilesAt this point in the chapter, you should feel pretty comfortable taking a guess at how to write ORC files. It really follows the exact same pattern that we have seen so far, in which we specify the format and then save the file: 在本章的这一点上，您应该对如何编写ORC文件进行猜测感到很自在。它实际上遵循我们到目前为止所看到的完全相同的模式，在该模式中，我们指定格式然后保存文件： 12// in ScalacsvFile.write.format("orc").mode("overwrite").save("/tmp/my-json-file.orc") 12# in PythoncsvFile.write.format("orc").mode("overwrite").save("/tmp/my-json-file.orc") SQL DatabasesSQL data sources are one of the more powerful connectors because there are a variety of systems to which you can connect (as long as that system speaks SQL). For instance you can connect to a MySQL database, a PostgreSQL database, or an Oracle database. You also can connect to SQLite, which is what we’ll do in this example. Of course, databases aren’t just a set of raw files, so there are more options to consider regarding how you connect to the database. Namely you’re going to need to begin considering things like authentication and connectivity (you’ll need to determine whether the network of your Spark cluster is connected to the network of your database system). SQL数据源是功能更强大的连接器之一，因为可以连接多种系统（只要该系统使用SQL即可）。例如，您可以连接到MySQL数据库，PostgreSQL数据库或Oracle数据库。您还可以连接到SQLite，这是我们在此示例中所做的。当然，数据库不仅是一组原始文件，因此，关于如何连接数据库，还有更多选项可供考虑。即您将需要开始考虑诸如身份验证和连接之类的事情（您需要确定Spark集群的网络是否已连接到数据库系统的网络）。 To avoid the distraction of setting up a database for the purposes of this book, we provide a reference sample that runs on SQLite. We can skip a lot of these details by using SQLite, because it can work with minimal setup on your local machine with the limitation of not being able to work in a distributed setting. If you want to work through these examples in a distributed setting, you’ll want to connect to another kind of database. 为了避免为了本书而设置数据库，我们提供了一个在SQLite上运行的参考示例。通过使用SQLite，我们可以跳过很多这些详细信息，因为它可以在本地计算机上以最少的设置工作，并且不能在分布式设置中工作。如果要在分布式环境中浏览这些示例，则需要连接到另一种数据库。 A PRIMER ON SQLITESQLite is the most used database engine in the entire world, and for good reason. It’s powerful, fast, and easy to understand. This is because a SQLite database is just a file. That’s going to make it very easy for you to get up and running because we include the source file in the official repository for this book. Simply download that file to your local machine, and you will be able to read from it and write to it. We’re using SQLite, but all of the code here works with more traditional relational databases, as well, like MySQL. The primary difference is in the properties that you include when you connect to the database. When we’re working with SQLite, there’s no notion of user or password.有充分的理由，SQLite是全世界使用最广泛的数据库引擎。它功能强大，快速且易于理解。这是因为SQLite数据库只是一个文件。这将使您非常容易地启动和运行，因为我们在本书的官方资源库中包含了源文件。只需将该文件下载到您的本地计算机上，您就可以对其进行读取和写入。我们使用的是SQLite，但此处的所有代码也适用于更传统的关系数据库，例如MySQL。主要区别在于连接数据库时所包含的属性。当我们使用SQLite时，没有用户或密码的概念。 WARNING 警告Although SQLite makes for a good reference example, it’s probably not what you want to use in production. Also, SQLite will not necessarily work well in a distributed setting because of its requirement to lock the entire database on write. The example we present here will work in a similar way using MySQL or PostgreSQL, as well. 尽管SQLite提供了很好的参考示例，但它并不是您想在生产中使用的功能。另外，由于需要在写入时锁定整个数据库，因此SQLite在分布式设置中不一定会很好地工作。我们在此提供的示例也可以使用MySQL或PostgreSQL以类似的方式工作。 To read and write from these databases, you need to do two things: include the Java Database Connectivity (JDBC) driver for you particular database on the spark classpath, and provide the proper JAR for the driver itself. For example, to be able to read and write from PostgreSQL, you might run something like this: 要从这些数据库读取和写入，您需要做两件事：在spark类路径上包含用于您的特定数据库的Java数据库连接（JDBC）驱动程序，并为驱动程序本身提供适当的JAR。例如，为了能够从PostgreSQL进行读取和写入，您可以运行以下命令： 123./bin/spark-shell \--driver-class-path postgresql-9.4.1207.jar \--jars postgresql-9.4.1207.jar Just as with our other sources, there are a number of options that are available when reading from and writing to SQL databases. Only some of these are relevant for our current example, but Table 9-6 lists all of the options that you can set when working with JDBC databases. 就像我们的其他来源一样，在读取和写入SQL数据库时，有许多可用的选项。其中只有一些与我们当前的示例相关，但是表9-6列出了在使用JDBC数据库时可以设置的所有选项。 Table 9-6. JDBC data source options Property Name Meaning url The JDBC URL to which to connect. The source-specific connection properties can be specified in the URL; for example, jdbc:postgresql://localhost/test?user=fred&amp;password=secret.要连接的JDBC URL。可以在URL中指定特定于源的连接属性。例如，jdbc:postgresql://localhost/test?user=fred&amp;password=secret dbtable The JDBC table to read. Note that anything that is valid in a FROM clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses.要读取的JDBC表。注意，可以使用在SQL查询的FROM子句中有效的任何东西。例如，除了完整表之外，您还可以在括号中使用子查询。 partitionColumn,lowerBound, upperBound If any one of these options is specified, then all others must be set as well. In addition, numPartitions must be specified. These properties describe how to partition the table when reading in parallel from multiple workers. partitionColumn must be a numeric column from the table in question. Notice that lowerBound and upperBound are used only to decide the partition stride, not for filtering the rows in the table. Thus, all rows in the table will be partitioned and returned. This option applies only to reading.如果指定了这些选项中的任何一个，则还必须设置所有其他选项。另外，必须指定numPartitions。这些属性描述了从多个 workers 并行读取时如何对表进行分区。partitionColumn必须是相关查询表的数值列。请注意，lowerBound和upperBound仅用于确定分区步幅，而不用于过滤表中的行。因此，表中的所有行都将被分区并返回。此选项仅适用于阅读。 numPartitions The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections. If the number of partitions to write exceeds this limit, we decrease it to this limit by calling coalesce(numPartitions) before writing.表读写中可用于并行处理的最大分区数。这也确定了并发JDBC连接的最大数量。如果要写入的分区数超过了此限制，我们可以通过在写入之前调用Coalesce（numPartitions）来将其降至此限制。 fetchsize The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers, which default to low fetch size (e.g., Oracle with 10 rows). This option applies only to reading.JDBC的获取大小，它确定每轮要获取多少行。这可以帮助提高JDBC驱动程序的性能，该驱动程序默认为较小的获取大小（例如，具有10行的Oracle）。此选项仅适用于读取数据。 batchsize The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing. The default is 1000.JDBC批处理大小，它确定每个回合要插入多少行。这可以帮助提高JDBC驱动程序的性能。此选项仅适用于写入数据。默认值为1000。 isolationLevel The transaction isolation level, which applies to current connection. It can be one of NONE, READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to standard transaction isolation levels defined by JDBC’s Connection object. The default is READ_UNCOMMITTED. This option applies only to writing. For more information, refer to the documentation in java.sql.Connection.事务隔离级别，适用于当前连接。它可以是NONE，READ_COMMITTED，READ_UNCOMMITTED，REPEATABLE_READ或SERIALIZABLE之一，对应于JDBC的Connection对象定义的标准事务隔离级别。默认值为READ_UNCOMMITTED。此选项仅适用于写入数据。有关更多信息，请参考java.sql.Connection中的文档。 truncate This is a JDBC writer-related option. When SaveMode.Overwrite is enabled, Spark truncates an existing table instead of dropping and re-creating it. This can be more efficient, and it prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. The default is false. This option applies only to writing.这是与JDBC写入器相关的选项。启用SaveMode.Overwrite时，Spark会截断现有表，而不是删除并重新创建它。这样可以更有效，并且可以防止删除表元数据（例如索引）。但是，在某些情况下（例如，新数据具有不同的模式时），它将不起作用。默认为false。此选项仅适用于写作。 createTableOptions This is a JDBC writer-related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., CREATE TABLE t (name string) ENGINE=InnoDB). This option applies only to writing.这是与JDBC写入器相关的选项。如果指定，则此选项允许在创建表时设置特定于数据库的表和分区选项（例如 CREATE TABLE t (name string) ENGINE=InnoDB ）。此选项仅适用于写入数据。 createTableColumnTypes The database column data types to use instead of the defaults, when creating the table. Data type in formation should be specified in the same format as CREATE TABLE columns syntax (e.g., “name CHAR(64), comments VARCHAR(1024)”). The specified types should be valid Spark SQL data types. This option applies only to writing.创建表时要使用的数据库列数据类型，而不是缺省值。格式中的数据类型应以与CREATE TABLE列语法相同的格式指定（例如，“name CHAR(64), comments VARCHAR(1024)”）。指定的类型应该是有效的Spark SQL数据类型。此选项仅适用于写作。 Reading from SQL DatabasesWhen it comes to reading a file, SQL databases are no different from the other data sources that we looked at earlier. As with those sources, we specify the format and options, and then load in the data: 在读取文件时，SQL数据库与我们之前看过的其他数据源没有什么不同。与这些源一样，我们指定格式和选项，然后加载数据： 12345// in Scalaval driver = "org.sqlite.JDBC"val path = "/data/flight-data/jdbc/my-sqlite.db"val url = s"jdbc:sqlite:/$&#123;path&#125;"val tablename = "flight_info" 12345# in Pythondriver = "org.sqlite.JDBC"path = "/data/flight-data/jdbc/my-sqlite.db"url = "jdbc:sqlite:" + pathtablename = "flight_info" After you have defined the connection properties, you can test your connection to the database itself to ensure that it is functional. This is an excellent troubleshooting technique to confirm that your database is available to (at the very least) the Spark driver. This is much less relevant for SQLite because that is a file on your machine but if you were using something like MySQL, you could test the connection with the following: 定义连接属性后，可以测试与数据库本身的连接以确保其正常运行。这是一种出色的故障排除技术，可确保您的数据库可用于（至少）Spark驱动程序。这与SQLite无关紧要，因为这是您计算机上的文件，但是如果您使用的是类似MySQL的文件，则可以使用以下命令测试连接： 1234import java.sql.DriverManagerval connection = DriverManager.getConnection(url)connection.isClosed()connection.close() If this connection succeeds, you’re good to go. Let’s go ahead and read the DataFrame from the SQL table: 如果此连接成功，那就很好了。让我们继续阅读SQL表中的DataFrame： 123// in Scalaval dbDataFrame = spark.read.format("jdbc").option("url", url).option("dbtable", tablename).option("driver", driver).load() 123# in PythondbDataFrame = spark.read.format("jdbc").option("url", url)\.option("dbtable", tablename).option("driver", driver).load() SQLite has rather simple configurations (no users, for example). Other databases, like PostgreSQL, require more configuration parameters. Let’s perform the same read that we just performed, except using PostgreSQL this time: SQLite具有相当简单的配置（例如，没有用户）。其他数据库（例如PostgreSQL）需要更多配置参数。让我们执行与刚刚执行的读取相同的操作，除了这次使用PostgreSQL： 1234567// in Scalaval pgDF = spark.read.format("jdbc").option("driver", "org.postgresql.Driver").option("url", "jdbc:postgresql://database_server").option("dbtable", "schema.tablename").option("user", "username").option("password","my-secret-password").load() 123456# in PythonpgDF = spark.read.format("jdbc")\.option("driver", "org.postgresql.Driver")\.option("url", "jdbc:postgresql://database_server")\.option("dbtable", "schema.tablename")\.option("user", "username").option("password", "my-secret-password").load() As we create this DataFrame, it is no different from any other: you can query it, transform it, and join it without issue. You’ll also notice that there is already a schema, as well. That’s because Spark gathers this information from the table itself and maps the types to Spark data types. Let’s get only the distinct locations to verify that we can query it as expected: 在创建此DataFrame时，它与其他任何对象都没有什么不同：您可以对其进行查询，转换和加入，而不会出现问题。您还会注意到，也已经有一个模式。那是因为Spark会从表格本身收集此信息，然后将类型映射为Spark数据类型。让我们仅获取不同的位置，以验证我们可以按预期查询它： 1dbDataFrame.select("DEST_COUNTRY_NAME").distinct().show(5) 123456789+-----------------+|DEST_COUNTRY_NAME|+-----------------+| Anguilla || Russia || Paraguay || Senegal || Sweden |+-----------------+ Awesome, we can query the database! Before we proceed, there are a couple of nuanced details that are worth understanding. 太好了，我们可以查询数据库了！在我们继续之前，有一些细微的细节值得理解。 Query Pushdown 查询向下推导First, Spark makes a best-effort attempt to filter data in the database itself before creating the DataFrame. For example, in the previous sample query, we can see from the query plan that it selects only the relevant column name from the table: 首先，Spark会尽最大努力在创建DataFrame之前过滤数据库本身中的数据。例如，在上一个示例查询中，我们可以从查询计划中看到它仅从表中选择相关的列名： 1dbDataFrame.select("DEST_COUNTRY_NAME").distinct().explain 12345== Physical Plan ==*HashAggregate(keys=[DEST_COUNTRY_NAME#8108], functions=[])+- Exchange hashpartitioning(DEST_COUNTRY_NAME#8108, 200) +- *HashAggregate(keys=[DEST_COUNTRY_NAME#8108], functions=[]) +- *Scan JDBCRelation(flight_info) [numPartitions=1] ... Spark can actually do better than this on certain queries. For example, if we specify a filter on our DataFrame, Spark will push that filter down into the database. We can see this in the explain plan under PushedFilters. 在某些查询中，Spark实际上比这更好。例如，如果我们在DataFrame上指定一个过滤器，Spark将把该过滤器下推到数据库中。我们可以在 PushedFilters 下的说明计划中看到这一点。 12// in ScaladbDataFrame.filter("DEST_COUNTRY_NAME in ('Anguilla', 'Sweden')").explain 12# in PythondbDataFrame.filter("DEST_COUNTRY_NAME in ('Anguilla', 'Sweden')").explain() 123== Physical Plan ==*Scan JDBCRel... PushedFilters: [*In(DEST_COUNTRY_NAME, [Anguilla,Sweden])],... Spark can’t translate all of its own functions into the functions available in the SQL database in which you’re working. Therefore, sometimes you’re going to want to pass an entire query into your SQL that will return the results as a DataFrame. Now, this might seem like it’s a bit complicated, but it’s actually quite straightforward. Rather than specifying a table name, you just specify a SQL query. Of course, you do need to specify this in a special way; you must wrap the query in parenthesis and rename it to something—in this case, I just gave it the same table name: Spark无法将其所有功能转换为您正在使用的SQL数据库中可用的功能。因此，有时您想要将整个查询传递到SQL中，该查询会将结果作为DataFrame返回。现在，这似乎有些复杂，但实际上非常简单。您只需指定一个SQL查询，而不是指定表名。当然，您确实需要以一种特殊的方式指定它。您必须将查询括在括号中并将其重命名为某种东西——在这种情况下，我只是给了它相同的表名： 12345// in Scalaval pushdownQuery = """(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info) AS flight_info"""val dbDataFrame = spark.read.format("jdbc").option("url", url).option("dbtable", pushdownQuery).option("driver", driver).load() 12345# in PythonpushdownQuery = """(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info) AS flight_info"""dbDataFrame = spark.read.format("jdbc")\.option("url", url).option("dbtable", pushdownQuery).option("driver", driver)\.load() Now when you query this table, you’ll actually be querying the results of that query. We can see this in the explain plan. Spark doesn’t even know about the actual schema of the table, just the one that results from our previous query: 现在，当您查询该表时，您实际上将在查询该查询的结果。我们可以在解释计划中看到这一点。Spark甚至不知道表的实际模式，仅知道我们先前查询的结果： 1dbDataFrame.explain() 12345== Physical Plan ==*Scan JDBCRelation((SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info) as flight_info) [numPartitions=1] [DEST_COUNTRY_NAME#788] ReadSchema: ... Reading from databases in parallelAll throughout this book, we have talked about partitioning and its importance in data processing. Spark has an underlying algorithm that can read multiple files into one partition, or conversely, read multiple partitions out of one file, depending on the file size and the “splitability” of the file type and compression. The same flexibility that exists with files, also exists with SQL databases except that you must configure it a bit more manually. What you can configure, as seen in the previous options, is the ability to specify a maximum number of partitions to allow you to limit how much you are reading and writing in parallel: 在本书中，我们都谈到了分区及其在数据处理中的重要性。Spark具有一种基础算法，可以根据文件大小以及文件类型和压缩的“可拆分性”将多个文件读取到一个分区中，或者反之，可以从一个文件读取多个分区。文件具有相同的灵活性，SQL数据库也具有相同的灵活性，只是您必须手动进行一些配置。如前面的选项所示，您可以配置的功能是指定最大分区数，以限制并行读取和写入的数量： 1234// in Scalaval dbDataFrame = spark.read.format("jdbc").option("url", url).option("dbtable", tablename).option("driver", driver).option("numPartitions", 10).load() 1234# in PythondbDataFrame = spark.read.format("jdbc")\.option("url", url).option("dbtable", tablename).option("driver", driver)\.option("numPartitions", 10).load() In this case, this will still remain as one partition because there is not too much data. However, this configuration can help you ensure that you do not overwhelm the database when reading and writing data: 在这种情况下，由于没有太多数据，因此仍将保留为一个分区。但是，此配置可以帮助您确保在读写数据时不会使数据库不知所措： 1dbDataFrame.select("DEST_COUNTRY_NAME").distinct().show() There are several other optimizations that unfortunately only seem to be under another API set. You can explicitly push predicates down into SQL databases through the connection itself. This optimization allows you to control the physical location of certain data in certain partitions by specifying predicates. That’s a mouthful, so let’s look at a simple example. We only need data from two countries in our data: Anguilla and Sweden. We could filter these down and have them pushed into the database, but we can also go further by having them arrive in their own partitions in Spark. 不幸的是，还有其他一些优化似乎只是在另一个API集之下。您可以通过连接本身将谓词显式向下推入SQL数据库。通过这种优化，您可以通过指定谓词来控制某些分区中某些数据的物理位置。这是一个大问题，所以让我们看一个简单的例子。我们只需要两个国家/地区的数据：安圭拉和瑞典。我们可以过滤掉它们并将它们推送到数据库中，但是我们也可以进一步通过将它们放入Spark中自己的分区中。 译者附谓词（predicate）——通常来说是函数的一种，是需要满足特定条件的函数。该条件就是“返回值是真值”，即返回的值必须为TRUE/FALSE/UNKNOWN） We do that by specifying a list of predicates when we create the data source: 我们通过在创建数据源时指定谓词列表来做到这一点： 12345678// in Scalaval props = new java.util.Propertiesprops.setProperty("driver", "org.sqlite.JDBC")val predicates = Array("DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden'","DEST_COUNTRY_NAME = 'Anguilla' OR ORIGIN_COUNTRY_NAME = 'Anguilla'")spark.read.jdbc(url, tablename, predicates, props).show()spark.read.jdbc(url, tablename, predicates, props).rdd.getNumPartitions // 2 12345678# in Pythonprops = &#123;"driver":"org.sqlite.JDBC"&#125;predicates = ["DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden'","DEST_COUNTRY_NAME = 'Anguilla' OR ORIGIN_COUNTRY_NAME = 'Anguilla'"]spark.read.jdbc(url, tablename, predicates=predicates, properties=props).show()spark.read.jdbc(url,tablename,predicates=predicates,properties=props)\.rdd.getNumPartitions() # 2 12345678+-----------------+-------------------+-----+|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|+-----------------+-------------------+-----+| Sweden | United States | 65 || United States | Sweden | 73 || Anguilla | United States | 21 || United States | Anguilla | 20 |+-----------------+-------------------+-----+ If you specify predicates that are not disjoint, you can end up with lots of duplicate rows. Here’s an example set of predicates that will result in duplicate rows: 如果您指定不互斥的谓词，则最终可能会出现很多重复的行。这是一组导致重复行的谓词示例： 1234567// in Scalaval props = new java.util.Propertiesprops.setProperty("driver", "org.sqlite.JDBC")val predicates = Array("DEST_COUNTRY_NAME != 'Sweden' OR ORIGIN_COUNTRY_NAME != 'Sweden'","DEST_COUNTRY_NAME != 'Anguilla' OR ORIGIN_COUNTRY_NAME != 'Anguilla'")spark.read.jdbc(url, tablename, predicates, props).count() // 510 123456# in Pythonprops = &#123;"driver":"org.sqlite.JDBC"&#125;predicates = ["DEST_COUNTRY_NAME != 'Sweden' OR ORIGIN_COUNTRY_NAME != 'Sweden'","DEST_COUNTRY_NAME != 'Anguilla' OR ORIGIN_COUNTRY_NAME != 'Anguilla'"]spark.read.jdbc(url, tablename, predicates=predicates, properties=props).count() Partitioning based on a sliding windowLet’s take a look to see how we can partition based on predicates. In this example, we’ll partition based on our numerical count column. Here, we specify a minimum and a maximum for both the first partition and last partition. Anything outside of these bounds will be in the first partition or final partition. Then, we set the number of partitions we would like total (this is the level of parallelism). 让我们看一下如何基于谓词进行分区。在此示例中，我们将基于数值计数列进行分区。在此，我们为第一个分区和最后一个分区都指定了最小值和最大值。这些范围之外的任何内容都将位于第一个分区或最终分区中。然后，我们设置希望的分区总数（这是并行度）。 Spark then queries our database in parallel and returns numPartitions partitions. We simply modify the upper and lower bounds in order to place certain values in certain partitions. No filtering is taking place like we saw in the previous example: 然后，Spark并行查询我们的数据库，并返回 numPartitions 分区。我们只需修改上限和下限，以便将某些值放置在某些分区中。不会像前面的示例中那样进行过滤： 12345// in Scalaval colName = "count"val lowerBound = 0Lval upperBound = 348113L // this is the max count in our databaseval numPartitions = 10 12345# in PythoncolName = "count"lowerBound = 0LupperBound = 348113L # this is the max count in our databasenumPartitions = 10 This will distribute the intervals equally from low to high: 这将从低到高平均分配时间间隔： 123// in Scalaspark.read.jdbc(url,tablename,colName,lowerBound,upperBound,numPartitions,props).count() // 255 1234# in Pythonspark.read.jdbc(url, tablename, column=colName, properties=props,lowerBound=lowerBound, upperBound=upperBound,numPartitions=numPartitions).count() # 255 Writing to SQL Databases Writing out to SQL databases is just as easy as before. You simply specify the URI and write out the data according to the specified write mode that you want. In the following example, we specify overwrite, which overwrites the entire table. We’ll use the CSV DataFrame that we defined earlier in order to do this: 写入SQL数据库就像以前一样容易。您只需指定URI并根据所需的指定写方式（mode）写出数据。在下面的示例中，我们指定覆盖，该覆盖将覆盖整个表。我们将使用我们先前定义的 CSV DataFrame 来做到这一点： 123// in Scalaval newPath = "jdbc:sqlite://tmp/my-sqlite.db"csvFile.write.mode("overwrite").jdbc(newPath, tablename, props) 123# in PythonnewPath = "jdbc:sqlite://tmp/my-sqlite.db"csvFile.write.jdbc(newPath, tablename, mode="overwrite", properties=props) Let’s look at the results: 我们来看一下结果： 12// in Scalaspark.read.jdbc(newPath, tablename, props).count() // 255 12# in Pythonspark.read.jdbc(newPath, tablename, properties=props).count() # 255 Of course, we can append to the table this new table just as easily: 当然，我们可以轻松地将此新表添加到表中： 12// in ScalacsvFile.write.mode("append").jdbc(newPath, tablename, props) 12# in PythoncsvFile.write.jdbc(newPath, tablename, mode="append", properties=props) Notice that count increases: 请注意，计数增加了： 12// in Scalaspark.read.jdbc(newPath, tablename, props).count() // 765 12# in Pythonspark.read.jdbc(newPath, tablename, properties=props).count() # 765 Text Files Spark also allows you to read in plain-text files. Each line in the file becomes a record in the DataFrame. It is then up to you to transform it accordingly. As an example of how you would do this,suppose that you need to parse some Apache log files to some more structured format, or perhaps you want to parse some plain text for natural-language processing. Text files make a great argument for the Dataset API due to its ability to take advantage of the flexibility of native types. Spark还允许您读取纯文本文件。文件中的每一行都成为DataFrame中的一条记录。然后由您自己进行相应的转换。作为如何执行此操作的示例，假设您需要将某些Apache日志文件解析为某种更结构化的格式，或者您可能想解析一些纯文本以进行自然语言处理。文本文件因其能够利用本地类型（native type）的灵活性而成为Dataset API的重要论据。 Reading Text Files Reading text files is straightforward: you simply specify the type to be textFile. With textFile, partitioned directory names are ignored. To read and write text files according to partitions, you should use text, which respects partitioning on reading and writing: 读取文本文件非常简单：您只需将类型指定为 textFile。使用 textFile，分区目录名将被忽略。要根据分区读取和写入文本文件，您应该使用文本，这在读写时要考虑分区： 12spark.read.textFile("/data/flight-data/csv/2010-summary.csv").selectExpr("split(value, ',') as rows").show() 12345678910+--------------------+| rows |+--------------------+|[DEST_COUNTRY_NAM...||[United States, R...|...|[United States, A...||[Saint Vincent an...||[Italy, United St...|+--------------------+ Writing Text FilesWhen you write a text file, you need to be sure to have only one string column; otherwise, the write will fail: 在写入文本文件时，您需要确保只有一个字符串列。否则，写入将失败： 1csvFile.select("DEST_COUNTRY_NAME").write.text("/tmp/simple-text-file.txt") If you perform some partitioning when performing your write (we’ll discuss partitioning in the next couple of pages), you can write more columns. However, those columns will manifest as directories in the folder to which you’re writing out to, instead of columns on every single file : 如果您在执行写入操作时进行了分区（我们将在接下来的几页中讨论分区），则可以写入更多列。但是，这些列将显示为您要写入的文件夹中的目录，而不是每个文件的列： 123// in ScalacsvFile.limit(10).select("DEST_COUNTRY_NAME", "count").write.partitionBy("count").text("/tmp/five-csv-files2.csv") 123# in PythoncsvFile.limit(10).select("DEST_COUNTRY_NAME", "count")\.write.partitionBy("count").text("/tmp/five-csv-files2py.csv") Advanced I/O Concepts We saw previously that we can control the parallelism of files that we write by controlling the partitions prior to writing. We can also control specific data layout by controlling two things: bucketing and partitioning (discussed momentarily). 先前我们看到，可以通过在写入之前控制分区来控制所写入文件的并行性。我们还可以通过控制两件事来控制特定的数据布局：存储和分区（暂时讨论）。 Splittable File Types and CompressionCertain file formats are fundamentally “splittable.” This can improve speed because it makes it possible for Spark to avoid reading an entire file, and access only the parts of the file necessary to satisfy your query. Additionally if you’re using something like Hadoop Distributed File System (HDFS), splitting a file can provide further optimization if that file spans multiple blocks. In conjunction with this is a need to manage compression. Not all compression schemes are splittable. How you store your data is of immense consequence when it comes to making your Spark jobs run smoothly. We recommend Parquet with gzip compression. 某些文件格式从根本上讲是“可拆分的”。这可以提高速度，因为它使Spark可以避免读取整个文件，而仅访问满足查询所需的文件部分。此外，如果您使用的是Hadoop分布式文件系统（HDFS）之类的文件，则如果文件跨越多个块，则拆分文件可以提供进一步的优化。与此相关，需要管理压缩。并非所有压缩方案都是可拆分的。当使Spark作业平稳运行时，如何存储数据将产生巨大的后果。我们建议使用gzip压缩的Parquet。 Reading Data in ParallelMultiple executors cannot read from the same file at the same time necessarily, but they can read different files at the same time. In general, this means that when you read from a folder with multiple files in it, each one of those files will become a partition in your DataFrame and be read in by available executors in parallel (with the remaining queueing up behind the others). 多个 worker 不一定必须同时读取同一文件，但是他们可以同时读取不同的文件。通常，这意味着当您从其中包含多个文件的文件夹中读取时，这些文件中的每个文件都将成为DataFrame中的一个分区，并由可用的 executor 并行读取（其余文件排在其他文件后面）。 Writing Data in ParallelThe number of files or data written is dependent on the number of partitions the DataFrame has at the time you write out the data. By default, one file is written per partition of the data. This means that although we specify a “file,” it’s actually a number of files within a folder, with the name of the specified file, with one file per each partition that is written. 写入的文件或数据的数量取决于您写出数据时DataFrame拥有的分区数量。默认情况下，每个数据分区写入一个文件。这意味着尽管我们指定了一个“文件”，但实际上它是一个文件夹中的许多文件，具有指定文件的名称，每个写入的分区每个文件一个。 For example, the following code: 例如下面的代码 ： 1csvFile.repartition(5).write.format(&quot;csv&quot;).save(&quot;/tmp/multiple.csv&quot;) will end up with five files inside of that folder. As you can see from the list call: 最终将在该文件夹中包含五个文件。从列表调用中可以看到： 1234567ls /tmp/multiple.csv/tmp/multiple.csv/part-00000-767df509-ec97-4740-8e15-4e173d365a8b.csv/tmp/multiple.csv/part-00001-767df509-ec97-4740-8e15-4e173d365a8b.csv/tmp/multiple.csv/part-00002-767df509-ec97-4740-8e15-4e173d365a8b.csv/tmp/multiple.csv/part-00003-767df509-ec97-4740-8e15-4e173d365a8b.csv/tmp/multiple.csv/part-00004-767df509-ec97-4740-8e15-4e173d365a8b.csv PartitioningPartitioning is a tool that allows you to control what data is stored (and where) as you write it. When you write a file to a partitioned directory (or table), you basically encode a column as a folder. What this allows you to do is skip lots of data when you go to read it in later, allowing you to read in only the data relevant to your problem instead of having to scan the complete dataset. These are supported for all file-based data sources: 分区是一种工具，可让您在写入数据时控制要存储的数据（以及存储在何处）。当您将文件写入分区目录（或表）时，基本上将一列编码为文件夹。这允许您执行的操作是稍后读入时跳过许多数据，从而仅读取与问题相关的数据，而不必扫描整个数据集。所有基于文件的数据源均支持以下功能： 123// in ScalacsvFile.limit(10).write.mode("overwrite").partitionBy("DEST_COUNTRY_NAME").save("/tmp/partitioned-files.parquet") 123# in PythoncsvFile.limit(10).write.mode("overwrite").partitionBy("DEST_COUNTRY_NAME")\.save("/tmp/partitioned-files.parquet") Upon writing, you get a list of folders in your Parquet “file”: 编写后，您会在Parquet“文件”中获得一个文件夹列表： 1$ ls /tmp/partitioned-files.parquet 123456...DEST_COUNTRY_NAME=Costa Rica/DEST_COUNTRY_NAME=Egypt/DEST_COUNTRY_NAME=Equatorial Guinea/DEST_COUNTRY_NAME=Senegal/DEST_COUNTRY_NAME=United States/ Each of these will contain Parquet files that contain that data where the previous predicate was true: 其中每个将包含Parquet文件，这些文件包含先前谓词为 true 的数据： 12$ ls /tmp/partitioned-files.parquet/DEST_COUNTRY_NAME=Senegal/part-00000-tid.....parquet This is probably the lowest-hanging optimization that you can use when you have a table that readers frequently filter by before manipulating. For instance, date is particularly common for a partition because, downstream, often we want to look at only the previous week’s data (instead of scanning the entire list of records). This can provide massive speedups for readers. 当您拥有一个表且读取器（reader）在操作这个表之前经常对其进行过滤，那么这可能是您可以使用的最容易优化。例如，日期在分区中尤为常见，因为在下游，我们通常只希望查看前一周的数据（而不是扫描整个记录列表）。这可以为读者提供巨大的提速。 Bucketing Bucketing is another file organization approach with which you can control the data that is specifically written to each file. This can help avoid shuffles later when you go to read the data because data with the same bucket ID will all be grouped together into one physical partition. This means that the data is prepartitioned according to how you expect to use that data later on, meaning you can avoid expensive shuffles when joining or aggregating. 存储桶是另一种文件组织方法，可用于控制专门写入每个文件的数据。这样可以避免以后再读取数据时发生洗牌（shuffle），因为具有相同存储区ID的数据将全部分组到一个物理分区中。这意味着将根据您以后使用数据的方式对数据进行预分区，这意味着您可以避免在合并或聚合时进行代价很高的洗牌（shuffle）。 Rather than partitioning on a specific column (which might write out a ton of directories), it’s probably worthwhile to explore bucketing the data instead. This will create a certain number of files and organize our data into those “buckets”: 与其在特定的列上进行分区（可能会写出大量的目录），不如探索对数据进行存储桶化。这将创建一定数量的文件，并将我们的数据组织到这些“存储桶”中： 1234val numberBuckets = 10val columnToBucketBy = "count"csvFile.write.format("parquet").mode("overwrite").bucketBy(numberBuckets, columnToBucketBy).saveAsTable("bucketedFiles") 123456$ ls /user/hive/warehouse/bucketedfiles/part-00000-tid-1020575097626332666-8....parquetpart-00000-tid-1020575097626332666-8....parquetpart-00000-tid-1020575097626332666-8....parquet... Bucketing is supported only for Spark-managed tables. For more information on bucketing and partitioning, watch this talk from Spark Summit 2017. 仅受Spark托管的表支持存储桶。有关存储分区和分区的更多信息，请观看Spark Summit 2017上的讨论。 译者附摘录： 本书第10章：《What Is SQL?》 One important note is the concept of managed versus unmanaged tables. Tables store two important pieces of information. The data within the tables as well as the data about the tables; that is, the metadata. You can have Spark manage the metadata for a set of files as well as for the data. When you define a table from files on disk, you are defining an unmanaged table. When you use saveAsTable on a DataFrame, you are creating a managed table for which Spark will track of all of the relevant information. 重要说明之一是托管表与非托管表的概念。表存储两个重要的信息。表中的数据以及有关表的数据；即元数据。您可以让Spark管理一组文件和数据的元数据。当您从磁盘上的文件定义表时，就是在定义非托管表。在DataFrame上使用 saveAsTable 时，您将创建一个托管表，Spark会为其跟踪所有相关信息。 This will read your table and write it out to a new location in Spark format. You can see this reflected in the new explain plan. In the explain plan, you will also notice that this writes to the default Hive warehouse location. You can set this by setting the spark.sql.warehouse.dir configuration to the directory of your choosing when you create your SparkSession. By default Spark sets this to /user/hive/warehouse: 这将读取您的表并将其以Spark格式写到新位置。您可以在新的计划说明（explain plan）中看到这一点。在计划说明（explain plan）中，您还将注意到这将写入默认的Hive仓库位置。您可以通过在创建SparkSession时将spark.sql.warehouse.dir配置设置为所选目录来进行设置。默认情况下，Spark将其设置为 /user/hive/warehouse： you can also see tables in a specific database by using the query show tables IN databaseName, where databaseName represents the name of the database that you want to query. 您还可以使用查询：show tables IN databaseName 查看特定数据库中的表，其中 databaseName 代表要查询的数据库的名称。 Writing Complex TypesAs we covered in Chapter 6, Spark has a variety of different internal types. Although Spark can work with all of these types, not every single type works well with every data file format. For instance, CSV files do not support complex types, whereas Parquet and ORC do. 如第6章所述，Spark具有多种不同的内部类型。尽管Spark可以使用所有这些类型，但是并非每种类型都能很好地适用于每种数据文件格式。例如，CSV文件不支持复杂类型，而Parquet和ORC支持。 Managing File Size Managing file sizes is an important factor not so much for writing data but reading it later on. When you’re writing lots of small files, there’s a significant metadata overhead that you incur managing all of those files. Spark especially does not do well with small files, although many file systems (like HDFS) don’t handle lots of small files well, either. You might hear this referred to as the “small file problem.” The opposite is also true: you don’t want files that are too large either, because it becomes inefficient to have to read entire blocks of data when you need only a few rows. 相对于写数据，管理文件大小对于稍后读取是一个十分重要的因素。当您写很多小文件时，管理所有这些文件会产生相当大的元数据开销。尽管许多文件系统（例如HDFS）也不能很好地处理许多小文件，但Spark尤其不适用于小文件。您可能会听到被称为“小文件问题”的情况。相反的情况也是如此：您也不想太大的文件，因为当您只需要几行数据时，不得不读取整个数据块是低效的。 Spark 2.2 introduced a new method for controlling file sizes in a more automatic way. We saw previously that the number of output files is a derivative of the number of partitions we had at write time (and the partitioning columns we selected). Now, you can take advantage of another tool in order to limit output file sizes so that you can target an optimum file size. You can use the maxRecordsPerFile option and specify a number of your choosing. This allows you to better control file sizes by controlling the number of records that are written to each file. For example, if you set an option for a writer as df.write.option(&quot;maxRecordsPerFile&quot;, 5000), Spark will ensure that files will contain at most 5,000 records. Spark 2.2引入了一种新方法，可以更自动地控制文件大小。先前我们看到输出文件的数量是写入时我们拥有的分区数量（以及我们选择的分区列）的派生数。现在，您可以利用另一个工具来限制输出文件的大小，以便您可以确定最佳的文件大小。您可以使用maxRecordsPerFile选项指定一个数字。这样可以通过控制写入每个文件的记录数来更好地控制文件大小。例如，如果您将写入器（writer）的选项设置为 df.write.option(&quot;maxRecordsPerFile&quot;, 5000)，Spark将确保文件最多包含5,000条记录。 ConclusionIn this chapter we discussed the variety of options available to you for reading and writing data in Spark. This covers nearly everything you’ll need to know as an everyday user of Spark. For the curious, there are ways of implementing your own data source; however, we omitted instructions for how to do this because the API is currently evolving to better support Structured Streaming. If you’re interested in seeing how to implement your own custom data sources, the Cassandra Connector is well organized and maintained and could provide a reference for the adventurous. 在本章中，我们讨论了可用于在Spark中读写数据的各种选项。这几乎涵盖了您作为Spark的日常用户需要了解的所有内容。出于好奇，有几种方法可以实现您自己的数据源。但是，我们省略了有关如何执行此操作的说明，因为API正在不断发展以更好地支持结构化流。如果您有兴趣了解如何实现自己的自定义数据源，则Cassandra Connector的组织和维护良好，可以为喜欢冒险的人提供参考。 In Chapter 10, we discuss Spark SQL and how it interoperates with everything else we’ve seen so far in the Structured APIs. 在第10章中，我们将讨论Spark SQL以及它如何与到目前为止在结构化API中看到的所有其他事物进行交互。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 10. Spark SQL]]></title>
    <url>%2F2019%2F10%2F20%2FChapter10_Spark-SQL(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 10. Spark SQL 译者：https://snaildove.github.ioSpark SQL is arguably one of the most important and powerful features in Spark. This chapter introduces the core concepts in Spark SQL that you need to understand. This chapter will not rewrite the ANSI-SQL specification or enumerate every single kind of SQL expression. If you read any other parts of this book, you will notice that we try to include SQL code wherever we include DataFrame code to make it easy to cross-reference with code samples. Other examples are available in the appendix and reference sections. Spark SQL可以说是Spark中最重要和最强大的功能之一。本章介绍您需要了解的Spark SQL核心概念。本章将不会重写ANSI-SQL规范或枚举每种SQL表达式。如果您阅读本书的其他部分，将会发现我们尝试在包含DataFrame代码的任何地方都包含SQL代码，以便于与代码示例进行交叉引用。附录和参考部分提供了其他示例。 In a nutshell, with Spark SQL you can run SQL queries against views or tables organized into databases. You also can use system functions or define user functions and analyze query plans in order to optimize their workloads. This integrates directly into the DataFrame and Dataset API, and as we saw in previous chapters, you can choose to express some of your data manipulations in SQL and others in DataFrames and they will compile to the same underlying code. 简而言之，使用Spark SQL，您可以对组织到数据库中的视图或表运行SQL查询。您还可以使用系统函数或定义用户函数并分析查询计划，以优化其工作量。它直接集成到DataFrame和Dataset API中，正如我们在前几章中所看到的，您可以选择在SQL中表达某些数据操作，在DataFrames中表达其他数据操作，它们将编译为相同的基础代码。 What Is SQL?SQL or Structured Query Language is a domain-specific language for expressing relational operations over data. It is used in all relational databases, and many “NoSQL” databases create their SQL dialect in order to make working with their databases easier. SQL is everywhere, and even though tech pundits prophesized its death, it is an extremely resilient data tool that many businesses depend on. Spark implements a subset of ANSI SQL:2003. This SQL standard is one that is available in the majority of SQL databases and this support means that Spark successfully runs the popular benchmark TPC-DS. SQL或结构化查询语言是一种特定领域的语言，用于表达对数据的关系操作。它在所有关系数据库中使用，许多“ NoSQL”数据库创建其SQL方言，以便于使用其数据库。SQL无处不在，即使技术专家预言了它的消亡，它还是许多企业所依赖的极其灵活的数据工具。Spark实现了ANSI SQL：2003的子集。此SQL标准是大多数SQL数据库中可用的标准，并且这种支持意味着Spark成功运行了流行的基准TPC-DS。 Big Data and SQL: Apache HiveBefore Spark’s rise, Hive was the de facto big data SQL access layer. Originally developed at Facebook, Hive became an incredibly popular tool across industry for performing SQL operations on big data. In many ways it helped propel Hadoop into different industries because analysts could run SQL queries. Although Spark began as a general processing engine with Resilient Distributed Datasets (RDDs), a large cohort of users now use Spark SQL. 在Spark崛起之前，Hive是事实上的大数据SQL访问层。Hive最初是在Facebook上开发的，已成为行业内非常流行的工具，用于对大数据执行SQL操作。它以多种方式帮助将Hadoop推向不同的行业，因为分析师可以运行SQL查询。尽管Spark最初是使用弹性分布式数据集（RDD）作为通用处理引擎，但现在有大量用户使用Spark SQL。 Big Data and SQL: Spark SQLWith the release of Spark 2.0, its authors created a superset of Hive’s support, writing a native SQL parser that supports both ANSI-SQL as well as HiveQL queries. This, along with its unique interoperability with DataFrames, makes it a powerful tool for all sorts of companies. For example, in late 2016, Facebook announced that it had begun running Spark workloads and seeing large benefits in doing so. In the words of the blog post’s authors: 随着Spark 2.0的发布，其作者创建了Hive支持的超集，编写了支持ANSI-SQL和HiveQL查询的本地SQL解析器。这以及它与DataFrames的独特互操作性，使其成为各种公司的强大工具。例如，在2016年末，Facebook宣布它已开始运行Spark工作负载，并看到这样做有很大的好处。用博客文章作者的话来说： We challenged Spark to replace a pipeline that decomposed to hundreds of Hive jobs into a single Spark job. Through a series of performance and reliability improvements, we were able to scale Spark to handle one of our entity ranking data processing use cases in production…. The Spark-based pipeline produced significant performance improvements (4.5–6x CPU, 3–4x resource reservation, and ~5x latency) compared with the old Hive-based pipeline, and it has been running in production for several months. 我们向Spark提出挑战，要求将分解成数百个Hive作业的管道替换为单个Spark作业。通过一系列的性能和可靠性改进，我们能够扩展Spark以处理生产中我们对实体排名数据处理用例之一的需求。与基于Hive的旧管道相比，基于Spark的管道显着提高了性能（4.5-6倍CPU，3-4倍资源预留和约5倍延迟），并且已经在生产中运行了几个月。 The power of Spark SQL derives from several key facts: SQL analysts can now take advantage of Spark’s computation abilities by plugging into the Thrift Server or Spark’s SQL interface, whereas data engineers and scientists can use Spark SQL where appropriate in any data flow. This unifying API allows for data to be extracted with SQL, manipulated as a DataFrame, passed into one of Spark MLlibs’ large-scale machine learning algorithms, written out to another data source, and everything in between. Spark SQL的强大功能来自几个关键事实：现在，SQL分析师可以通过插入Thrift Server或Spark的SQL接口来利用Spark的计算能力，而数据工程师和科学家可以在任何数据流中酌情使用Spark SQL。这个统一的API允许使用SQL提取数据，将其作为DataFrame进行操作，传递到Spark MLlibs的大型机器学习算法之一中，写出到另一个数据源中，以及介于两者之间的所有内容。 NOTE 注意Spark SQL is intended to operate as an online analytic processing (OLAP) database, not an online transaction processing (OLTP) database. This means that it is not intended to perform extremely low-latency queries. Even though support for in-place modifications is sure to be something that comes up in the future, it’s not something that is currently available. Spark SQL旨在用作联机分析处理（OLAP）数据库，而不是联机事务处理（OLTP）数据库。这意味着它不打算执行极低延迟的查询。即使将来肯定会支持就地修改，但目前尚不支持。 Spark’s Relationship to HiveSpark SQL has a great relationship with Hive because it can connect to Hive metastores. The Hive metastore is the way in which Hive maintains table information for use across sessions. With Spark SQL, you can connect to your Hive metastore (if you already have one) and access table metadata to reduce file listing when accessing information. This is popular for users who are migrating from a legacy Hadoop environment and beginning to run all their workloads using Spark. Spark SQL与Hive有着密切的关系，因为它可以连接到Hive元存储。Hive元存储库是Hive维护表信息以供跨会话使用的方式。使用Spark SQL，您可以连接到Hive元存储（如果已经拥有一个）并访问表元数据以减少访问信息时的文件列表。非常受遗留的Hadoop环境迁移并开始使用Spark运行其所有工作负载的用户欢迎。 The Hive metastoreTo connect to the Hive metastore, there are several properties that you’ll need. First, you need to set the Metastore version (spark.sql.hive.metastore.version) to correspond to the proper Hive metastore that you’re accessing. By default, this value is 1.2.1. You also need to set spark.sql.hive.metastore.jars if you’re going to change the way that the HiveMetastoreClient is initialized. Spark uses the default versions, but you can also specify Maven repositories or a classpath in the standard format for the Java Virtual Machine (JVM). In addition, you might need to supply proper class prefixes in order to communicate with different databases that store the Hive metastore. You’ll set these as shared prefixes that both Spark and Hive will share (spark.sql.hive.metastore.sharedPrefixes). 要连接到 Hive Metastore，您需要几个属性。首先，您需要将 Metastore 版本（spark.sql.hive.metastore.version）设置为与您正在访问的正确的 Hive Metastore 相对应。默认情况下，此值为1.2.1。如果您要更改 HiveMetastoreClient 的初始化方式，则还需要设置 spark.sql.hive.metastore.jars 。Spark使用默认版本，但是您也可以为Java虚拟机（JVM）以标准格式指定Maven存储库或类路径。此外，您可能需要提供适当的类前缀才能与存储Hive元存储库的其他数据库进行通信。您将这些设置为Spark和Hive都将共享的共享前缀（spark.sql.hive.metastore.sharedPrefixes）。 If you’re connecting to your own metastore, it’s worth checking the documentation for further updates and more information. 如果您要连接到自己的元存储库，则值得查看文档以获取更多更新和更多信息。 How to Run Spark SQL QueriesSpark provides several interfaces to execute SQL queries. Spark提供了几个接口来执行SQL查询。 Spark SQL CLI The Spark SQL CLI is a convenient tool with which you can make basic Spark SQL queries in local mode from the command line. Note that the Spark SQL CLI cannot communicate with the Thrift JDBC server. To start the Spark SQL CLI, run the following in the Spark directory: Spark SQL CLI是一种方便的工具，您可以使用它从命令行在本地模式下进行基本的Spark SQL查询。请注意，Spark SQL CLI无法与Thrift JDBC服务器通信。要启动Spark SQL CLI，请在Spark目录中运行以下命令： 1./bin/spark-sql You configure Hive by placing your hive-site.xml, core-site.xml, and hdfs-site.xml files in conf/. For a complete list of all available options, you can run ./bin/spark-sql --help. 您可以通过将 hive-site.xml，core-site.xml 和 hdfs-site.xml 文件放在 conf/ 中来配置Hive。有关所有可用选项的完整列表，可以运行./bin/spark-sql --help。 Spark’s Programmatic SQL InterfaceIn addition to setting up a server, you can also execute SQL in an ad hoc manner via any of Spark’s language APIs. You can do this via the method sql on the SparkSession object. This returns a DataFrame, as we will see later in this chapter. For example, in Python or Scala, we can run the following: 除了设置服务器之外，您还可以通过任意Spark语言API以临时方式执行SQL。您可以通过SparkSession对象上的sql方法执行此操作。这将返回一个DataFrame，我们将在本章后面看到。例如，在Python或Scala中，我们可以运行以下命令： 1spark.sql("SELECT 1 + 1").show() The command spark.sql(&quot;SELECT 1 + 1&quot;) returns a DataFrame that we can then evaluate programmatically. Just like other transformations, this will not be executed eagerly but lazily. This is an immensely powerful interface because there are some transformations that are much simpler to express in SQL code than in DataFrames. 命令 spark.sql(&quot;SELECT 1 + 1&quot;) 返回一个DataFrame，然后我们可以通过编程对其求值。就像其他转换一样，这不会急于执行，而是懒惰地执行。这是一个非常强大的接口，因为在SQL代码中表达的某些转换比在DataFrames中表达的转换要简单得多。 You can express multiline queries quite simply by passing a multiline string into the function. For example, you could execute something like the following code in Python or Scala: 通过将多行字符串传递到函数中，可以非常简单地表达多行查询。例如，您可以在Python或Scala中执行类似以下代码的操作： 12spark.sql("""SELECT user_id, department, first_name FROM professors WHERE department IN(SELECT name FROM department WHERE created_date &gt;= '2016-01-01')""") Even more powerful, you can completely interoperate between SQL and DataFrames, as you see fit. For instance, you can create a DataFrame, manipulate it with SQL, and then manipulate it again as a DataFrame. It’s a powerful abstraction that you will likely find yourself using quite a bit: 更加强大的是，您可以根据需要在SQL和DataFrame之间完全进行互操作。例如，您可以创建一个DataFrame，使用SQL对其进行操作，然后再次将其作为DataFrame进行操作。这是一个强大的抽象，您可能会发现自己经常使用： 12345678// in Scalaspark.read.json("/data/flight-data/json/2015-summary.json").createOrReplaceTempView("some_sql_view") // DF =&gt; SQLspark.sql("""SELECT DEST_COUNTRY_NAME, sum(count)FROM some_sql_view GROUP BY DEST_COUNTRY_NAME""").where("DEST_COUNTRY_NAME like 'S%'").where("`sum(count)` &gt; 10").count() // SQL =&gt; DF 123456789# in Pythonspark.read.json("/data/flight-data/json/2015-summary.json")\.createOrReplaceTempView("some_sql_view") # DF =&gt; SQLspark.sql("""SELECT DEST_COUNTRY_NAME, sum(count)FROM some_sql_view GROUP BY DEST_COUNTRY_NAME""")\.where("DEST_COUNTRY_NAME like 'S%'").where("`sum(count)` &gt; 10")\.count() # SQL =&gt; DF SparkSQL Thrift JDBC/ODBC ServerSpark provides a Java Database Connectivity (JDBC) interface by which either you or a remote program connects to the Spark driver in order to execute Spark SQL queries. A common use case might be a for a business analyst to connect business intelligence software like Tableau to Spark. The Thrift JDBC/Open Database Connectivity (ODBC) server implemented here corresponds to the HiveServer2 in Hive 1.2.1. You can test the JDBC server with the beeline script that comes with either Spark or Hive 1.2.1. Spark提供了Java数据库连接（JDBC）接口，可通过该接口，您自己或远程程序连接到Spark驱动程序以执行Spark SQL查询。对于业务分析师来说，一个常见的用例可能是将Tableau之类的商业智能软件连接到Spark。此处实现的Thrift JDBC/开放数据库连接（ODBC）服务器对应于Hive 1.2.1中的HiveServer2。您可以使用Spark或Hive 1.2.1附带的beeline脚本测试JDBC服务器。 To start the JDBC/ODBC server, run the following in the Spark directory : 要启动 JDBC/ODBC 服务器，请在Spark目录中运行以下命令： 1./sbin/start-thriftserver.sh This script accepts all bin/spark-submit command-line options. To see all available options for configuring this Thrift Server, run ./sbin/start-thriftserver.sh --help. By default, the server listens on localhost:10000. You can override this through environmental variables or system properties 该脚本接受所有 bin/spark-submit 命令行选项。要查看用于配置此Thrift Server的所有可用选项，请运行./sbin/start-thriftserver.sh --help。默认情况下，服务器在 localhost:10000 上侦听。您可以通过环境变量或系统属性来覆盖它。 For environment configuration, use this: 对于环境配置，请使用以下命令： 12345export HIVE_SERVER2_THRIFT_PORT=&lt;listening-port&gt;export HIVE_SERVER2_THRIFT_BIND_HOST=&lt;listening-host&gt;./sbin/start-thriftserver.sh \--master &lt;master-uri&gt; \... For system properties: 对于系统属性： 12345./sbin/start-thriftserver.sh \--hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \--hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \--master &lt;master-uri&gt;... You can then test this connection by running the following commands: 然后，您可以通过运行以下命令来测试此连接： 12./bin/beelinebeeline&gt; !connect jdbc:hive2://localhost:10000 Beeline will ask you for a username and password. In nonsecure mode, simply type the username on your machine and a blank password. For secure mode, follow the instructions given in the beeline documentation. Beeline会要求您提供用户名和密码。在非安全模式下，只需在计算机上键入用户名和空白密码即可。对于安全模式，请遵循 beeline 文档中给出的说明。 CatalogThe highest level abstraction in Spark SQL is the Catalog. The Catalog is an abstraction for the storage of metadata about the data stored in your tables as well as other helpful things like databases, tables, functions, and views. The catalog is available in the org.apache.spark.sql.catalog.Catalog package and contains a number of helpful functions for doing things like listing tables, databases, and functions. We will talk about all of these things shortly. It’s very self-explanatory to users, so we will omit the code samples here but it’s really just another programmatic interface to Spark SQL. This chapter shows only the SQL being executed; thus, if you’re using the programmatic interface, keep in mind that you need to wrap everything in a spark.sql function call to execute the relevant code. Spark SQL中最高级别的抽象是Catalog。Catalog是用于存储相关表中存储的数据以及其他有用信息（如数据库，表，函数和视图）的元数据的抽象。该目录可在org.apache.spark.sql.catalog.Catalog包中找到，并包含许多有用的函数，用于执行诸如列出表，数据库和函数之类的操作。我们很快将讨论所有这些事情。这对用户来说不言自明，因此我们在这里省略了代码示例，但实际上它只是Spark SQL的另一个编程接口。本章仅显示正在执行的SQL。因此，如果您使用的是编程接口，请记住，您需要将所有内容包装在spark.sql函数调用中以执行相关代码。 TablesTo do anything useful with Spark SQL, you first need to define tables. Tables are logically equivalent to a DataFrame in that they are a structure of data against which you run commands. We can join tables, filter them, aggregate them, and perform different manipulations that we saw in previous chapters. The core difference between tables and DataFrames is this: you define DataFrames in the scope of a programming language, whereas you define tables within a database. This means that when you create a table (assuming you never changed the database), it will belong to the default database. We discuss databases more fully later on in the chapter. 为了对Spark SQL做任何有用的事情，您首先需要定义表。表在逻辑上等效于DataFrame，因为它们是运行命令所依据的数据结构。我们可以联接表，对其进行过滤，对其进行汇总，并执行在上一章中看到的不同操作。表和DataFrames之间的核心区别在于：您可以在编程语言范围内定义DataFrames，而可以在数据库中定义表。这意味着在创建表时（假设您从未更改过数据库），该表将属于默认数据库。我们将在本章后面更全面地讨论数据库。 An important thing to note is that in Spark 2.X, tables always contain data. There is no notion of a temporary table, only a view, which does not contain data. This is important because if you go to drop a table, you can risk losing the data when doing so. 需要注意的重要一点是，在Spark 2.X中，表始终包含数据。没有临时表的概念，只有一个不包含数据的视图。这很重要，因为如果要删除表，则可能会丢失数据。 Spark-Managed TablesOne important note is the concept of managed versus unmanaged tables. Tables store two important pieces of information. The data within the tables as well as the data about the tables; that is, the metadata. You can have Spark manage the metadata for a set of files as well as for the data. When you define a table from files on disk, you are defining an unmanaged table. When you use saveAsTable on a DataFrame, you are creating a managed table for which Spark will track of all of the relevant information. 重要说明之一是托管表与非托管表的概念。表存储两个重要的信息。表中的数据以及有关表的数据；即元数据。您可以让Spark管理一组文件和数据的元数据。当您从磁盘上的文件定义表时，就是在定义非托管表。在DataFrame上使用 saveAsTable 时，您将创建一个托管表，Spark会为其跟踪所有相关信息。 This will read your table and write it out to a new location in Spark format. You can see this reflected in the new explain plan. In the explain plan, you will also notice that this writes to the default Hive warehouse location. You can set this by setting the spark.sql.warehouse.dir configuration to the directory of your choosing when you create your SparkSession. By default Spark sets this to /user/hive/warehouse: 这将读取您的表并将其以Spark格式写到新位置。您可以在新的计划说明（explain plan）中看到这一点。在计划说明（explain plan）中，您还将注意到这将写入默认的Hive仓库位置。您可以通过在创建SparkSession时将spark.sql.warehouse.dir配置设置为所选目录来进行设置。默认情况下，Spark将其设置为 /user/hive/warehouse： you can also see tables in a specific database by using the query show tables IN databaseName, where databaseName represents the name of the database that you want to query. 您还可以使用查询：show tables IN databaseName 查看特定数据库中的表，其中 databaseName 代表要查询的数据库的名称。 If you are running on a new cluster or local mode, this should return zero results. 如果您在新的集群或本地模式上运行，则应返回零结果。 Creating TablesYou can create tables from a variety of sources. Something fairly unique to Spark is the capability of reusing the entire Data Source API within SQL. This means that you do not need to define a table and then load data into it; Spark lets you create one on the fly. You can even specify all sorts of sophisticated options when you read in a file. For example, here’s a simple way to read in the flight data we worked with in previous chapters: 您可以从多种来源创建表。Spark相当独特的功能是可以在SQL中重用整个数据源API。这意味着您无需定义表然后再将数据加载到表中。Spark可让您即时创建一个。读取文件时，甚至可以指定各种复杂的选项。例如，这是一种读取我们先前章节中使用的航班数据的简单方法： 123CREATE TABLE flights (DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)USING JSON OPTIONS (path &apos;/data/flight-data/json/2015-summary.json&apos;) USING AND STORED AS The specification of the USING syntax in the previous example is of significant importance. If you do not specify the format, Spark will default to a Hive SerDe configuration. This has performance implications for future readers and writers because Hive SerDes are much slower than Spark’s native serialization. Hive users can also use the STORED AS syntax to specify that this should be a Hive table.前面示例中的USING语法规范非常重要。如果未指定格式，Spark将默认为Hive SerDe配置。由于Hive SerDes比Spark的本地序列化要慢得多，因此这对将来的读取器（reader）和写入器（writer）都有性能影响。Hive用户还可以使用STORED AS语法来指定此表应为Hive表。 You can also add comments to certain columns in a table, which can help other developers understand the data in the tables: 您还可以将注释添加到表中的某些列，这可以帮助其他开发人员理解表中的数据： 12345CREATE TABLE flights_csv (DEST_COUNTRY_NAME STRING,ORIGIN_COUNTRY_NAME STRING COMMENT &quot;remember, the US will be most prevalent&quot;,count LONG)USING csv OPTIONS (header true, path &apos;/data/flight-data/csv/2015-summary.csv&apos;) It is possible to create a table from a query as well : 也可以通过查询创建表： 1CREATE TABLE flights_from_select USING parquet AS SELECT * FROM flights In addition, you can specify to create a table only if it does not currently exist: 此外，您可以指定仅在当前不存在的情况下创建表： NOTE 注意In this example, we are creating a Hive-compatible table because we did not explicitly specify the format via USING. We can also do the following 在此示例中，我们将创建一个兼容Hive的表，因为我们没有通过USING明确指定格式。我们还可以执行以下操作 12CREATE TABLE IF NOT EXISTS flights_from_selectAS SELECT * FROM flights Finally, you can control the layout of the data by writing out a partitioned dataset, as we saw in Chapter 9: 最后，您可以通过写出分区的数据集来控制数据的布局，如我们在第9章中所看到的： 12CREATE TABLE partitioned_flights USING parquet PARTITIONED BY (DEST_COUNTRY_NAME)AS SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 5 These tables will be available in Spark even through sessions; temporary tables do not currently exist in Spark. You must create a temporary view, which we demonstrate later in this chapter. 这些表甚至可以通过会话在Spark中使用；临时表目前在Spark中不存在。您必须创建一个临时视图，我们将在本章稍后进行演示。 Creating External TablesAs we mentioned in the beginning of this chapter, Hive was one of the first big data SQL systems, and Spark SQL is completely compatible with Hive SQL (HiveQL) statements. One of the use cases that you might encounter is to port your legacy Hive statements to Spark SQL. Luckily, you can, for the most part, just copy and paste your Hive statements directly into Spark SQL. For example, in the example that follows, we create an unmanaged table. Spark will manage the table’s metadata; however, the files are not managed by Spark at all. You create this table by using the CREATE EXTERNAL TABLE statement. 如本章开头所述，Hive是最早的大数据SQL系统之一，Spark SQL与Hive SQL（HiveQL）语句完全兼容。您可能会遇到的一种使用情况是将旧的Hive语句移植到Spark SQL。幸运的是，在大多数情况下，您只需将Hive语句直接复制并粘贴到Spark SQL中即可。例如，在下面的示例中，我们创建一个 非托管表。Spark将管理表格的元数据；但是，这些文件完全不受Spark管理。通过使用CREATE EXTERNAL TABLE语句创建此表。 You can view any files that have already been defined by running the following command: 您可以通过运行以下命令来查看任何已定义的文件： 123CREATE EXTERNAL TABLE hive_flights (DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; LOCATION &apos;/data/flight-data-hive/&apos; You can also create an external table from a select clause: 您还可以从select子句创建外部表： 12CREATE EXTERNAL TABLE hive_flights_2ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;LOCATION &apos;/data/flight-data-hive/&apos; AS SELECT * FROM flights Inserting into TablesInsertions follow the standard SQL syntax: 插入遵循标准的SQL语法： 12INSERT INTO flights_from_selectSELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 20 You can optionally provide a partition specification if you want to write only into a certain partition. Note that a write will respect a partitioning scheme, as well (which may cause the above query to run quite slowly); however, it will add additional files only into the end partitions: 如果您只想写入某个分区，则可以选择提供分区规范。注意，写操作也将遵循分区方案（这可能导致上述查询运行得很慢）。但是，它只会将其他文件添加到最终分区中： 1234INSERT INTO partitioned_flightsPARTITION (DEST_COUNTRY_NAME=&quot;UNITED STATES&quot;)SELECT count, ORIGIN_COUNTRY_NAME FROM flightsWHERE DEST_COUNTRY_NAME=&apos;UNITED STATES&apos; LIMIT 12 Describing Table MetadataWe saw earlier that you can add a comment when creating a table. You can view this by describing the table metadata, which will show us the relevant comment: 前面我们看到，您可以在创建表时添加注释。您可以通过描述表的元数据来查看此信息，这将向我们显示相关注释： 1DESCRIBE TABLE flights_csv You can also see the partitioning scheme for the data by using the following (note, however, that this works only on partitioned tables): 您还可以通过使用以下内容查看数据的分区方案（但是请注意，这仅适用于分区表）： 1SHOW PARTITIONS partitioned_flights Refreshing Table MetadataMaintaining table metadata is an important task to ensure that you’re reading from the most recent set of data. There are two commands to refresh table metadata. REFRESH TABLE refreshes all cached entries (essentially, files) associated with the table. If the table were previously cached, it would be cached lazily the next time it is scanned: 维护表元数据是一项重要的任务，以确保您正在从最新的数据集中进行读取。有两个命令可以刷新表元数据。REFRESH TABLE刷新与该表关联的所有缓存条目（实质上是文件）。如果该表先前已被缓存，则下次扫描时将被延迟缓存： 1REFRESH table partitioned_flights Another related command is REPAIR TABLE, which refreshes the partitions maintained in the catalog for that given table. This command’s focus is on collecting new partition information—an example might be writing out a new partition manually and the need to repair the table accordingly: 另一个相关的命令是REPAIR TABLE，它刷新给定表在目录中维护的分区。该命令的重点是收集新的分区信息——例如，可能是手动写出新分区，并且需要相应地修复表： 1MSCK REPAIR TABLE partitioned_flights Dropping TablesYou cannot delete tables: you can only “drop” them. You can drop a table by using the DROP keyword. If you drop a managed table (e.g., flights_csv), both the data and the table definition will be removed: 您不能 delete 表：只能“drop”它们。您可以使用DROP关键字删除表。如果您删除托管表（例如，flights_csv），则数据和表定义都将被删除： 1DROP TABLE flights_csv; WARNING 警告Dropping a table deletes the data in the table, so you need to be very careful when doing this. 删除表会删除表中的数据，因此在执行此操作时需要非常小心。 If you try to drop a table that does not exist, you will receive an error. To only delete a table if it already exists, use DROP TABLE IF EXISTS. 如果尝试删除不存在的表，则会收到错误消息。要仅删除已存在的表，请使用DROP TABLE IF EXISTS。 1DROP TABLE IF EXISTS flights_csv; WARNING 警告This deletes the data in the table, so exercise caution when doing this. 这会删除表中的数据，因此请谨慎操作。 Dropping unmanaged tablesIf you are dropping an unmanaged table (e.g., hive_flights), no data will be removed but you will no longer be able to refer to this data by the table name. 如果要删除非托管表（例如hive_flights），则不会删除任何数据，但是您将不再能够通过表名引用该数据。 Caching TablesJust like DataFrames, you can cache and uncache tables. You simply specify which table you would like using the following syntax: 就像DataFrames一样，您可以缓存和取消缓存表。您只需使用以下语法指定要使用的表： 1CACHE TABLE flights Here’s how you uncache them: 解除缓存的方法如下： 1UNCACHE TABLE FLIGHTS ViewsNow that you created a table, another thing that you can define is a view. A view specifies a set of transformations on top of an existing table—basically just saved query plans, which can be convenient for organizing or reusing your query logic. Spark has several different notions of views. Views can be global, set to a database, or per session. 现在，您已经创建了一个表，您可以定义的另一件事是视图。视图在现有表的顶部指定一组转换（基本上只是保存的查询计划），可以方便地组织或重用查询逻辑。Spark有几种不同的视图概念。视图可以是全局视图，设置为数据库视图或每个会话。 Creating ViewsTo an end user, views are displayed as tables, except rather than rewriting all of the data to a new location, they simply perform a transformation on the source data at query time. This might be a filter, select, or potentially an even larger GROUP BY or ROLLUP. For instance, in the following example, we create a view in which the destination is United States in order to see only those flights: 对于末端用户，视图显示为表，除了将所有数据重写到新位置之外，它们只是在查询时对源数据执行转换。这可能是一个筛选器，一个选择，甚至可能是更大的GROUP BY 或 ROLLUP。例如，在以下示例中，我们创建一个目的地为美国的视图，以便仅查看那些航班： 12CREATE VIEW just_usa_view ASSELECT * FROM flights WHERE dest_country_name = &apos;United States&apos; Like tables, you can create temporary views that are available only during the current session and are not registered to a database: 像表一样，您可以创建仅在当前会话期间可用且未注册到数据库的临时视图： 12CREATE TEMP VIEW just_usa_view_temp ASSELECT * FROM flights WHERE dest_country_name = &apos;United States&apos; Or, it can be a global temp view. Global temp views are resolved regardless of database and are viewable across the entire Spark application, but they are removed at the end of the session: 或者，它可以是全局临时视图。无论使用哪种数据库，都可以解析全局临时视图，并且可以在整个Spark应用程序中查看它们，但是在会话结束时将其删除： 1234CREATE GLOBAL TEMP VIEW just_usa_global_view_temp ASSELECT * FROM flights WHERE dest_country_name = &apos;United States&apos;SHOW TABLES You can also specify that you would like to overwrite a view if one already exists by using the keywords shown in the sample that follows. We can overwrite both temp views and regular views: 您还可以使用下面的示例中显示的关键字，指定要覆盖的视图（如果已经存在）。我们可以覆盖临时视图和常规视图： 12CREATE OR REPLACE TEMP VIEW just_usa_view_temp ASSELECT * FROM flights WHERE dest_country_name = &apos;United States&apos; Now you can query this view just as if it were another table: 现在，您可以查询此视图，就像它是另一个表一样： 1SELECT * FROM just_usa_view_temp A view is effectively a transformation and Spark will perform it only at query time. This means that it will only apply that filter after you actually go to query the table (and not earlier). Effectively, views are equivalent to creating a new DataFrame from an existing DataFrame. 视图实际上是一种转换，Spark仅在查询时执行。这意味着它只会在您实际查询表之后（而不是更早）才应用该过滤器。实际上，视图等效于从现有DataFrame创建新DataFrame。 In fact, you can see this by comparing the query plans generated by Spark DataFrames and Spark SQL. In DataFrames, we would write the following: 实际上，您可以通过比较Spark DataFrames和Spark SQL生成的查询计划来看到这一点。在DataFrames中，我们将编写以下内容： 1234val flights = spark.read.format(&quot;json&quot;).load(&quot;/data/flight-data/json/2015-summary.json&quot;)val just_usa_df = flights.where(&quot;dest_country_name = &apos;United States&apos;&quot;)just_usa_df.selectExpr(&quot;*&quot;).explain In SQL, we would write (querying from our view) this: 在SQL中，我们将这样编写（从我们的视图中查询）： 1EXPLAIN SELECT * FROM just_usa_view Or, equivalently: 或者，等效地： 1EXPLAIN SELECT * FROM flights WHERE dest_country_name = &apos;United States&apos; Due to this fact, you should feel comfortable in writing your logic either on DataFrames or SQL— whichever is most comfortable and maintainable for you. 由于这个事实，在DataFrames或SQL上编写逻辑时应该感到很自在——无论哪种方法对您来说都是最舒适和可维护的。 Dropping ViewsYou can drop views in the same way that you drop tables; you simply specify that what you intend to drop is a view instead of a table. The main difference between dropping a view and dropping a table is that with a view, no underlying data is removed, only the view definition itself : 您可以像删除表一样删除视图。您只需指定要删除的是视图而不是表。删除视图和删除表之间的主要区别在于，使用视图时，不会删除任何基础数据，只会删除视图定义本身： 1DROP VIEW IF EXISTS just_usa_view; DatabasesDatabases are a tool for organizing tables. As mentioned earlier, if you do not define one, Spark will use the default database. Any SQL statements that you run from within Spark (including DataFrame commands) execute within the context of a database. This means that if you change the database, any user-defined tables will remain in the previous database and will need to be queried differently. 数据库是用于组织表的工具。如前所述，如果您未定义数据库，Spark将使用默认数据库。您在Spark中运行的所有SQL语句（包括DataFrame命令）都在数据库的上下文中执行。这意味着，如果您更改数据库，则任何用户定义的表都将保留在先前的数据库中，并且需要以其他方式查询。 WARNING 警告This can be a source of confusion, especially if you’re sharing the same context or session for your coworkers, so be sure to set your databases appropriately. 这可能会引起混乱，尤其是如果您要为同事共享相同的上下文或会话时，请确保正确设置数据库。 You can see all databases by using the following command: 您可以使用以下命令查看所有数据库： 1SHOW DATABASES Creating DatabasesCreating databases follows the same patterns you’ve seen previously in this chapter; however, hereyou use the CREATE DATABASE keywords: 创建数据库的方式与本章前面介绍的相同。但是，您在这里使用CREATE DATABASE关键字： 1CREATE DATABASE some_db Setting the DatabaseYou might want to set a database to perform a certain query. To do this, use the USE keyword followed by the database name: 您可能需要设置数据库以执行特定查询。为此，请使用USE关键字，后跟数据库名称： 1USE some_db After you set this database, all queries will try to resolve table names to this database. Queries that were working just fine might now fail or yield different results because you are in a different database: 设置该数据库后，所有查询将尝试将表名解析为该数据库。现在，运行良好的查询可能会失败或产生不同的结果，因为您位于其他数据库中： 12SHOW tablesSELECT * FROM flights -- fails with table/view not found However, you can query different databases by using the correct prefix: 但是，您可以使用正确的前缀来查询其他数据库： 1SELECT * FROM default.flights You can see what database you’re currently using by running the following command: 通过运行以下命令，您可以查看当前正在使用的数据库： 1SELECT current_database() You can, of course, switch back to the default database: 您当然可以切换回默认数据库： 1USE default; Dropping DatabasesDropping or removing databases is equally as easy: you simply use the DROP DATABASE keyword: 删除或删除数据库同样容易：您只需使用DROP DATABASE关键字： 1DROP DATABASE IF EXISTS some_db; Select StatementsQueries in Spark support the following ANSI SQL requirements (here we list the layout of the SELECT expression): Spark中的查询支持以下ANSI SQL要求（此处列出了SELECT表达式的布局）： 12345678910111213141516171819202122232425SELECT [ALL|DISTINCT] named_expression[, named_expression, ...] FROM relation[, relation, ...][lateral_view[, lateral_view, ...]] [WHERE boolean_expression] [aggregation [HAVING boolean_expression]] [ORDER BY sort_expressions] [CLUSTER BY expressions] [DISTRIBUTE BY expressions] [SORT BY sort_expressions] [WINDOW named_window[, WINDOW named_window, ...]] [LIMIT num_rows]named_expression:: expression [AS alias]relation: | join_relation | (table_name|query|relation) [sample] [AS alias] : VALUES (expressions)[, (expressions), ...] [AS (column_name[, column_name, ...])]expressions: : expression[, expression, ...]sort_expressions: : expression [ASC|DESC][, expression [ASC|DESC], ...] case…when…then StatementsOftentimes, you might need to conditionally replace values in your SQL queries. You can do this by using a case…when…then…end style statement. This is essentially the equivalent of programmatic if statements: 通常，您可能需要有条件地替换SQL查询中的值。您可以通过使用case … when … then … end 类型语句来实现。从本质上讲，这等效于程序化if语句： 12345SELECT CASE WHEN DEST_COUNTRY_NAME = &apos;UNITED STATES&apos; THEN 1 WHEN DEST_COUNTRY_NAME = &apos;Egypt&apos; THEN 0 ELSE -1 ENDFROM partitioned_flights Advanced TopicsNow that we defined where data lives and how to organize it, let’s move on to querying it. A SQL query is a SQL statement requesting that some set of commands be run. SQL statements can define manipulations, definitions, or controls. The most common case are the manipulations, which is the focus of this book. 现在我们定义了数据的存放位置以及如何组织数据，让我们继续进行数据查询。SQL查询是一条SQL语句，它要求运行某些命令集。SQL语句可以定义操作，进行定义或定义控制流（control）。最常见的情况是操作，这是本书的重点。 Complex TypesComplex types are a departure from standard SQL and are an incredibly powerful feature that does not exist in standard SQL. Understanding how to manipulate them appropriately in SQL is essential. There are three core complex types in Spark SQL: structs, lists, and maps. 复杂类型与标准SQL背道而驰，并且是标准SQL中不存在的强大功能。了解如何在SQL中适当地操作它们至关重要。 Spark SQL中存在三种核心复杂类型：结构，列表和映射。 StructsStructs are more akin to maps. They provide a way of creating or querying nested data in Spark. To create one, you simply need to wrap a set of columns (or expressions) in parentheses: 结构更类似于映射。它们提供了一种在Spark中创建或查询嵌套数据的方法。要创建一个，只需要将一组列（或表达式）括在括号中： 12CREATE VIEW IF NOT EXISTS nested_data ASSELECT (DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME) as country, count FROM flights Now, you can query this data to see what it looks like: 现在，您可以查询此数据以查看其外观： 1SELECT * FROM nested_data You can even query individual columns within a struct—all you need to do is use dot syntax: 您甚至可以查询结构中的各个列——您所需要做的就是使用点语法： 1SELECT country.DEST_COUNTRY_NAME, count FROM nested_data If you like, you can also select all the subvalues from a struct by using the struct’s name and select all of the subcolumns. Although these aren’t truly subcolumns, it does provide a simpler way to think about them because we can do everything that we like with them as if they were a column: 如果愿意，您还可以使用结构的名称从结构中选择所有子值，然后选择所有子列。尽管这些并不是真正的子列，但是它确实提供了一种更简单的方式来考虑它们，因为我们可以像对待专栏一样做我们喜欢的所有事情： 1SELECT country.*, count FROM nested_data ListsIf you’re familiar with lists in programming languages, Spark SQL lists will feel familiar. There are several ways to create an array or list of values. You can use the collect_list function, which creates a list of values. You can also use the function collect_set, which creates an array without duplicate values. These are both aggregation functions and therefore can be specified only in aggregations: 如果您熟悉编程语言中的列表，Spark SQL列表将很熟悉。有几种创建数组或值列表的方法。您可以使用collect_list函数创建一个值列表。您还可以使用函数collect_set创建一个没有重复值的数组。这些都是聚合函数，因此只能在聚合中指定： 123SELECT DEST_COUNTRY_NAME as new_name, collect_list(count) as flight_counts,collect_set(ORIGIN_COUNTRY_NAME) as origin_setFROM flights GROUP BY DEST_COUNTRY_NAME You can, however, also create an array manually within a column, as shown here: 但是，您也可以在列中手动创建数组，如下所示： 1SELECT DEST_COUNTRY_NAME, ARRAY(1, 2, 3) FROM flights You can also query lists by position by using a Python-like array query syntax: 您还可以使用类似Python的数组查询语法按位置查询列表： 12SELECT DEST_COUNTRY_NAME as new_name, collect_list(count)[0]FROM flights GROUP BY DEST_COUNTRY_NAME You can also do things like convert an array back into rows. You do this by using the explode function. To demonstrate, let’s create a new view as our aggregation: 您还可以执行将数组转换回行的操作。您可以通过使用展开函数来实现。为了演示，让我们创建一个新的视图作为汇总： 译者附explode，直译成“爆炸”不合场景，因此此处意译为：展开，如有不当，欢迎指出。 123CREATE OR REPLACE TEMP VIEW flights_agg ASSELECT DEST_COUNTRY_NAME, collect_list(count) as collected_countsFROM flights GROUP BY DEST_COUNTRY_NAME Now let’s explode the complex type to one row in our result for every value in the array. The DEST_COUNTRY_NAME will duplicate for every value in the array, performing the exact opposite of the original collect and returning us to the original DataFrame: 现在，对于数组中的每个值，让我们将复杂类型展开（explode）为一行。DEST_COUNTRY_NAME将为数组中的每个值重复，执行与原始collection相反的操作，并将返回到原始DataFrame： 1SELECT explode(collected_counts), DEST_COUNTRY_NAME FROM flights_agg FunctionsIn addition to complex types, Spark SQL provides a variety of sophisticated functions. You can find most of these functions in the DataFrames function reference; however, it is worth understanding how to find these functions in SQL, as well. To see a list of functions in Spark SQL, you use the SHOW FUNCTIONS statement: 除了复杂的类型，Spark SQL还提供了各种复杂巧妙的函数。您可以在DataFrames函数参考中找到大多数这些函数。但是，也值得了解如何在SQL中找到这些函数。要查看Spark SQL中的函数列表，请使用SHOW FUNCTIONS语句： 1SHOW FUNCTIONS You can also more specifically indicate whether you would like to see the system functions (i.e., those built into Spark) as well as user functions: 您还可以更具体地指出是否要查看系统函数（即Spark内置的函数）以及用户函数： 1SHOW SYSTEM FUNCTIONS User functions are those defined by you or someone else sharing your Spark environment. These are the same user-defined functions that we talked about in earlier chapters (we will discuss how to create them later on in this chapter): 用户函数是您或共享您的Spark环境的其他人定义的函数。这些是与我们在前几章中讨论过的用户定义函数一样的函数（我们将在本章稍后讨论如何创建它们）： 1SHOW USER FUNCTIONS You can filter all SHOW commands by passing a string with wildcard (*) characters. Here, we can see all functions that begin with “s”: 您可以通过传递带有通配符（*）字符的字符串来过滤所有SHOW命令。在这里，我们可以看到所有以“ s”开头的函数： 1SHOW FUNCTIONS &quot;s*&quot;; Optionally, you can include the LIKE keyword, although this is not necessary: （可选）您可以包括LIKE关键字，尽管这不是必需的： 1SHOW FUNCTIONS LIKE &quot;collect*&quot;; Even though listing functions is certainly useful, often you might want to know more about specific functions themselves. To do this, use the DESCRIBE keyword, which returns the documentation for a specific function. 即使列出函数肯定有用，但通常您可能想进一步了解特定函数本身。为此，请使用DESCRIBE关键字，该关键字返回特定函数的文档。 译者附例子： 12&gt;DESCRIBE FUNCTION collect_list&gt; User-defined functionsAs we saw in Chapters 3 and 4, Spark gives you the ability to define your own functions and use them in a distributed manner. You can define functions, just as you did before, writing the function in the language of your choice and then registering it appropriately: 正如我们在第3章和第4章中看到的那样，Spark使您能够定义自己的函数并以分布式方式使用它们。您可以像以前一样定义函数，以您选择的语言编写函数，然后适当地注册它： 1234def power3(number:Double):Double = number * number * numberspark.udf.register(&quot;power3&quot;, power3(_:Double):Double)SELECT count, power3(count) FROM flights You can also register functions through the Hive CREATE TEMPORARY FUNCTION syntax. 您还可以通过Hive CREATE TEMPORARY FUNCTION语法注册函数。 SubqueriesWith subqueries, you can specify queries within other queries. This makes it possible for you to specify some sophisticated logic within your SQL. In Spark, there are two fundamental subqueries. Correlated subqueries use some information from the outer scope of the query in order to supplement information in the subquery. Uncorrelated subqueries include no information from the outer scope. Each of these queries can return one (scalar subquery) or more values. Spark also includes support for predicate subqueries, which allow for filtering based on values. 使用子查询，您可以在其他查询中指定查询。这使您可以在SQL中指定一些复杂的逻辑。在Spark中，有两个基本子查询。关联子查询使用查询外部范围中的某些信息来补充子查询中的信息。不相关的子查询不包含来自外部范围的信息。这些查询中的每个查询都可以返回一个（标量子查询）或多个值。Spark还包括对谓词子查询的支持，该谓词子查询允许基于值进行过滤。 Uncorrelated predicate subqueriesFor example, let’s take a look at a predicate subquery. In this example, this is composed of two uncorrelated queries. The first query is just to get the top five country destinations based on the data we have: 例如，让我们看一下谓词子查询。在此示例中，这由两个不相关的查询组成。第一个查询只是根据我们拥有的数据获取前五个国家/地区的目的地： 12SELECT dest_country_name FROM flightsGROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5 This gives us the following result: 这给我们以下结果： 123456789+-----------------+|dest_country_name|+-----------------+| United States || Canada || Mexico || United Kingdom|| Japan |+-----------------+ Now we place this subquery inside of the filter and check to see if our origin country exists in that list: 现在，我们将此子查询放入过滤器中，并检查该列表中是否存在我们的原籍国： 123SELECT * FROM flightsWHERE origin_country_name IN (SELECT dest_country_name FROM flightsGROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5) This query is uncorrelated because it does not include any information from the outer scope of the query. It’s a query that you can run on its own. 该查询是不相关的，因为它不包含来自查询外部范围的任何信息。您可以单独运行该查询。 Correlated predicate subqueriesCorrelated predicate subqueries allow you to use information from the outer scope in your inner query. For example, if you want to see whether you have a flight that will take you back from your destination country, you could do so by checking whether there is a flight that has the destination country as an origin and a flight that had the origin country as a destination: 相关谓词子查询使您可以在内部查询中使用外部作用域中的信息。例如，如果您想查看是否有将您从目的地国家带回国的航班，则可以通过检查是否有一个以目的地国家为出发地的航班以及是否有一个将国家作为出发地的航班来进行。作为目的地： 12345SELECT * FROM flights f1WHERE EXISTS (SELECT 1 FROM flights f2 WHERE f1.dest_country_name = f2.origin_country_name)AND EXISTS (SELECT 1 FROM flights f2 WHERE f2.dest_country_name = f1.origin_country_name) EXISTS just checks for some existence in the subquery and returns true if there is a value. You can flip this by placing the NOT operator in front of it. This would be equivalent to finding a flight to a destination from which you won’t be able to return! EXISTS只是检查子查询中是否存在，如果有值，则返回true。您可以通过将NOT运算符放在其前面来翻转它。这等同于找到飞往您将无法返回的目的地的航班！ Uncorrelated scalar queriesUsing uncorrelated scalar queries, you can bring in some supplemental information that you might not have previously. For example, if you wanted to include the maximum value as its own column from the entire counts dataset, you could do this: 使用不相关的标量查询，您可以引入一些以前可能没有的补充信息。例如，如果要在整个计数数据集中将最大值作为自己的列包括在内，则可以执行以下操作： 1SELECT *, (SELECT max(count) FROM flights) AS maximum FROM flights Miscellaneous FeaturesThere are some features in Spark SQL that don’t quite fit in previous sections of this chapter, so we’re going to include them here in no particular order. These can be relevant when performing optimizations or debugging your SQL code. Spark SQL中的某些函数与本章前面的部分不太吻合，因此我们将以不特定的顺序将其包含在此处。这些在执行优化或调试SQL代码时可能是相关的。 ConfigurationsThere are several Spark SQL application configurations, which we list in Table 10-1. You can set these either at application initialization or over the course of application execution (like we have seen with shuffle partitions throughout this book). 有几种Spark SQL应用程序配置，我们在表10-1中列出。您可以在应用程序初始化时或在应用程序执行过程中进行设置（就像我们在本书中看到的随机排序分区一样）。 Table 10-1. Spark SQL configurations Property Name Default Meaning spark.sql.inMemoryColumnarStorage.compressed true When set to true, Spark SQL automatically selects a compression codec for each column based on statistics of the data.设置为true时，Spark SQL根据数据的统计信息自动为每一列选择一个压缩编解码器。 spark.sql.inMemoryColumnarStorage.batchSize 10000 Controls the size of batches for columnar caching. Larger batch sizes can improve memory utilization and compression, but risk OutOfMemoryErrors (OOMs) when caching data.控制用于列式缓存的批处理的大小。较大的批处理大小可以提高内存利用率和压缩率，但是在缓存数据时会出现OutOfMemoryErrors（OOM）。 spark.sql.files.maxPartitionBytes 134217728(128 MB) The maximum number of bytes to pack into a single partition when reading files.读取文件时打包到单个分区中的最大字节数。 spark.sql.files.openCostInBytes 4194304(4 MB) The estimated cost to open a file, measured by the number of bytes that could be scanned in the same time. This is used when putting multiple files into a partition. It is better to overestimate; that way the partitions with small files will be faster than partitions with bigger files (which is scheduled first).打开文件的估计成本，用可以同时扫描的字节数来衡量。将多个文件放入一个分区时使用。最好高估一下；这样，具有较小文件的分区将比具有较大文件的分区（首先安排）更快。 spark.sql.broadcastTimeout 300 Timeout in seconds for the broadcast wait time in broadcast.广播中的广播等待时间超时（以秒为单位）。 spark.sql.autoBroadcastJoinThreshold 10485760(10 MB) Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. You can disable broadcasting by setting this value to -1. Note that currently statistics are supported only for Hive Metastore tables for which the command ANALYZE TABLE COMPUTE STATISTICS noscan has been run.配置表的最大大小（以字节为单位），该表在执行连接时将广播到所有工作程序节点。您可以通过将此值设置为-1来禁用广播。请注意，当前仅对运行了ANALYZE TABLE COMPUTE STATISTICS noscan命令的Hive Metastore表支持统计信息。 spark.sql.shuffle.partitions 200 Configures the number of partitions to use when shuffling data for joins or aggregations.配置在对连接或聚集进行数据洗牌时要使用的分区数。 Setting Configuration Values in SQLWe talk about configurations in Chapter 15, but as a preview, it’s worth mentioning how to set configurations from SQL. Naturally, you can only set Spark SQL configurations that way, but here’s how you can set shuffle partitions: 我们将在第15章中讨论配置，但是作为预览，值得一提的是如何从SQL设置配置。当然，您只能以这种方式设置Spark SQL配置，但是这里是设置洗牌（shuffle）分区的方法： 1SET spark.sql.shuffle.partitions=20 ConclusionIt should be clear from this chapter that Spark SQL and DataFrames are very closely related and that you should be able to use nearly all of the examples throughout this book with only small syntactical tweaks. This chapter illustrated more of the Spark SQL–related specifics. Chapter 11 focuses on a new concept: Datasets that allow for type-safe structured transformations. 从本章中应该清楚地知道，Spark SQL和DataFrames是密切相关的，并且您应该能够通过很少的语法调整就可以使用本书中几乎所有的示例。本章说明了更多与Spark SQL相关的细节。第11章关注于一个新概念：允许类型安全的结构化转换的Dataset。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 27 Regression]]></title>
    <url>%2F2019%2F09%2F01%2FChapter27_Regression(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 27 Regression 回归 译者：https://snaildove.github.io Regression is a logical extension of classification. Rather than just predicting a single value from a set of values, regression is the act of predicting a real number (or continuous variable) from a set of features (represented as numbers). 回归是分类的逻辑延伸。回归不是仅仅从一组值中预测单个值，而是从一组特征（表示为数字）预测实数（或连续变量）的行为。 Regression can be harder than classification because, from a mathematical perspective, there are an infinite number of possible output values. Furthermore, we aim to optimize some metric of error between the predicted and true value, as opposed to an accuracy rate. Aside from that, regression and classification are fairly similar. For this reason, we will see a lot of the same underlying concepts applied to regression as we did with classification. 回归可能比分类更难，因为从数学角度来看，存在无限数量的可能输出值。此外，我们的目标是优化预测值和真值之间的一些误差度量，而不是准确率。除此之外，回归和分类非常相似。出于这个原因，我们将看到许多与回归相同的基本概念应用于分类。 Use Cases 使用案例The following is a small set of regression use cases that can get you thinking about potential regression problems in your own domain: 以下是一小组回归用例，可以让您考虑自己领域中潜在的回归问题： Predicting movie viewership 预测电影收视率 Given information about a movie and the movie-going public, such as how many people have watched the trailer or shared it on social media, you might want to predict how many people are likely to watch the movie when it comes out. 如果给出有关电影和电影公众的信息，例如有多少人观看了预告片或在社交媒体上分享了预告片，您可能想要预测有多少人在电影上映时可能会看电影。 Predicting company revenue 预测公司收入 Given a current growth trajectory, the market, and seasonality, you might want to predict how much revenue a company will gain in the future. 如果给出目前的增长轨迹，市场和季节性，您可能希望预测公司未来将获得多少收入。 Predicting crop yield 预测作物产量 Given information about the particular area in which a crop is grown, as well as the current weather throughout the year, you might want to predict the total crop yield for a particular plot of land. 如果给出有关作物种植的特定区域以及全年当前天气的信息，您可能希望预测特定土地的总产量。 Regression Models in MLlib 在MLlib中的回归模型There are several fundamental regression models in MLlib. Some of these models are carryovers from Chapter 26. Others are only relevant to the regression problem domain. This list is current as of Spark 2.2 but will grow : MLlib 中有几个基本的回归模型。其中一些模型是第26章的遗留物。其他模型只与回归问题内的领域有关。此列表是 Spark 2.2 的最新列表，但会增长： Linear regression 线性回归 Generalized linear regression 广义线性回归 Isotonic regression 保序回归 Decision trees 决策树 Random forest 随机森林 Gradient-boosted trees 梯度提升树 Survival regression 生存回归 This chapter will cover the basics of each of these particular models by providing: 本章将通过提供以下内容来介绍每种特定模型的基础知识： A simple explanation of the model and the intuition behind the algorithm 模型的简单解释和算法背后的直觉 Model hyperparameters (the different ways that we can initialize the model) 模型超参数（我们可以初始化模型的不同方式） Training parameters (parameters that affect how the model is trained) 训练参数（影响模型训练方式的参数） Prediction parameters (parameters that affect how predictions are made) 预测参数（影响预测方式的参数） You can search over the hyperparameters and training parameters using a ParamGrid, as we saw in Chapter 24. 您可以使用 ParamGrid 搜索超参数和训练参数，如第24章所述。 Model Scalability 模型可伸缩性The regression models in MLlib all scale to large datasets. Table 27-1 is a simple model scalability scorecard that will help you in choosing the best model for your particular task (if scalability is your core consideration). These will depend on your configuration, machine size, and other factors. MLlib中的回归模型全都可以扩展到大型数据集。表27-1是一个简单的模型可伸缩性记分卡，可帮助您为特定任务选择最佳模型（如果可扩展性是您的核心考虑因素）。这些将取决于您的配置，机器大小和其他因素。 Table 27-1. Regression scalability reference 表27-1。回归可伸缩性参考 Model Number features Training examples Linear regression 1 to 10 million No limit Generalized linear regression 4,096 No limit Isotonic regression N/A Millions Decision trees 1,000s No limit Random forest 10,000s No limit Gradient-boosted trees 1,000s No limit Survival regression 1 to 10 million No limit NOTE 注意 Like our other advanced analytics chapters, this one cannot teach you the mathematical underpinnings of every model. See Chapter 3 in ISL and ESL for a review of regression.像我们的其他高级分析章节一样，这个章节不能教你每个模型的数学基础。 有关回归的评论，请参阅ISL和ESL的第3章。Let’s read in some sample data that we will use throughout the chapter: 让我们读一下我们将在本章中使用的一些示例数据： 12// in Scalaval df = spark.read.load("/data/regression") 12# in Pythondf = spark.read.load("/data/regression") Linear Regression 线性回归Linear regression assumes that a linear combination of your input features (the sum of each feature multiplied by a weight) results along with an amount of Gaussian error in the output. This linear assumption (along with Gaussian error) does not always hold true, but it does make for a simple, interpretable model that’s hard to overfit. Like logistic regression, Spark implements ElasticNet regularization for this, allowing you to mix L1 and L2 regularization. 线性回归假设输入特征的线性组合（每个特征的总和乘以权重）与输出中的高斯误差量一起产生。这种线性假设（连同高斯误差）并不总是成立，但它确实构成了一个难以过度拟合的简单，可解释的模型。与逻辑回归一样，Spark 为此实现了 ElasticNet 正则化，允许您混合L1和L2正则化。 See ISL 3.2 and ESL 3.2 for more information. 有关详细信息，请参阅 ISL 3.2和 ESL 3.2。 Model Hyperparameters 模型超参数Linear regression has the same model hyperparameters as logistic regression. See Chapter 26 for more information. 线性回归具有与逻辑回归相同的模型超参数。有关更多信息，请参见第26章。 Training Parameters 训练参数Linear regression also shares all of the same training parameters from logistic regression. Refer back to Chapter 26 for more on this topic. 线性回归还与逻辑回归共享所有相同的训练参数。有关此主题的更多信息，请参阅第26章。 Example 示例Here’s a short example of using linear regression on our sample dataset: 以下是在样本数据集上使用线性回归的简短示例： 123456// in Scalaimport org.apache.spark.ml.regression.LinearRegressionval lr = new LinearRegression().setMaxIter(10).setRegParam(0.3)\.setElasticNetParam(0.8)println(lr.explainParams())val lrModel = lr.fit(df) 123456# in Pythonfrom pyspark.ml.regression import LinearRegressionlr = LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)print lr.explainParams()lrModel = lr.fit(df) Training Summary 训练摘要Just as in logistic regression, we get detailed training information back from our model. The code font method is a simple shorthand for accessing these metrics. It reports several conventional metrics for measuring the success of a regression model, allowing you to see how well your model is actually fitting the line. 就像在逻辑回归中一样，我们从模型中获取详细的训练信息。code font 方法是访问这些指标的简单简写。它报告了几个用于衡量回归模型成功的常规指标，使您可以看到模型实际拟合生产线的程度。 The summary method returns a summary object with several fields. Let’s go through these in turn. The residuals are simply the weights for each of the features that we input into the model. The objective history shows how our training is going at every iteration. The root mean squared error is a measure of how well our line is fitting the data, determined by looking at the distance between each predicted value and the actual value in the data. The R-squared variable is a measure of the proportion of the variance of the predicted variable that is captured by the model. summary方法返回包含多个字段的摘要对象。让我们依次讨论这些问题。残差只是我们输入模型的每个特征的权重。目标历史显示了我们的训练在每次迭代中的进展情况。均方根误差是衡量我们的线拟合数据的程度，通过查看每个预测值与数据中实际值之间的距离来确定。 R平方变量是模型捕获的预测变量的方差比例的度量。 There are a number of metrics and summary information that may be relevant to your use case. This section demonstrates the API, but does not comprehensively cover every metric (consult the API documentation for more information). 有许多可能与您的使用案例相关的指标和摘要信息。本节演示API，但不全面涵盖每个指标（有关更多信息，请参阅API文档）。 Here are some of the attributes of the model summary for linear regression: 以下是线性回归模型摘要的一些属性： 123456// in Scalaval summary = lrModel.summarysummary.residuals.show()println(summary.objectiveHistory.toSeq.toDF.show())println(summary.rootMeanSquaredError)println(summary.r2) 1234567# in Pythonsummary = lrModel.summarysummary.residuals.show()print summary.totalIterationsprint summary.objectiveHistoryprint summary.rootMeanSquaredErrorprint summary.r2 Generalized Linear Regression 广义线性回归The standard linear regression that we saw in this chapter is actually a part of a family of algorithms called generalized linear regression. Spark has two implementations of this algorithm. One is optimized for working with very large sets of features (the simple linear regression covered previously in this chapter), while the other is more general, includes support for more algorithms, and doesn’t currently scale to large numbers of features. 我们在本章中看到的标准线性回归实际上是一类称为广义线性回归的算法的一部分。 Spark有两种这种算法的实现。一个针对处理非常大的特征集（本章前面介绍的简单线性回归）进行了优化，而另一个则更为通用，包括对更多算法的支持，并且目前不能扩展到大量特征。 The generalized form of linear regression gives you more fine-grained control over what kind of regression model you use. For instance, these allow you to select the expected noise distribution from a variety of families, including Gaussian (linear regression), binomial (logistic regression), poisson (poisson regression), and gamma (gamma regression). The generalized models also support setting a link function that specifies the relationship between the linear predictor and the mean of the distribution function. Table 27-2 shows the available link functions for each family. 线性回归的广义形式使您可以更精细地控制所使用的回归模型。例如，这些允许您从各种族（family）中选择预期的噪声分布，包括高斯（线性回归），二项式（逻辑回归），泊松（泊松回归）和伽玛（伽马回归）。广义模型还支持设置连接函数（link function），该函数指定线性预测器与分布函数的均值之间的关系。表27-2显示了每个系列的可用连接函数（link function）。 Table 27-2. Regression families, response types, and link functions 表27-2 回归族（families），响应类型和连接函数（link function） Family Response type Supported links Gaussian Continuous Identity*, Log, Inverse Binomial Binary Logit*, Probit, CLogLog Poisson Count Log*, Identity, Sqrt Gamma Continuous Inverse*, Idenity, Log Tweedie Zero-inflated continuous Power link function The asterisk signifies the canonical link function for each family. 星号表示每个族（family）的规范（canonical）连接函数（link function）。 See ISL 3.2 and ESL 3.2 for more information on generalized linear models. 有关广义线性模型的更多信息，请参见 ISL 3.2 和 ESL 3.2。 WARNING 警告 A fundamental limitation as of Spark 2.2 is that generalized linear regression only accepts a maximum of 4,096 features for inputs. This will likely change for later versions of Spark, so be sure to refer to the documentation.Spark 2.2 的一个基本限制是广义线性回归仅接受最多4,096个输入特征。 对于更高版本的Spark，这可能会有所改变，因此请务必参考文档。Model Hyperparameters 模型超参数These are configurations that we specify to determine the basic structure of the model itself. In addition to fitIntercept and regParam (mentioned in “Regression”), generalized linear regression includes several other hyperparameters: 这些是我们指定的配置，用于确定模型本身的基本结构。除了 fitIntercept 和 regParam（在“回归”中提到）之外，广义线性回归还包括其他几个超参数： familyA description of the error distribution to be used in the model. Supported options are Poisson, binomial, gamma, Gaussian, and Tweedie. 要在模型中使用的错误分布的描述。支持的选项包括泊松，二项，伽马，高斯和特威迪（Tweedie）。 linkThe name of link function which provides the relationship between the linear predictor and the mean of the distribution function. Supported options are cloglog, probit, logit, inverse, sqrt, identity, and log (default: identity). 连接函数的名称，它提供线性预测器与分布函数均值之间的关系。支持的选项包括 cloglog，probit，logit，inverse，sqrt，identity 和 log（默认值：identity）。 solverThe solver algorithm to be used for optimization. The only currently supported solver is irls (iteratively reweighted least squares). 用于优化的解算器算法。目前唯一支持的解算器是 irls（迭代重新加权最小二乘）。 variancePowerThe power in the variance function of the Tweedie distribution, which characterizes the relationship between the variance and mean of the distribution. Only applicable to the Tweedie family. Supported values are 0 and [1, Infinity). The default is 0. 特威迪（Tweedie）分布的方差函数中的幂，其表征方差和分布均值之间的关系。仅适用于特威迪（Tweedie）分布族。支持的值为 0 和 $[1，\infty]$。默认值为0。 linkPowerThe index in the power link function for the Tweedie family. 特威迪（Tweedie）分布族连接函数（link function）的幂的索引。 Training Parameters 训练参数The training parameters are the same that you will find for logistic regression. Consult Chapter 26 for more information. 训练参数与逻辑回归相同。有关更多信息，请参阅第26章。 Prediction Parameters 预测参数This model adds one prediction parameter: 该模型添加了一个预测参数： linkPredictionColA column name that will hold the output of our link function for each prediction. 一个列名，用于保存每个预测的连接函数（link function）的输出。 Example 例子Here’s an example of using GeneralizedLinearRegression: 以下是使用 GeneralizedLinearRegression 的示例： 12345678910// in Scalaimport org.apache.spark.ml.regression.GeneralizedLinearRegressionval glr = new GeneralizedLinearRegression().setFamily("gaussian").setLink("identity").setMaxIter(10).setRegParam(0.3).setLinkPredictionCol("linkOut")println(glr.explainParams())val glrModel = glr.fit(df) 12345678910# in Pythonfrom pyspark.ml.regression import GeneralizedLinearRegressionglr = GeneralizedLinearRegression()\.setFamily("gaussian")\.setLink("identity")\.setMaxIter(10)\.setRegParam(0.3)\.setLinkPredictionCol("linkOut")print glr.explainParams()glrModel = glr.fit(df) Training Summary 训练摘要As for the simple linear model in the previous section, the training summary provided by Spark for the generalized linear model can help you ensure that your model is a good fit for the data that you used as the training set. It is important to note that this does not replace running your algorithm against a proper test set, but it can provide more information. This information includes a number of different potential metrics for analyzing the fit of your algorithm, including some of the most common success metrics: 对于上一节中的简单线性模型，Spark为广义线性模型提供的训练摘要可以帮助您确保模型非常拟合您用作训练集的数据。重要的是要注意，这不会取代运行您的算法与正确的测试集，但它可以提供更多信息。此信息包括许多用于分析算法拟合的不同潜在指标，包括一些最常见的成功指标： R squared The coefficient of determination; a measure of fit.决定系数；拟合的一种度量。 The residuals 残差 The difference between the label and the predicted value.标签和预测值之间的差异。 Be sure to inspect the summary object on the model to see all the available methods. 请务必检查模型上的摘要对象以查看所有可用方法。 Decision Trees 决策树Decision trees as applied to regression work fairly similarly to decision trees applied to classification. The main difference is that decision trees for regression output a single number per leaf node instead of a label (as we saw with classification). The same interpretability properties and model structure still apply. In short, rather than trying to train coeffiecients to model a function, decision tree regression simply creates a tree to predict the numerical outputs. This is of significant consequence because unlike generalized linear regression, we can predict nonlinear functions in the input data. This also creates a significant risk of overfitting the data, so we need to be careful when tuning and evaluating these models. 应用于回归的决策树与应用于分类的决策树非常相似。主要区别在于回归的决策树每个叶节点输出一个数字而不是标签（正如我们在分类中看到的那样）。相同的可解释性属性和模型结构仍然适用。简而言之，决策树回归不是试图训练系数来模拟函数，而是简单地创建一个树来预测数值输出。这是重要的结果，因为与广义线性回归不同，我们可以预测输入数据中的非线性函数。这也会产生过度拟合数据的重大风险，因此在调整和评估这些模型时需要小心。 We also covered decision trees in Chapter 26 (refer to “Decision Trees”). For more information on this topic, consult ISL 8.1 and ESL 9.2 . 我们还在第26章介绍了决策树（参见“决策树”）。有关该主题的更多信息，请参阅 ISL 8.1和 ESL 9.2。 Model Hyperparameters 模型超参数The model hyperparameters that apply decision trees for regression are the same as those for classification except for a slight change to the impurity parameter. See Chapter 26 for more information on the other hyperparameters: 应用决策树进行回归的模型超参数与分类相同，只是杂质参数略有变化。有关其他超参数的更多信息，请参见第26章： impurity 不纯度 The impurity parameter represents the metric (information gain) for whether or not the model should split at a particular leaf node with a particular value or keep it as is. The only metric currently supported for regression trees is “variance.” 不纯度参数表示模型是否应在具有特定值的特定叶节点处分割或保持原样的度量（信息增益）。目前支持回归树的唯一指标是“方差（variance）”。 Training Parameters 训练参数In addition to hyperparameters, classification and regression trees also share the same training parameters. See “Training Parameters” in Chapter 26 for these parameters. 除了超参数，分类和回归树也共享相同的训练参数。有关这些参数，请参见第26章中的“训练参数”。 Example 示例Here’s a short example of using a decision tree regressor: 以下是使用决策树回归程序的简短示例： 12345// in Scalaimport org.apache.spark.ml.regression.DecisionTreeRegressorval dtr = new DecisionTreeRegressor()println(dtr.explainParams())val dtrModel = dtr.fit(df) 12345# in Pythonfrom pyspark.ml.regression import DecisionTreeRegressordtr = DecisionTreeRegressor()print dtr.explainParams()dtrModel = dtr.fit(df) Random Forests and Gradient-Boosted Trees 随机森林和梯度提升树The random forest and gradient-boosted tree models can be applied to both classification and regression. As a review, these both follow the same basic concept as the decision tree, except rather than training one tree, many trees are trained to perform a regression. In the random forest model, many de-correlated trees are trained and then averaged. With gradient-boosted trees, each tree makes a weighted prediction (such that some trees have more predictive power for some classes over others). Random forest and gradient-boosted tree regression have the same model hyperparameters and training parameters as the corresponding classification models, except for the purity measure (as is the case with DecisionTreeRegressor). 随机森林和梯度提升树模型可以应用于分类和回归。作为回顾，这些都遵循与决策树相同的基本概念，除了训练一棵树，训练许多树进行回归。在随机森林模型中，训练去相关的树然后进行平均。使用梯度提升树，每棵树进行加权预测（这样一些树对某些类具有比其他树更多的预测能力）。随机森林和梯度提升树回归具有与相应分类模型相同的模型超参数和训练参数，除了纯度测量（如“DecisionTreeRegressor”的情况）。 See ISL 8.2 and ESL 10.1 for more information on tree ensembles. 有关树集合的更多信息，请参见 ISL 8.2 和 ESL 10.1。 Model Hyperparameters 模型超参数These models share many of the same parameters as we saw in the previous chapter as well as for regression decision trees. Refer back to “Model Hyperparameters” in Chapter 26 for a thorough explanation of these parameters. As for a single regression tree, however, the only impurity metric currently supported is variance. 这些模型共享许多与我们在前一章中看到的相同的参数以及回归决策树。有关这些参数的详细说明，请参阅第26章中的“模型超参数”。但是，对于单个回归树，当前支持的唯一不纯度的度量（impurity metric）是方差。 Training Parameters 训练参数These models support the same checkpointInterval parameter as classification trees, as described in Chapter 26. 这些模型支持与分类树相同的checkpointInterval参数，如第26章所述。 ExampleHere’s a small example of how to use these two models to perform a regression: 以下是如何使用这两个模型执行回归的一个小示例： 123456789// in Scalaimport org.apache.spark.ml.regression.RandomForestRegressorimport org.apache.spark.ml.regression.GBTRegressorval rf = new RandomForestRegressor()println(rf.explainParams())val rfModel = rf.fit(df)val gbt = new GBTRegressor()println(gbt.explainParams())val gbtModel = gbt.fit(df) 123456789# in Pythonfrom pyspark.ml.regression import RandomForestRegressorfrom pyspark.ml.regression import GBTRegressorrf = RandomForestRegressor()print rf.explainParams()rfModel = rf.fit(df)gbt = GBTRegressor()print gbt.explainParams()gbtModel = gbt.fit(df) Advanced Methods 高级方法The preceding methods are highly general methods for performing a regression. The models are by no means exhaustive, but do provide the essential regression types that many folks use. This next section will cover some of the more specialized regression models that Spark includes. We omit code examples simply because they follow the same patterns as the other algorithms. 前述方法是用于执行回归的高度通用的方法。这些模型并非详尽无遗，但确实提供了许多人使用的基本回归类型。下一节将介绍Spark包含的一些更专业的回归模型。我们省略代码示例只是因为它们遵循与其他算法相同的模式。 Survival Regression (Accelerated Failure Time) 生存回归（加速失败时间）Statisticians use survival analysis to understand the survival rate of individuals, typically in controlled experiments. Spark implements the accelerated failure time model, which, rather than describing the actual survival time, models the log of the survival time. This variation of survival regression is implemented in Spark because the more well-known Cox Proportional Hazard’s model is semi-parametric and does not scale well to large datasets. By contrast, accelerated failure time does because each instance (row) contributes to the resulting model independently. Accelerated failure time does have different assumptions than the Cox survival model and therefore one is not necessarily a drop-in replacement for the other. Covering these differing assumptions is outside of the scope of this book. See L. J. Wei’s paper on accelerated failure time for more information. 统计学家使用生存分析来了解个体的存活率，通常是在对照实验中。Spark实现了加速失败时间模型，该模型不是描述实际生存时间，而是模拟生存时间的对数。这种生存回归的变体（variation ）在Spark中实现，因为更为人熟知的 Cox Proportional Hazard 模型是半参数的，并且不能很好地扩展到大型数据集。相比之下，加速失败时间确实存在，因为每个实例（行）都独立地对结果模型做出贡献。加速失败时间确实具有与 Cox 生存模型不同的假设，因此一个不一定是另一个的直接替代品。涵盖这些不同的假设超出了本书的范围。见 L. J. Wei的论文 关于加速失败时间以获取更多信息。 The requirement for input is quite similar to that of other regressions. We will tune coefficients according to feature values. However, there is one departure, and that is the introduction of a censor variable column. A test subject censors during a scientific study when that individual drops out of a study, since their state at the end of the experiment may be unknown. This is important because we cannot assume an outcome for an individual that censors (doesn’t report that state to the researchers) at some intermediate point in a study. 输入要求与其他回归非常相似。我们将根据特征值调整系数。然而，有一个偏离，那就是引入一个检查变量列。当一个人退出研究时，测试对象在科学研究期间进行检查，因为他们在实验结束时的状态可能是未知的。这很重要，因为我们无法假设在研究的某个中间点审查（不向研究人员报告该状态）的个人的结果。 See more about survival regression with AFT in the documentation. 在文档中查看有关AFT的生存回归的更多信息。 Isotonic Regression 保序回归Isotonic regression is another specialized regression model, with some unique requirements. Essentially, isotonic regression specifies a piecewise linear function that is always monotonically increasing. It cannot decrease. This means that if your data is going up and to the right in a given plot, this is an appropriate model. If it varies over the course of input values, then this is not appropriate. The illustration of isotonic regression’s behavior in Figure 27-1 makes it much easier to understand. 保序回归是另一种专门的回归模型，具有一些独特的要求。本质上，保序回归指定了一个单调递增的分段线性函数。它不能减少。这意味着如果您的数据在给定的图中向上和向右，这是一个合适的模型。如果它在输入值的过程中变化，那么这是不合适的。图27-1中保序回归的行为说明使其更容易理解。 Notice how this gets a better fit than the simple linear regression. See more about how to use this model in the Spark documentation. 注意这比简单的线性回归更合适。在Spark文档中查看有关如何使用此模型的更多信息。 Evaluators and Automating Model Tuning 评估器和自动化模型调整Regression has the same core model tuning functionality that we saw with classification. We can specify an evaluator, pick a metric to optimize for, and then train our pipeline to perform that parameter tuning on our part. The evaluator for regression, unsurprisingly, is called the RegressionEvaluator and allows us to optimize for a number of common regression success metrics. Just like the classification evaluator, RegressionEvaluator expects two columns, a column representing the prediction and another representing the true label. The supported metrics to optimize for are the root mean squared error (“rmse”), the mean squared error (“mse”), the $r^2$ metric (“r2”), and the mean absolute error (“mae”). 回归具有与分类相同且关键的模型调整功能。我们可以指定一个评估器，选择一个要优化的度量，然后训练我们的管道来执行我们的参数调整。毫无疑问，回归评估器称为 RegressionEvaluator，它允许我们针对许多常见的回归成功度量进行优化。就像分类评估器一样，RegressionEvaluator 需要两列，一列代表预测值，另一列代表真实标签。要优化的支持度量是均方根误差（“rmse”），均方误差（“mse”），$ r ^ 2 $ metric（“r2”）和平均绝对误差（“mae”） ）。 To use RegressionEvaluator, we build up our pipeline, specify the parameters we would like to test, and then run it. Spark will automatically select the model that performs best and return this to us: 要使用 RegressionEvaluator，我们构建我们的管道，指定我们想要测试的参数，然后运行它。 Spark 会自动选择性能最佳的模型并将其返回给我们：123456789101112131415161718192021// in Scalaimport org.apache.spark.ml.evaluation.RegressionEvaluatorimport org.apache.spark.ml.regression.GeneralizedLinearRegressionimport org.apache.spark.ml.Pipelineimport org.apache.spark.ml.tuning.&#123;CrossValidator, ParamGridBuilder&#125;val glr = new GeneralizedLinearRegression().setFamily("gaussian").setLink("identity")val pipeline = new Pipeline().setStages(Array(glr))val params = new ParamGridBuilder().addGrid(glr.regParam, Array(0, 0.5, 1)).build()val evaluator = new RegressionEvaluator().setMetricName("rmse").setPredictionCol("prediction").setLabelCol("label")val cv = new CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(params).setNumFolds(2) // should always be 3 or more but this dataset is smallval model = cv.fit(df) 123456789101112131415161718# in Pythonfrom pyspark.ml.evaluation import RegressionEvaluatorfrom pyspark.ml.regression import GeneralizedLinearRegressionfrom pyspark.ml import Pipelinefrom pyspark.ml.tuning import CrossValidator, ParamGridBuilderglr = GeneralizedLinearRegression().setFamily("gaussian").setLink("identity")pipeline = Pipeline().setStages([glr])params = ParamGridBuilder().addGrid(glr.regParam, [0, 0.5, 1]).build()evaluator = RegressionEvaluator()\.setMetricName("rmse")\.setPredictionCol("prediction")\.setLabelCol("label")cv = CrossValidator()\.setEstimator(pipeline)\.setEvaluator(evaluator)\.setEstimatorParamMaps(params)\.setNumFolds(2) # should always be 3 or more but this dataset is smallmodel = cv.fit(df) Metrics 衡量指标Evaluators allow us to evaluate and fit a model according to one specific metric, but we can also access a number of regression metrics via the RegressionMetrics object. As for the classification metrics in the previous chapter, RegressionMetrics operates on RDDs of (prediction, label) pairs. For instance, let’s see how we can inspect the results of the previously trained model. 评估器允许我们根据一个特定指标评估和拟合模型，但我们也可以通过 RegressionMetrics 对象访问许多回归指标。 对于前一章中的分类度量，RegressionMetrics 对（预测，标签）数据对的RDD进行操作。 例如，让我们看看我们如何检查以前训练过的模型的结果。 1234567891011// in Scalaimport org.apache.spark.mllib.evaluation.RegressionMetricsval out = model.transform(df).select("prediction", "label").rdd.map(x =&gt; (x(0).asInstanceOf[Double], x(1).asInstanceOf[Double]))val metrics = new RegressionMetrics(out)println(s"MSE = &#123;metrics.meanSquaredError&#125;")println(s"RMSE = &#123;metrics.rootMeanSquaredError&#125;")println(s"R-squared = &#123;metrics.r2&#125;")println(s"MAE = &#123;metrics.meanAbsoluteError&#125;")println(s"Explained variance = $&#123;metrics.explainedVariance&#125;") 12345678910# in Pythonfrom pyspark.mllib.evaluation import RegressionMetricsout = model.transform(df)\.select("prediction", "label").rdd.map(lambda x: (float(x[0]), float(x[1])))metrics = RegressionMetrics(out)print "MSE: " + str(metrics.meanSquaredError)print "RMSE: " + str(metrics.rootMeanSquaredError)print "R-squared: " + str(metrics.r2)print "MAE: " + str(metrics.meanAbsoluteError)print "Explained variance: " + str(metrics.explainedVariance) Consult the Spark documentation for the latest methods. 有关最新方法，请参阅Spark文档。 Conclusion 结论In this chapter, we covered the basics of regression in Spark, including how we train models and how we measure success. In the next chapter, we’ll take a look at recommendation engines, one of the more popular applications of MLlib. 在本章中，我们介绍了Spark中回归的基础知识，包括我们如何训练模型以及如何衡量成功。 在下一章中，我们将介绍推荐引擎，这是 MLlib 更受欢迎的应用之一。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 26 Classification]]></title>
    <url>%2F2019%2F09%2F01%2FChapter26_Classification(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 26 Classification 分类Classification is the task of predicting a label, category, class, or discrete variable given some input features. The key difference from other ML tasks, such as regression, is that the output label has a finite set of possible values (e.g., three classes). 分类是在给定一些输入特征的情况下预测标签，类别，类或离散变量的任务。与其他ML任务（例如回归）的主要区别在于输出标签具有一组有限的可能值（例如，三个类）。 Use Cases 用户案例Classification has many use cases, as we discussed in Chapter 24. Here are a few more to consider as a reinforcement of the multitude of ways classification can be used in the real world. 分类有许多用户案例，正如我们在第24章中讨论的那样。这里还有一些需要考虑的因素，可以加强分类在现实世界中的使用方式。 Predicting credit risk 预测信用风险 A financing company might look at a number of variables before offering a loan to a company or individual. Whether or not to offer the loan is a binary classification problem. 在向公司或个人提供贷款之前，融资公司可能会考虑许多变量。是否提供贷款是二元分类问题。 News classification 新闻分类 An algorithm might be trained to predict the topic of a news article (sports, politics, business, etc.). 可以训练算法来预测新闻文章（体育，政治，商业等）的主题。 Classifying human activity 对人类活动进行分类 By collecting data from sensors such as a phone accelerometer or smart watch, you can predict the person’s activity. The output will be one of a finite set of classes (e.g., walking, sleeping, standing, or running). 通过从传感器（如手机加速度计或智能手表）收集数据，您可以预测人员的活动。输出将是一组有限的类（例如，步行，睡觉， 站立或跑步）。 Types of Classification 分类算法的类别Before we continue, let’s review several different types of classification. 在继续之前，让我们回顾几种不同类型的分类。 Binary Classification 二元分类The simplest example of classification is binary classification, where there are only two labels you can predict. One example is fraud analytics, where a given transaction can be classified as fraudulent or not; or email spam, where a given email can be classified as spam or not spam. 最简单的分类示例是二元分类，其中只有两个标签可以预测。一个例子是欺诈分析，其中给定的交易可以被分类为欺诈性或非欺诈性;或电子邮件垃圾邮件，其中给定的电子邮件可分类为垃圾邮件或非垃圾邮件。 Multiclass Classification 多类别的分类Beyond binary classification lies multiclass classification, where one label is chosen from more than two distinct possible labels. A typical example is Facebook predicting the people in a given photo or a meterologist predicting the weather (rainy, sunny, cloudy, etc.). Note how there is always a finite set of classes to predict; it’s never unbounded. This is also called multinomial classification. 除了二元分类之外，还有多类分类，其中一个标签是从两个以上不同的可能标签中选择的。一个典型的例子是Facebook预测给定照片中的人或预测天气（雨天，晴天，阴天等）的气象学家。注意如何去预测总是有一组有限的类来；它永远不会无限制。这也称为多项分类。 Multilabel Classification 多标签的分类Finally, there is multilabel classification, where a given input can produce multiple labels. For example, you might want to predict a book’s genre based on the text of the book itself. While this could be multiclass, it’s probably better suited for multilabel because a book may fall into multiple genres. Another example of multilabel classification is identifying the number of objects that appear in an image. Note that in this example, the number of output predictions is not necessarily fixed, and could vary from image to image. 最后，存在多标签分类，其中给定输入可以产生多个标签。例如，您可能希望根据书本身的文本来预测书籍的类型。虽然这可能是多类的，但它可能更适合多标签，因为一本书可能属于多种类型。多标签分类的另一个例子是识别出现在图像中的对象的数量。请注意，在此示例中，输出预测的数量不一定是固定的，并且可能因图像而异。 Classification Models in MLlib 在MLlib中的分类模型Spark has several models available for performing binary and multiclass classification out of the box. The following models are available for classification in Spark : Spark有几种可用于执行二元和多分类的模型，这些模型开箱即用。Spark中可以使用以下模型进行分类： Logistic regression 逻辑回归 Decision trees 决策树 Random forests 随机森林 Gradient-boosted trees 梯度提升树 Spark does not support making multilabel predictions natively. In order to train a multilabel model, you must train one model per label and combine them manually. Once manually constructed, there are built-in tools that support measuring these kinds of models (discussed at the end of the chapter). Spark不支持原生进行多标签预测。为了训练多标签模型， 您必须为每个标签训练一个模型并手动组合它们。一旦手动构建，就有内置工具支持测量这些模型（在本章末尾讨论）。 This chapter will cover the basics of each of these models by providing: 本章将通过提供以下内容介绍每种模型的基础知识： A simple explanation of the model and the intuition behind it 模型的简单解释及其背后的直觉 Model hyperparameters (the different ways we can initialize the model) 模型超参数（我们可以初始化模型的不同方式） Training parameters (parameters that affect how the model is trained) 训练参数（影响模型训练方式的参数） Prediction parameters (parameters that affect how predictions are made) 预测参数（影响预测方式的参数） You can set the hyperparameters and training parameters in a ParamGrid as we saw in Chapter 24. 您可以在第24章中看到，在 ParamGrid 中设置超参数和训练参数。 Model Scalability 模型伸缩性Model scalability is an important consideration when choosing your model. In general, Spark has great support for training large-scale machine learning models (note, these are large scale; on single node workloads there are a number of other tools that also perform well). Table 26-1 is a simple model scalability scorecard to use to find the best model for your particular task (if scalability is your core consideration). The actual scalability will depend on your configuration, machine size, and other specifics but should make for a good heuristic. 选择模型时，模型可伸缩性是一个重要的考虑因素。总的来说，Spark非常支持训练大型机器学习模型（注意，这些是大规模的;在单节点工作负载上，还有许多其他工具也表现良好）。表26-1是一个简单的模型可伸缩性记分卡，用于查找特定任务的最佳模型（如果可扩展性是您的核心考虑因素）。实际的可扩展性将取决于您的配置，机器数量和其他细节，但应该是一个良好的启发式。 Table 26-1. Model scalability reference 模型伸缩性参考表 Model模型 Features count特征计数 Training examples训练例子 Output classes输出类别 Logistic regression逻辑回归 1 to 10 million No limit Features x Classes &lt; 10 million Decision trees决策树 1,000s No limit Features x Classes &lt; 10,000s Random forest 10,000s随机森林 10,000s No limit Features x Classes &lt; 100,000s Gradient-boosted trees梯度提升树 1,000s No limit Features x Classes &lt; 10,000s We can see that nearly all these models scale to large collections of input data and there is ongoing work to scale them even further. The reason no limit is in place for the number of training examples is because these are trained using methods like stochastic gradient descent and L-BFGS. These methods are optimized specifically for working with massive datasets and to remove any constraints that might exist on the number of training examples you would hope to learn on. 我们可以看到，几乎所有这些模型都可以扩展到输入数据的大量集合，并且正在进行进一步扩展它们的工作。 对训练样本数量没有限制的原因是因为这些是使用随机梯度下降和 L-BFGS 等方法训练的。 这些方法专门优化用于处理大量数据集，并移除可能存在于您希望学习的训练示例数量上的任何约束。 Let’s start looking at the classification models by loading in some data: 让我们开始通过加载一些数据来查看分类模型： 123// in Scalaval bInput = spark.read.format("parquet").load("/data/binary-classification").selectExpr("features", "cast(label as double) as label") 123# in PythonbInput = spark.read.format("parquet").load("/data/binary-classification")\.selectExpr("features", "cast(label as double) as label") NOTE 注意Like our other advanced analytics chapters, this one cannot teach you the mathematical underpinnings of every model. See Chapter 4 in ISL and ESL for a review of classification. 像我们的其他高级分析章节一样，这个章节不能教你每个模型的数学基础。 有关分类的评论，请参阅 ISL 和 ESL 中的第4章。 Logistic Regression Logistic回归Logistic regression is one of the most popular methods of classification. It is a linear method that combines each of the individual inputs (or features) with specific weights (these weights are generated during the training process) that are then combined to get a probability of belonging to a particular class. These weights are helpful because they are good representations of feature importance; if you have a large weight, you can assume that variations in that feature have a significant effect on the outcome (assuming you performed normalization). A smaller weight means the feature is less likely to be important. 逻辑回归是最流行的分类方法之一。它是一种线性方法，将每个单独的输入（或特征）与特定权重（这些权重在训练过程中生成）组合在一起，然后将这些权重组合起来以获得属于特定类别的概率。这些权重是有用的，因为它们是特征重要性的良好表示；如果你的权重很大，你可以假设该特征的变化对结果有显着影响（假设你进行了标准化）。较小的权重意味着该特征不太可能重要。 See ISL 4.3 and ESL 4.4 for more information. 有关更多信息，请参阅 ISL 4.3 和 ESL 4.4。 Model Hyperparameters 模型超参数Model hyperparameters are configurations that determine the basic structure of the model itself. The following hyperparameters are available for logistic regression: 模型超参数是确定模型本身的基本结构的配置项。以下超参数可用于逻辑回归： family Can be multinomial (two or more distinct labels; multiclass classification) or binary (only two distinct labels; binary classification). 可以是多项式（两个或多个不同的标签;多类分类）或二元（仅两个不同的标签;二元分类）。 elasticNetParam A floating-point value from 0 to 1. This parameter specifies the mix of L1 and L2 regularization according to elastic net regularization (which is a linear combination of the two). Your choice of L1 or L2 depends a lot on your particular use case but the intuition is as follows: L1 regularization (a value of 1) will create sparsity in the model because certain feature weights will become zero (that are of little consequence to the output). For this reason, it can be used as a simple feature-selection method. On the other hand, L2 regularization (a value of 0) does not create sparsity because the corresponding weights for particular features will only be driven toward zero, but will never completely reach zero. ElasticNet gives us the best of both worlds—we can choose a value between 0 and 1 to specify a mix of L1 and L2 regularization. For the most part, you should be tuning this by testing different values. 从0到1的浮点值。该参数根据弹性网络正则化（两者的线性组合）指定L1和L2正则化的混合。您对L1或L2的选择很大程度上取决于您的特定使用案例，但直觉如下：L1正则化（值为1）将在模型中产生稀疏性，因为某些特征权重将变为零（这对于输出）。因此，它可以用作简单的特征选择方法。另一方面，L2正则化（值为0）不会产生稀疏性，因为特定特征的相应权重将仅被驱动为零，但永远不会完全达到零。 ElasticNet 为我们提供了两全其美的优势——我们可以选择0到1之间的值来指定L1和L2正则化的混合。在大多数情况下，您应该通过测试不同的值来调整它。 fitIntercept Can be true or false. This hyperparameter determines whether or not to fit the intercept or the arbitrary number that is added to the linear combination of inputs and weights of the model. Typically you will want to fit the intercept if we haven’t normalized our training data. 可以是真是假。该超参数确定是否拟合截距或添加到模型的输入和权重的线性组合的任意数。通常，如果我们没有标准化训练数据，您将需要拟合截距。 regParam A value ≥ 0. that determines how much weight to give to the regularization term in the objective function. Choosing a value here is again going to be a function of noise and dimensionality in our dataset. In a pipeline, try a wide range of values (e.g., 0, 0.01, 0.1, 1). 值≥0，用于确定目标函数中正则化项的权重。在这里选择一个值将再次成为我们数据集中噪声和维度的函数。在管道中，尝试各种值（例如，0,0.01,0.1,1）。 standardization Can be true or false, whether or not to standardize the inputs before passing them into the model.See Chapter 25 for more information. 无论是否在将输入传递到模型之前对输入进行标准化，都可以是真或假。有关更多信息，请参见第25章。 Training Parameters 训练参数Training parameters are used to specify how we perform our training. Here are the training parameters for logistic regression. 训练参数用于指定我们如何执行训练。以下是逻辑回归的训练参数。 maxIter Total number of iterations over the data before stopping. Changing this parameter probably won’t change your results a ton, so it shouldn’t be the first parameter you look to adjust. The default is 100. 停止前数据的迭代总数。更改此参数可能不会大幅度改变您的结果，因此它不应该是您要调整的第一个参数。默认值为100。 tol This value specifies a threshold by which changes in parameters show that we optimized our weights enough, and can stop iterating. It lets the algorithm stop before maxIter iterations. The default value is $1.0E-6$. This also shouldn’t be the first parameter you look to tune. 此值指定一个阈值，通过该阈值，参数的变化表明我们已经足够优化了权重，并且可以停止迭代。它允许算法在maxIter 迭代之前停止。默认值为$1.0E-6$。这也不应该是你想要调整的第一个参数。 weightCol The name of a weight column used to weigh certain rows more than others. This can be a useful tool if you have some other measure of how important a particular training example is and have a weight associated with it. For example, you might have 10,000 examples where you know that some labels are more accurate than others. You can weigh the labels you know are correct more than the ones you don’t. 用于比其他行加更多权重的一些行的权重列名称（译者注：每个样本的权重不一样，这里权重列指的是样本的权重向量）。如果您对特定训练示例的重要程度以及与之相关的权重有其他衡量标准，那么这可能是一个有用的工具。例如，您可能有10,000个示例，其中您知道某些标签比其他标签更准确。您可以对您知道的准确的标签比您不知道的标签加更多权重。 Prediction Parameters 预测参数These parameters help determine how the model should actually be making predictions at prediction time, but do not affect training. Here are the prediction parameters for logistic regression: 这些参数有助于确定模型在预测时应该如何实际进行预测，但不会影响训练。以下是逻辑回归的预测参数： threshold A Double in the range of 0 to 1. This parameter is the probability threshold for when a given class should be predicted. You can tune this parameter according to your requirements to balance between false positives and false negatives. For instance, if a mistaken prediction would be costly—you might want to make its prediction threshold very high. 双精度范围为0到1，此参数是应该预测给定类的概率阈值。您可以根据您的要求调整此参数，以平衡预测阳性但是预测错误的样本（假阳）和预测阴性的但是预测错误的样本（假阴）。例如，如果错误的预测成本很高——您可能希望将其预测阈值设置得非常高。 thresholds This parameter lets you specify an array of threshold values for each class when using multiclass classification. It works similarly to the single threshold parameter described previously. 使用多类分类时，此参数允许您为每个类指定阈值数组。它的工作方式类似于前面描述的单个阈值参数。 ExampleHere’s a simple example using the LogisticRegression model. Notice how we didn’t specify any parameters because we’ll leverage the defaults and our data conforms to the proper column naming. In practice, you probably won’t need to change many of the parameters: 这是使用 LogisticRegression 模型的简单示例。请注意我们没有指定任何参数，因为我们将利用默认值并且我们的数据符合正确的列命名。实际上，您可能不需要更改许多参数： 12345// in Scalaimport org.apache.spark.ml.classification.LogisticRegressionval lr = new LogisticRegression()println(lr.explainParams()) // see all parametersval lrModel = lr.fit(bInput) 12345# in Pythonfrom pyspark.ml.classification import LogisticRegressionlr = LogisticRegression()print lr.explainParams() # see all parameterslrModel = lr.fit(bInput) Once the model is trained you can get information about the model by taking a look at the coefficients and the intercept. The coefficients correspond to the individual feature weights (each feature weight is multiplied by each respective feature to compute the prediction) while the intercept is the value of the italics-intercept (if we chose to fit one when specifying the model). Seeing the coefficients can be helpful for inspecting the model that you built and comparing how features affect the prediction: 训练模型后，您可以通过查看系数和截距来获得有关模型的信息。系数对应于各个特征权重（每个特征权重乘以每个相应的特征以计算预测），而截距是斜体截距的值（如果我们在指定模型时选择拟合一个）。查看系数有助于检查您构建的模型并比较特征如何影响预测： 123// in Scalaprintln(lrModel.coefficients)println(lrModel.intercept) 123# in Pythonprint lrModel.coefficientsprint lrModel.intercept For a multinomial model (the current one is binary), lrModel.coefficientMatrix and lrModel.interceptVector can be used to get the coefficients and intercept. These will return Matrix and Vector types representing the values or each of the given classes. 对于多项模型（当前的二元模型），可以使用 lrModel.coefficientMatrix 和 lrModel.interceptVector 来获取系数和截距。这些将返回表示值或每个给定类的 Matrix 和 Vector 类型。 Model Summary 模型摘要Logistic regression provides a model summary that gives you information about the final, trained model. This is analogous to the same types of summaries we see in many R language machine learning packages. The model summary is currently only available for binary logistic regression problems, but multiclass summaries will likely be added in the future. Using the binary summary, we can get all sorts of information about the model itself including the area under the ROC curve, the f measure by threshold, the precision, the recall, the recall by thresholds, and the ROC curve. Note that for the area under the curve, instance weighting is not taken into account, so if you wanted to see how you performed on the values you weighed more highly, you’d have to do that manually. This will probably change in future Spark versions. You can see the summary using the following APIs: Logistic 回归提供了一个模型摘要，为您提供有关最终训练模型的信息。这类似于我们在许多R语言机器学习包中看到的相同类型的摘要。模型摘要目前仅适用于二元逻辑回归问题，但将来可能会添加多类摘要。使用二元汇总，我们可以获得有关模型本身的各种信息，包括 ROC 曲线下的面积，阈值的 f 度量，精度，召回率，阈值召回率和ROC 曲线。请注意，对于曲线下方的区域，不考虑实例权重，因此如果您想要查看权重更高的值的执行情况，则必须手动执行此操作。这可能会在未来的Spark版本中发生变化。您可以使用以下API查看摘要： 1234567// in Scalaimport org.apache.spark.ml.classification.BinaryLogisticRegressionSummaryval summary = lrModel.summaryval bSummary = summary.asInstanceOf[BinaryLogisticRegressionSummary]println(bSummary.areaUnderROC)bSummary.roc.show()bSummary.pr.show() 12345# in Pythonsummary = lrModel.summaryprint summary.areaUnderROCsummary.roc.show()summary.pr.show() The speed at which the model descends to the final result is shown in the objective history. We can access this through the objective history on the model summary : 模型下降到最终结果的速度显示在目标历史中。我们可以通过模型摘要的目标历史来访问它： 1summary.objectiveHistory This is an array of doubles that specify how, over each training iteration, we are performing with respect to our objective function. This information is helpful to see if we have sufficient iterations or need to be tuning other parameters. 这是一个双精度数组，用于指定在每次训练迭代中我们对目标函数的执行方式。此信息有助于查看是否有足够的迭代或需要调整其他参数。 Decision TreesDecision trees are one of the more friendly and interpretable models for performing classification because they’re similar to simple decision models that humans use quite often. For example, if you have to predict whether or not someone will eat ice cream when offered, a good feature might be whether or not that individual likes ice cream. In pseudocode, if person.likes(“ice_cream”), they will eat ice cream; otherwise, they won’t eat ice cream. A decision tree creates this type of structure with all the inputs and follows a set of branches when it comes time to make a prediction. This makes it a great starting point model because it’s easy to reason about, easy to inspect, and makes very few assumptions about the structure of the data. In short, rather than trying to train coeffiecients in order to model a function, it simply creates a big tree of decisions to follow at prediction time. This model also supports multiclass classification and provides outputs as predictions and probabilities in two different columns. 决策树是用于执行分类的更友好且可解释的模型之一，因为它们类似于人类经常使用的简单决策模型。 例如，如果您必须预测某人是否会在提供冰淇淋时吃冰淇淋，那么一个好的特征可能就是这个人是否喜欢冰淇淋。在伪代码中，如果是 person.likes(“ice_cream”)，他们会吃冰淇淋; 否则，他们不会吃冰淇淋。决策树使用所有输入创建此类结构，并在进行预测时遵循一组分支。 这使它成为一个很好的起点模型，因为它易于推理，易于检查，并且对数据结构做出很少的假设。简而言之，它不是试图训练系数来模拟一个函数，而是简单地在预测时创建一个大的决策树。 该模型还支持多类分类，并在两个不同的列中提供输出作为预测和概率。 While this model is usually a great start, it does come at a cost. It can overfit data extremely quickly. By that we mean that, unrestrained, the decision tree will create a pathway from the start based on every single training example. That means it encodes all of the information in the training set in the model. This is bad because then the model won’t generalize to new data (you will see poor test set prediction performance). However, there are a number of ways to try and rein in the model by limiting its branching structure (e.g., limiting its height) to get good predictive power. 虽然这种模式通常是一个很好的开始，但确实需要付出代价。它可以非常快速地过度拟合数据。我们的意思是，无拘无束，决策树将从一开始就根据每个训练样例创建一条路径。这意味着它会对模型中训练集中的所有信息进行编码。这很糟糕，因为那时模型不会泛化到新数据（您将看到不良的测试集预测性能）。然而，有许多方法可以通过限制其分支结构（例如，限制其高度）来尝试并控制模型以获得良好的预测能力。 See ISL 8.1 and ESL 9.2 for more information. 有关更多信息，请参见 ISL 8.1 和 ESL 9.2。 Model Hyperparameters 模型超参数There are many different ways to configure and train decision trees. Here are the hyperparameters that Spark’s implementation supports : 有许多不同的方法来配置和训练决策树。以下是Spark实现支持的超参数： maxDepth Since we’re training a tree, it can be helpful to specify a max depth in order to avoid overfitting to the dataset (in the extreme, every row ends up as its own leaf node). The default is 5. 由于我们正在训练树，因此指定最大深度以避免过度拟合数据集会很有帮助（在极端情况下，每一行最终都是自己的叶节点）。默认值为 5。 maxBins In decision trees, continuous features are converted into categorical features and maxBins determines how many bins should be created from continous features. More bins gives a higher level of granularity. The value must be greater than or equal to 2 and greater than or equal to the number of categories in any categorical feature in your dataset. The default is 32. 在决策树中，连续特征将转换为分类特征，maxBins 将确定应从连续特征创建的分箱数（分桶数）。更多的箱（桶）提供更高级别的粒度。该值必须大于或等于2且大于或等于数据集中任何分类特征中的类别数。默认值为32. impurity To build up a “tree” you need to configure when the model should branch. Impurity represents the metric (information gain) to determine whether or not the model should split at a particular leaf node. This parameter can be set to either be “entropy” or “gini” (default), two commonly used impurity metrics. 要构建“树”，您需要配置模型何时应该分支。杂质表示用于确定模型是否应在特定叶节点处拆分的衡量指标（信息增益）。此参数可以设置为“信息熵”或“基尼系数”（默认），两个常用的不纯度的衡量指标。 minInfoGain This parameter determines the minimum information gain that can be used for a split. A higher value can prevent overfitting. This is largely something that needs to be determined from testing out different variations of the decision tree model. The default is zero. 此参数确定可用于拆分的最小信息增益。较高的值可以防止过度拟合。这很大程度上需要通过测试决策树模型的不同变体来确定。默认值为零。 minInstancePerNode This parameter determines the minimum number of training instances that need to end in a particular node. Think of this as another manner of controlling max depth. We can prevent overfitting by limiting depth or we can prevent it by specifying that at minimum a certain number of training values need to end up in a particular leaf node. If it’s not met we would “prune” the tree until that requirement is met. A higher value can prevent overfitting. The default is 1, but this can be any value greater than 1. 此参数确定需要在特定节点中结束的最小训练实例数。可以将其视为控制最大深度的另一种方式。我们可以通过限制深度来防止过度拟合，或者我们可以通过指定至少一定数量的训练值需要在特定叶节点中结束防止过度拟合。如果不满足，我们将“修剪”树，直到满足该要求。较高的值可以防止过度拟合。默认值为1，但这可以是大于1的任何值。 Training Parameters 训练参数These are configurations we specify in order to manipulate how we perform our training. Here is the training parameter for decision trees: 这些是我们指定的配置，以便控制我们如何执行训练。 以下是决策树的训练参数： checkpointInterval 检查点间隔时间 Checkpointing is a way to save the model’s work over the course of training so that if nodes in the cluster crash for some reason, you don’t lose your work. A value of 10 means the model will get checkpointed every 10 iterations. Set this to -1 to turn off checkpointing. This parameter needs to be set together with a checkpointDir (a directory to checkpoint to) and with useNodeIdCache=true. Consult the Spark documentation for more information on checkpointing. 检查点是一种在训练过程中保存模型工作的方法，这样如果群集中的节点由于某种原因而崩溃，您就不会丢失工作。 值 10 表示模型将每10次迭代检查一次。 将此值设置为 -1 可关闭检查点。 此参数需要与checkpointDir（检查点的目录）和 useNodeIdCache=true 一起设置。 有关检查点的更多信息，请参阅Spark文档。 Prediction Parameters 预测参数There is only one prediction parameter for decision trees: thresholds. Refer to the explanation for thresholds under “Logistic Regression”. 决策树只有一个预测参数：阈值。 请参阅“Logistic 回归”下的阈值（thresholds ）说明。 Here’s a minimal but complete example of using a decision tree classifier: 这是使用决策树分类器的最小但完整的示例： 12345// in Scalaimport org.apache.spark.ml.classification.DecisionTreeClassifierval dt = new DecisionTreeClassifier()println(dt.explainParams())val dtModel = dt.fit(bInput) 12345# in Pythonfrom pyspark.ml.classification import DecisionTreeClassifierdt = DecisionTreeClassifier()print dt.explainParams()dtModel = dt.fit(bInput) Random Forest and Gradient-Boosted Trees 随机森林与梯度提升树These methods are extensions of the decision tree. Rather than training one tree on all of the data, you train multiple trees on varying subsets of the data. The intuition behind doing this is that various decision trees will become “experts” in that particular domain while others become experts in others. By combining these various experts, you then get a “wisdom of the crowds” effect, where the group’s performance exceeds any individual. In addition, these methods can help prevent overfitting. 这些方法是决策树的扩展。您可以在不同的数据子集上训练多个树，而不是在所有数据上训练一棵树。这样做的直觉是，各种决策树将成为该特定领域的“专家”，而其他决策树则成为其他领域的专家。通过将这些不同的专家结合起来，您可以获得“群众智慧”的效果，即群体的表现超过任何个体。此外，这些方法可以帮助防止过度拟合。 Random forests and gradient-boosted trees are two distinct methods for combining decision trees. In random forests, we simply train a lot of trees and then average their response to make a prediction. With gradient-boosted trees, each tree makes a weighted prediction (such that some trees have more predictive power for some classes than others). They have largely the same parameters, which we note below. One current limitation is that gradient-boosted trees currently only support binary labels. 随机森林和梯度提升树是组合决策树的两种不同方法。在随机森林中，我们只是训练了很多树，然后平均他们的反馈来做出预测。对于梯度提升树，每棵树都进行加权预测（这样一些树对某些类具有比其他树更强的预测能力）。它们的参数大致相同，我们在下面说明。目前的一个限制是梯度提升树目前仅支持二元标签。 NOTE 注意There are several popular tools for learning tree-based models. For example, the XGBoost library provides an integration package for Spark that can be used to run it on Spark. 有几种流行的工具可用于学习基于树的模型。例如，XGBoost 库为Spark提供了一个集成包，可用于在Spark上运行它。 See ISL 8.2 and ESL 10.1 for more information on these tree ensemble models. 有关这些树集合模型的更多信息，请参见 ISL 8.2 和 ESL 10.1。 Model Hyperparameters 模型超参数Random forests and gradient-boosted trees provide all of the same model hyperparameters supported by decision trees. In addition, they add several of their own. 随机森林和梯度提升树提供决策树支持的所有相同的模型超参数。此外，他们还添加了几个自己的。 Random forest only 只有随机森林numTrees 树的数量 The total number of trees to train. 要训练的树木总数。 featureSubsetStrategy 特征子集的策略 This parameter determines how many features should be considered for splits. This can be a variety of different values including “auto”, “all”, “sqrt”, “log2”, or a number “n.” When your input is “n” the model will use n * number of features during training. When n is in the range (1, number of features), the model will use n features during training. There’s no one-size-fits-all solution here, so it’s worth experimenting with different values in your pipeline. 此参数确定要为拆分考虑的功能数量。这可以是各种不同的值，包括“auto”，“all”，“sqrt”，“log2”或数字“n”。当您的输入为“n”时，模型将在训练期间使用：n乘以特征数量。当n在范围（1，特征数量）范围内时，模型将在训练期间使用n个特征。这里没有一个通用的解决方案，因此值得在您的管道中试验不同的值。 Gradient-boosted trees (GBT) only 仅梯度提升树（GBT）lossType 损失函数类型 This is the loss function for gradient-boosted trees to minimize during training. Currently, only logistic loss is supported. 这是梯度提升树在训练期间最小化的损失函数。目前，仅支持logistic损失。 maxIter 最大迭代次数 Total number of iterations over the data before stopping. Changing this probably won’t change your results a ton, so it shouldn’t be the first parameter you look to adjust. The default is 100. 停止前数据的迭代总数。改变这个可能不会明显改变你的结果，所以它不应该是你想要调整的第一个参数。默认值为100。 stepSize 每步的大小（即学习率的大小） This is the learning rate for the algorithm. A larger step size means that larger jumps are made between training iterations. This can help in the optimization process and is something that should be tested in training. The default is 0.1 and this can be any value from 0 to 1. 这是算法的学习率。较大的步长意味着在训练迭代之间进行较大的跳跃。这有助于优化过程，并且应该在训练中进行测试。默认值为 0.1，可以是 0 到 1 之间的任何值。 Training Parameters 训练参数There is only one training parameter for these models, checkpointInterval. Refer back to the explanation under “Decision Trees” for details on checkpointing. 这些模型只有一个训练参数，checkpointInterval。有关检查点的详细信息，请参阅“决策树”下的说明。 Prediction Parameters 预测参数These models have the same prediction parameters as decision trees. Consult the prediction parameters under that model for more information. 这些模型具有与决策树相同的预测参数。有关更多信息，请参阅该模型下的预测参数。 Here’s a short code example of using each of these classifiers: 这是使用每个分类器的简短代码示例： 1234567891011// in Scalaimport org.apache.spark.ml.classification.RandomForestClassifierval rfClassifier = new RandomForestClassifier()println(rfClassifier.explainParams())val trainedModel = rfClassifier.fit(bInput)// in Scalaimport org.apache.spark.ml.classification.GBTClassifierval gbtClassifier = new GBTClassifier()println(gbtClassifier.explainParams())val trainedModel = gbtClassifier.fit(bInput) 1234567891011# in Pythonfrom pyspark.ml.classification import RandomForestClassifierrfClassifier = RandomForestClassifier()print rfClassifier.explainParams()trainedModel = rfClassifier.fit(bInput)# in Pythonfrom pyspark.ml.classification import GBTClassifiergbtClassifier = GBTClassifier()print gbtClassifier.explainParams()trainedModel = gbtClassifier.fit(bInput) Naive Bayes 朴素贝叶斯Naive Bayes classifiers are a collection of classifiers based on Bayes’ theorem. The core assumption behind the models is that all features in your data are independent of one another. Naturally, strict independence is a bit naive, but even if this is violated, useful models can still be produced. Naive Bayes classifiers are commonly used in text or document classification, although it can be used as a more general-purpose classifier as well. There are two different model types: either a multivariate Bernoulli model, where indicator variables represent the existence of a term in a document; or the multinomial model, where the total counts of terms are used. 朴素贝叶斯分类器是基于贝叶斯定理的分类器集合。模型背后的核心假设是数据中的所有特征都是相互独立的。当然，严格的独立性有点天真，但即使违反了这一点，仍然可以制作出有用的模型。朴素贝叶斯分类器通常用于文本或文档分类，尽管它也可以用作更通用的分类器。有两种不同的模型类型：多变量伯努利模型，其中指标变量（indicator variable）表示文档中术语（terms）的存在；或多项式模型，其中使用术语（terms）的总计数。 One important note when it comes to Naive Bayes is that all input features must be non-negative. See ISL 4.4 and ESL 6.6 for more background on these models. 朴素贝叶斯的一个重要注意事项是所有输入功能必须是非负的。有关这些内容的更多背景信息，请参阅 ISL 4.4 和ESL 6.6 楷模。 Model Hyperparameters 模型超参数These are configurations we specify to determine the basic structure of the models:这些是我们指定的配置，用于决定模型的基本结构： modelType 模型类型 Either “bernoulli” or “multinomial.” See the previous section for more information on this choice. “伯努利”或“多项式”。有关此选择的更多信息，请参阅上一节。 weightCol 权重列 Allows weighing different data points differently. Refer back to “Training Parameters” for the explanation of this hyperparameter. 允许对不同的数据点进行不一样的加权。有关此超参数的说明，请参阅“训练参数”。 Training Parameters 训练参数These are configurations that specify how we perform our training:这些是指定我们如何执行训练的配置： smoothing 平滑 This determines the amount of regularization that should take place using additive smoothing. This helps smooth out categorical data and avoid overfitting on the training data by changing the expected probability for certain classes. The default value is 1. 这决定了应使用加法平滑进行正则化的数量。这有助于平滑分类数据，并通过改变某些类的预期概率来避免过度拟合训练数据。默认值为1。 Prediction Parameters 预测参数Naive Bayes shares the same prediction parameter, thresholds, as all of our other models. Refer back to the previous explanation for threshold to see how to use this. Here’s an example of using a Naive Bayes classifier. 朴素贝叶斯与我们所有其他模型共享相同的预测参数，阈值。请参阅前面的阈值（threshold ）说明，了解如何使用它。这是使用朴素贝叶斯分类器的示例。 12345// in Scalaimport org.apache.spark.ml.classification.NaiveBayesval nb = new NaiveBayes()println(nb.explainParams())val trainedModel = nb.fit(bInput.where("label != 0")) 12345# in Pythonfrom pyspark.ml.classification import NaiveBayesnb = NaiveBayes()print nb.explainParams()trainedModel = nb.fit(bInput.where("label != 0")) WARNING 警告Note that in this example dataset, we have features that have negative values. In this case, the rows with negative features correspond to rows with label “0”. Therefore we’re just going to filter them out (via the label) instead of processing them further to demonstrate the naive bayes API. 请注意，在此示例数据集中，我们具有负值的特征。 在这种情况下，具有负特征的行对应于标记为“0”的行。 因此，我们只是将它们过滤掉（通过标签），而不是进一步处理它们以演示朴素的贝叶斯API。 Evaluators for Classification and Automating Model Tuning 分类和自动化模型调整评估器As we saw in Chapter 24, evaluators allow us to specify the metric of success for our model. An evaluator doesn’t help too much when it stands alone; however, when we use it in a pipeline, we can automate a grid search of our various parameters of the models and transformers—trying all combinations of the parameters to see which ones perform the best. Evaluators are most useful in this pipeline and parameter grid context. For classification, there are two evaluators, and they expect two columns: a predicted label from the model and a true label. For binary classification we use the BinaryClassificationEvaluator. This supports optimizing for two different metrics “areaUnderROC” and areaUnderPR.” For multiclass classification, we need to use the MulticlassClassificationEvaluator, which supports optimizing for “f1”, “weightedPrecision”, “weightedRecall”, and “accuracy”. 正如我们在第24章中看到的那样，评估器（evaluator）允许我们为模型指定成功的衡量。评估器（evaluator）独立时并没有太多帮助；然而，当我们在管道中使用它时，我们可以自动对模型和转换器的各种参数进行网格搜索——尝试参数的所有组合以查看哪些参数表现最佳。评估器（evaluator）在此管道和参数网格上下文中最有用。对于分类，有两个评估器（evaluator），他们期望有两列：来自模型的预测标签和真实标签。对于二元分类，我们使用 BinaryClassificationEvaluator。这支持优化两个不同的衡量指标 “areaUnderROC” 和 “areaUnderPR“。对于多类分类，我们需要使用MulticlassClassificationEvaluator，它支持优化 ”f1“，”weightedPrecision“，”weightedRecall“ 和 ”accuracy“。 To use evaluators, we build up our pipeline, specify the parameters we would like to test, and then run it and see the results. See Chapter 24 for a code example. 要使用评估器（evaluator），我们构建我们的管道，指定我们想要测试的参数，然后运行它并查看结果。有关代码示例，请参见第24章。 Detailed Evaluation Metrics 详细的评估指标MLlib also contains tools that let you evaluate multiple classification metrics at once. Unfortunately, these metrics classes have not been ported over to Spark’s DataFrame-based ML package from the underlying RDD framework. So, at the time of this writing, you still have to create an RDD to use these. In the future, this functionality will likely be ported to DataFrames and the following may no longer be the best way to see metrics (although you will still be able to use these APIs). MLlib 还包含一些工具，可让您一次评估多个分类指标。遗憾的是，这些衡量标准类尚未从基础RDD框架移植到Spark的基于DataFrame的ML包。因此，在撰写本文时，您仍然需要创建一个RDD来使用它们。将来，此功能可能会移植到DataFrames，以下可能不再是查看指标的最佳方式（尽管您仍然可以使用这些API）。 There are three different classification metrics we can use: 我们可以使用三种不同的分类指标： Binary classification metrics二进制分类指标 Multiclass classification metrics多类分类指标 Multilabel classification metrics多标签分类指标 All of these measures follow the same approximate style. We’ll compare generated outputs with true values and the model calculates all of the relevant metrics for us. Then we can query the object for the values for each of the metrics: 所有这些措施都遵循相同的近似风格。我们将生成的输出与真值进行比较，模型为我们计算所有相关指标。然后我们可以在对象中查询每个指标的值： 123456// in Scalaimport org.apache.spark.mllib.evaluation.BinaryClassificationMetricsval out = model.transform(bInput).select("prediction", "label").rdd.map(x =&gt; (x(0).asInstanceOf[Double], x(1).asInstanceOf[Double]))val metrics = new BinaryClassificationMetrics(out) 123456# in Pythonfrom pyspark.mllib.evaluation import BinaryClassificationMetricsout = model.transform(bInput)\.select("prediction", "label")\.rdd.map(lambda x: (float(x[0]), float(x[1])))metrics = BinaryClassificationMetrics(out) Once we’ve done that, we can see typical classification success metrics on this metric’s object using a similar API to the one we saw with logistic regression : 完成后，我们可以使用与逻辑回归看到的类似的API，在此衡量标准对象上看到典型的分类成功衡量标准： 12345// in Scalametrics.areaUnderPRmetrics.areaUnderROCprintln("Receiver Operating Characteristic")metrics.roc.toDF().show() 12345# in Pythonprint metrics.areaUnderPRprint metrics.areaUnderROCprint "Receiver Operating Characteristic"metrics.roc.toDF().show() One-vs-Rest Classifier 1对其余的分类There are some MLlib models that don’t support multiclass classification. In these cases, users can leverage a one-vs-rest classifier in order to perform multiclass classification given only a binary classifier. The intuition behind this is that for every class you hope to predict, the one-vs-rest classifier will turn the problem into a binary classification problem by isolating one class as the target class and grouping all of the other classes into one. Thus the prediction of the class becomes binary (is it this class or not this class?). 有些MLlib模型不支持多类分类。在这些情况下，用户可以利用 one-vs-rest 分类器，以便仅在给定二元分类器的情况下执行多类分类。这背后的直觉是，对于您希望预测的每个类，one-vs-rest 分类器将把一个类隔离为目标类并将所有其他类分组为一个，从而将问题转化为二元分类问题。因此，类的预测变为二元（这个类是否是这个类？）。 One-vs-rest is implemented as an estimator. For the base classifier it takes instances of the classifier and creates a binary classification problem for each of the classes. The classifier for class i is trained to predict whether the label is i or not, distinguishing class i from all other classes. Predictions are done by evaluating each binary classifier and the index of the most confident classifier is output as the label. One-vs-rest 实现为估计器（estimator）。对于基类分类器，它接受分类器的实例并为每个类创建二元分类问题。训练 i 类的分类器来预测标签是否为 i ，将类 i 与所有其他类区分开来。通过评估每个二元分类器来完成预测，并且输出最自信的分类器的下标作为标签。 See the Spark documentation for a nice example of the use of one-vs-rest. 请参阅Spark文档，了解使用 one-vs-rest 的一个很好的例子）。 Multilayer Perceptron 多层感知器The multilayer perceptron is a classifier based on neural networks with a configurable number of layers (and layer sizes). We will discuss it in Chapter 31. 多层感知机是基于具有可配置数量的层（和层大小）的神经网络的分类器。我们将在第31章讨论它。 Conclusion 结论In this chapter we covered the majority of tools Spark provides for classification: predicting one of a finite set of labels for each data point based on its features. In the next chapter, we’ll look at regression, where the required output is continuous instead of categorical. 在本章中，我们介绍了Spark为分类提供的大多数工具：根据每个数据点的特征为每个数据点预测一组有限标签。在下一章中，我们将看回归，其中所需的输出是连续的而不是分类的。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 28 Recommendation]]></title>
    <url>%2F2019%2F08%2F31%2FChapter28_Recommendation(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 28 Recommendation 推荐 译者：https://snaildove.github.ioThe task of recommendation is one of the most intuitive. By studying people’s explicit preferences (through ratings) or implicit preferences (through observed behavior), you can make recommendations on what one user may like by drawing similarities between the user and other users, or between the products they liked and other products. Using the underlying similarities, recommendation engines can make new recommendations to other users. 推荐任务是最直观的。通过研究人们的显性偏好（通过评分）或隐含偏好（通过观察到的行为），您可以通过绘制用户与其他用户之间或他们喜欢的产品与其他产品之间的相似性来对用户可能喜欢的内容提出建议。利用潜在的相似性，推荐引擎可以向其他用户提出新的推荐。 Use Cases 使用案例Recommendation engines are one of the best use cases for big data. It’s fairly easy to collect training data about users’ past preferences at scale, and this data can be used in many domains to connect users with new content. Spark is an open source tool of choice used across a variety of companies for large-scale recommendations: 推荐引擎是大数据的最佳用例之一。可以相当容易地大规模收集有关用户过去偏好的训练数据，并且可以在许多域中使用此数据来将用户与新内容连接起来。 Spark是一种开源工具，可供各种公司使用，用于大规模推荐： Movie recommendations 电影推荐Amazon, Netflix, and HBO all want to provide relevant film and TV content to their users. Netflix utilizes Spark, to make large scale movie recommendations to their users. 亚马逊，Netflix 和 HBO 都希望向用户提供相关的电影和电视内容。 Netflix 利用 Spark 为其用户制作大型电影推荐。 Course recommendations 课程建议A school might want to recommend courses to students by studying what courses similar students have liked or taken. Past enrollment data makes for a very easy to collect training dataset for this task. 学校可能希望通过研究类似学生喜欢或去上的课程向学生推荐课程。过去的注册数据可以很容易地收集此任务的训练数据集。 In Spark, there is one workhorse recommendation algorithm, Alternating Least Squares (ALS). This algorithm leverages a technique called collaborative filtering, which makes recommendations based only on which items users interacted with in the past. That is, it does not require or use any additional features about the users or the items. It supports several ALS variants (e.g., explicit or implicit feedback). Apart from ALS, Spark provides Frequent Pattern Mining for finding association rules in market basket analysis. Finally, Spark’s RDD API also includes a lower-level matrix factorization method that will not be covered in this book. 在Spark中，有一种主力推荐算法，即交替最小二乘（Alternating Least Squares: ALS）。该算法利用称为协同过滤的技术，该技术仅基于用户过去互动的商品进行推荐。也就是说，它不需要或使用关于用户或商品的任何额外特征。它支持几种ALS变体（例如，显式或隐式反馈）。除了ALS之外，Spark还提供频繁模式挖掘，用于在市场购物篮分析中查找关联规则。最后，Spark 的 RDD API 还包括一个较低级别的矩阵分解方法，本书不会介绍。 Collaborative Filtering with Alternating Least Squares 使用交替最小二乘法进行协同过滤ALS finds a -dimensional feature vector for each user and item such that the dot product of each user’s feature vector with each item’s feature vector approximates the user’s rating for that item. Therefore this only requires an input dataset of existing ratings between user-item pairs, with three columns: a user ID column, an item ID column (e.g., a movie), and a rating column. The ratings can either be explicit—a numerical rating that we aim to predict directly—or implicit—in which case each rating represents the strength of interactions observed between a user and item (e.g., number of visits to a particular page), which measures our level of confidence in the user’s preference for that item. Given this input DataFrame, the model will produce feature vectors that you can use to predict users’ ratings for items they have not yet rated. ALS 为每个用户和商品找到一个维度特征向量，使得每个用户的特征向量与每个商品的特征向量的点积近似于该用户对该商品的评分。因此，这仅需要用户—商品对之间的现有评分的输入数据集，具有三列：用户ID列，商品ID列（例如，电影）和评分列。评分可以是显性的——我们旨在直接或隐含地预测的数字评分——或者隐形的——在这种情况下，每个评分表示在用户和商品之间观察到的互动的强度（例如，对特定页面的访问次数），这度量我们对用户对该商品的偏好的信心程度。给定此输入DataFrame，模型将生成特征向量，您可以使用这些特征向量来预测用户对尚未评分的商品的评分。 One issue to note in practice is that this algorithm does have a preference for serving things that are very common or that it has a lot of information on. If you’re introducing a new product that no users have expressed a preference for, the algorithm isn’t going to recommend it to many people. Additionally, if new users are onboarding onto the platform, they may not have any ratings in the training set. Therefore, the algorithm won’t know what to recommend them. These are examples of what we call the cold start problem, which we discuss later on in the chapter. 在实践中需要注意的一个问题是，该算法确实倾向于提供非常常见的事物或者具有大量信息的事物。如果您正在推出一种没有用户表示偏好的新产品，该算法不会向许多人推荐它。此外，如果新用户加入平台，他们可能在训练集中没有任何评分。因此，算法将不知道推荐它们的内容。这些是我们称之为冷启动问题的例子，我们将在本章后面讨论。 In terms of scalability, one reason for Spark’s popularity for this task is that the algorithm and implementation in MLlib can scale to millions of users, millions of items, and billions of ratings. 在可扩展性方面，Spark在此任务中受欢迎的一个原因是MLlib中的算法和实现可以扩展到数百万用户，数百万项和数十亿的评分。 Model Hyperparameters 模型超参数These are configurations that we can specify to determine the structure of the model as well as the specific collaborative filtering problem we wish to solve : 这些是我们可以指定的配置，用于确定模型的结构以及我们希望解决的特定协同过滤问题： rank 排名（评分排序） The rank term determines the dimension of the feature vectors learned for users and items. This should normally be tuned through experimentation. The core trade-off is that by specifying too high a rank, the algorithm may overfit the training data; but by specifying a low rank, then it may not make the best possible predictions. The default value is 10. 排名（评分排序）术语确定为用户和商品学习的特征向量的维度。这通常应该通过实验来调整。核心权衡是通过指定过高的等级，算法可能过度拟合训练数据；但是通过指定低等级，则可能无法做出最佳预测。默认值为10。 alpha When training on implicit feedback (behavioral observations), the alpha sets a baseline confidence for preference. This has a default of 1.0 and should be driven through experimentation. 在对隐式反馈（行为观察）进行训练时，alpha设置了偏好的基准置信度。它的默认值为1.0，应该通过实验来驱动。 regParam Controls regularization to prevent overfitting. You should test out different values for the regularization parameter to find the optimal value for your problem. The default is 0.1. 控制正规化以防止过度拟合。您应该测试正则化参数的不同值，以找到问题的最优值。默认值为0.1。 implicitPrefs This Boolean value specifies whether you are training on implicit (true) or explicit (false) (refer back to the preceding discussion for an explanation of the difference between explicit and implicit). This value should be set based on the data that you’re using as input to the model. If the data is based off passive endorsement of a product (say, via a click or page visit), then you should use implicit preferences. In contrast, if the data is an explicit rating (e.g., the user gave this restaurant 4/5 stars), you should use explicit preferences. Explicit preferences are the default. 此布尔值指定您是在训练隐式（true）还是显式（false）（请参阅前面的讨论，以解释显式和隐式之间的区别）。应根据您用作模型输入的数据设置此值。如果数据基于产品的被动认可（例如，通过点击或页面访问），那么您应该使用隐式偏好。相反，如果数据是显示的评分（例如，用户给这家餐厅 4/5 星），您应该使用显示的偏好。显式偏好是默认值。 nonnegative If set to true, this parameter configures the model to place non-negative constraints on the leastsquares problem it solves and only return non-negative feature vectors. This can improve performance in some applications. The default value is false. 如果设置为 true，则此参数将模型配置为对其解决的最小二乘问题设置非负约束，并仅返回非负特征向量。这可以提高某些应用程序的性能。默认值为false。 Training Parameters 训练参数The training parameters for alternating least squares are a bit different from those that we have seen in other models. That’s because we’re going to get more low-level control over how the data is distributed across the cluster. The groups of data that are distributed around the cluster are called blocks. Determining how much data to place in each block can have a significant impact on the time it takes to train the algorithm (but not the final result). A good rule of thumb is to aim for approximately one to five million ratings per block. If you have less data than that in each block, more blocks will not improve the algorithm’s performance. 交替最小二乘的训练参数与我们在其他模型中看到的训练参数略有不同。那是因为我们将对数据在集群中的分布方式进行更多的低级控制。围绕群集分布的数据组称为块。确定每个块中放置多少数据会对训练算法所花费的时间产生重大影响（但不是最终结果）。一个好的经验法则是每块大约有一到五百万的评分。如果数据少于每个块中的数据，则更多的块不会提高算法的性能。 numUserBlocks This determines how many blocks to split the users into. The default is 10. 这决定了将用户分成多少个块。默认值为10。 numItemBlocks This determines how many blocks to split the items into. The default is 10. 这决定了将商品拆分的块数。默认值为10。 maxIter Total number of iterations over the data before stopping. Changing this probably won’t change your results a ton, so this shouldn’t be the first parameter you adjust. The default is 10. An example of when you might want to increase this is that after inspecting your objective history and noticing that it doesn’t flatline after a certain number of training iterations. 停止前数据的迭代总数。改变这个可能不会剧烈改变你的结果，所以这不应该是你调整的第一个参数。默认值为10。 您可能希望增加此值的一个示例是检查您的目标历史记录并注意到在一定数量的训练迭代后它不会变平。 checkpointInterval Checkpointing allows you to save model state during training to more quickly recover from node failures. You can set a checkpoint directory using SparkContext.setCheckpointDir. 检查点允许您在训练期间保存模型状态，以便更快地从节点故障中恢复。您可以使用SparkContext.setCheckpointDir 设置检查点目录。 seed Specifying a random seed can help you replicate your results. 指定随机种子可以帮助您复制结果。 Prediction Parameters 预测参数Prediction parameters determine how a trained model should actually make predictions. In our case, there’s one parameter: the cold start strategy (set through coldStartStrategy). This setting determines what the model should predict for users or items that did not appear in the training set. The cold start challenge commonly arises when you’re serving a model in production, and new users and/or items have no ratings history, and therefore the model has no recommendation to make. It can also occur when using simple random splits as in Spark’s CrossValidator or TrainValidationSplit, where it is very common to encounter users and/or items in the evaluation set that are not in the training set. 预测参数确定训练模型应如何实际进行预测。在我们的例子中，有一个参数：冷启动策略（通过 coldStartStrategy 设置）。此设置确定模型应为未出现在训练集中的用户或商品预测的内容。当您在生产中为模型提供服务时，通常会出现冷启动挑战，并且新用户和/或商品没有评分历史记录，因此该模型无需建议。当使用 Spark 的 CrossValidator 或 TrainValidationSplit 中的简单随机拆分时也会发生这种情况，在这种情况下，在评估集中遇到不在训练集中的用户和/或商品是很常见的。 By default, Spark will assign NaN prediction values when it encounters a user and/or item that is not present in the actual model. This can be useful because you design your overall system to fall back to some default recommendation when a new user or item is in the system. However, this is undesirable during training because it will ruin the ability for your evaluator to properly measure the success of your model. This makes model selection impossible. Spark allows users to set the coldStartStrategy parameter to drop in order to drop any rows in the DataFrame of predictions that contain NaN values. The evaluation metric will then be computed over the non-NaN data and will be valid. drop and nan (the default) are the only currently supported cold-start strategies. 默认情况下，Spark 会在遇到实际模型中不存在的用户和/或商品时分配 NaN 预测值。这可能很有用，因为您将整个系统设计为在系统中有新用户或商品时回退到某个默认建议。但是，这在训练期间是不合需要的，因为它会破坏评估者正确测量模型成功的能力。这使得模型选择不可能。 Spark 允许用户将 coldStartStrategy 参数设置为drop，以便删除包含 NaN 值的预测的DataFrame中的任何行。然后将根据非NaN数据计算评估度量并且该评估度量将是有效的。 drop和nan（默认值）是目前唯一支持的冷启动策略。 ExampleThis example will make use of a dataset that we have not used thus far in the book, the MovieLens movie rating dataset. This dataset, naturally, has information relevant for making movie recommendations. We will first use this dataset to train a model: 此示例将使用我们迄今为止尚未使用的数据集，即 MovieLens 电影评分数据集。当然，该数据集具有与制作电影推荐相关的信息。我们将首先使用此数据集来训练模型： 12345678910111213141516171819// in Scalaimport org.apache.spark.ml.recommendation.ALSval ratings = spark.read.textFile("/data/sample_movielens_ratings.txt").selectExpr("split(value , '::') as col").selectExpr("cast(col[0] as int) as userId","cast(col[1] as int) as movieId","cast(col[2] as float) as rating","cast(col[3] as long) as timestamp")val Array(training, test) = ratings.randomSplit(Array(0.8, 0.2))val als = new ALS().setMaxIter(5).setRegParam(0.01).setUserCol("userId").setItemCol("movieId").setRatingCol("rating")println(als.explainParams())val alsModel = als.fit(training)val predictions = alsModel.transform(test) 1234567891011121314151617181920# in Pythonfrom pyspark.ml.recommendation import ALSfrom pyspark.sql import Rowratings = spark.read.text("/data/sample_movielens_ratings.txt")\.rdd.toDF()\.selectExpr("split(value , '::') as col")\.selectExpr("cast(col[0] as int) as userId","cast(col[1] as int) as movieId","cast(col[2] as float) as rating","cast(col[3] as long) as timestamp")training, test = ratings.randomSplit([0.8, 0.2])als = ALS()\.setMaxIter(5)\.setRegParam(0.01)\.setUserCol("userId")\.setItemCol("movieId")\.setRatingCol("rating")print als.explainParams()alsModel = als.fit(training)predictions = alsModel.transform(test) We can now output the top recommendations for each user or movie. The model’s recommendForAllUsers method returns a DataFrame of a userId, an array of recommendations, as well as a rating for each of those movies. recommendForAllItems returns a DataFrame of a movieId, as well as the top users for that movie: 我们现在可以为每个用户或电影输出最佳推荐。该模型的 suggestForAllUsers 方法返回 userId 的DataFrame，推荐的数组，以及每部电影的评分。 suggestForAllItems 返回 movieId 的 DataFrame，以及该影片的排名靠前用户： 12345// in ScalaalsModel.recommendForAllUsers(10).selectExpr("userId", "explode(recommendations)").show()alsModel.recommendForAllItems(10).selectExpr("movieId", "explode(recommendations)").show() 12345# in PythonalsModel.recommendForAllUsers(10)\.selectExpr("userId", "explode(recommendations)").show()alsModel.recommendForAllItems(10)\.selectExpr("movieId", "explode(recommendations)").show() Evaluators for Recommendation 推荐的评估器When covering the cold-start strategy, we can set up an automatic model evaluator when working with ALS. One thing that may not be immediately obvious is that this recommendation problem is really just a kind of regression problem. Since we’re predicting values (ratings) for given users, we want to optimize for reducing the total difference between our users’ ratings and the true values. We can do this using the same RegressionEvaluator that we saw in Chapter 27. You can place this in a pipeline to automate the training process. When doing this, you should also set the cold-start strategy to be drop instead of NaN and then switch it back to NaN when it comes time to actually make predictions in your production system: 在涵盖冷启动策略时，我们可以在使用 ALS 时设置自动模型评估程序。有一件事可能不是很明显，这个推荐问题实际上只是一种回归问题。由于我们正在预测给定用户的价值（评分），因此我们希望优化以减少用户评分与真实值之间的总差异。我们可以使用我们在第27章中看到的相同的 RegressionEvaluator 来完成此操作。您可以将其置于管道中以自动化训练过程。执行此操作时，您还应将冷启动策略设置为 drop 而不是 NaN，然后在生产系统中实际进行预测时将其切换回 NaN： 12345678// in Scalaimport org.apache.spark.ml.evaluation.RegressionEvaluatorval evaluator = new RegressionEvaluator().setMetricName("rmse").setLabelCol("rating").setPredictionCol("prediction")val rmse = evaluator.evaluate(predictions)println(s"Root-mean-square error = $rmse") 12345678# in Pythonfrom pyspark.ml.evaluation import RegressionEvaluatorevaluator = RegressionEvaluator()\.setMetricName("rmse")\.setLabelCol("rating")\.setPredictionCol("prediction")rmse = evaluator.evaluate(predictions)print("Root-mean-square error = %f" % rmse) Metrics 衡量指标Recommendation results can be measured using both the standard regression metrics and some recommendation-specific metrics. It should come as no surprise that there are more sophisticated ways of measuring recommendation success than simply evaluating based on regression. These metrics are particularly useful for evaluating your final model. 可以使用标准回归衡量指标和一些特定于推荐的指标来衡量推荐结果。毫无疑问，有更多复杂的方法来衡量推荐成功，而不仅仅是基于回归进行评估。这些指标对于评估最终模型特别有用。 Regression Metrics 回归的衡量指标We can recycle the regression metrics for recommendation. This is because we can simply see how close each prediction is to the actual rating for that user and item: 我们可以重复利用回归指标以进行推荐。这是因为我们可以简单地看到每个预测与该用户和商品的实际评分的接近程度： 12345// in Scalaimport org.apache.spark.mllib.evaluation.&#123;RankingMetrics, RegressionMetrics&#125;val regComparison = predictions.select("rating", "prediction").rdd.map(x =&gt; (x.getFloat(0).toDouble,x.getFloat(1).toDouble))val metrics = new RegressionMetrics(regComparison) 12345# in Pythonfrom pyspark.mllib.evaluation import RegressionMetricsregComparison = predictions.select("rating", "prediction")\.rdd.map(lambda x: (x(0), x(1)))metrics = RegressionMetrics(regComparison) Ranking Metrics 评分的衡量指标More interestingly, we also have another tool: ranking metrics. A RankingMetric allows us to compare our recommendations with an actual set of ratings (or preferences) expressed by a given user. RankingMetric does not focus on the value of the rank but rather whether or not our algorithm recommends an already ranked item again to a user. This does require some data preparation on our part. You may want to refer to Part II for a refresher on some of the methods. First, we need to collect a set of highly ranked movies for a given user. In our case, we’re going to use a rather low threshold: movies ranked above 2.5. Tuning this value will largely be a business decision : 更有趣的是，我们还有另一个工具：排名指标。 RankingMetric 允许我们将我们的推荐与给定用户表达的实际评分（或偏好）进行比较。 RankingMetric 不关注评分的值，而是关注我们的算法是否再次向用户推荐已经评分的商品。 这确实需要我们做一些数据准备。 您可能需要参考第二部分来了解一些方法。首先，我们需要为给定用户收集一组评分很高的电影。在我们的例子中，我们将使用相当低的门槛：电影评分高于2.5。调整此值很大程度上取决于业务决策： 1234567// in Scalaimport org.apache.spark.mllib.evaluation.&#123;RankingMetrics, RegressionMetrics&#125;import org.apache.spark.sql.functions.&#123;col, expr&#125;val perUserActual = predictions.where("rating &gt; 2.5").groupBy("userId").agg(expr("collect_set(movieId) as movies")) 1234567# in Pythonfrom pyspark.mllib.evaluation import RankingMetrics, RegressionMetricsfrom pyspark.sql.functions import col, exprperUserActual = predictions\.where("rating &gt; 2.5")\.groupBy("userId")\.agg(expr("collect_set(movieId) as movies")) At this point, we have a collection of users, along with a truth set of previously ranked movies for each user. Now we will get our top 10 recommendations from our algorithm on a per-user basis. We will then see if the top 10 recommendations show up in our truth set. If we have a well-trained model, it will correctly recommend the movies a user already liked. If it doesn’t, it may not have learned enough about each particular user to successfully reflect their preferences: 此时，我们有一组用户，以及对每个用户进行过评分的电影的真值集合。现在，我们将根据每个用户的算法获得我们的十大推荐。然后我们将看看前十条推荐是否出现在我们的真实集中。如果我们有一个训练有素的模型，它将正确推荐用户已经喜欢过的电影。如果没有，则可能没有充分了解每个特定用户去成功反映他们的偏好： 12345// in Scalaval perUserPredictions = predictions.orderBy(col("userId"), col("prediction").desc).groupBy("userId").agg(expr("collect_list(movieId) as movies")) 12345# in PythonperUserPredictions = predictions\.orderBy(col("userId"), expr("prediction DESC"))\.groupBy("userId")\.agg(expr("collect_list(movieId) as movies")) Now we have two DataFrames, one of predictions and another the top-ranked items for a particular user. We can pass them into the RankingMetrics object. This object accepts an RDD of these combinations, as you can see in the following join and RDD conversion: 现在我们有两个 DataFrame，一个是预测，另一个是特定用户评分靠前的商品。我们可以将它们传递给RankingMetrics 对象。此对象接受这些组合的 RDD，如以下连接和 RDD 转换中所示： 1234567// in Scalaval perUserActualvPred = perUserActual.join(perUserPredictions, Seq("userId")).map(row =&gt; ( row(1).asInstanceOf[Seq[Integer]].toArray, row(2).asInstanceOf[Seq[Integer]].toArray.take(15)))val ranks = new RankingMetrics(perUserActualvPred.rdd) 1234# in PythonperUserActualvPred = perUserActual.join(perUserPredictions, ["userId"]).rdd\.map(lambda row: (row[1], row2))ranks = RankingMetrics(perUserActualvPred) Now we can see the metrics from that ranking. For instance, we can see how precise our algorithm is with the mean average precision. We can also get the precision at certain ranking points, for instance, to see where the majority of the positive recommendations fall : 现在我们可以看到该评分的指标。例如，我们可以看到我们的算法与平均精度的精确度。例如，我们还可以获得某些评分的精确度，以查看大多数积极建议的落点： 123// in Scalaranks.meanAveragePrecisionranks.precisionAt(5) 123# in Pythonranks.meanAveragePrecisionranks.precisionAt(5) Frequent Pattern Mining 频繁模式挖掘In addition to ALS, another tool that MLlib provides for creating recommendations is frequent pattern mining. Frequent pattern mining, sometimes referred to as market basket analysis, looks at raw data and finds association rules. For instance, given a large number of transactions it might identify that users who buy hot dogs almost always purchase hot dog buns. This technique can be applied in the recommendation context, especially when people are filling shopping carts (either on or offline). Spark implements the FP-growth algorithm for frequent pattern mining. See the Spark documentation and ESL 14.2 for more information about this algorithm. 除了 ALS 之外，MLlib 为创建推荐提供的另一个工具是频繁的模式挖掘。 频繁模式挖掘（有时称为市场购物篮分析）会查看原始数据并查找关联规则。 例如，鉴于大量交易，它可能确定购买热狗的用户几乎总是购买热狗面包。 此技术可应用于在推荐背景，尤其是当人们填充购物车（在线或离线）时。 Spark实现了用于频繁模式挖掘的 FP 增长算法。 有关此算法的更多信息，请参阅Spark文档和 ESL 14.2。 Conclusion 结论In this chapter, we discussed one of Spark’s most popular machine learning algorithms in practice—alternating least squares for recommendation. We saw how we can train, tune, and evaluate this model. In the next chapter, we’ll move to unsupervised learning and discuss clustering. 在本章中，我们讨论了Spark在实践中最受欢迎的机器学习算法之一 ——用于推荐的交替最小二乘法。 我们看到了如何训练，调整和评估这个模型。 在下一章中，我们将转向无监督学习并讨论聚类。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 25 Preprocessing and Feature Engineering]]></title>
    <url>%2F2019%2F08%2F26%2FChapter25_PreprocessingAndFeature(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 25 Preprocessing and Feature Engineering 预处理和特征工程Any data scientist worth her salt knows that one of the biggest challenges (and time sinks) in advanced analytics is preprocessing. It’s not that it’s particularly complicated programming, but rather that it requires deep knowledge of the data you are working with and an understanding of what your model needs in order to successfully leverage this data. This chapter covers the details of how you can use Spark to perform preprocessing and feature engineering. We’ll walk through the core requirements you’ll need to meet in order to train an MLlib model in terms of how your data is structured. We will then discuss the different tools Spark makes available for performing this kind of work. 值得任何数据科学家都知道的是高级分析中最大的挑战之一（和时间汇集）是预处理。这不是特别复杂的编程，而是需要深入了解您正在使用的数据，并了解您的模型需要什么才能成功利用这些数据。本章介绍了如何使用 Spark 执行预处理和功能工程的详细信息。我们将逐步介绍您需要满足的核心要求，以便根据数据的结构来训练 MLlib 模型。然后，我们将讨论 Spark 可用于执行此类工作的不同工具。 Formatting Models According to Your Use Case 根据您的使用案例格式化模型To preprocess data for Spark’s different advanced analytics tools, you must consider your end objective. The following list walks through the requirements for input data structure for each advanced analytics task in MLlib: 要为 Spark 的不同高级分析工具预处理数据，您必须考虑最终目标。以下列表详细研究了 MLlib 中每个高级分析任务的输入数据结构要求： In the case of most classification and regression algorithms, you want to get your data into a column of type Double to represent the label and a column of type Vector (either dense or sparse) to represent the features. 对于大多数分类和回归算法，您希望将数据放入 Double 类型的列中以表示标签，并使用Vector类型（密集或稀疏）来表示特征。 In the case of recommendation, you want to get your data into a column of users, a column of items (say movies or books), and a column of ratings. 在推荐的案例中，你想将数据放入到一列用户，一列项目（比如电影或书籍）和一列评分中去。 In the case of unsupervised learning, a column of type Vector (either dense or sparse) is needed to represent the features. 在无监督学习的情况下，需要一个Vector类型（密集或稀疏）来表示特征。 In the case of graph analytics, you will want a DataFrame of vertices and a DataFrame of edges. 在图形分析的情况下，您将需要顶点的 DataFrame 和边的 DataFrame 。 The best way to get your data in these formats is through transformers. Transformers are functions thataccept a DataFrame as an argument and return a new DataFrame as a response. This chapter willfocus on what transformers are relevant for particular use cases rather than attempting to enumerateevery possible transformer. 以这些格式获取数据的最佳方式是通过转换器。转换器是接受DataFrame作为参数并返回一个新的DataFrame的函数。本章将重点介绍转换器与特定用户案例相关的内容，而不是试图列举所有可能的转换器。 NOTE 注意Spark provides a number of transformers as part of the org.apache.spark.ml.feature package. The corresponding package in Python is pyspark.ml.feature. New transformers are constantly popping up in Spark MLlib and therefore it is impossible to include a definitive list in this book. The most up-to-date information can be found on the Spark documentation site. Spark 提供了许多转换器作为 org.apache.spark.ml.feature 包的一部分。 Python 中相应的包是pyspark.ml.feature 。新的转换器不断出现在 Spark MLlib 中，因此不可能在本书中包含明确的列表。可以在Spark文档站点上找到最新信息。 Before we proceed, we’re going to read in several different sample datasets, each of which has different properties we will manipulate in this chapter : 在我们继续之前，我们将阅读几个不同的样本数据集，每个样本数据集都有不同的属性，我们将在本章中操作： 1234567891011// in Scalaval sales = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/data/retail-data/by-day/*.csv").coalesce(5).where("Description IS NOT NULL")val fakeIntDF = spark.read.parquet("/data/simple-ml-integers")var simpleDF = spark.read.json("/data/simple-ml")val scaleDF = spark.read.parquet("/data/simple-ml-scaling") 12345678910# in Pythonsales = spark.read.format("csv")\.option("header", "true")\.option("inferSchema", "true")\.load("/data/retail-data/by-day/*.csv")\.coalesce(5)\.where("Description IS NOT NULL")fakeIntDF = spark.read.parquet("/data/simple-ml-integers")simpleDF = spark.read.json("/data/simple-ml")scaleDF = spark.read.parquet("/data/simple-ml-scaling") In addition to this realistic sales data, we’re going to use several simple synthetic datasets as well. FakeIntDF, simpleDF, and scaleDF all have very few rows. This will give you the ability to focus on the exact data manipulation we are performing instead of the various inconsistencies of an particular dataset. Because we’re going to be accessing the sales data a number of times, we’re going to cache it so we can read it efficiently from memory as opposed to reading it from disk every time we need it. Let’s also check out the first several rows of data in order to better understand what’s in the dataset: 除了这些现实生活中的销售数据，我们还将使用几个简单的合成数据集。 FakeIntDF，simpleDF 和 scaleDF 都只有很少的行。 这将使您能够专注于我们正在执行的确切数据操作，而不是特定数据集的各种不一致性。 因为我们将多次访问销售数据，所以我们将对其进行缓存，以便我们可以从内存中有效地读取它，而不是每次需要时从磁盘读取它。 我们还要查看前几行数据，以便更好地了解数据集中的内容： 12sales.cache()sales.show() 1234567+---------+---------+--------------------+--------+-------------------+---------|InvoiceNo|StockCode| Description |Quantity| InvoiceDate |UnitPr...+---------+---------+--------------------+--------+-------------------+---------| 580538 | 23084 | RABBIT NIGHT LIGHT | 48 |2011-12-05 08:38:00| 1......| 580539 | 22375 |AIRLINE BAG VINTA...| 4 |2011-12-05 08:39:00| 4...+---------+---------+--------------------+--------+-------------------+--------- NOTE 注意It is important to note that we filtered out null values here. MLlib does not always play nicely with null values at this point in time. This is a frequent cause for problems and errors and a great first step when you are debugging. Improvements are also made with every Spark release to improve algorithm handling of null values. 请务必注意，我们在此处过滤掉了空值。 在这个时间点，MLlib并不总能很好地处理空值。 这是导致问题和错误的常见原因，也是调试时的第一步。 每个Spark版本都进行了改进，以改进空值的算法处理。 Transformers 转换器We discussed transformers in the previous chapter, but it’s worth reviewing them again here. Transformers are functions that convert raw data in some way. This might be to create a new interaction variable (from two other variables), to normalize a column, or to simply turn it into a Double to be input into a model. Transformers are primarily used in preprocessing or feature generation. 我们在前一章讨论过转换器，但值得再次回顾一下。转换器是以某种方式转换原始数据的函数。这可能是创建一个新的交互变量（来自其他两个变量），标准化一个列，或者简单地将其转换为 Double 以输入到模型中。转换器主要用于预处理或特征生成。 Spark’s transformer only includes a transform method. This is because it will not change based on the input data. Figure 25-1 is a simple illustration. On the left is an input DataFrame with the column to be manipulated. On the right is the input DataFrame with a new column representing the output transformation. Spark 的转换器只包含一种转换方法。这是因为它不会根据输入数据而改变。图25-1是一个简单的图示。左侧是输入 DataFrame，其中包含要操作的列。右侧是输入DataFrame，其中一个新列表示输出转换。 The Tokenizer is an example of a transformer. It tokenizes a string, splitting on a given character, and has nothing to learn from our data; it simply applies a function. We’ll discuss the tokenizer in more depth later in this chapter, but here’s a small code snippet showing how a tokenizer is built to accept the input column, how it transforms the data, and then the output from that transformation: Tokenizer是转换器的一个例子。它符号化一个字符串，分裂给定的文本，并且没有任何东西可以从我们的数据中学习；它只是应用一个功能。我们将在本章后面更深入地讨论符号生成器，但这里有一个小代码片段，显示如何构建标记生成器以接受输入列，如何转换数据，然后来自该转换的输出： 1234// in Scalaimport org.apache.spark.ml.feature.Tokenizerval tkn = new Tokenizer().setInputCol("Description")tkn.transform(sales.select("Description")).show(false) 123456789+-----------------------------------+------------------------------------------+| Description | tok_7de4dfc81ab7__output |+-----------------------------------+------------------------------------------+| RABBIT NIGHT LIGHT | [rabbit, night, light] || DOUGHNUT LIP GLOSS | [doughnut, lip, gloss] |...|AIRLINE BAG VINTAGE WORLD CHAMPION | [airline, bag, vintage, world, champion] ||AIRLINE BAG VINTAGE JET SET BROWN | [airline, bag, vintage, jet, set, brown] |+-----------------------------------+------------------------------------------+ Estimators for Preprocessing 预处理的估记器Another tool for preprocessing are estimators. An estimator is necessary when a transformation you would like to perform must be initialized with data or information about the input column (often derived by doing a pass over the input column itself). For example, if you wanted to scale the values in our column to have mean zero and unit variance, you would need to perform a pass over the entire data in order to calculate the values you would use to normalize the data to mean zero and unit variance. In effect, an estimator can be a transformer configured according to your particular input data. In simplest terms, you can either blindly apply a transformation (a “regular” transformer type) or perform a transformation based on your data (an estimator type). Figure 25-2 is a simple illustration of an estimator fitting to a particular input dataset, generating a transformer that is then applied to the input dataset to append a new column (of the transformed data). 预处理的另一个工具是估记器。当您想要执行的转换必须使用有关输入列的数据或信息进行初始化时（通常通过对输入列本身进行传递），必须使用估计器。例如，如果要将列中的值缩放为具有均值零和单位方差，则需要对整个数据执行传递，以便计算用于将数据标准化为零和单位方差的值。实际上，估计器可以是根据您的特定输入数据配置的转换器。简单来说，您可以盲目地应用转换（“常规”转换器类型）或根据您的数据执行转换（估计器类型）。图25-2是拟合特定输入数据集的估计器的简单图示，生成转换器，然后将其应用于输入数据集以附加（已经转换的数据的）新列。 An example of this type of estimator is the StandardScaler, which scales your input column according to the range of values in that column to have a zero mean and a variance of 1 in each dimension. For that reason it must first perform a pass over the data to create the transformer. Here’s a sample code snippet showing the entire process, as well as the output: 此类估计器的一个示例是 StandardScaler，它根据该列中的值范围缩放输入列，使其在每个维度中具有零均值和方差1。因此，它必须首先对数据执行传递以创建变换器。这是一个示例代码片段，显示整个过程以及输出： 1234// in Scalaimport org.apache.spark.ml.feature.StandardScalerval ss = new StandardScaler().setInputCol("features")ss.fit(scaleDF).transform(scaleDF).show(false) 1234567+---+--------------+------------------------------------------------------------+|id | features | stdScal_d66fbeac10ea__output |+---+--------------+------------------------------------------------------------+| 0 |[1.0,0.1,-1.0]|[1.1952286093343936,0.02337622911060922,-0.5976143046671968]|...| 1 |[3.0,10.1,3.0]| [3.5856858280031805,2.3609991401715313,1.7928429140015902] |+---+--------------+------------------------------------------------------------+ We will use both estimators and transformers throughout and cover more about these particular estimators (and add examples in Python) later on in this chapter. 我们将在整个过程中使用估计器和转换器，并在本章后面详细介绍这些特定的估计器（并在Python中添加示例）。 Transformer Properties 转换器属性All transformers require you to specify, at a minimum, the inputCol and the outputCol, which represent the column name of the input and output, respectively. You set these with setInputCol and setOutputCol. There are some defaults (you can find these in the documentation), but it is a best practice to manually specify them yourself for clarity. In addition to input and output columns, all transformers have different parameters that you can tune (whenever we mention a parameter in this chapter you must set it with a set() method). In Python, we also have another method to set these values with keyword arguments to the object’s constructor. We exclude these from the examples in the next chapter for consistency. Estimators require you to fit the transformer to your particular dataset and then call transform on the resulting object. 所有转换器都要求您至少指定 inputCol 和 outputCol，它们分别表示输入和输出的列名。您可以使用setInputCol 和 setOutputCol 设置它们。有一些默认值（您可以在文档中找到这些默认值），但为了清晰起见，最好自己手动指定它们。除了输入和输出列之外，所有转换器都有不同的参数可以调整（每当我们在本章中提到参数时，必须使用 set() 方法设置它）。在 Python 中，我们还有另一种方法可以使用关键字参数为对象的构造函数设置这些值。为了保持一致性，我们将在下一章的示例中排除这些。估计器要求您将转换器拟合特定数据集，然后对结果对象调用transform。 NOTE 注意Spark MLlib stores metadata about the columns it uses in each DataFrame as an attribute on the column itself. This allows it to properly store (and annotate) that a column of Doubles may actually represent a series of categorical variables instead of continuous values. However, metadata won’t show up when you print the schema or the DataFrame. Spark MLlib 将有关其在每个DataFrame中使用的列的元数据存储为列本身的属性。 这允许它正确地存储（和注释）双列列实际上可以表示一系列分类变量而不是连续值。 但是，打印模式（schema）或 DataFrame 时，元数据不会显示。 High-Level Transformers 高层（接口的）转换器High-level transformers, such as the RFormula we saw in the previous chapter, allow you to concisely specify a number of transformations in one. These operate at a “high level”, and allow yow to avoid doing data manipulations or transformations one by one. In general, you should try to use the highest level transformers you can, in order to minimize the risk of error and help you focus on the business problem instead of the smaller details of implementation. While this is not always possible, it’s a good objective. 高级变换器，例如我们在前一章中看到的RFormula，允许您在一个变换器中简明地指定多个变换。它们在“高级别”运行，并允许您避免逐个进行数据操作或转换。通常，您应该尝试使用最高级别的变换器，以便最大限度地降低出错风险，并帮助您专注于业务问题而不是较小的实现细节。虽然这并非总是可行，但这是一个很好的目标。 RFormulaThe RFormula is the easiest transfomer to use when you have “conventionally” formatted data. Spark borrows this transformer from the R language to make it simple to declaratively specify a set of transformations for your data. With this transformer, values can be either numerical or categorical and you do not need to extract values from strings or manipulate them in any way. The RFormula will automatically handle categorical inputs (specified as strings) by performing something called one-hot encoding. In brief, one-hot encoding converts a set of values into a set of binary columns specifying whether or not the data point has each particular value (we’ll discuss one-hot encoding in more depth later in the chapter). With the RFormula, numeric columns will be cast to Double but will not be one-hot encoded. If the label column is of type String, it will be first transformed to Double with StringIndexer. 当您拥有“常规”格式的数据时，RFormula 是最容易使用的变换器。 Spark 从 R 语言借用了这个转换器，使得声明性地为数据指定一组转换变得简单。使用此转换器，值可以是数值类型或类别类型，您不需要从字符串中提取值或以任何方式操纵它们。 RFormula 将通过执行称为独热编码的操作自动处理类别类型输入（指定为字符串）。简而言之，独热编码将一组值转换为一组二进制列，指定数据点是否具有每个特定值（我们将在本章后面更深入地讨论独热编码）。使用 RFormula，数字列将转换为 Double，但不会进行独热编码。如果 label 列的类型为 String，则它将首先使用 StringIndexer 转换为 Double。 WARNING 警告Automatic casting of numeric columns to Double without one-hot encoding has some important implications. If you have numerically valued categorical variables, they will only be cast to Double, implicitly specifying an order. It is important to ensure the input types correspond to the expected conversion. If you have categorical variables that really have no order relation, they should be cast to String. You can also manually index columns (see “Working with Categorical Features”). 将数字列自动转换为 Double 而不使用独热编码具有一些重要作用。如果您有数值分类变量，它们将仅转换为Double，隐式指定顺序。确保输入类型与预期转换相对应非常重要。如果您的分类变量确实没有顺序关系，则应将它们强制转换为 String。您也可以手动索引列（请参阅本文的“使用分类功能”小节）。 The RFormula allows you to specify your transformations in declarative syntax. It is simple to use once you understand the syntax. Currently, RFormula supports a limited subset of the R operators that in practice work quite well for simple transformations. The basic operators are : RFormula 允许您在声明性语法中指定转换。一旦理解了语法，它就很容易使用。目前，RFormula 支持 R 运算符的有限子集，这些运算符在实际上对于简单变换非常有效。基本运算符是： 运算符operator 含义 ~ Separate target and terms分隔目标和项 + Concat terms; “+ 0” means removing the intercept (this means that the y-intercept of the line that we will fit will be 0)拼接项；“+0” 意味着移除截距（这意思是我们将拟合的直线的 y轴截距将会是0） - Remove a term; “- 1” means removing the intercept (this means that the y-intercept of the line that we will fit will be 0—yes, this does the same thing as “+ 0”移除一个项；“-1” 与 “-0” 做同样的事情 : Interaction (multiplication for numeric values, or binarized categorical values)交互（数值乘法或二进制分类值） . All columns except the target/dependent variable除目标/因变量之外的所有列 RFormula also uses default columns of label and features to label, you guessed it, the label and the set of features that it outputs (for supervised machine learning). The models covered later on in this chapter by default require those column names, making it easy to pass the resulting transformed DataFrame into a model for training. If this doesn’t make sense yet, don’t worry—it’ll become clear once we actually start using models in later chapters. RFormula还使用标签和特征的默认列来标记，您猜对了，标签和它输出的特征集（用于监督机器学习）。 默认情况下，本章后面介绍的模型需要这些列名称，以便将生成的转换后的 DataFrame 传递到训练模型中。 如果这还没有意义，请不要担心——一旦我们真正开始在后面的章节中使用模型，它就会变得清晰。 Let’s use RFormula in an example. In this case, we want to use all available variables (the .) and then specify an interaction between value1 and color and value2 and color as additional features to generate: 我们在一个例子中使用 RFormula。 在这种案例下，我们希望使用所有可用变量（.），然后指定 value1 和 color 以及 value2 和 color 之间的交互作为生成的附加功能： 1234// in Scalaimport org.apache.spark.ml.feature.RFormulaval supervised = new RFormula().setFormula("lab ~ . + color:value1 + color:value2")supervised.fit(simpleDF).transform(simpleDF).show() 1234# in Pythonfrom pyspark.ml.feature import RFormulasupervised = RFormula(formula="lab ~ . + color:value1 + color:value2")supervised.fit(simpleDF).transform(simpleDF).show() 123456789+-----+----+------+------------------+--------------------+-----+|color| lab|value1| value2 | features |label|+-----+----+------+------------------+--------------------+-----+|green|good| 1 |14.386294994851129|(10,[1,2,3,5,8],[...| 1.0 || blue| bad| 8 |14.386294994851129|(10,[2,3,6,9],[8....| 0.0 |...| red | bad| 1 | 38.97187133755819|(10,[0,2,3,4,7],[...| 0.0 || red | bad| 2 |14.386294994851129|(10,[0,2,3,4,7],[...| 0.0 |+-----+----+------+------------------+--------------------+-----+ SQL TransformersA SQLTransformer allows you to leverage Spark’s vast library of SQL-related manipulations just as you would a MLlib transformation. Any SELECT statement you can use in SQL is a valid transformation. The only thing you need to change is that instead of using the table name, you should just use the keyword THIS. You might want to use SQLTransformer if you want to formally codify some DataFrame manipulation as a preprocessing step, or try different SQL expressions for features during hyperparameter tuning. Also note that the output of this transformation will be appended as a column to the output DataFrame. SQLTransformer 允许您像使用 MLlib 转换一样利用 Spark 庞大的 SQL 相关操作库。 在 SQL 中可以使用的任何 SELECT 语句都是有效的转换。 您需要更改的唯一事情是，您应该只使用关键字 THIS，而不是使用表名。 如果要将某些 DataFrame 操作正式编码为预处理步骤，或者在超参数调整期间尝试使用不同的 SQL 表达式，则可能需要使用 SQLTransformer。 另请注意，此转换的输出将作为列附加到输出 DataFrame。 You might want to use an SQLTransformer in order to represent all of your manipulations on the very rawest form of your data so you can version different variations of manipulations as transformers. This gives you the benefit of building and testing varying pipelines, all by simply swapping out transformers. The following is a basic example of using SQLTransformer: 您可能希望使用 SQLTransformer 来表示对最新形式的数据的所有操作，以便您可以将不同的操作变体版本作为变换器。 通过简单地更换转换器，这为您提供了构建和测试不同管道的利好。 以下是使用 SQLTransformer 的基本示例： 123456789// in Scalaimport org.apache.spark.ml.feature.SQLTransformerval basicTransformation = new SQLTransformer().setStatement("""SELECT sum(Quantity), count(*), CustomerIDFROM __THIS__GROUP BY CustomerID""")basicTransformation.transform(sales).show() 123456789# in Pythonfrom pyspark.ml.feature import SQLTransformerbasicTransformation = SQLTransformer()\.setStatement("""SELECT sum(Quantity), count(*), CustomerIDFROM __THIS__GROUP BY CustomerID""")basicTransformation.transform(sales).show() Here’s a sample of the output: 这是样本的输出： 1234567-------------+--------+----------+|sum(Quantity)|count(1)|CustomerID|+-------------+--------+----------+| 119 | 62 | 14452.0 |...| 138 | 18 | 15776.0 |+-------------+--------+----------+ For extensive samples of these transformations, refer back to Part II. 有关这些转换的大量示例，请参阅第II部分。 VectorAssemblerThe VectorAssembler is a tool you’ll use in nearly every single pipeline you generate. It helps concatenate all your features into one big vector you can then pass into an estimator. It’s used typically in the last step of a machine learning pipeline and takes as input a number of columns of Boolean, Double, or Vector. This is particularly helpful if you’re going to perform a number of manipulations using a variety of transformers and need to gather all of those results together. VectorAssembler是您将在几乎每个生成的管道中使用的工具。 它有助于将所有特征连接成一个大向量，然后传递给估计器。 它通常用于机器学习管道的最后一步，并将多列Boolean，Double或Vector作为输入。 如果您要使用各种转换器执行大量操作并且需要将所有这些结果收集在一起，这将特别有用。 The output from the following code snippet will make it clear how this works: 以下代码段的输出将清楚说明其工作原理： 1234// in Scalaimport org.apache.spark.ml.feature.VectorAssemblerval va = new VectorAssembler().setInputCols(Array("int1", "int2", "int3"))va.transform(fakeIntDF).show() 123# in Pythonfrom pyspark.ml.feature import VectorAssemblerva = VectorAssembler().setInputCols(["int1", "int2", "int3"])va.transform(fakeIntDF).show() 1234567+----+----+----+--------------------------------------------+|int1|int2|int3|VectorAssembler_403ab93eacd5585ddd2d__output|+----+----+----+--------------------------------------------+| 1 | 2 | 3 | [1.0,2.0,3.0] || 4 | 5 | 6 | [4.0,5.0,6.0] || 7 | 8 | 9 | [7.0,8.0,9.0] |+----+----+----+--------------------------------------------+ Working with Continuous Features 使用连续特征Continuous features are just values on the number line, from positive infinity to negative infinity. There are two common transformers for continuous features. First, you can convert continuous features into categorical features via a process called bucketing, or you can scale and normalize your features according to several different requirements. These transformers will only work on Double types, so make sure you’ve turned any other numerical values to Double: 连续特征只是数字线上的值，从正无穷大到负无穷大。 连续功能有两种常见的变压器。 首先，您可以通过称为bucketing 的过程将连续要素转换为分类要素，或者您可以根据多种不同要求对要素进行缩放和规范化。 这些变换器只适用于Double类型，因此请确保您已将任何其他数值转换为Double： 12// in Scalaval contDF = spark.range(20).selectExpr("cast(id as double)") 12# in PythoncontDF = spark.range(20).selectExpr("cast(id as double)") Bucketing 分桶The most straightforward approach to bucketing or binning is using the Bucketizer. This will split a given continuous feature into the buckets of your designation. You specify how buckets should be created via an array or list of Double values. This is useful because you may want to simplify the features in your dataset or simplify their representations for interpretation later on. For example, imagine you have a column that represents a person’s weight and you would like to predict some value based on this information. In some cases, it might be simpler to create three buckets of “overweight,” “average,” and “underweight.” 最直接的分组或分级方法是使用Bucketizer。这会将给定的连续特征分成您指定的桶。您可以指定如何通过数组或Double值列表创建存储区。这很有用，因为您可能希望简化数据集中的功能，或者稍后简化其表示以进行解释。例如，假设您有一个代表一个人体重的列，并且您希望根据此信息预测某些值。在某些情况下，创建三个“超重”，“平均”和“体重不足”的桶可能更简单。 To specify the bucket, set its borders. For example, setting splits to 5.0, 10.0, 250.0 on our contDF will actually fail because we don’t cover all possible input ranges. When specifying your bucket points, the values you pass into splits must satisfy three requirements: 要指定桶，请设置其边界。例如，在我们的 contDF 上设置拆分为5.0,10.0,250.0实际上会失败，因为我们没有涵盖所有可能的输入范围。指定桶点时，传递给拆分的值必须满足三个要求： The minimum value in your splits array must be less than the minimum value in your DataFrame. splits 数组中的最小值必须小于DataFrame中的最小值。 The maximum value in your splits array must be greater than the maximum value in your DataFrame. splits 数组中的最大值必须大于DataFrame中的最大值。 You need to specify at a minimum three values in the splits array, which creates two buckets. 您需要在 splits 数组中至少指定三个值，这会创建两个桶。 WARNING 警告The Bucketizer can be confusing because we specify bucket borders via the splits method, but these are not actually splits. Bucketizer 可能会让人感到困惑，因为我们通过 splits 方法指定了bucket边界，但这些实际上并不是分裂。 To cover all possible ranges, scala.Double.NegativeInfinity might be another split option, with scala.Double.PositiveInfinity to cover all possible ranges outside of the inner splits. In Python we specify this in the following way: float(&quot;inf&quot;), float(&quot;-inf&quot;). 为了覆盖所有可能的范围，scala.Double.NegativeInfinity 可能是另一个分裂选项，scala.Double.PositiveInfinity 可以覆盖内部分裂之外的所有可能范围。在 Python 中，我们通过以下方式指定它：float(“inf”)，float(“-inf”)。 In order to handle nullor NaN values, we must specify the handleInvalid parameter as a certain value. We can either keep those values (keep), error or null , or skip those rows. Here’s an example of using bucketing: 为了处理 null 或 NaN 值，我们必须将 handleInvalid 参数指定为特定值。我们可以保留这些值（keep），error 或 null，或跳过这些行。以下是使用 bucketing 的示例： 12345// in Scalaimport org.apache.spark.ml.feature.Bucketizerval bucketBorders = Array(-1.0, 5.0, 10.0, 250.0, 600.0)val bucketer = new Bucketizer().setSplits(bucketBorders).setInputCol("id")bucketer.transform(contDF).show() 12345# in Pythonfrom pyspark.ml.feature import BucketizerbucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]bucketer = Bucketizer().setSplits(bucketBorders).setInputCol("id")bucketer.transform(contDF).show() 123456789+----+---------------------------------------+| id |Bucketizer_4cb1be19f4179cc2545d__output|+----+---------------------------------------+| 0.0| 0.0 |...|10.0| 2.0 ||11.0| 2.0 |...+----+---------------------------------------+ In addition to splitting based on hardcoded values, another option is to split based on percentiles in our data. This is done with QuantileDiscretizer, which will bucket the values into user-specified buckets with the splits being determined by approximate quantiles values. For instance, the 90th quantile is the point in your data at which 90% of the data is below that value. You can control how finely the buckets should be split by setting the relative error for the approximate quantiles calculation using setRelativeError. Spark does this is by allowing you to specify the number of buckets you would like out of the data and it will split up your data accordingly. The following is an example: 除了基于硬编码值进行分裂外，另一种选择是根据数据中的百分位数进行分裂。 这是通过 QuantileDiscretizer 完成的，它将值存储到用户指定的桶中，分裂由近似的分位数值确定。 例如，第90个分位数是数据中90％的数据低于该值的点。 您可以通过使用 setRelativeError 设置近似分位数计算的相对误差来控制分割桶的精确程度。 Spark 这样做是为了让你能够指定数据中你想要的桶数，它会相应地分割你的数据。 以下是一个例子： 12345// in Scalaimport org.apache.spark.ml.feature.QuantileDiscretizerval bucketer = new QuantileDiscretizer().setNumBuckets(5).setInputCol("id")val fittedBucketer = bucketer.fit(contDF)fittedBucketer.transform(contDF).show() 12345# in Pythonfrom pyspark.ml.feature import QuantileDiscretizerbucketer = QuantileDiscretizer().setNumBuckets(5).setInputCol("id")fittedBucketer = bucketer.fit(contDF)fittedBucketer.transform(contDF).show() 123456789101112+----+----------------------------------------+| id |quantileDiscretizer_cd87d1a1fb8e__output|+----+----------------------------------------+| 0.0| 0.0 |...| 6.0| 1.0 || 7.0| 2.0 |...|14.0| 3.0 ||15.0| 4.0 |...+----+----------------------------------------+ Advanced bucketing techniquesThe techniques descriubed here are the most common ways of bucketing data, but there are a number of other ways that exist in Spark today. All of these processes are the same from a data flow perspective: start with continuous data and place them in buckets so that they become categorical. Differences arise depending on the algorithm used to compute these buckets. The simple examples we just looked at are easy to intepret and work with, but more advanced techniques such as locality sensitivity hashing (LSH) are also available in MLlib. 这里描述的技术是最常见的数据分桶方式，但今天Spark中还有许多其他方法。从数据流的角度来看，所有这些过程都是相同的：从连续数据开始并将它们放在桶中，以便它们有分类。根据用于计算这些桶的算法而产生差异。我们刚看到的简单示例很容易解释和使用，但MLlib中也提供了更高级的技术，如局部敏感哈希（LSH）。 Scaling and NormalizationWe saw how we can use bucketing to create groups out of continuous variables. Another common task is to scale and normalize continuous data. While not always necessary, doing so is usually a best practice. You might want to do this when your data contains a number of columns based on different scales. For instance, say we have a DataFrame with two columns: weight (in ounces) and height (in feet). If you don’t scale or normalize, the algorithm will be less sensitive to variations in height because height values in feet are much lower than weight values in ounces. That’s an example where you should scale your data. 我们看到了如何使用分桶从连续变量中创建组。另一个常见任务是缩放和标准化连续数据。虽然并非总是必要，但这样做通常是最佳做法。当您的数据包含基于不同比例的多个列时，您可能希望这样做。例如，假设我们有一个包含两列的DataFrame：weight（以盎司为单位）和height（以英尺为单位）。如果不进行缩放或标准化，则算法对高度变化不太敏感，因为以英尺为单位的高度值远低于以盎司为单位的重量值。这是您应该缩放数据的示例。 An example of normalization might involve transforming the data so that each point’s value is a representation of its distance from the mean of that column. Using the same example from before, we might want to know how far a given individual’s height is from the mean height. Many algorithms assume that their input data is normalized. 标准化的一个示例可能涉及转换数据，以便每个点的值表示其与该列平均值的距离。使用之前相同的示例，我们可能想知道给定个体的高度与平均高度的距离。许多算法假设他们的输入数据被标准化。 As you might imagine, there are a multitude of algorithms we can apply to our data to scale or normalize it. Enumerating them all is unnecessary here because they are covered in many other texts and machine learning libraries. If you’re unfamiliar with the concept in detail, check out any of the books referenced in the previous chapter. Just keep in mind the fundamental goal—we want our data on the same scale so that values can easily be compared to one another in a sensible way. In MLlib, this is always done on columns of type Vector. MLlib will look across all the rows in a given column (of type Vector) and then treat every dimension in those vectors as its own particular column. It will then apply the scaling or normalization function on each dimension separately. A simple example might be the following vectors in a column: 正如您可能想象的那样，我们可以将大量算法应用于我们的数据来扩展或规范化它。在这里列举所有这些都是不必要的，因为它们包含在许多其他文本和机器学习库中。如果您不熟悉这个概念，请查看上一章中引用的任何书籍。请记住基本目标——我们希望我们的数据具有相同的比例，以便可以以合理的方式轻松地将值进行比较。在MLlib中，这总是在Vector类型的列上完成。 MLlib 将查看给定列（Vector类型）中的所有行，然后将这些向量中的每个维度视为其自己的特定列。然后，它将分别在每个维度上应用缩放或归一化功能。一个简单的例子可能是列中的以下向量： 1,23,4 When we apply our scaling (but not normalization) function, the “3” and the “1” will be adjusted according to those two values while the “2” and the “4” will be adjusted according to one another. This is commonly referred to as component-wise comparisons. 当我们应用我们的缩放（但不是归一化）功能时，将根据这两个值调整“3”和“1”，而“2”和“4”将根据彼此进行调整。这通常被称为分量比较。 StandardScalerThe StandardScaler standardizes a set of features to have zero mean and a standard deviation of 1. The flag withStd will scale the data to unit standard deviation while the flag withMean (false by default) will center the data prior to scaling it. StandardScaler 将一组特征标准化为零均值和标准差为1. 标志 withStd 将数据缩放到单位标准差，而标志withMean（默认为false）将在缩放之前将数据中心化。 WARNING 警告Centering can be very expensive on sparse vectors because it generally turns them into dense vectors, so be careful before centering your data. 在稀疏向量上居中可能非常昂贵，因为它通常会将它们变成密集矢量，因此在对数据中心化之前要小心。 Here’s an example of using a StandardScaler: 以下是使用 StandardScaler 的示例： 1234// in Scalaimport org.apache.spark.ml.feature.StandardScalerval sScaler = new StandardScaler().setInputCol("features")sScaler.fit(scaleDF).transform(scaleDF).show() 1234# in Pythonfrom pyspark.ml.feature import StandardScalersScaler = StandardScaler().setInputCol("features")sScaler.fit(scaleDF).transform(scaleDF).show() The output is shown below: 1234567+---+--------------+------------------------------------------------------------+|id | features | StandardScaler_41aaa6044e7c3467adc3__output |+---+--------------+------------------------------------------------------------+|0 |[1.0,0.1,-1.0]|[1.1952286093343936,0.02337622911060922,-0.5976143046671968]|...|1 |[3.0,10.1,3.0]|[3.5856858280031805,2.3609991401715313,1.7928429140015902] |+---+--------------+------------------------------------------------------------+ MinMaxScalerThe MinMaxScaler will scale the values in a vector (component wise) to the proportional values on a scale from a given min value to a max value. If you specify the minimum value to be 0 and the maximum value to be 1, then all the values will fall in between 0 and 1: MinMaxScaler 会将向量（基于元素）中的值按照从给定最小值到最大值的比例缩放到比例值。如果将最小值指定为0并将最大值指定为1，则所有值都将介于0和1之间： 12345// in Scalaimport org.apache.spark.ml.feature.MinMaxScalerval minMax = new MinMaxScaler().setMin(5).setMax(10).setInputCol("features")val fittedminMax = minMax.fit(scaleDF)fittedminMax.transform(scaleDF).show() 12345# in Pythonfrom pyspark.ml.feature import MinMaxScalerminMax = MinMaxScaler().setMin(5).setMax(10).setInputCol("features")fittedminMax = minMax.fit(scaleDF)fittedminMax.transform(scaleDF).show() 1234567+---+--------------+----------------------------------------------------------+|id | features | MaxAbsScaler_402587e1d9b6f268b927__output |+---+--------------+----------------------------------------------------------+|0 |[1.0,0.1,-1.0]|[0.3333333333333333,0.009900990099009901,-0.3333333333333]|... |1 |[3.0,10.1,3.0]| [1.0,1.0,1.0] |+---+--------------+----------------------------------------------------------+ ElementwiseProductThe ElementwiseProduct allows us to scale each value in a vector by an arbitrary value. For example, given the vector below and the row “1, 0.1, -1” the output will be “10, 1.5, -20.” Naturally the dimensions of the scaling vector must match the dimensions of the vector inside the relevant column: ElementwiseProduct 允许我们通过任意值缩放向量中的每个值。例如，给定下面的向量和行“1,0.1，-1”，输出将是“10,1.5，-20。”当然，缩放向量的尺寸必须与相关列内的向量尺寸相匹配： 12345678// in Scalaimport org.apache.spark.ml.feature.ElementwiseProductimport org.apache.spark.ml.linalg.Vectorsval scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)val scalingUp = new ElementwiseProduct().setScalingVec(scaleUpVec).setInputCol("features")scalingUp.transform(scaleDF).show() 12345678# in Pythonfrom pyspark.ml.feature import ElementwiseProductfrom pyspark.ml.linalg import VectorsscaleUpVec = Vectors.dense(10.0, 15.0, 20.0)scalingUp = ElementwiseProduct()\.setScalingVec(scaleUpVec)\.setInputCol("features")scalingUp.transform(scaleDF).show() 1234567+---+--------------+-----------------------------------------------+| id| features |ElementwiseProduct_42b29ea5a55903e9fea6__output|+---+--------------+-----------------------------------------------+| 0 |[1.0,0.1,-1.0]| [10.0,1.5,-20.0] |...| 1 |[3.0,10.1,3.0]| [30.0,151.5,60.0] |+---+--------------+-----------------------------------------------+ NormalizerThe normalizer allows us to scale multidimensional vectors using one of several power norms, set through the parameter “p”. For example, we can use the Manhattan norm (or Manhattan distance) with p = 1, Euclidean norm with p = 2, and so on. The Manhattan distance is a measure of distance where you can only travel from point to point along the straight lines of an axis (like the streets in Manhattan). Normalizer（归一化器又称：标准化）允许我们使用几个强大的范数之一来缩放多维向量，通过参数“p”设置。例如，我们可以使用曼哈顿范数（或曼哈顿距离），其中p = 1，欧几里德范数，p = 2，依此类推。曼哈顿距离是距离的度量，您只能沿着轴的直线（如曼哈顿的街道）从一个点到另一个点行进。 Here’s an example of using the Normalizer: 以下是使用 Normalizer 的示例： 1234// in Scalaimport org.apache.spark.ml.feature.Normalizerval manhattanDistance = new Normalizer().setP(1).setInputCol("features")manhattanDistance.transform(scaleDF).show() 1234# in Pythonfrom pyspark.ml.feature import NormalizermanhattanDistance = Normalizer().setP(1).setInputCol("features")manhattanDistance.transform(scaleDF).show() 123456789+---+--------------+-------------------------------+| id| features |normalizer_1bf2cd17ed33__output|+---+--------------+-------------------------------+| 0 |[1.0,0.1,-1.0]| [0.47619047619047... || 1 | [2.0,1.1,1.0]| [0.48780487804878... || 0 |[1.0,0.1,-1.0]| [0.47619047619047... || 1 | [2.0,1.1,1.0]| [0.48780487804878... || 1 |[3.0,10.1,3.0]| [0.18633540372670... |+---+--------------+-------------------------------+ Working with Categorical FeaturesThe most common task for categorical features is indexing. Indexing converts a categorical variable in a column to a numerical one that you can plug into machine learning algorithms. While this is conceptually simple, there are some catches that are important to keep in mind so that Spark can do this in a stable and repeatable manner. 分类特征的最常见任务是索引。索引将列中的分类变量转换为可插入机器学习算法的数字变量。虽然这在概念上很简单，但仍有一些隐患事项要记住，以便 Spark 可以以稳定和可重复的方式执行此操作。 In general, we recommend re-indexing every categorical variable when pre-processing just for consistency’s sake. This can be helpful in maintaining your models over the long run as your encoding practices may change over time. 通常，我们建议在预处理时为每个分类变量重新编制索引以保持一致性。从长远来看，这有助于维护模型，因为编码实践可能会随着时间的推移而发生变化。 StringIndexerThe simplest way to index is via the StringIndexer, which maps strings to different numerical IDs. Spark’s StringIndexer also creates metadata attached to the DataFrame that specify what inputs correspond to what outputs. This allows us later to get inputs back from their respective index values: 索引的最简单方法是通过 StringIndexer，它将字符串映射到不同的数字 ID。 Spark 的 StringIndexer 还创建附加到 DataFrame 的元数据，用于指定哪些输入对应于哪些输出。这允许我们稍后从各自的索引值获取输入： 12345// in Scalaimport org.apache.spark.ml.feature.StringIndexerval lblIndxr = new StringIndexer().setInputCol("lab").setOutputCol("labelInd")val idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)idxRes.show() 12345# in Pythonfrom pyspark.ml.feature import StringIndexerlblIndxr = StringIndexer().setInputCol("lab").setOutputCol("labelInd")idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)idxRes.show() 1234567+-----+----+------+------------------+--------+|color| lab|value1| value2 |labelInd|+-----+----+------+------------------+--------+|green|good| 1 |14.386294994851129| 1.0 |...| red | bad| 2 |14.386294994851129| 0.0 |+-----+----+------+------------------+--------+ We can also apply StringIndexer to columns that are not strings, in which case, they will be converted to strings before being indexed: 我们也可以将 StringIndexer 应用于非字符串的列，在这种情况下，它们将在被索引之前转换为字符串： 12345// in Scalaval valIndexer = new StringIndexer().setInputCol("value1").setOutputCol("valueInd")valIndexer.fit(simpleDF).transform(simpleDF).show() 123# in PythonvalIndexer = StringIndexer().setInputCol("value1").setOutputCol("valueInd")valIndexer.fit(simpleDF).transform(simpleDF).show() 1234567+-----+----+------+------------------+--------+|color| lab|value1| value2 |valueInd|+-----+----+------+------------------+--------+|green|good| 1 |14.386294994851129| 1.0 |...| red | bad| 2 |14.386294994851129| 0.0 |+-----+----+------+------------------+--------+ Keep in mind that the StringIndexer is an estimator that must be fit on the input data. This means it must see all inputs to select a mapping of inputs to IDs. If you train a StringIndexer on inputs “a,” “b,” and “c” and then go to use it against input “d,” it will throw an error by default. Another option is to skip the entire row if the input value was not a value seen during training. Going along with the previous example, an input value of “d” would cause that row to be skipped entirely. We can set this option before or after training the indexer or pipeline. More options may be added to this feature in the future but as of Spark 2.2, you can only skip or throw an error on invalid inputs. 请记住，StringIndexer 是一个必须适合输入数据的估计器（estimator）。这意味着它必须查看所有输入以选择输入到 ID 的映射。如果你在输入“a”，“b”和“c”上训练一个 StringIndexer，然后对输入“d”的背景下使用它，它默认会抛出一个错误。如果输入值不是训练期间看到的值，则另一个选项是跳过整行。沿用前面的示例，输入值“d”将导致完全跳过该行。我们可以在训练索引器或管道之前或之后设置此选项。将来可能会向此功能添加更多选项，但从Spark 2.2开始，您只能跳过或在无效输入上抛出错误。 12valIndexer.setHandleInvalid("skip")valIndexer.fit(simpleDF).setHandleInvalid("skip") Converting Indexed Values Back to TextWhen inspecting your machine learning results, you’re likely going to want to map back to the original values. Since MLlib classification models make predictions using the indexed values, this conversion is useful for converting model predictions (indices) back to the original categories. We can do this with IndexToString. You’ll notice that we do not have to input our value to the String key; Spark’s MLlib maintains this metadata for you. You can optionally specify the outputs. 检查机器学习结果时，您可能希望映射回原始值。由于 MLlib 分类模型使用索引值进行预测，因此此转换对于将模型预测（索引）转换回原始类别非常有用。我们可以使用 IndexToString 来做到这一点。您会注意到我们不必将我们的值输入String键; Spark的MLlib为您维护这个元数据。您可以选择指定输出。 1234// in Scalaimport org.apache.spark.ml.feature.IndexToStringval labelReverse = new IndexToString().setInputCol("labelInd")labelReverse.transform(idxRes).show() 1234# in Pythonfrom pyspark.ml.feature import IndexToStringlabelReverse = IndexToString().setInputCol("labelInd")labelReverse.transform(idxRes).show() 1234567+-----+----+------+------------------+--------+--------------------------------+|color| lab|value1| value2 |labelInd|IndexToString_415...2a0d__output|+-----+----+------+------------------+--------+--------------------------------+|green|good| 1 |14.386294994851129| 1.0 | good |...| red | bad| 2 |14.386294994851129| 0.0 | bad |+-----+----+------+------------------+--------+--------------------------------+ Indexing in VectorsVectorIndexer is a helpful tool for working with categorical variables that are already found inside of vectors in your dataset. This tool will automatically find categorical features inside of your input vectors and convert them to categorical features with zero-based category indices. For example, in the following DataFrame, the first column in our Vector is a categorical variable with two different categories while the rest of the variables are continuous. By setting maxCategories to 2 in our VectorIndexer, we are instructing Spark to take any column in our vector with two or less distinct values and convert it to a categorical variable. This can be helpful when you know how many unique values there are in your largest category because you can specify this and it will automatically index the values accordingly. Conversely, Spark changes the data based on this parameter, so if you have continuous variables that don’t appear particularly continuous (lots of repeated values) these can be unintentionally converted to categorical variables if there are too few unique values. VectorIndexer 是一个有用的工具，用于处理已在数据集中的向量中找到的分类变量。此工具将自动查找输入向量内的分类特征，并将其转换为具有从零开始的类别索引的分类特征。例如，在以下 DataFrame 中，Vector 中的第一列是具有两个不同类别的分类变量，而其余变量是连续的。通过在我们的 VectorIndexer 中将 maxCategories 设置为2，我们指示 Spark 在我们的向量中使用两个或更少不同的值并将其转换为分类变量。当您知道最大类别中有多少个唯一值时，这会很有用，因为您可以指定它，并相应地自动索引值。相反，Spark 会根据此参数更改数据，因此如果连续变量看起来不是特别连续（许多重复值），如果唯一值太少，这些变量可能会无意中转换为分类变量。 12345678910111213// in Scalaimport org.apache.spark.ml.feature.VectorIndexerimport org.apache.spark.ml.linalg.Vectorsval idxIn = spark.createDataFrame(Seq((Vectors.dense(1, 2, 3),1),(Vectors.dense(2, 5, 6),2),(Vectors.dense(1, 8, 9),3))).toDF("features", "label")val indxr = new VectorIndexer().setInputCol("features").setOutputCol("idxed").setMaxCategories(2)indxr.fit(idxIn).transform(idxIn).show() 123456789101112# in Pythonfrom pyspark.ml.feature import VectorIndexerfrom pyspark.ml.linalg import VectorsidxIn = spark.createDataFrame([(Vectors.dense(1, 2, 3),1),(Vectors.dense(2, 5, 6),2),(Vectors.dense(1, 8, 9),3)]).toDF("features", "label")indxr = VectorIndexer()\.setInputCol("features")\.setOutputCol("idxed")\.setMaxCategories(2)indxr.fit(idxIn).transform(idxIn).show() 1234567+-------------+-----+-------------+| features |label| idxed |+-------------+-----+-------------+|[1.0,2.0,3.0]| 1 |[0.0,2.0,3.0]||[2.0,5.0,6.0]| 2 |[1.0,5.0,6.0]||[1.0,8.0,9.0]| 3 |[0.0,8.0,9.0]|+-------------+-----+-------------+ One-Hot Encoding 独热编码Indexing categorical variables is only half of the story. One-hot encoding is an extremely common data transformation performed after indexing categorical variables. This is because indexing does not always represent our categorical variables in the correct way for downstream models to process. For instance, when we index our “color” column, you will notice that some colors have a higher value (or index number) than others (in our case, blue is 1 and green is 2). 索引分类变量只是故事的一半。独热编码是在对分类变量建立索引之后执行的极其常见的数据转换。这是因为索引并不总是以下游模型处理的正确方式表示我们的分类变量。例如，当我们索引 “color” 列时，您会注意到某些颜色的值（或索引号码）高于其他颜色（在我们的例子中，蓝色为1，绿色为2）。 This is incorrect because it gives the mathematical appearance that the input to the machine learning algorithm seems to specify that green &gt; blue, which makes no sense in the case of the current categories. To avoid this, we use OneHotEncoder, which will convert each distinct value to a Boolean flag (1 or 0) as a component in a vector. When we encode the color value, then we can see these are no longer ordered, making them easier for downstream models (e.g., a linear model) to process: 这是不正确的，因为它套上了数学的外衣，机器学习算法的输入似乎指定绿色&gt;蓝色，这在当前类别的情况下没有意义。为避免这种情况，我们使用 OneHotEncoder，它将每个不同的值转换为布尔（Boolean）标志（1或0）作为向量中的元素。当我们对颜色值进行编码时，我们可以看到它们不再有序，这使得下游模型（例如，线性模型）更容易处理： 123456// in Scalaimport org.apache.spark.ml.feature.&#123;StringIndexer, OneHotEncoder&#125;val lblIndxr = new StringIndexer().setInputCol("color").setOutputCol("colorInd")val colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select("color"))val ohe = new OneHotEncoder().setInputCol("colorInd")ohe.transform(colorLab).show() 123456# in Pythonfrom pyspark.ml.feature import OneHotEncoder, StringIndexerlblIndxr = StringIndexer().setInputCol("color").setOutputCol("colorInd")colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select("color"))ohe = OneHotEncoder().setInputCol("colorInd")ohe.transform(colorLab).show() 123456789+-----+--------+------------------------------------------+|color|colorInd|OneHotEncoder_46b5ad1ef147bb355612__output|+-----+--------+------------------------------------------+|green| 1.0 | (2,[1],[1.0]) || blue| 2.0 | (2,[],[]) |...| red | 0.0 | (2,[0],[1.0]) || red | 0.0 | (2,[0],[1.0]) |+-----+--------+------------------------------------------+ Text Data Transformers 文本数据转换器Text is always tricky input because it often requires lots of manipulation to map to a format that a machine learning model will be able to use effectively. There are generally two kinds of texts you’ll see: free-form text and string categorical variables. This section primarily focuses on free-form text because we already discussed categorical variables. 文本总是很棘手的输入，因为它经常需要大量的操作才能映射到机器学习模型能够有效使用的格式。您将看到通常有两种文本：自由格式文本和字符串分类变量。本节主要关注自由格式文本，因为我们已经讨论了分类变量。 Tokenizing Text 文本符号化Tokenization is the process of converting free-form text into a list of “tokens” or individual words. The easiest way to do this is by using the Tokenizer class. This transformer will take a string of words, separated by whitespace, and convert them into an array of words. For example, in our dataset we might want to convert the Description field into a list of tokens. 符号化是将自由格式文本转换为“符号”或单个单词列表的过程。最简单的方法是使用 Tokenizer 类。这个转换器将采用一串由空格分隔的单词，并将它们转换为单词数组。例如，在我们的数据集中，我们可能希望将Description 字段转换为标记列表。 12345// in Scalaimport org.apache.spark.ml.feature.Tokenizerval tkn = new Tokenizer().setInputCol("Description").setOutputCol("DescOut")val tokenized = tkn.transform(sales.select("Description"))tokenized.show(false) 12345# in Pythonfrom pyspark.ml.feature import Tokenizertkn = Tokenizer().setInputCol("Description").setOutputCol("DescOut")tokenized = tkn.transform(sales.select("Description"))tokenized.show(20, False) 123456789+-----------------------------------+------------------------------------------+| Description DescOut |+-----------------------------------+------------------------------------------+| RABBIT NIGHT LIGHT | [rabbit, night, light] || DOUGHNUT LIP GLOSS | [doughnut, lip, gloss] |...|AIRLINE BAG VINTAGE WORLD CHAMPION | [airline, bag, vintage, world, champion] ||AIRLINE BAG VINTAGE JET SET BROWN | [airline, bag, vintage, jet, set, brown] |+-----------------------------------+------------------------------------------+ We can also create a Tokenizer that is not just based white space but a regular expression with the RegexTokenizer. The format of the regular expression should conform to the Java Regular Expression (RegEx) syntax: 我们还可以创建一个 Tokenizer，它不仅仅是基于空格，而是使用 RegexTokenizer 的正则表达式。正则表达式的格式应符合 Java 正则表达式（RegEx）语法： 12345678// in Scalaimport org.apache.spark.ml.feature.RegexTokenizerval rt = new RegexTokenizer().setInputCol("Description").setOutputCol("DescOut").setPattern(" ") // simplest expression.setToLowercase(true)rt.transform(sales.select("Description")).show(false) 12345678# in Pythonfrom pyspark.ml.feature import RegexTokenizerrt = RegexTokenizer()\.setInputCol("Description")\.setOutputCol("DescOut")\.setPattern(" ")\.setToLowercase(True)rt.transform(sales.select("Description")).show(20, False) 123456789+-----------------------------------+------------------------------------------+| Description DescOut |+-----------------------------------+------------------------------------------+| RABBIT NIGHT LIGHT | [rabbit, night, light] || DOUGHNUT LIP GLOSS | [doughnut, lip, gloss] |...|AIRLINE BAG VINTAGE WORLD CHAMPION | [airline, bag, vintage, world, champion] ||AIRLINE BAG VINTAGE JET SET BROWN | [airline, bag, vintage, jet, set, brown] |+-----------------------------------+------------------------------------------+ Another way of using the RegexTokenizer is to use it to output values matching the provided pattern instead of using it as a gap. We do this by setting the gaps parameter to false. Doing this with a space as a pattern returns all the spaces, which is not too useful, but if we made our pattern capture individual words, we could return those: 使用 RegexTokenizer 的另一种方法是使用它来输出与提供的模式匹配的值，而不是将其用作间隔。我们通过将gaps 参数设置为 false 来完成此操作。使用空格作为模式执行此操作将返回所有空格，这不是太有用，但如果我们使模式捕获单个单词，我们可以返回这些： 123456789// in Scalaimport org.apache.spark.ml.feature.RegexTokenizerval rt = new RegexTokenizer().setInputCol("Description").setOutputCol("DescOut").setPattern(" ").setGaps(false).setToLowercase(true)rt.transform(sales.select("Description")).show(false) 123456789# in Pythonfrom pyspark.ml.feature import RegexTokenizerrt = RegexTokenizer()\.setInputCol("Description")\.setOutputCol("DescOut")\.setPattern(" ")\.setGaps(False)\.setToLowercase(True)rt.transform(sales.select("Description")).show(20, False) 123456789+-----------------------------------+------------------+| Description DescOut |+-----------------------------------+------------------+| RABBIT NIGHT LIGHT | [ , ] || DOUGHNUT LIP GLOSS | [ , , ] |...|AIRLINE BAG VINTAGE WORLD CHAMPION | [ , , , , ] ||AIRLINE BAG VINTAGE JET SET BROWN | [ , , , , ] |+-----------------------------------+------------------+ Removing Common Words 移除常见词A common task after tokenization is to filter stop words, common words that are not relevant in many kinds of analysis and should thus be removed. Frequently occurring stop words in English include “the,” “and,” and “but”. Spark contains a list of default stop words you can see by calling the following method, which can be made case insensitive if necessary (as of Spark 2.2, supported languages for stopwords are “danish,” “dutch,” “english,” “finnish,” “french,” “german,” “hungarian,” “italian,” “norwegian,” “portuguese,” “russian,” “spanish,” “swedish,” and “turkish”): 符号化（tokenization ）后的一个常见任务是过滤停用词，这些词在多种分析中不相关，因此应该被删除。英语中经常出现的停用词包括“the”，“and”和“but”。Spark包含一个默认停止词列表，您可以通过调用以下方法查看，如果需要，可以使其不区分大小写（从Spark 2.2开始 ，支持的停用词语言是“丹麦语”，“荷兰语”，“英语”，“芬兰语”，“法语”，“德语”，“匈牙利语”，“意大利语”，“挪威语”，“葡萄牙语”，“俄语”，“西班牙语”，“瑞典语 ，“和”土耳其语“）: 1234567// in Scalaimport org.apache.spark.ml.feature.StopWordsRemoverval englishStopWords = StopWordsRemover.loadDefaultStopWords("english")val stops = new StopWordsRemover().setStopWords(englishStopWords).setInputCol("DescOut")stops.transform(tokenized).show() 1234567# in Pythonfrom pyspark.ml.feature import StopWordsRemoverenglishStopWords = StopWordsRemover.loadDefaultStopWords("english")stops = StopWordsRemover()\.setStopWords(englishStopWords)\.setInputCol("DescOut")stops.transform(tokenized).show() The following output shows how this works: 下面的输出展示了这是如何工作的： 1234567+--------------------+--------------------+------------------------------------+| Description | DescOut |StopWordsRemover_4ab18...6ed__output|+--------------------+--------------------+------------------------------------+...|SET OF 4 KNICK KN...|[set, of, 4, knic...| [set, 4, knick, k... |...+--------------------+--------------------+------------------------------------+ Notice how the word of is removed in the output column. That’s because it’s such a common word that it isn’t relevant to any downstream manipulation and simply adds noise to our dataset. 注意如何在输出列中删除单词。这是因为它是一个常见的词，它与任何下游操作无关，只是简单地为我们的数据集添加噪声。 Creating Word Combinations 创建词的组合Tokenizing our strings and filtering stop words leaves us with a clean set of words to use as features. It is often of interest to look at combinations of words, usually by looking at colocated words. Word combinations are technically referred to as n-grams—that is, sequences of words of length n. An ngram of length 1 is called a unigrams; those of length 2 are called bigrams, and those of length 3 are called trigrams (anything above those are just four-gram, five-gram, etc.), Order matters with n-gram creation, so converting a sentence with three words into bigram representation would result in two bigrams. The goal when creating n-grams is to better capture sentence structure and more information than can be gleaned by simply looking at all words individually. Let’s create some n-grams to illustrate this concept. 对字符串进行符号化（Tokenizing）并过滤停用词会给我们留下一组简洁的用作特征的单词。通常通过查看共现的单词来查看单词的组合通常是有意义的。单词组合在技术上被称为 n-gram，即长度为n的单词序列。长度为1的 ngram 称为单元组（unigram）；长度为2的那些被称为二元组（bigram），而长度为3的那些被称为三元组（trigram）（任何高于那些只有 four-gram， five-gram等），顺序与 n-gram 创建有关，所以将一个带三个单词的句子转换成 bigram 代表将产生两个 bigram。创建 n-gram 时的目标是更好地捕获句子结构和更多信息，而不是通过简单地单独查看所有单词来收集信息。让我们创建一些 n-gram 来说明这个概念。 The bigrams of “Big Data Processing Made Simple” are: “大数据处理变得简单”的2元组是： “Big Data” “Data Processing” “Processing Made” “Made Simple” While the trigrams are: 而三元组是： “Big Data Processing” “Data Processing Made” “Procesing Made Simple” With n-grams, we can look at sequences of words that commonly co-occur and use them as inputs to a machine learning algorithm. These can create better features than simply looking at all of the words individually (say, tokenized on a space character): 使用 n-gram，我们可以查看通常共同出现的单词序列，并将它们用作机器学习算法的输入。 这些可以创建比单独查看所有单词更好的特征（例如，在空格字符上符号化）： 123456// in Scalaimport org.apache.spark.ml.feature.NGramval unigram = new NGram().setInputCol("DescOut").setN(1)val bigram = new NGram().setInputCol("DescOut").setN(2)unigram.transform(tokenized.select("DescOut")).show(false)bigram.transform(tokenized.select("DescOut")).show(false) 123456# in Pythonfrom pyspark.ml.feature import NGramunigram = NGram().setInputCol("DescOut").setN(1)bigram = NGram().setInputCol("DescOut").setN(2)unigram.transform(tokenized.select("DescOut")).show(False)bigram.transform(tokenized.select("DescOut")).show(False) 123456789+-----------------------------------------+------------------------------------- DescOut | ngram_104c4da6a01b__output ...+-----------------------------------------+-------------------------------------| [rabbit, night, light] | [rabbit, night, light] ...| [doughnut, lip, gloss] | [doughnut, lip, gloss] ......|[airline, bag, vintage, world, champion] |[airline, bag, vintage, world, cha...|[airline, bag, vintage, jet, set, brown] |[airline, bag, vintage, jet, set, ...+-----------------------------------------+------------------------------------- And the result for bigrams: 二元组的结果： 123456789+------------------------------------------+------------------------------------ DescOut | ngram_6e68fb3a642a__output ...+------------------------------------------+------------------------------------| [rabbit, night, light] | [rabbit night, night light] ...| [doughnut, lip, gloss] | [doughnut lip, lip gloss] ......|[airline, bag, vintage, world, champion] | [airline bag, bag vintage, vintag...|[airline, bag, vintage, jet, set, brown] | [airline bag, bag vintage, vintag...+------------------------------------------+------------------------------------ Converting Words into Numerical Representations 将单词转换为数字表示Once you have word features, it’s time to start counting instances of words and word combinations for use in our models. The simplest way is just to include binary counts of a word in a given document (in our case, a row). Essentially, we’re measuring whether or not each row contains a given word. This is a simple way to normalize for document sizes and occurrence counts and get numerical features that allow us to classify documents based on content. In addition, we can count words using a CountVectorizer, or reweigh them according to the prevalence of a given word in all the documents using a TF–IDF transformation (discussed next). 一旦你有了单词功能，就可以开始计算单词和单词组合的实例，以便在我们的模型中使用。最简单的方法是在给定文档中包含单词的二进制计数（在我们的例子中是一行）。基本上，我们测量每行是否包含给定的单词。这是一种标准化文档大小和出现次数的简单方法，并获得允许我们根据内容对文档进行分类的数值特征。此外，我们可以使用 CountVectorizer 对单词进行计数，或者使用 TF-IDF 转换根据所有文档中给定单词的普遍程度对它们进行重新加权（下面将讨论）。 A CountVectorizer operates on our tokenized data and does two things: CountVectorizer 对我们的符号化数据进行操作，并做两件事： During the fit process, it finds the set of words in all the documents and then counts the occurrences of those words in those documents. 在拟合过程中，它在所有文档中找到一组单词，然后计算这些单词在这些文档中的出现次数。 It then counts the occurrences of a given word in each row of the DataFrame column during the transformation process and outputs a vector with the terms that occur in that row. 然后，它在转换过程中计算DataFrame列的每一行中给定单词的出现次数，并输出带有该行中出现的词语（term）的向量。 Conceptually this tranformer treats every row as a document and every word as a term and the total collection of all terms as the vocabulary. These are all tunable parameters, meaning we can set the minimum term frequency (minTF) for the term to be included in the vocabulary (effectively removing rare words from the vocabulary); minimum number of documents a term must appear in (minDF) before being included in the vocabulary (another way to remove rare words from the vocabulary); and finally, the total maximum vocabulary size (vocabSize). Lastly, by default the CountVectorizer will output the counts of a term in a document. To just return whether or not a word exists in a document, we can use setBinary(true). Here’s an example of using CountVectorizer: 从概念上讲，这个转换器将每一行视为一个文档，将每个单词（word）视为一个术语（term），并将所有术语的总集合视为词汇（vocabulary）。这些都是可调参数，这意味着我们可以设置词汇中包含的术语的最小术语频率（minTF）（有效地从词汇表中删除稀有词）; 术语在被包含在词汇表中之前必须出现的次数满足（minDF）中的最小文档数量（从词汇表中删除稀有词汇的另一种方式）；最后，总的最大词汇量大小（vocabSize）。最后，默认情况下，CountVectorizer 将输出文档中术语的计数。要返回文档中是否存在单词，我们可以使用setBinary（true）。以下是使用CountVectorizer 的示例： 12345678910// in Scalaimport org.apache.spark.ml.feature.CountVectorizerval cv = new CountVectorizer().setInputCol("DescOut").setOutputCol("countVec").setVocabSize(500).setMinTF(1).setMinDF(2)val fittedCV = cv.fit(tokenized)fittedCV.transform(tokenized).show(false) 123456789# in Pythonfrom pyspark.ml.feature import CountVectorizercv = CountVectorizer()\.setInputCol("DescOut")\.setOutputCol("countVec")\.setVocabSize(500)\.setMinTF(1)\.setMinDF(2)fittedCV = cv.fit(tokenized)fittedCV.transform(tokenized).show(False) While the output looks a little complicated, it’s actually just a sparse vector that contains the total vocabulary size, the index of the word in the vocabulary, and then the counts of that particular word: 虽然输出看起来有点复杂，但它实际上只是一个稀疏向量，它包含总词汇量大小，词汇表中单词的索引，然后是特定单词的计数： 123456789+---------------------------------+--------------------------------------------+ DescOut | countVec |+---------------------------------+--------------------------------------------+| [rabbit, night, light] | (500,[150,185,212],[1.0,1.0,1.0]) || [doughnut, lip, gloss] | (500,[462,463,492],[1.0,1.0,1.0]) |...|[airline, bag, vintage, world,...| (500,[2,6,328],[1.0,1.0,1.0]) ||[airline, bag, vintage, jet, s...|(500,[0,2,6,328,405],[1.0,1.0,1.0,1.0,1.0]) |+---------------------------------+--------------------------------------------+ Term frequency–inverse document frequency 词频—逆文档频率Another way to approach the problem of converting text into a numerical representation is to use term frequency–inverse document frequency (TF–IDF). In simplest terms, TF–IDF measures how often a word occurs in each document, weighted according to how many documents that word occurs in. The result is that words that occur in a few documents are given more weight than words that occur in many documents. In practice, a word like “the” would be weighted very low because of its prevalence while a more specialized word like “streaming” would occur in fewer documents and thus would be weighted higher. In a way, TF–IDF helps find documents that share similar topics. 解决将文本转换为数字表示的问题的另一种方法是使用词频 - 逆文档频率（TF-IDF）。简单来说，TF-IDF测量每个文档中单词出现的频率，根据单词出现的文档数加权。结果是，少数文档中出现的单词比许多文档中出现的单词更重要。 在实践中，像“the”这样的单词由于其普遍性而被加权得非常低，而像“streaming”这样的更专业的单词将在更少的文档中出现，因此将被加权更高。在某种程度上，TF-IDF有助于查找共享相似主题的文档。 Let’s take a look at an example—first, we’ll inspect some of the documents in our data containing the word “red”: 让我们看一个例子——首先，我们将检查包含单词“red”的数据中的一些文档： 123456// in Scalaval tfIdfIn = tokenized.where("array_contains(DescOut, 'red')").select("DescOut").limit(10)tfIdfIn.show(false) 123456# in PythontfIdfIn = tokenized\.where("array_contains(DescOut, 'red')")\.select("DescOut")\.limit(10)tfIdfIn.show(10, False) 12345678+---------------------------------------+ DescOut |+---------------------------------------+|[gingham, heart, , doorstop, red] |...|[red, retrospot, oven, glove] ||[red, retrospot, plate] |+---------------------------------------+ We can see some overlapping words in these documents, but these words provide at least a rough topic-like representation. Now let’s input that into TF–IDF. To do this, we’re going to hash each word and convert it to a numerical representation, and then weigh each word in the voculary according to the inverse document frequency. Hashing is a similar process as CountVectorizer, but is irreversible—that is, from our output index for a word, we cannot get our input word (multiple words might map to the same output index): 我们可以在这些文档中看到一些重叠的单词，但这些单词至少提供了一个粗略的主题表示。现在让我们输入TF-IDF。为此，我们将对每个单词进行哈希并将其转换为数字表示，然后根据逆文档频率对单词中的每个单词进行加权。哈希是与 CountVectorizer 类似的过程，但是不可逆转——也就是说，从单词的输出索引，我们无法得到输入词（多个单词可能映射到相同的输出索引）： 12345678910// in Scalaimport org.apache.spark.ml.feature.&#123;HashingTF, IDF&#125;val tf = new HashingTF().setInputCol("DescOut").setOutputCol("TFOut").setNumFeatures(10000)val idf = new IDF().setInputCol("TFOut").setOutputCol("IDFOut").setMinDocFreq(2) 12345678910# in Pythonfrom pyspark.ml.feature import HashingTF, IDFtf = HashingTF()\.setInputCol("DescOut")\.setOutputCol("TFOut")\.setNumFeatures(10000)idf = IDF()\.setInputCol("TFOut")\.setOutputCol("IDFOut")\.setMinDocFreq(2) 12// in Scalaidf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show(false) 12# in Pythonidf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show(10, False) While the output is too large to include here, notice that a certain value is assigned to “red” and that this value appears in every document. Also note that this term is weighted extremely low because it appears in every document. The output format is a sparse Vector we can subsequently input into a machine learning model in a form like this: 虽然输出太大而不能包含在此处，但请注意某个值被指定为 “red”，并且该值出现在每个文档中。另请注意，此术语（term）的权重极低，因为它出现在每个文档中。输出格式是一个稀疏的 Vector，我们可以随后以这样的形式输入到机器学习模型中： 1(10000,[2591,4291,4456],[1.0116009116784799,0.0,0.0]) This vector is represented using three different values: the total vocabulary size, the hash of every word appearing in the document, and the weighting of each of those terms. This is similar to the CountVectorizer output. 该向量使用三个不同的值表示：总词汇量大小，文档中出现的每个单词的哈希值，以及每个术语的权重。这类似于 CountVectorizer 输出。 Word2VecWord2Vec is a deep learning–based tool for computing a vector representation of a set of words. The goal is to have similar words close to one another in this vector space, so we can then make generalizations about the words themselves. This model is easy to train and use, and has been shown to be useful in a number of natural language processing applications, including entity recognition, disambiguation, parsing, tagging, and machine translation. Word2Vec 是一种基于深度学习的工具，用于计算一组单词的向量表示。目标是在这个向量空间中使相似的单词彼此接近，这样我们就可以对单词本身进行概括。该模型易于训练和使用，并且已被证明在许多自然语言处理应用中是有用的，包括实体识别，消歧，解析，标记和机器翻译。 Word2Vec is notable for capturing relationships between words based on their semantics. For example, if v~king, v~queen, v~man, and v~women represent the vectors for those four words, then we will often get a representation where v~king - v~man + v~woman ~= v~queen. To do this, Word2Vec uses a technique called “skip-grams” to convert a sentence of words into a vector representation (optionally of a specific size). It does this by building a vocabulary, and then for every sentence, it removes a token and trains the model to predict the missing token in the “n-gram” representation. Word2Vec works best with continuous, free-form text in the form of tokens. Word2Vec以基于语义捕获单词之间的关系而着称。例如，如果v~king，v~ques，v~man和v~woman代表这四个单词的向量，那么我们经常会得到一个表示 v~king - v~man + v~woman~ = v 〜女王。为此，Word2Vec使用一种名为“skip-gram”的技术将单词的句子转换为向量表示（可选地具有特定大小）。它通过构建词汇表来实现这一点，然后对于每个句子，它会删除一个子并训练模型以预测“n-gram”表示中的丢失子. Word2Vec 最适用于连续的自由格式文本，其形式为：子。 Here’s a simple example from the documentation: 以下是文档中的一个简单示例： 12345678910111213141516171819202122232425// in Scalaimport org.apache.spark.ml.feature.Word2Vecimport org.apache.spark.ml.linalg.Vectorimport org.apache.spark.sql.Row// Input data: Each row is a bag of words from a sentence or document.val documentDF = spark.createDataFrame(Seq( "Hi I heard about Spark".split(" "), "I wish Java could use case classes".split(" "), "Logistic regression models are neat".split(" ")).map(Tuple1.apply)).toDF("text")// Learn a mapping from words to Vectors.val word2Vec = new Word2Vec().setInputCol("text").setOutputCol("result").setVectorSize(3).setMinCount(0)val model = word2Vec.fit(documentDF)val result = model.transform(documentDF)result.collect().foreach &#123; case Row(text: Seq[_], features: Vector) =&gt; println(s"Text: [$&#123;text.mkString(", ")&#125;] =&gt; \nVector: $features\n")&#125; 1234567891011121314151617 #in Pythonfrom pyspark.ml.feature import Word2Vec# Input data: Each row is a bag of words from a sentence or document.documentDF = spark.createDataFrame( [("Hi I heard about Spark".split(" "), ), ("I wish Java could use case classes".split(" "), ), ("Logistic regression models are neat".split(" "), ) ], ["text"])# Learn a mapping from words to Vectors.word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol="text",outputCol="result")model = word2Vec.fit(documentDF)result = model.transform(documentDF)for row in result.collect(): text, vector = row print("Text: [%s] =&gt; \nVector: %s\n" % (", ".join(text), str(vector))) 123456Text: [Hi, I, heard, about, Spark] =&gt;Vector: [-0.008142343163490296,0.02051363289356232,0.03255096450448036]Text: [I, wish, Java, could, use, case, classes] =&gt;Vector: [0.043090314205203734,0.035048123182994974,0.023512658663094044]Text: [Logistic, regression, models, are, neat] =&gt;Vector: [0.038572299480438235,-0.03250147425569594,-0.01552378609776497] Spark’s Word2Vec implementation includes a variety of tuning parameters that can be found in the documentation. Spark 的 Word2Vec 实现包括各种调整参数，这可以在文档中找到。 Feature Manipulation特征操作While nearly every transformer in ML manipulates the feature space in some way, the following algorithms and tools are automated means of either expanding the input feature vectors or reducing them to a lower number of dimensions. 虽然 ML 中的几乎每个转换器都以某种方式操纵特征空间，但以下算法和工具是扩展输入特征向量或将它们减少到较低维数的自动化方法。 PCA 主成分分析Principal Components Analysis (PCA) is a mathematical technique for finding the most important aspects of our data (the principal components). It changes the feature representation of our data by creating a new set of features (“aspects”). Each new feature is a combination of the original features. The power of PCA is that it can create a smaller set of more meaningful features to be input into your model, at the potential cost of interpretability. 主成分分析（PCA）是一种用于查找数据最重要层面（主要成分）的数学技术。它通过创建一组新特征（“方面”）来更改数据的特征表示。每个新特征都是原始特征的组合。PCA的强大之处在于它可以创建一组更小的更有意义的特征，以便以可解释的潜在成本输入到您的模型中。 You’d want to use PCA if you have a large input dataset and want to reduce the total number of features you have. This frequently comes up in text analysis where the entire feature space is massive and many of the features are largely irrelevant. Using PCA, we can find the most important combinations of features and only include those in our machine learning model. PCA takes a parameter , specifying the number of output features to create. Generally, this should be much smaller than your input vectors’ dimension. 如果您有大量输入数据集并希望减少所拥有的特征总数，则需要使用 PCA。这经常出现在文本分析中，其中整个特征空间是巨大的，并且许多特征在很大程度上是无关紧要的。使用 PCA，我们可以找到最重要的特征组合，并且只包括我们的机器学习模型中的特征组合。 PCA 接受一个参数，指定要创建的输出要素的数量。通常，这应该比输入向量的维度小得多。 NOTE 注意Picking the right is nontrivial and there’s no prescription we can give. Check out the relevant chapters in ESL and ISL for more information. 挑选对的特征是非常重要的，我们无法给予处方。有关更多信息，请查看 ESL 和 ISL 中的相关章节。 Let’s train PCA with a of 2: 让我们训练 PCA 的 2： 1234// in Scalaimport org.apache.spark.ml.feature.PCAval pca = new PCA().setInputCol("features").setK(2)pca.fit(scaleDF).transform(scaleDF).show(false) 1234# in Pythonfrom pyspark.ml.feature import PCApca = PCA().setInputCol("features").setK(2)pca.fit(scaleDF).transform(scaleDF).show(20, False) 1234567+---+--------------+------------------------------------------+|id | features | pca_7c5c4aa7674e__output |+---+--------------+------------------------------------------+|0 |[1.0,0.1,-1.0]|[0.0713719499248418,-0.4526654888147822] |...|1 |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060150646]|+---+--------------+------------------------------------------+ Interaction 相互作用In some cases, you might have domain knowledge about specific variables in your dataset. For example, you might know that a certain interaction between the two variables is an important variable to include in a downstream estimator. The feature transformer Interaction allows you to create an interaction between two variables manually. It just multiplies the two features together—something that a typical linear model would not do for every possible pair of features in your data. This transformer is currently only available directly in Scala but can be called from any language using the RFormula. We recommend users just use RFormula instead of manually creating interactions. 在某些情况下，您可能拥有有关数据集中特定变量的领域知识。例如，您可能知道两个变量之间的某种相互作用是包含在下游估算器中的重要变量。特征转换器 Interaction 允许您手动创建两个变量之间的交互。它只是将两个特征相乘——这是典型的线性模型不能为数据中的每个可能的特征对做的事情。此转换器目前只能在 Scala 中直接使用，但可以使用 RFormula 从任何语言调用。我们推荐用户只使用 RFormula 而不是手动创建交互。 Polynomial Expansion 多项式扩展Polynomial expansion is used to generate interaction variables of all the input columns. With polynomial expansion, we specify to what degree we would like to see various interactions. For example, for a degree-2 polynomial, Spark takes every value in our feature vector, multiplies it by every other value in the feature vector, and then stores the results as features. For instance, if we have two input features, we’ll get four output features if we use a second degree polynomial (2x2). If we have three input features, we’ll get nine output features (3x3). If we use a third-degree polynomial, we’ll get 27 output features (3x3x3) and so on. This transformation is useful when you want to see interactions between particular features but aren’t necessarily sure about which interactions to consider. 多项式展开用于生成所有输入列的交互变量。通过多项式展开，我们指定了我们希望看到各种交互的维度。例如，对于维度为2的多项式，Spark 会获取特征向量中的每个值，将其乘以特征向量中的每个其他值，然后将结果存储为特征。例如，如果我们有两个输入特征，如果我们使用二次多项式（2x2），我们将得到四个输出特征。如果我们有三个输入特征，我们将获得九个输出特征（3x3）。如果我们使用三次多项式，我们将获得27个输出特征（3x3x3），依此类推。当您想要查看特定之间的交互但不一定确定要考虑哪些交互时，此转换很有用。 WARNING 警告Polynomial expansion can greatly increase your feature space, leading to both high computational costs and overfitting. Use it with caution, especially for higher degrees. 多项式扩展可以极大地增加您的特征空间，从而导致高计算成本和过度拟合。请谨慎使用，特别是对于更高的度数。 Here’s an example of a second degree polynomial: 这是二次多项式的一个例子： 1234// in Scalaimport org.apache.spark.ml.feature.PolynomialExpansionval pe = new PolynomialExpansion().setInputCol("features").setDegree(2)pe.transform(scaleDF).show(false) 1234# in Pythonfrom pyspark.ml.feature import PolynomialExpansionpe = PolynomialExpansion().setInputCol("features").setDegree(2)pe.transform(scaleDF).show() 1234567+---+--------------+-----------------------------------------------------------+|id | features | poly_9b2e603812cb__output |+---+--------------+-----------------------------------------------------------+| 0 |[1.0,0.1,-1.0]|[1.0,1.0,0.1,0.1,0.010000000000000002,-1.0,-1.0,-0.1,1.0] |...| 1 |[3.0,10.1,3.0]|[3.0,9.0,10.1,30.299999999999997,102.00999999999999,3.0... |+---+--------------+-----------------------------------------------------------+ Feature SelectionOften, you will have a large range of possible features and want to select a smaller subset to use for training. For example, many features might be correlated, or using too many features might lead to overfitting. This process is called feature selection. There are a number of ways to evaluate feature importance once you’ve trained a model but another option is to do some rough filtering beforehand. Spark has some simple options for doing that, such as ChiSqSelector. 通常，您将拥有大量可能的特征，并希望选择较小的子集用于训练。例如，许多特征可能是相关的，或者使用太多特征可能会导致过拟合。此过程称为特征选择。一旦您训练了模型，有很多方法可以评估特征重要性，但另一种方法是事先进行粗略过滤。 Spark有一些简单的选项，比如 ChiSqSelector （卡方选择器）。 ChiSqSelector 卡方选择器ChiSqSelector leverages a statistical test to identify features that are not independent from the label we are trying to predict, and drop the uncorrelated features. It’s often used with categorical data in order to reduce the number of features you will input into your model, as well as to reduce the dimensionality of text data (in the form of frequencies or counts). Since this method is based on the Chi-Square test, there are several different ways we can pick the “best” features. The methods are numTopFeatures, which is ordered by p-value; percentile, which takes a proportion of the input features (instead of just the top N features); and fpr, which sets a cut off p-value. ChiSqSelector 利用统计测试来识别与我们试图预测的标签无关的特征，并删除不相关的特征。它通常与分类数据一起使用，以减少您将输入到模型中的特征数量，以及减少文本数据的维度（以频率或计数的形式）。由于此方法基于卡方检验，因此有几种不同的方法可以选择“最佳”特征。方法是 numTopFeatures，按p值排序；百分位数，它占用一部分输入特征（而不仅仅是前N个特征）; 和 fpr，它设置了一个截止的p值。 We will demonstrate this with the output of the CountVectorizer created earlier in this chapter: 我们将使用本章前面创建的 CountVectorizer 的输出来演示这一点： 12345678910111213// in Scalaimport org.apache.spark.ml.feature.&#123;ChiSqSelector, Tokenizer&#125;val tkn = new Tokenizer().setInputCol("Description").setOutputCol("DescOut")val tokenized = tkn.transform(sales.select("Description", "CustomerId")).where("CustomerId IS NOT NULL")val prechi = fittedCV.transform(tokenized)val chisq = new ChiSqSelector().setFeaturesCol("countVec").setLabelCol("CustomerId").setNumTopFeatures(2)chisq.fit(prechi).transform(prechi).drop("customerId", "Description", "DescOut").show() 1234567891011121314# in Pythonfrom pyspark.ml.feature import ChiSqSelector, Tokenizertkn = Tokenizer().setInputCol("Description").setOutputCol("DescOut")tokenized = tkn\.transform(sales.select("Description", "CustomerId"))\.where("CustomerId IS NOT NULL")prechi = fittedCV.transform(tokenized)\.where("CustomerId IS NOT NULL")chisq = ChiSqSelector()\.setFeaturesCol("countVec")\.setLabelCol("CustomerId")\.setNumTopFeatures(2)chisq.fit(prechi).transform(prechi)\.drop("customerId", "Description", "DescOut").show() Advanced Topics 高级主题There are several advanced topics surrounding transformers and estimators. Here we touch on the two most common, persisting transformers as well as writing custom ones. Persisting Transformers Once you’ve used an estimator to configure a transformer, it can be helpful to write it to disk and simply load it when necessary (e.g., for use in another Spark session). We saw this in the previous chapter when we persisted an entire pipeline. To persist a transformer individually, we use the write method on the fitted transformer (or the standard transformer) and specify the location : 围绕转换器和估计器有几个高级主题。 在这里，我们触及两个最常见的，持久化的转换器以及编写自定义的转换器。 持久化的转换器一旦使用了估计器来配置转换器，将其写入磁盘并在必要时简单地加载（例如，用于另一个Spark会话）会很有帮助。 我们在上一章中看到了这一点，当时我们持久化整个管道。 为了单独持久化转换器，我们在已经拟合的转换器（或标准转换器）上使用写入方法并指定位置： 123// in Scalaval fittedPCA = pca.fit(scaleDF)fittedPCA.write.overwrite().save("/tmp/fittedPCA") 123# in PythonfittedPCA = pca.fit(scaleDF)fittedPCA.write().overwrite().save("/tmp/fittedPCA") We can then load it back in: 我们可以加载回来： 1234// in Scalaimport org.apache.spark.ml.feature.PCAModelval loadedPCA = PCAModel.load("/tmp/fittedPCA")loadedPCA.transform(scaleDF).show() 1234# in Pythonfrom pyspark.ml.feature import PCAModelloadedPCA = PCAModel.load("/tmp/fittedPCA")loadedPCA.transform(scaleDF).show() Writing a Custom Transformer 编写自定义转换器Writing a custom transformer can be valuable when you want to encode some of your own business logic in a form that you can fit into an ML Pipeline, pass on to hyperparameter search, and so on. In general you should try to use the built-in modules (e.g., SQLTransformer) as much as possible because they are optimized to run efficiently. But sometimes we do not have that luxury. Let’s create a simple tokenizer to demonstrate: 当您想要以适合ML管道的形式编码自己的一些业务逻辑，传递给超参数搜索等时，编写自定义转换器可能很有价值。 通常，您应该尝试尽可能多地使用内置模块（例如，SQLTransformer），因为它们经过优化可以高效运行。 但有时我们没有那么奢侈。 让我们创建一个简单的符号化器来演示： 123456789101112131415161718192021222324252627282930import org.apache.spark.ml.UnaryTransformerimport org.apache.spark.ml.util.&#123;DefaultParamsReadable, DefaultParamsWritable,Identifiable&#125;import org.apache.spark.sql.types.&#123;ArrayType, StringType, DataType&#125;import org.apache.spark.ml.param.&#123;IntParam, ParamValidators&#125;class MyTokenizer(override val uid: String) extends UnaryTransformer[String, Seq[String], MyTokenizer] with DefaultParamsWritable &#123; def this() = this(Identifiable.randomUID("myTokenizer")) val maxWords: IntParam = new IntParam(this, "maxWords", "The max number of words to return.", ParamValidators.gtEq(0)) def setMaxWords(value: Int): this.type = set(maxWords, value) def getMaxWords: Integer = $(maxWords) override protected def createTransformFunc: String =&gt; Seq[String] = ( inputString: String) =&gt; &#123; inputString.split("\\s").take($(maxWords)) &#125; override protected def validateInputType(inputType: DataType): Unit = &#123; require(inputType == StringType, s"Bad input type: $inputType. Requires String.") &#125; override protected def outputDataType: DataType = new ArrayType(StringType,true)&#125; // this will allow you to read it back in by using this object.object MyTokenizer extends DefaultParamsReadable[MyTokenizer]val myT = new MyTokenizer().setInputCol("someCol").setMaxWords(2)myT.transform(Seq("hello world. This text won't show.").toDF("someCol")).show() It is also possible to write a custom estimator where you must customize the transformation based on the actual input data. However, this isn’t as common as writing a standalone transformer and is therefore not included in this book. A good way to do this is to look at one of the simple estimators we saw before and modify the code to suit your use case. A good place to start might be the StandardScaler. 也可以编写自定义估计器，您必须根据实际输入数据自定义转换。但是，这并不像编写独立转换器那么常见，因此不包含在本书中。这样做的一个好方法是查看我们之前看到的一个简单估计器并修改代码以适合您的用例。一个好的起点可能是StandardScaler。 Conclusion 结论This chapter gave a whirlwind tour of many of the most common preprocessing transformations Spark has available. There are several domain-specific ones we did not have enough room to cover (e.g., Discrete Cosine Transform), but you can find more information in the documentation. This area of Spark is also constantly growing as the community develops new ones. 本章对Spark提供的许多最常见的预处理转换进行了快速之旅。有几个特定于某些领域的，我们没有足够的空间来覆盖（例如，离散余弦变换），但您可以在文档中找到更多信息。随着社区发展新领域，Spark的这一领域也在不断发展。 Another important aspect of this feature engineering toolkit is consistency. In the previous chapter we covered the pipeline concept, an essential tool to package and train end-to-end ML workflows. In the next chapter we will start going through the variety of machine learning tasks you may have and what algorithms are available for each one. 此特征工程工具包的另一个重要方面是一致性。在上一章中，我们介绍了管道概念，它是打包和训练端到端ML工作流程的重要工具。在下一章中，我们将开始介绍您可能拥有的各种机器学习任务以及每种机器可用的算法。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 24 Advanced Analytics and Machine Learning Overview]]></title>
    <url>%2F2019%2F08%2F20%2FChapter24_Advanced-Analytics-and-Machine-Learning-Overview(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 24 Advanced Analytics and Machine Learning OverviewThus far, we have covered fairly general data flow APIs. This part of the book will dive deeper into some of the more specific advanced analytics APIs available in Spark. Beyond large-scale SQL analysis and streaming, Spark also provides support for statistics, machine learning, and graph analytics. These encompass a set of workloads that we will refer to as advanced analytics. This part of the book will cover advanced analytics tools in Spark, including : 到目前为止，我们已经涵盖了相当通用的数据流 API。本书的这一部分将深入探讨 Spark 中可用的一些更具体的高级分析 API。除了大规模的 SQL 分析和流媒体，Spark 还提供对统计，机器学习和图形分析的支持。这些包含一套工作负荷，我们将其称为高级分析。本书的这一部分将介绍Spark中的高级分析工具，包括： Preprocessing your data (cleaning data and feature engineering) 预处理数据（清理数据和特征工程） Supervised learning 监督学习 Recommendation learning 推荐学习 Unsupervised engines 无人监督引擎 Graph analytics 图形分析 Deep learning 深度学习 This chapter offers a basic overview of advanced analytics, some example use cases, and a basic advanced analytics workflow. Then we’ll cover the analytics tools just listed and teach you how to apply them. 本章提供高级分析的基本概述，一些用户案例的示例和基本的高级分析工作流。然后我们将介绍刚刚列出的分析工具，并教您如何应用它们。 WARNING 警告This book is not intended to teach you everything you need to know about machine learning from scratch. We won’t go into strict mathematical definitions and formulations—not for lack of importance but simply because it’s too much information to include. This part of the book is not an algorithm guide that will teach you the mathematical underpinnings of every available algorithm nor the in-depth implementation strategies used. The chapters included here serve as a guide for users, with the purpose of outlining what you need to know to use Spark’s advanced analytics APIs. 本书无意向您介绍从头开始学习机器学习所需的所有知识。我们不会进入严格的数学定义和表述——不是因为缺乏重要性，而仅仅因为它包含太多的信息。本书的这一部分不是一个算法指南，它将教你每个可用算法的数学基础，也不会使用深入的实现策略。此处包含的章节可作为用户指南，旨在概述使用Spark的高级分析API需要了解的内容。 A Short Primer on Advanced Analytics 高级分析的简短入门Advanced analytics refers to a variety of techniques aimed at solving the core problem of deriving insights and making predictions or recommendations based on data. The best ontology for machine learning is structured based on the task that you’d like to perform. The most common tasks include: 高级分析是指旨在解决获取洞察力并根据数据进行预测或推荐的核心问题的各种技术。机器学习的最佳本体是基于您想要执行的任务而构建的。最常见的任务包括： Supervised learning, including classification and regression, where the goal is to predict a label for each data point based on various features. 监督学习，包括分类和回归，其目标是基于各种特征预测每个数据点的标签。 Recommendation engines to suggest products to users based on behavior. 推荐引擎，根据行为向用户推荐产品。 Unsupervised learning, including clustering, anomaly detection, and topic modeling, where the goal is to discover structure in the data. 无监督学习，包括聚类，异常检测和主题建模，其目标是发现数据中的结构。 Graph analytics tasks such as searching for patterns in a social network. 图形分析任务，例如在社交网络中搜索模式。 Before discussing Spark’s APIs in detail, let’s review each of these tasks along with some common machine learning and advanced analytics use cases. While we have certainly tried to make this introduction as accessible as possible, at times you may need to consult other resources in order to fully understand the material. O’Reilly should we link to or mention any specific ones? Additionally, we will cite the following books throughout the next few chapters because they are great resources for learning more about the individual analytics (and, as a bonus, they are freely available on the web): 在详细讨论Spark的API之前，让我们回顾一下这些任务以及一些常见的机器学习和高级分析用户案例。虽然我们确实试图尽可能地使用这种介绍，但有时您可能需要咨询其他资源以便完全理解材料。 O’Reilly应该链接或提及任何特定的？此外，我们将在接下来的几章中引用以下书籍，因为它们是了解有关个人分析的更多资源（并且，作为奖励，它们可以在网上免费获得）： An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. We refer to this book as “ISL.” Gareth James，Daniela Witten，Trevor Hastie 和 Robert Tibshirani 的统计学习导论简介。我们将这本书称为“ISL”。 Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman. We refer to this book as “ESL.” Trevor Hastie，Robert Tibshirani 和 Jerome Friedman 的统计学习基础。我们将这本书称为“ESL”。 Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. We refer to this book as “DLB.” Ian Goodfellow，Yoshua Bengio 和 Aaron Courville 的深度学习。我们将这本书称为“DLB”。 Supervised Learning 监督学习Supervised learning is probably the most common type of machine learning. The goal is simple: using historical data that already has labels (often called the dependent variables), train a model to predict the values of those labels based on various features of the data points. One example would be to predict a person’s income (the dependent variables) based on age (a feature). This training process usually proceeds through an iterative optimization algorithm such as gradient descent. The training algorithm starts with a basic model and gradually improves it by adjusting various internal parameters (coefficients) during each training iteration. The result of this process is a trained model that you can use to make predictions on new data. There are a number of different tasks we’ll need to complete as part of the process of training and making predictions, such as measuring the success of trained models before using them in the field, but the fundamental principle is simple: train on historical data, ensure that it generalizes to data we didn’t train on, and then make predictions on new data. 有监督的学习可能是最常见的机器学习类型。目标很简单：使用已经有标签的历史数据（通常称为因变量），训练模型根据数据点的各种特征预测这些标签的值。一个例子是根据年龄（特征）预测一个人的收入（因变量）。该训练过程通常通过诸如梯度下降的迭代优化算法进行。训练算法从基本模型开始，并通过在每次训练迭代期间调整各种内部参数（系数）来逐步改进它。此过程的结果是经过训练的模型，您可以使用该模型对新数据进行预测。作为训练和预测过程的一部分，我们需要完成许多不同的任务，例如在现场使用之前测量训练模型的成功，但基本原则很简单：训练历史数据，确保它推广到我们没有训练的数据，然后对新数据进行预测。 We can further organize supervised learning based on the type of variable we’re looking to predict. We’ll get to that next. 我们可以根据我们想要预测的变量类型进一步组织有监督的学习。我们接下来会谈到这一点。 Classification 分类One common type of supervised learning is classification. Classification is the act of training an algorithm to predict a dependent variable that is categorical (belonging to a discrete, finite set of values). The most common case is binary classification, where our resulting model will make a prediction that a given item belongs to one of two groups. The canonical example is classifying email spam. Using a set of historical emails that are organized into groups of spam emails and not spam emails, we train an algorithm to analyze the words in, and any number of properties of, the historical emails and make predictions about them. Once we are satisfied with the algorithm’s performance, we use that model to make predictions about future emails the model has never seen before. 一种常见的监督学习类型是分类。分类是训练算法以预测分类的因变量（属于离散的，有限的值集）的行为。最常见的情况是二元分类，其中我们得到的模型将预测给定项目属于两个组中的一个。典型的例子是对垃圾邮件进行分类。使用一组历史电子邮件，这些电子邮件被组织成垃圾邮件组而不是垃圾邮件，我们训练一种算法来分析历史电子邮件中的单词和任意数量的属性，并对它们进行预测。一旦我们对算法的性能感到满意，我们就会使用该模型来预测模型以前从未见过的未来电子邮件。 When we classify items into more than just two categories, we call this multiclass classification. For example, we may have four different categories of email (as opposed to the two categories in the previous paragraph): spam, personal, work related, and other. There are many use cases for classification, including: 当我们将项目分类为两个以上的类别时，我们称之为多类分类。例如，我们可能有四种不同类别的电子邮件（与前一段中的两个类别相对）：垃圾邮件，个人电子邮件，与工作相关的邮件和其他类别。分类有很多用户案例，包括： Predicting disease 预测疾病 A doctor or hospital might have a historical dataset of behavioral and physiological attributes of a set of patients. They could use this dataset to train a model on this historical data (and evaluate its success and ethical implications before applying it) and then leverage it to predict whether or not a patient has heart disease or not. This is an example of binary classification (healthy heart, unhealthy heart) or multiclass classification (healthy heart, or one of several different diseases).医生或医院可能具有一组患者的行为和生理属性的历史数据集。他们可以使用此数据集来训练该历史数据的模型（并在应用之前评估其成功和伦理影响），然后利用它来预测患者是否患有心脏病。这是二元分类（健康的心脏，不健康的心脏）或多类分类（健康的心脏，或几种不同的疾病之一）的例子。 Classifying images 分类图像 There are a number of applications from companies like Apple, Google, or Facebook that can predict who is in a given photo by running a classification model that has been trained on historical images of people in your past photos. Another common use case is to classify images or label the objects in images. Apple，Google或Facebook等公司提供的许多应用程序可以通过运行已经过过去照片中人物历史图像训练的分类模型来预测给定照片中的人物。另一个常见用户案例是对图像进行分类或标记图像中的对象。 Predicting customer churn 预测客户流失 A more business-oriented use case might be predicting customer churn—that is, which customers are likely to stop using a service. You can do this by training a binary classifier on past customers that have churned (and not churned) and using it to try and predict whether or not current customers will churn. 更加面向业务的用户案例可能预测客户流失——也就是说，哪些客户可能会停止使用服务。您可以通过对过去已经搅动（而非搅动）的客户训练二元分类器并使用它来尝试预测当前客户是否会流失来实现此目的。 Buy or won’t buy 买或不买 Companies often want to predict whether visitors of their website will purchase a given product. They might use information about users’ browsing pattern or attributes such as location in order to drive this prediction. 公司通常希望预测其网站的访问者是否会购买特定产品。他们可能会使用有关用户浏览模式或属性（如位置）的信息来推动此预测。 There are many more use cases for classification beyond these examples. We will introduce more use cases, as well as Spark’s classification APIs, in Chapter 26. 除了这些示例之外，还有更多用于分类的用户案例。我们将在第26章介绍更多用户案例以及 Spark 的分类API。 Regression 回归In classification, our dependent variable is a set of discrete values. In regression, we instead try to predict a continuous variable (a real number). In simplest terms, rather than predicting a category, we want to predict a value on a number line. The rest of the process is largely the same, which is why they’re both forms of supervised learning. We will train on historical data to make predictions about data we have never seen. Here are some typical examples: 在分类中，我们的因变量是一组离散值。在回归中，我们改为尝试预测连续变量（实数）。简单来说，我们希望预测数字行上的值，而不是预测类别。其余的过程基本相同，这就是为什么它们都是监督学习的形式。我们将对历史数据进行训练，以预测我们从未见过的数据。以下是一些典型示例： Predicting sales 预测销售额 A store may want to predict total product sales on given data using historical sales data. There are a number of potential input variables, but a simple example might be using last week’s sales data to predict the next day’s data. 商店可能希望使用历史销售数据预测给定数据的总产品销售额。有许多潜在的输入变量，但一个简单的例子可能是使用上周的销售数据来预测第二天的数据。 Predicting height 预测身高 Based on the heights of two individuals, we might want to predict the heights of their potential children. 基于两个人的高度，我们可能想要预测他们可能孩子的高度。 Predicting the number of viewers of a show 预测节目观众的数量 A media company like Netflix might try to predict how many of their subscribers will watch a particular show. 像Netflix这样的媒体公司可能会试图预测他们有多少用户会观看特定的节目。 We will introduce more use cases, as well as Spark’s methods for regression, in Chapter 27. 我们将在第27章介绍更多用户案例以及Spark的回归方法。 Recommendation 推荐Recommendation is one of the most intuitive applications of advanced analytics. By studying people’s explicit preferences (through ratings) or implicit ones (through observed behavior) for various products or items, an algorithm can make recommendations on what a user may like by drawing similarities between the users or items. By looking at these similarities, the algorithm makes recommendations to users based on what similar users liked, or what other products resemble the ones the user already purchased. Recommendation is a common use case for Spark and well suited to big data. Here are some example use cases: 推荐是高级分析最直观的应用之一。通过研究人们对各种产品或项目的明确偏好（通过评级）或隐性（通过观察到的行为），算法可以通过绘制用户或项目之间的相似性来对用户可能喜欢的内容做出推荐。通过查看这些相似性，该算法基于类似用户喜欢的内容或其他产品类似于用户已购买的产品向用户推荐。推荐是Spark的常见用户案例，非常适合大数据。以下是一些示例用户案例： Movie recommendations 电影推荐 Netflix uses Spark, although not necessarily its built-in libraries, to make large-scale movie recommendations to its users. It does this by studying what movies users watch and do not watch in the Netflix application. In addition, Netflix likely takes into consideration how similar a given user’s ratings are to other users’. Netflix使用Spark，尽管不一定是其内置库，向用户提供大规模的电影推荐。它是通过研究用户在Netflix应用程序中观看和不观看的电影来实现的。此外，Netflix可能会考虑给定的用户评级与其他用户的相似程度。 Product recommendations 产品推荐 Amazon uses product recommendations as one of its main tools to increase sales. For instance, based on the items in our shopping cart, Amazon may recommend other items that were added to similar shopping carts in the past. Likewise, on every product page, Amazon shows similar products purchased by other users. 亚马逊将产品推荐作为增加销售额的主要工具之一。例如，根据购物车中的商品，亚马逊可能会推荐过去添加到类似购物车中的其他商品。同样，在每个产品页面上，亚马逊都会显示其他用户购买的类似产品。 We will introduce more recommendation use cases, as well as Spark’s methods for generating recommendations, in Chapter 28. 我们将在第28章介绍更多推荐用户案例，以及 Spark 用于生成推荐的方法。 Unsupervised Learning 无监督学习Unsupervised learning is the act of trying to find patterns or discover the underlying structure in a given set of data. This differs from supervised learning because there is no dependent variable (label) to predict. 无监督学习是试图在给定数据集中找到模式或发现底层结构的行为。这与监督学习不同，因为没有因变量（标签）来预测。 Some example use cases for unsupervised learning include: 用于无监督学习的一些示例用户案例包括： Anomaly detection 异常检测 Given some standard event type often occuring over time, we might want to report when a nonstandard type of event occurs. For example, a security officer might want to receive notifications when a strange object (think vehicle, skater, or bicyclist) is observed on a pathway. 鉴于某些标准事件类型经常出现，我们可能希望报告何时发生非标准类型的事件。例如，当在道路上观察到奇怪的物体（想想车辆，滑板或骑自行车者）时，安保人员可能希望接收通知。 User segmentation 用户细分 Given a set of user behaviors, we might want to better understand what attributes certain users share with other users. For instance, a gaming company might cluster users based on properties like the number of hours played in a given game. The algorithm might reveal that casual players have very different behavior than hardcore gamers, for example, and allow the company to offer different recommendations or rewards to each player. 给定一组用户行为，我们可能希望更好地了解某些用户与其他用户共享的属性。例如，游戏公司可以基于诸如在给定游戏中玩的小时数之类的属性来聚集用户。例如，该算法可能揭示休闲玩家与硬核游戏玩家的行为非常不同，并允许公司向每个玩家提供不同的推荐或奖励。 Topic modeling 主题建模 Given a set of documents, we might analyze the different words contained therein to see if there is some underlying relation between them. For example, given a number of web pages on data analytics, a topic modeling algorithm can cluster them into pages about machine learning, SQL, streaming, and so on based on groups of words that are more common in one topic than in others. Intuitively, it is easy to see how segmenting customers could help a platform cater better to each set of users. However, it may be hard to discover whether or not this set of user segments is “correct”. For this reason, it can be difficult to determine whether a particular model is good or not. We will discuss unsupervised learning in detail in Chapter 29. 给定一组文档，我们可以分析其中包含的不同单词，看看它们之间是否存在某种潜在的关系。例如，给定大量关于数据分析的网页，主题建模算法可以基于在一个主题中比在其他主题中更常见的单词组将它们聚类到关于机器学习，SQL，流等的页面中。直观地，很容易看出细分客户如何帮助平台更好地满足每组用户。但是，可能很难发现这组用户段是否“正确”。因此，可能难以确定特定模型是否良好。我们将在第29章详细讨论无监督学习。 Graph Analytics 图表分析While less common than classification and regression, graph analytics is a powerful tool. Fundamentally, graph analytics is the study of structures in which we specify vertices (which are objects) and edges (which represent the relationships between those objects). For example, the vertices might represent people and products, and edges might represent a purchase. By looking at the properties of vertices and edges, we can better understand the connections between them and the overall structure of the graph. Since graphs are all about relationships, anything that specifies a relationship is a great use case for graph analytics. Some examples include: 虽然不如分类和回归常见，但图形分析是一种强大的工具。从根本上说，图形分析是对结构的研究，其中我们指定顶点（对象）和边（表示这些对象之间的关系）。例如，顶点可能代表人和产品，边可能代表购买。通过查看顶点和边的属性，我们可以更好地理解它们之间的连接以及图的整体结构。由于图表都是关系，因此指定关系的任何内容都是图表分析的一个很好的用户案例。一些例子包括： Fraud prediction 欺诈预测 Capital One uses Spark’s graph analytics capabilities to better understand fraud networks. By using historical fraudulent information (like phone numbers, addresses, or names) they discover fraudulent credit requests or transactions. For instance, any user accounts within two hops of a fraudulent phone number might be considered suspicious. Capital One使用Spark的图形分析功能来更好地了解欺诈网络。通过使用历史欺诈信息（如电话号码，地址或姓名），他们会发现欺诈性信用请求或交易。例如，欺诈性电话号码两跳内的任何用户帐户可能被视为可疑。 Anomaly detection 异常检测 By looking at how networks of individuals connect with one another, outliers and anomalies can be flagged for manual analysis. For instance, if typically in our data each vertex has ten edges associated with it and a given vertex only has one edge, that might be worth investigating as something strange. 通过观察个体网络如何相互连接，可以标记离群点和异常值以进行手动分析。例如，如果通常在我们的数据中，每个顶点都有10个与之关联的边，并且给定的顶点只有一个边，那么可能值得调查一些奇怪的东西。 Classification 分类 Given some facts about certain vertices in a network, you can classify other vertices according to their connection to the original node. For instance, if a certain individual is labeled as an influencer in a social network, we could classify other individuals with similar network structures as influencers. 给定关于网络中某些顶点的一些事实，您可以根据它们与原始节点的连接对其他顶点进行分类。例如，如果某个人被标记为社交网络中的影响者，我们可以将具有类似网络结构的其他个人归类为影响者。 Recommendation 推荐 Google’s original web recommendation algorithm, PageRank, is a graph algorithm that analyzes website relationships in order to rank the importance of web pages. For example, a web page that has a lot of links to it is ranked as more important than one with no links to it. We’ll discuss more examples of graph analytics in Chapter 30. Google 的原始网络推荐算法 PageRank 是一种图表算法，可以分析网站关系，以便对网页的重要性进行排名。例如，具有大量链接指向的网页被排名为比没有链接指向的网页更重要。我们将在第30章讨论更多图形分析的例子。 The Advanced Analytics Process 高级分析流程You should have a firm grasp of some fundamental use cases for machine learning and advanced analytics. However, finding a use case is only a small part of the actual advanced analytics process. There is a lot of work in preparing your data for analysis, testing different ways of modeling it, and evaluating these models. This section will provide structure to the overall analytics process and the steps we have to take to not just perform one of the tasks just outlined, but actually evaluate success objectively in order to understand whether or not we should apply our model to the real world (Figure 24-1). 您应该牢牢掌握机器学习和高级分析的一些基本用户案例。但是，查找用户案例只是实际高级分析过程的一小部分。在准备分析数据，测试不同的建模方法以及评估这些模型方面，有很多工作要做。本节将提供整体分析过程的结构，以及我们必须采取的步骤，不仅要执行刚刚概述的任务之一，而且要客观地评估成功，以便了解我们是否应该将我们的模型应用于现实世界（图24-1）。 The overall process involves, the following steps (with some variation): 整个过程涉及以下步骤（有一些变化）： Gathering and collecting the relevant data for your task. 归拢和收集任务的相关数据。 Cleaning and inspecting the data to better understand it. 清理和检查数据以更好地理解它。 Performing feature engineering to allow the algorithm to leverage the data in a suitable form (e.g., converting the data to numerical vectors). 执行特征工程以允许算法以适当的形式利用数据（例如，将数据转换为数字向量）。 Using a portion of this data as a training set to train one or more algorithms to generate some candidate models. 使用该数据的一部分作为训练集来训练一个或多个算法以生成一些候选模型。 Evaluating and comparing models against your success criteria by objectively measuring results on a subset of the same data that was not used for training. This allows you to better understand how your model may perform in the wild. 通过客观地测量未用于训练的相同数据的子集的结果，评估和比较模型与您的成功标准。这使您可以更好地了解模型在新数据上的表现。 Leveraging the insights from the above process and/or using the model to make predictions, detect anomalies, or solve more general business challenges. These steps won’t be the same for every advanced analytics task. However, this workflow does serve as a general framework for what you’re going to need to be successful with advanced analytics. Just as we did with the various advanced analytics tasks earlier in the chapter, let’s break down the process to better understand the overall objective of each step. 利用上述过程中的见解和/或使用模型进行预测，检测异常或解决更常见的业务挑战。每个高级分析任务的这些步骤都不相同。但是，此工作流程确实可作为使用高级分析成功所需的一般框架。正如我们在本章前面对各种高级分析任务所做的那样，让我们分解过程以更好地理解每个步骤的总体目标。 Data collection 数据收集Naturally it’s hard to create a training set without first collecting data. Typically this means at least gathering the datasets you’ll want to leverage to train your algorithm. Spark is an excellent tool for this because of its ability to speak to a variety of data sources and work with data big and small. 当然，在没有首先收集数据的情况下很难创建训练集。通常，这意味着至少要收集您想要用来训练算法的数据集。 Spark 是一个很好的工具，因为它能够与各种数据源对话并处理大小数据。 Data cleaning 数据清理After you’ve gathered the proper data, you’re going to need to clean and inspect it. This is typically done as part of a process called exploratory data analysis, or EDA. EDA generally means using interactive queries and visualization methods in order to better understand distributions, correlations, and other details in your data. During this process you may notice you need to remove some values that may have been misrecorded upstream or that other values may be missing. Whatever the case, it’s always good to know what is in your data to avoid mistakes down the road. The multitude of Spark functions in the structured APIs will provide a simple way to clean and report on your data. 在收集了适当的数据后，您将需要清理并检查它。这通常作为称为探索性数据分析（EDA）的过程的一部分来完成。 EDA通常意味着使用交互式查询和可视化方法，以便更好地理解数据中的分布，相关性和其他详细信息。在此过程中，您可能会注意到需要删除可能在上游错误记录的某些值或者可能缺少其他值。无论如何，最好知道数据中的内容，以避免错误发生。结构化API中的众多 Spark 函数将提供一种清理和报告数据的简单方法。 Feature engineering 特征工程Now that you collected and cleaned your dataset, it’s time to convert it to a form suitable for machine learning algorithms, which generally means numerical features. Proper feature engineering can often make or break a machine learning application, so this is one task you’ll want to do carefully. The process of feature engineering includes a variety of tasks, such as normalizing data, adding variables to represent the interactions of other variables, manipulating categorical variables, and converting them to the proper format to be input into our machine learning model. In MLlib, Spark’s machine learning library, all variables will usually have to be input as vectors of doubles (regardless of what they actually represent). We cover the process of feature engineering in great depth in Chapter 25. As you will see in that chapter, Spark provides the essentials you’ll need to manipulate your data using a variety of machine learning statistical techniques. 现在您已经收集并清理了数据集，现在是时候将其转换为适合机器学习算法的形式，这通常意味着数字特征。合适的特征工程通常可以创建或破坏机器学习应用程序，因此这是您需要仔细完成的一项任务。特征工程的过程包括各种任务，例如规范化数据，添加变量以表示其他变量的交互，控制分类变量，以及将它们转换为适当的格式以输入到我们的机器学习模型中。在 Spark 的机器学习库 MLlib 中，所有变量通常都必须作为双精度向量输入（无论它们实际代表什么）。我们将在第25章深入介绍特征工程的过程。正如您将在本章中看到的，Spark 提供了使用各种机器学习统计技术操作数据所需的基本知识。 NOTE 注意The following few steps (training models, model tuning, and evaluation) are not relevant to all use cases. This is a general workflow that may vary significantly based on the end objective you would like to achieve. 以下几个步骤（训练模型，模型调优和评估）与所有用户案例无关。 这是一个通用工作流程，可能会根据您希望实现的最终目标而有很大差异。 Training models 训练模型At this point in the process we have a dataset of historical information (e.g., spam or not spam emails) and a task we would like to complete (e.g., classifying spam emails). Next, we will want to train a model to predict the correct output, given some input. During the training process, the parameters inside of the model will change according to how well the model performed on the input data. For instance, to classify spam emails, our algorithm will likely find that certain words are better predictors of spam than others and therefore weight the parameters associated with those words higher. In the end, the trained model will find that certain words should have more influence (because of their consistent association with spam emails) than others. The output of the training process is what we call a model. Models can then be used to gain insights or to make future predictions. To make predictions, you will give the model an input and it will produce an output based on a mathematical manipulation of these inputs. Using the classification example, given the properties of an email, it will predict whether that email is spam or not by comparing to the historical spam and not spam emails that it was trained on. 在此过程中，我们拥有历史信息（例如，垃圾邮件或非垃圾邮件）的数据集以及我们想要完成的任务（例如，对垃圾邮件进行分类）。接下来，我们将需要训练模型来预测正确的输出，给定一些输入。在训练过程中，模型内部的参数将根据模型在输入数据上的执行情况而变化。例如，为了对垃圾邮件进行分类，我们的算法可能会发现某些单词比其他单词更好地预测垃圾邮件，因此将与这些单词相关联的参数加权更高。最后，受过训练的模型会发现某些单词应该比其他单词具有更大的影响力（因为它们与垃圾邮件的一致关联）。训练过程的输出就是我们所说的模型。然后可以使用模型来获得洞察力或进行未来预测。要进行预测，您将为模型提供输入，并根据这些输入的数学操作生成输出。使用分类这个例子，在给定电子邮件的属性的情况下，它将通过与历史垃圾邮件进行比较来预测该电子邮件是否为垃圾邮件，而不是通过与之进行过训练的垃圾邮件进行比较。 However, just training a model isn’t the objective—we want to leverage our model to produce insights. Thus, we must answer the question: how do we know our model is any good at what it’s supposed to do? That’s where model tuning and evaluation come in. 然而，仅仅训练模型并不是目标——我们希望利用我们的模型来产生洞察力。因此，我们必须回答这个问题：我们怎么知道我们的模型对它应该做的事情有多好？这就是模型调整和评估的用武之地。 Model tuning and evaluation 模型调整和评估You likely noticed earlier that we mentioned that you should split your data into multiple portions and use only one for training. This is an essential step in the machine learning process because when you build an advanced analytics model you want that model to generalize to data it has not seen before. Splitting our dataset into multiple portions allows us to objectively test the effectiveness of the trained model against a set of data that it has never seen before. The objective is to see if your model understands something fundamental about this data process or whether or not it just noticed the things particular to only the training set (sometimes called overfitting). That’s why it is called a test set. In the process of training models, we also might take another, separate subset of data and treat that as another type of test set, called a validation set, in order to try out different hyperparameters (parameters that affect the training process) and compare different variations of the same model without overfitting to the test set. 您之前可能已经注意到我们提到您应该将数据分成多个部分，并且只使用一个部分进行训练。这是机器学习过程中必不可少的一步，因为当您构建高级分析模型时，您希望该模型能够泛化到（推广到）之前从未见过的数据。将我们的数据集拆分成多个部分使我们能够客观地测试训练模型对一组前所未有的数据的有效性。目标是看你的模型是否理解这个数据过程的基本内容，或者它是否只注意到训练集特有的东西（有时称为过度拟合）。这就是为什么它被称为测试集。在训练模型的过程中，我们也可以采用另一个单独的数据子集，并将其视为另一种类型的测试集，称为验证集，以便尝试不同的超参数（影响训练过程的参数）并比较相同模型的不同变体（比如：维度相同，超参数不同）而不会过度拟合测试集。 WARNING 警告Following proper training, validation, and test set best practices is essential to successfully using machine learning. It’s easy to end up overfitting (training a model that does not generalize well to new data) if we do not properly isolate these sets of data. We cannot cover this problem in depth in this book, but almost any machine learning book will cover this topic. 经过适当的训练，验证和测试集最佳实践对于成功使用机器学习至关重要。如果我们没有正确地隔离这些数据集，很容易最终过度拟合（训练一个不能很好地泛化到新数据的模型）。我们无法在本书中深入探讨这个问题，但几乎所有的机器学习书都将涵盖这一主题。 To continue with the classification example we referenced previously, we have three sets of data: a training set for training models, a validation set for testing different variations of the models that we’re training, and lastly, a test set we will use for the final evaluation of our different model variations to see which one performed the best. 为了继续我们之前引用的分类示例，我们有三组数据：训练模型的训练集，用于测试我们正在训练的模型的不同变体的验证集，最后对于最后评估我们不同的模型变体，我们将使用测试集看看哪一个表现最好。 Leveraging the model and/or insights 利用模型和/或洞悉力After running the model through the training process and ending up with a well-performing model, you are now ready to use it! Taking your model to production can be a significant challenge in and of itself. We will discuss some tactics later on in this chapter. 在通过训练过程运行模型并最终获得性能良好的模型后，您现在可以使用它了！将您的模型投入生产可能是一项重大挑战。我们将在本章后面讨论一些策略。 Spark’s Advanced Analytics Toolkit Spark的高级分析工具包The previous overview is just an example workflow and doesn’t encompass all use cases or potential workflows. In addition, you probably noticed that we did not discuss Spark almost at all. This section will discuss Spark’s advanced analytics capabilities. Spark includes several core packages and many external packages for performing advanced analytics. The primary package is MLlib, which provides an interface for building machine learning pipelines. 前面的概述只是一个示例工作流程，并不包含所有用户案例或潜在的工作流程。另外，您可能已经注意到我们几乎没有讨论过 Spark。本节将讨论 Spark 的高级分析功能。 Spark 包含多个核心软件包和许多用于执行高级分析的外部软件包。主要包是 MLlib ，它提供了用于构建机器学习管道的接口。 What Is MLlib? 什么是MLlib？MLlib is a package, built on and included in Spark, that provides interfaces for gathering and cleaning data, feature engineering and feature selection, training and tuning large-scale supervised and unsupervised machine learning models, and using those models in production. MLlib 是一个基于 Spark 构建的软件包，它提供了用于收集和清理数据，特征工程和特征选择，训练和调优大规模监督和无监督机器学习模型以及在生产中使用这些模型的接口。 WARNING 警告MLlib actually consists of two packages that leverage different core data structures. The package org.apache.spark.ml includes an interface for use with DataFrames. This package also offers a high-level interface for building machine learning pipelines that help standardize the way in which you perform the preceding steps. The lower-level package, org.apache.spark.mllib, includes interfaces for Spark’s low-level RDD APIs. This book will focus exclusively on the DataFrame API. The RDD API is the lower-level interface, which is in maintenance mode (meaning it will only receive bug fixes, not new features) at this time. It has also been covered fairly extensively in older books on Spark and is therefore omitted here. MLlib 实际上由两个利用不同核心数据结构的包组成。包 org.apache.spark.ml 包含一个使用 DataFrames 的接口。该软件包还提供了一个用于构建机器学习管道的高层次接口，有助于标准化执行上述步骤的方式。较低层次的包 org.apache.spark.mllib 包含 Spark 的低层次 RDD API 的接口。本书将专注于 DataFrame API。 RDD API 是较低层次的接口，它处于维护模式（意味着它目前只接收错误修复，而不是新功能）。它在Spark的旧书中也得到了相当广泛的介绍，因此在此省略。 When and why should you use MLlib (versus scikit-learn, TensorFlow, or foo package) 何时以及为什么要使用MLlib（与scikit-learn，TensorFlow或foo包相比）At a high level, MLlib might sound like a lot of other machine learning packages you’ve probably heard of, such as scikit-learn for Python or the variety of R packages for performing similar tasks. So why should you bother with MLlib at all? There are numerous tools for performing machine learning on a single machine, and while there are several great options to choose from, these single machine tools do have their limits either in terms of the size of data you can train on or the processing time. 在很高的层次上，MLlib 可能听起来像你可能听说过的许多其他机器学习包，例如用于 Python 的 scikit-learn 或用于执行类似任务的各种 R 包。那你为什么要操心 MLlib 呢？有许多工具可以在一台机器上进行机器学习，虽然有几个很好的选择可供选择，但这些单机在你可以训练的数据大小或处理时间方面都有其局限性。 This means single-machine tools are usually complementary to MLlib. When you hit those scalability issues, take advantage of Spark’s abilities. 这意味着单机工具通常是 MLlib 的补充。当您遇到这些可伸缩性问题时，请充分利用Spark的功能。 There are two key use cases where you want to leverage Spark’s ability to scale. First, you want to leverage Spark for preprocessing and feature generation to reduce the amount of time it might take to produce training and test sets from a large amount of data. Then you might leverage single-machine learning libraries to train on those given data sets. Second, when your input data or model size become too difficult or inconvenient to put on one machine, use Spark to do the heavy lifting. Spark makes distributed machine learning very simple. 有两个关键用户案例，在这些案例中您希望利用Spark的伸缩能力。首先，您希望利用Spark进行预处理和特征生成，以减少从大量数据生成训练和测试集所需的时间。然后，您可以利用单机学习库来训练那些给定的数据集。其次，当您的输入数据或模型大小变得太难或不方便放在一台机器上时，请使用Spark来完成繁重的工作。 Spark使分布式机器学习变得非常简单。 An important caveat to all of this is that while training and data preparation are made simple, there are still some complexities you will need to keep in mind, especially when it comes to deploying a trained model. For example, Spark does not provide a built-in way to serve low-latency predictions from a model, so you may want to export the model to another serving system or a custom application to do that. MLlib is generally designed to allow inspecting and exporting models to other tools where possible. 所有这一切的一个重要警告是，虽然训练和数据准备工作变得简单，但仍需要记住一些复杂性，特别是在部署经过训练的模型时。例如，Spark 不提供从模型提供低延迟预测的内置方法，因此您可能希望将模型导出到另一个服务系统或自定义应用程序来执行此操作。 MLlib 通常设计为允许在可能的情况下检查和导出模型到其他工具。 High-Level MLlib ConceptsIn MLlib there are several fundamental “structural” types: transformers, estimators, evaluators, and pipelines. By structural, we mean you will think in terms of these types when you define an end-to-end machine learning pipeline. They’ll provide the common language for defining what belongs in what part of the pipeline. Figure 24-2 illustrates the overall workflow that you will follow when developing machine learning models in Spark. 在 MLlib 中，有几种基本的“结构”类型：转换器（transformer），估计器（estimator），评估器（evaluator）和管道（pipeline）。 通过结构，我们的意思是当您定义端到端机器学习管道时，您将考虑这些类型。 它们将提供用于定义属于管道的哪些部分的通用语言。 图24-2说明了在 Spark 中开发机器学习模型时要遵循的整体工作流程。 Transformers are functions that convert raw data in some way. This might be to create a new interaction variable (from two other variables), normalize a column, or simply change an Integer into a Double type to be input into a model. An example of a transformer is one that converts string categorical variables into numerical values that can be used in MLlib. Transformers are primarily used in preprocessing and feature engineering. Transformers take a DataFrame as input and produce a new DataFrame as output, as illustrated in Figure 24-3. 转换器（transformer）是以某种方式转换原始数据的函数。 这可能是创建一个新的交互变量（来自另外两个变量），规范化一个列，或者只是将一个 Integer 更改为 Double 类型以输入到模型中。 转换器（transformer）的示例是将字符串分类变量转换为可在 MLlib 中使用的数值的转换器（transformer）。 转换器（transformer）主要用于预处理和特征工程。 转换器（transformer）将 DataFrame 作为输入并生成一个新的 DataFrame 作为输出，如图 24-3 所示。 Estimators are one of two kinds of things. First, estimators can be a kind of transformer that is initialized with data. For instance, to normalize numerical data we’ll need to initialize our transformation with some information about the current values in the column we would like to normalize. This requires two passes over our data—the initial pass generates the initialization values and the second actually applies the generated function over the data. In the Spark’s nomenclature, algorithms that allow users to train a model from data are also referred to as estimators. 估计器（estimator）是两种事物之一。首先，估计器（estimator）可以是一种用数据初始化的转换器（transformer）。例如，为了标准化数值数据，我们需要使用有关我们想要标准化的列中的当前值的一些信息来初始化我们的转换。这需要对数据进行两次传递——初始传递生成初始化值，第二次实际应用于在数据上生成的函数。在Spark的命名法中，允许用户从数据训练模型的算法也称为估计器（estimator）。 An evaluator allows us to see how a given model performs according to criteria we specify like a receiver operating characteristic (ROC) curve. After we use an evaluator to select the best model from the ones we tested, we can then use that model to make predictions. 评估器（evaluator ）允许我们根据我们指定的标准（如受试者特性（ROC）曲线）查看给定模型的执行情况。在我们使用评估器（evaluator ）从我们测试的模型中选择最佳模型之后，我们可以使用该模型进行预测。 From a high level we can specify each of the transformations, estimations, and evaluations one by one, but it is often easier to specify our steps as stages in a pipeline. This pipeline is similar to scikit-learn’s pipeline concept. 从高层次我们可以逐个指定每个转换（ transformation），估计（estimation）和评估（evaluation），但通常更容易将我们的步骤指定为管道中的阶段。此管道类似于 scikit-learn 的管道概念。 Low-level data types 低层次数据类型In addition to the structural types for building pipelines, there are also several lower-level data types you may need to work with in MLlib (Vector being the most common). Whenever we pass a set of features into a machine learning model, we must do it as a vector that consists of Doubles. This vector can be either sparse (where most of the elements are zero) or dense (where there are many unique values). Vectors are created in different ways. To create a dense vector, we can specify an array of all the values. To create a sparse vector, we can specify the total size and the indices and values of the non-zero elements. Sparse is the best format, as you might have guessed, when the majority of values are zero as this is a more compressed representation. Here is an example of how to manually create a Vector: 除了构建管道的结构类型之外，还有一些您可能需要在 MLlib 中使用的较低层次的数据类型（向量（Vector）是最常见的）。每当我们将一组特征传递给机器学习模型时，我们必须将其作为由双精度（ Double ）组成的向量。此向量可以是稀疏的（大多数元素为零）或密集（有许多唯一值）。向量（Vector）以不同方式创建。要创建密集向量，我们可以指定所有值的数组。要创建稀疏向量，我们可以指定总大小以及非零元素的索引和值。当大多数值为零时，稀疏是最好的格式，正如您可能已经猜到的那样，因为这是一种更加压缩的表示。以下是如何手动创建向量（Vector）的示例： 123456789101112131415// in Scalaimport org.apache.spark.ml.linalg.Vectorsval denseVec = Vectors.dense(1.0, 2.0, 3.0)val size = 3val idx = Array(1,2) // locations of non-zero elements in vectorval values = Array(2.0,3.0)val sparseVec = Vectors.sparse(size, idx, values)sparseVec.toDensedenseVec.toSparse# in Pythonfrom pyspark.ml.linalg import VectorsdenseVec = Vectors.dense(1.0, 2.0, 3.0)size = 3idx = [1, 2] # locations of non-zero elements in vectorvalues = [2.0, 3.0]sparseVec = Vectors.sparse(size, idx, values) WARNING 警告Confusingly, there are similar datatypes that refer to ones that can be used in DataFrames and others that can only be used in RDDs. The RDD implementations fall under the mllib package while the DataFrame implementations fall under ml. 令人困惑的是，有类似的数据类型可以参考在 DataFrame 中使用的数据类型以及其他只能在RDD中使用的数据类型。 RDD 实现属于 mllib 包，而 DataFrame 实现属于 ml 。 MLlib in ActionNow that we have described some of the core pieces you can expect to come across, let’s create a simple pipeline to demonstrate each of the components. We’ll use a small synthetic dataset that will help illustrate our point. Let’s read the data in and see a sample before talking about it further: 现在我们已经描述了您可能会遇到的一些核心部分，让我们创建一个简单的管道来演示每个组件。 我们将使用一个小的合成数据集来帮助说明我们的观点。 在进一步讨论之前，让我们读取数据并查看示例： Here’s a sample of the data: 这是一个数据样本： 123// in Scalavar df = spark.read.json("/data/simple-ml")df.orderBy("value2").show() 123# in Pythondf = spark.read.json("/data/simple-ml")df.orderBy("value2").show() 123456789+-----+----+------+------------------+|color| lab|value1| value2|+-----+----+------+------------------+|green|good| 1|14.386294994851129||green| bad| 16|14.386294994851129|| blue| bad| 8|14.386294994851129|| blue| bad| 8|14.386294994851129|+-----+----+------+------------------+only showing top 20 rows This dataset consists of a categorical label with two values (good or bad), a categorical variable (color), and two numerical variables. While the data is synthetic, let’s imagine that this dataset represents a company’s customer health. The “color” column represents some categorical health rating made by a customer service representative. The “lab” column represents the true customer health. The other two values are some numerical measures of activity within an application (e.g., minutes spent on site and purchases). Suppose that we want to train a classification model where we hope to predict a binary variable—the label—from the other values. 此数据集由具有两个值（好或坏），分类变量（颜色）和两个数值变量的分类标签组成。虽然数据是合成的，但我们假设这个数据集代表了公司的客户健康状况。 “color”列表示由客户服务代表做出的某些分类健康评级。 “lab”列代表真正的客户健康状况。其他两个值是应用程序内活动的一些数字度量（例如，在站点和购买上花费的分钟数）。假设我们想要训练一个分类模型，我们希望从其他值预测二进制变量——标签。 TIP 提示Apart from JSON, there are some specific data formats commonly used for supervised learning, including LIBSVM. These formats have real valued labels and sparse input data. Spark can read and write for these formats using its data source API. Here’s an example of how to read in data from a libsvm file using that Data Source API. 除了 JSON 之外，还有一些通常用于监督学习的特定数据格式，包括 LIBSVM 。这些格式具有实值标签和稀疏输入数据。 Spark 可以使用其数据源 API 读取和写入这些格式。以下是如何使用该 Data Source API 从 libsvm 文件读取数据的示例。 1spark.read.format("libsvm").load("/data/sample_libsvm_data.txt") For more information on LIBSVM, see the documentation. 有关 LIBSVM 的更多信息，请参阅文档。 Feature Engineering with Transformers 使用Transformer进行特征工程As already mentioned, transformers help us manipulate our current columns in one way or another. Manipulating these columns is often in pursuit of building features (that we will input into our model). 如前所述，转换器（transformer）帮助我们以某种方式操纵当前列。操纵这些列通常是为了追求构建特征（我们将输入到我们的模型中）。 Transformers exist to either cut down the number of features, add more features, manipulate current ones, or simply to help us format our data correctly. Transformers add new columns to DataFrames. When we use MLlib, all inputs to machine learning algorithms (with several exceptions discussed in later chapters) in Spark must consist of type Double (for labels) and Vector[Double] (for features). 转换器（transformer）可以减少特征数量，添加更多特征，控制当前特征，或者只是帮助我们正确地格式化数据。转换器（transformer）向 DataFrames 添加新列。当我们使用 MLlib 时，Spark 中机器学习算法的所有输入（在后面的章节中讨论了几个例外）必须由类型 Double（用于标签）和 Vector[Double]（用于特征）。 The current dataset does not meet that requirement and therefore we need to transform it to the proper format. To achieve this in our example, we are going to specify an RFormula. This is a declarative language for specifying machine learning transformations and is simple to use once you understand the syntax. RFormula supports a limited subset of the R operators that in practice work quite well for simple models and manipulations (we demonstrate the manual approach to this problem in Chapter 25). The basic RFormula operators are: 当前数据集不符合该要求，因此我们需要将其转换为适当的格式。为了在我们的示例中实现这一点，我们将指定一个 RFormula。这是一种用于指定机器学习转换的声明性语言，一旦理解了语法，就很容易使用。 RFormula 支持R运算符的有限子集，这些运算符实际上对于简单模型和操作非常有效（我们在第25章中演示了解决此问题的手动方法）。基本的 RFormula 运算符（operator）是： 运算符operator 含义 ~ Separate target and terms分隔目标和项 + Concat terms; “+ 0” means removing the intercept (this means that the y-intercept of the line that we will fit will be 0)拼接项；“+0” 意味着移除截距（这意思是我们将拟合的直线的 y轴截距将会是0） - Remove a term; “- 1” means removing the intercept (this means that the y-intercept of the line that we will fit will be 0—yes, this does the same thing as “+ 0”移除一个项；“-1” 与 “-0” 做同样的事情 : Interaction (multiplication for numeric values, or binarized categorical values)交互（数值乘法或二进制分类值） . All columns except the target/dependent variable除目标/因变量之外的所有列 In order to specify transformations with this syntax, we need to import the relevant class. Then we go through the process of defining our formula. In this case we want to use all available variables (the .) and also add in the interactions between value1 and color and value2 and color, treating those as new features: 为了使用此语法指定转换（transformation），我们需要导入相关的类。 然后我们将完成定义公式的过程。 在这种情况下，我们想要使用所有可用的变量（.），并添加 value1 和 color 以及 value2 和 color 之间的交互，将它们视为新功能： 123// in Scalaimport org.apache.spark.ml.feature.RFormulaval supervised = new RFormula().setFormula("lab ~ . + color:value1 + color:value2") 123# in Pythonfrom pyspark.ml.feature import RFormulasupervised = RFormula(formula="lab ~ . + color:value1 + color:value2") At this point, we have declaratively specified how we would like to change our data into what we will train our model on. The next step is to fit the RFormula transformer to the data to let it discover the possible values of each column. Not all transformers have this requirement but because RFormula will automatically handle categorical variables for us, it needs to determine which columns are categorical and which are not, as well as what the distinct values of the categorical columns are. For this reason, we have to call the fit method. Once we call fit, it returns a “trained” version of our transformer we can then use to actually transform our data. 在这一点上，我们已声明指定我们如何将数据更改为我们将训练模型的内容。 下一步是将 RFormula 转换器（transformer）拟合数据，以便发现每列的可能值。 并非所有转换器（transformer）都有此要求，但由于RFormula 会自动为我们处理分类变量，因此需要确定哪些列是分类的，哪些不是，以及分类列的不同值是什么。 出于这个原因，我们必须调用 fit 方法。 一旦我们调用了 fit，它就会返回我们转换器（transformer）的“经过训练”的版本，然后我们可以使用它来实际转换我们的数据。 NOTE 注意We’re using the RFormula transformer because it makes performing several transformations extremely easy to do. In Chapter 25, we’ll show other ways to specify a similar set of transformations and outline the component parts of the RFormula when we cover the specific transformers in MLlib. 我们正在使用 RFormula 转换器（transformer），因为它使得执行几次变换非常容易。 在第25章中，我们将展示指定一组类似的转换（transformation）的其他方法，并在我们覆盖 MLlib 中的特定转换器（transformer）时概述 RFormula 的组成部分。 Now that we covered those details, let’s continue on and prepare our DataFrame: 现在我们已经介绍了这些细节，让我们继续并准备我们的 DataFrame： 1234// in Scalaval fittedRF = supervised.fit(df)val preparedDF = fittedRF.transform(df)preparedDF.show() 1234# in PythonfittedRF = supervised.fit(df)preparedDF = fittedRF.transform(df)preparedDF.show() Here’s the output from the training and transformation process: 以下是训练和转换过程的输出： 1234567+-----+----+------+------------------+----------------------------------------------------------------------+-----+|color|lab |value1|value2 |features |label|+-----+----+------+------------------+----------------------------------------------------------------------+-----+|green|good|1 |14.386294994851129|(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129]) |1.0 ||blue |bad |8 |14.386294994851129|(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129]) |0.0 ||blue |bad |12 |14.386294994851129|(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129]) |0.0 |+-----+----+------+------------------+----------------------------------------------------------------------+-----+ In the output we can see the result of our transformation—a column called features that has our previously raw data. What’s happening behind the scenes is actually pretty simple. RFormula inspects our data during the fit call and outputs an object that will transform our data according to the specified formula, which is called an RFormulaModel. This “trained” transformer always has the word Model in the type signature. 在输出中，我们可以看到转换的结果——一个名为 features 的列，它包含我们以前的原始数据。幕后发生的事实上非常简单。 RFormula 在 fit 调用期间检查我们的数据，并输出一个对象，该对象将根据指定的公式（称为 RFormulaModel ）转换我们的数据。这种“训练有素”的转换器（transformer）在类型签名中始终具有单词Model。 When we use this transformer, Spark automatically converts our categorical variable to Doubles so that we can input it into a (yet to be specified) machine learning model. In particular, it assigns a numerical value to each possible color category, creates additional features for the interaction variables between colors and value1/value2, and puts them all into a single vector. We then call transform on that object in order to transform our input data into the expected output data. 当我们使用这个转换器时，Spark会自动将我们的分类变量转换为双精度变量，以便我们可以将它输入到（尚未指定的）机器学习模型中。 特别是，它为每个可能的颜色类别指定一个数值，为颜色和 value1/value2 之间的交互变量创建额外的特征，并将它们全部放入一个单独向量中。 然后，我们调用该对象的变换，以便将输入数据转换为预期的输出数据。 Thus far you (pre)processed the data and added some features along the way. Now it is time to actually train a model (or a set of models) on this dataset. In order to do this, you first need to prepare a test set for evaluation. 到目前为止，您（之前）处理过数据并在此过程中添加了一些特征。现在是时候在这个数据集上实际训练一个模型（或一组模型）了。为此，您首先需要准备一个用于评估的测试集。 TIP 提示Having a good test set is probably the most important thing you can do to ensure you train a model you can actually use in the real world (in a dependable way). Not creating a representative test set or using your test set for hyperparameter tuning are surefire ways to create a model that does not perform well in real-world scenarios. Don’t skip creating a test set—it’s a requirement to know how well your model actually does! 拥有一个好的测试集可能是你可以做的最重要的事情，以确保你训练一个你可以在现实世界中实际使用的模型（以可靠的方式）。 不创建代表性测试集或使用测试集进行超参数调整是创建在实际场景中表现不佳的模型的绝对方法。 不要跳过创建测试集——这是要求了解模型的实际效果！ Let’s create a simple test set based off a random split of the data now (we’ll be using this test set throughout the remainder of the chapter): 让我们现在根据数据的随机分割创建一个简单的测试集（我们将在本章的其余部分使用这个测试集）： 12// in Scalaval Array(train, test) = preparedDF.randomSplit(Array(0.7, 0.3)) 12# in Pythontrain, test = preparedDF.randomSplit([0.7, 0.3]) Estimators 估计Now that we have transformed our data into the correct format and created some valuable features, it’s time to actually fit our model. In this case we will use a classification algorithm called logistic regression. To create our classifier we instantiate an instance of LogisticRegression, using the default configuration or hyperparameters. We then set the label columns and the feature columns; the column names we are setting—label and features—are actually the default labels for all estimators in Spark MLlib, and in later chapters we omit them: 现在我们已经将数据转换为正确的格式并创建了一些有价值的特征，现在是时候实际拟合我们的模型了。 在这种情况下，我们将使用称为逻辑回归的分类算法。 要创建我们的分类器，我们使用默认配置或超参数来实例化LogisticRegression 的实例。 然后我们设置标签列和特征列；我们设置的列名称——标签和特征——实际上是Spark MLlib 中所有估计器的默认标签，在后面的章节中我们忽略它们： 123// in Scalaimport org.apache.spark.ml.classification.LogisticRegressionval lr = new LogisticRegression().setLabelCol("label").setFeaturesCol("features") 123# in Pythonfrom pyspark.ml.classification import LogisticRegressionlr = LogisticRegression(labelCol="label",featuresCol="features") Before we actually go about training this model, let’s inspect the parameters. This is also a great way to remind yourself of the options available for each particular model: 在我们真正开始训练这个模型之前，让我们检查参数。 这也是提醒自己每种特定模型可用选项的好方法： 12// in Scalaprintln(lr.explainParams()) 12# in Pythonprint lr.explainParams() While the output is too large to reproduce here, it shows an explanation of all of the parameters for Spark’s implementation of logistic regression. The explainParams method exists on all algorithms available in MLlib. 虽然输出太大而无法在此重现，但它显示了 Spark 实现逻辑回归的所有参数的解释。 在 MLlib 中可用的所有算法都有 explainParams 方法。 Upon instantiating an untrained algorithm, it becomes time to fit it to data. In this case, this returns a LogisticRegressionModel: 在实例化未经训练的算法时，就可以将它与数据相弥合。在这种情况下，这将返回 LogisticRegressionModel： 12// in Scalaval fittedLR = lr.fit(train) 12# in PythonfittedLR = lr.fit(train) This code will kick off a Spark job to train the model. As opposed to the transformations that you saw throughout the book, the fitting of a machine learning model is eager and performed immediately. 此代码将启动 Spark 作业以训练模型。 与您在本书中看到的转换相反，机器学习模型的拟合非常迫切并且立即执行。 Once complete, you can use the model to make predictions. Logically this means tranforming features into labels. We make predictions with the transform method. For example, we can transform our training dataset to see what labels our model assigned to the training data and how those compare to the true outputs. This, again, is just another DataFrame we can manipulate. Let’s perform that prediction with the following code snippet: 完成后，您可以使用该模型进行预测。 从逻辑上讲，这意味着将特征转换为标签。 我们使用transform方法进行预测。 例如，我们可以转换训练数据集，以查看我们的模型分配给训练数据的标签以及这些标签与真实输出的比较。 这又是我们可以操作的另一个DataFrame。 让我们使用以下代码片段执行该预测： 1fittedLR.transform(train).select("label", "prediction").show() This results in: 结果： 1234567+-----+----------+|label|prediction|+-----+----------+| 0.0 | 0.0 |...| 0.0 | 0.0 |+-----+----------+ Our next step would be to manually evaluate this model and calculate performance metrics like the true positive rate, false negative rate, and so on. We might then turn around and try a different set of parameters to see if those perform better. However, while this is a useful process, it can also be quite tedious. Spark helps you avoid manually trying different models and evaluation criteria by allowing you to specify your workload as a declarative pipeline of work that includes all your transformations as well as tuning your hyperparameters. 我们的下一步是手动评估此模型并计算性能指标，如真阳性率，假阴性率等。 然后我们可以转去尝试一组不同的参数，看看这些参数是否表现更好。 然而，虽然这是一个有用的过程，但它也可能非常繁琐。 Spark允许您将工作负载指定为声明式的管道，其中包含所有转换以及调整超参数的工作，从而帮助您避免手动尝试不同的模型和评估标准。 A REVIEW OF HYPERPARAMETERS 超参数回顾Although we mentioned them previously, let’s more formally define hyperparameters. Hyperparameters are configuration parameters that affect the training process, such as model architecture and regularization. They are set prior to starting training. For instance, logistic regression has a hyperparameter that determines how much regularization should be performed on our data through the training phase (regularization is a technique that pushes models against overfitting data). You’ll see in the next couple of pages that we can set up our pipeline to try different hyperparameter values (e.g., different regularization values) in order to compare different variations of the same model against one another.虽然我们之前提到过它们，但我们更正式地定义了超参数。 超参数是影响训练过程的配置参数，例如模型架构和正则化。 它们在开始训练之前设定。 例如，逻辑回归有一个超参数，用于确定应通过训练阶段对我们的数据执行多少正则化（正则化是一种推动模型对抗过度拟合数据的技术）。 您将在接下来的几页中看到，我们可以设置我们的管道以尝试不同的超参数值（例如，不同的正则化值），以便比较相同模型相对于彼此的不同变化。 Pipelining Our Workflow 流水线我们的工作流程As you probably noticed, if you are performing a lot of transformations, writing all the steps and keeping track of DataFrames ends up being quite tedious. That’s why Spark includes the Pipeline concept. A pipeline allows you to set up a dataflow of the relevant transformations that ends with an estimator that is automatically tuned according to your specifications, resulting in a tuned model ready for use. Figure 24-4 illustrates this process. 正如您可能已经注意到的，如果您正在执行大量转换，那么编写所有步骤并跟踪 DataFrames 最终会非常繁琐。 这就是 Spark 包含 Pipeline 概念的原因。 管道允许您设置相关转换的数据流，该数据流以估计器（estimator）结束，而估计器（estimator）根据您设置的规范自动调整，从而使得可以使用的调整模型。 图24-4说明了这个过程。 Note that it is essential that instances of transformers or models are not reused across different pipelines. Always create a new instance of a model before creating another pipeline. 请注意，转换器（transformer）或模型的实例不能在不同的管道中重复使用。 始终在创建另一个管道之前创建模型的新实例。 In order to make sure we don’t overfit, we are going to create a holdout test set and tune our hyperparameters based on a validation set (note that we create this validation set based on the original dataset, not the preparedDF used in the previous pages) : 为了确保我们不过度拟合，我们将创建一个留出法（holdout） 测试集并基于验证集调整我们的超参数（请注意，我们基于原始数据集创建此验证集，而不是之前页面使用的 preparedDF）： 12// in Scalaval Array(train, test) = df.randomSplit(Array(0.7, 0.3)) 12# in Pythontrain, test = df.randomSplit([0.7, 0.3]) Now that you have a holdout set, let’s create the base stages in our pipeline. A stage simply represents a transformer or an estimator. In our case, we will have two estimators. The RFomula will first analyze our data to understand the types of input features and then transform them to create new features. Subsequently, the LogisticRegression object is the algorithm that we will train to produce a model: 既然你有一个 留出法（holdout） 集合，让我们在我们的管道中创建基础阶段。 阶段只代表转换器（transformer）或估计器（estimator）。 在我们的例子中，我们将有两个估计器（estimator）。 RFomula 将首先分析我们的数据，以了解输入特征的类型，然后转换它们以创建新特征。 随后，LogisticRegression 对象是我们将训练生成模型的算法： 123// in Scalaval rForm = new RFormula()val lr = new LogisticRegression().setLabelCol("label").setFeaturesCol("features") 123# in PythonrForm = RFormula()lr = LogisticRegression().setLabelCol("label").setFeaturesCol("features") We will set the potential values for the RFormula in the next section. Now instead of manually using our transformations and then tuning our model we just make them stages in the overall pipeline, as in the following code snippet: 我们将在下一节中设置 RFormula 的潜在值。 现在，不是手动使用我们的转换然后调整我们的模型，而是在整个管道中创建它们，如下面的代码片段所示： 1234// in Scalaimport org.apache.spark.ml.Pipelineval stages = Array(rForm, lr)val pipeline = new Pipeline().setStages(stages) 1234# in Pythonfrom pyspark.ml import Pipelinestages = [rForm, lr]pipeline = Pipeline().setStages(stages) Training and Evaluation 训练和评估Now that you arranged the logical pipeline, the next step is training. In our case, we won’t train just one model (like we did previously); we will train several variations of the model by specifying different combinations of hyperparameters that we would like Spark to test. We will then select the best model using an Evaluator that compares their predictions on our validation data. We can test different hyperparameters in the entire pipeline, even in the RFormula that we use to manipulate the raw data. This code shows how we go about doing that: 既然您已经安排了逻辑管道，那么下一步就是训练。 在我们的例子中，我们不会只训练一个模型（就像我们之前做过的那样）; 我们将通过指定我们希望 Spark 测试的超参数的不同组合来训练模型的几种变体。 然后，我们将使用评估器（Evaluator） 选择最佳模型，该评估器（evaluator ）将对其验证数据的预测进行比较。 我们可以在整个管道中测试不同的超参数，甚至可以在我们用来操作原始数据的 RFormula 中测试。 此代码显示了我们如何做到这一点： 123456789// in Scalaimport org.apache.spark.ml.tuning.ParamGridBuilderval params = new ParamGridBuilder().addGrid(rForm.formula, Array("lab ~ . + color:value1","lab ~ . + color:value1 + color:value2")).addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0)).addGrid(lr.regParam, Array(0.1, 2.0)).build() 123456789# in Pythonfrom pyspark.ml.tuning import ParamGridBuilderparams = ParamGridBuilder()\.addGrid(rForm.formula, ["lab ~ . + color:value1","lab ~ . + color:value1 + color:value2"])\.addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\.addGrid(lr.regParam, [0.1, 2.0])\.build() In our current paramter grid, there are three hyperparameters that will diverge from the defaults:在我们当前的参数网格中，有三个超参数与默认值不同： Two different versions of the RFormula 两种不同版本的 RFormula Three different options for the ElasticNet parameter ElasticNet 参数的三个不同选项 Two different options for the regularization parameter 正则化参数的两个不同选项 This gives us a total of 12 different combinations of these parameters, which means we will be training 12 different versions of logistic regression. We explain the ElasticNet parameter as well as the regularization options in Chapter 26. 这给了我们这些参数的12种不同组合，这意味着我们将训练12种不同版本的逻辑回归。我们将在第26章中解释ElasticNet参数以及正则化选项。 Now that the grid is built, it’s time to specify our evaluation process. The evaluator allows us to automatically and objectively compare multiple models to the same evaluation metric. There are evaluators for classification and regression, covered in later chapters, but in this case we will use the BinaryClassificationEvaluator, which has a number of potential evaluation metrics, as we’ll discuss in Chapter 26. In this case we will use areaUnderROC, which is the total area under the receiver operating characteristic, a common measure of classification performance: 现在网格已经建成，是时候指定我们的评估过程了。评估器（evaluator ）允许我们自动客观地将多个模型在同一评估指标进行比较。有分类和回归的评估器（evaluator ），在后面的章节中有介绍，但在这种情况下，我们将使用 BinaryClassificationEvaluator，它有许多潜在的评估指标，我们将在第26章讨论。在这种情况下，我们将使用 areaUnderROC，是受试者特性曲线下的总面积，是分类性能的常用度量： 123456// in Scalaimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluatorval evaluator = new BinaryClassificationEvaluator().setMetricName("areaUnderROC").setRawPredictionCol("prediction").setLabelCol("label") 123456# in Pythonfrom pyspark.ml.evaluation import BinaryClassificationEvaluatorevaluator = BinaryClassificationEvaluator()\.setMetricName("areaUnderROC")\.setRawPredictionCol("prediction")\.setLabelCol("label") Now that we have a pipeline that specifies how our data should be transformed, we will perform model selection to try out different hyperparameters in our logistic regression model and measure success by comparing their performance using the areaUnderROC metric. 既然我们有一个管道来指定我们的数据应该如何转换，我们将执行模型选择以在我们的逻辑回归模型中尝试不同的超参数，并通过使用 areaUnderROC 指标比较它们的性能来衡量成功。 As we discussed, it is a best practice in machine learning to fit hyperparameters on a validation set (instead of your test set) to prevent overfitting. For this reason, we cannot use our holdout test set (that we created before) to tune these parameters. Luckily, Spark provides two options for performing hyperparameter tuning automatically. We can use TrainValidationSplit, which will simply perform an arbitrary random split of our data into two different groups, or CrossValidator, which performs K-fold cross-validation by splitting the dataset into k non-overlapping, randomly partitioned folds: 正如我们所讨论的，机器学习中的最佳实践是在验证集（而不是测试集）上拟合超参数以防止过度拟合。出于这个原因，我们不能使用我们的留出法（holdout） 测试集（我们之前创建的）来调整这些参数。幸运的是，Spark 提供了两种自动执行超参数调整的选项。我们可以使用 TrainValidationSplit，它只是将我们的数据任意随机拆分成两个不同的组，或 CrossValidator，它通过将数据集拆分为 k 个非重叠，随机分区的部分来执行K折交叉验证： 1234567// in Scalaimport org.apache.spark.ml.tuning.TrainValidationSplitval tvs = new TrainValidationSplit().setTrainRatio(0.75) // also the default..setEstimatorParamMaps(params).setEstimator(pipeline).setEvaluator(evaluator) 1234567# in Pythonfrom pyspark.ml.tuning import TrainValidationSplittvs = TrainValidationSplit()\.setTrainRatio(0.75)\.setEstimatorParamMaps(params)\.setEstimator(pipeline)\.setEvaluator(evaluator) Let’s run the entire pipeline we constructed. To review, running this pipeline will test out every version of the model against the validation set. Note the type of tvsFitted is TrainValidationSplitModel. Any time we fit a given model, it outputs a “model” type : 让我们运行我们构建的整个管道。 要进行检查，运行此管道将根据验证集测试模型的每个版本。 请注意，tvsFitted 的类型是 TrainValidationSplitModel 。 只要我们拟合一个给定的模型，它就会输出一个“模型”类型： 12// in Scalaval tvsFitted = tvs.fit(train) 12# in PythontvsFitted = tvs.fit(train) And of course evaluate how it performs on the test set! 当然还要评估它在测试集上的表现！ 1evaluator.evaluate(tvsFitted.transform(test)) // 0.9166666666666667 We can also see a training summary for some models. To do this we extract it from the pipeline, cast it to the proper type, and print our results. The metrics available on each model are discussed throughout the next several chapters. Here’s how we can see the results: 我们还可以查看某些模型的训练摘要。 为此，我们从管道中提取它，将其转换为合适的类型，然后打印我们的结果。 在接下来的几章中将讨论每种模型上可用的衡量指标。 以下是我们如何看到结果： 1234567// in Scalaimport org.apache.spark.ml.PipelineModelimport org.apache.spark.ml.classification.LogisticRegressionModelval trainedPipeline = tvsFitted.bestModel.asInstanceOf[PipelineModel]val TrainedLR = trainedPipeline.stages(1).asInstanceOf[LogisticRegressionModel]val summaryLR = TrainedLR.summarysummaryLR.objectiveHistory // 0.6751425885789243, 0.5543659647777687, 0.473776... The objective history shown here provides details related to how our algorithm performed over each training iteration. This can be helpful because we can note the progress our algorithm is making toward the best model. Large jumps are typically expected at the beginning, but over time the values should become smaller and smaller, with only small amounts of variation between the values. 此处显示的目标历史记录提供了与我们的算法如何在每次训练迭代中执行的详细信息。 这可能有用，因为我们可以记录我们的算法朝向最佳模型所取得的进展。 通常在开始时预期会出现大跳跃，但随着时间的推移，值会变得越来越小，值之间只有少量变化。 Persisting and Applying Models 持久化和应用模型Now that we trained this model, we can persist it to disk to use it for prediction purposes later on: 现在我们已经训练了这个模型，我们可以将它保存到磁盘，以便以后用于预测目的： 1tvsFitted.write.overwrite().save("/tmp/modelLocation") After writing out the model, we can load it into another Spark program to make predictions. To do this, we need to use a “model” version of our particular algorithm to load our persisted model from disk. If we were to use CrossValidator, we’d have to read in the persisted version as the CrossValidatorModel, and if we were to use LogisticRegression manually we would have to use LogisticRegressionModel. In this case, we use TrainValidationSplit, which outputs TrainValidationSplitModel: 在写出模型之后，我们可以将它加载到另一个 Spark 程序中进行预测。 为此，我们需要使用特定算法的“模型”版本从磁盘加载持久化模型。 如果我们使用 CrossValidator，我们必须在持久化版本中读取CrossValidatorModel，如果我们手动使用 LogisticRegression，我们将不得不使用LogisticRegressionModel。 在这个案例下，我们使用 TrainValidationSplit，它输出TrainValidationSplitModel： 1234// in Scalaimport org.apache.spark.ml.tuning.TrainValidationSplitModelval model = TrainValidationSplitModel.load("/tmp/modelLocation")model.transform(test) Deployment PatternsIn Spark there are several different deployment patterns for putting machine learning models into production. Figure 24-5 illustrates common workflows. 在 Spark 中，有几种不同的部署模式可用于将机器学习模型投入生产。 图 24-5 说明了常见的工作流程。 Here are the various options for how you might go about deploying a Spark model. These are the general options you should be able to link to the process illustrated in Figure 24-5. 以下是有关如何部署 Spark 模型的各种选项。这些是您应该能够链接到图24-5中所示流程的通用选项。 Train your machine learning (ML) model offline and then supply it with offline data. In this context, we mean offline data to be data that is stored for analysis, and not data that you need to get an answer from quickly. Spark is well suited to this sort of deployment. 离线训练您的机器学习（ML）模型，然后为其提供离线数据。在这种情况下，我们将离线数据称为存储用于分析的数据，而不是快速获得答案所需的数据。 Spark 非常适合这种部署。 Train your model offline and then put the results into a database (usually a key-value store). This works well for something like recommendation but poorly for something like classification or regression where you cannot just look up a value for a given user but must calculate one based on the input. 离线训练模型，然后将结果放入数据库（通常是键值存储）。这对于像推荐这样的东西很有效，但对于像分类或回归这样的东西来说效果很差，你不仅可以查找给定用户的值，而且必须根据输入计算一个值。 Train your ML algorithm offline, persist the model to disk, and then use that for serving. This is not a low-latency solution if you use Spark for the serving part, as the overhead of starting up a Spark job can be high, even if you’re not running on a cluster. Additionally this does not parallelize well, so you’ll likely have to put a load balancer in front of multiple model replicas and build out some REST API integration yourself. There are some interesting potential solutions to this problem, but no standards currently exist for this sort of model serving. 离线训练 ML 算法，将模型保存到磁盘，然后使用它进行服务。如果您将 Spark 用作服务部分，这不是一个低延迟的解决方案，因为即使您没有在集群上运行，启动Spark作业的开销也很高。此外，这并不能很好地并行化，因此您可能必须在多个模型副本之前放置一个负载均衡器，并自己构建一些REST API集成。这个问题有一些有趣的潜在解决方案，但目前没有这种模型服务的标准。 Manually (or via some other software) convert your distributed model to one that can run much more quickly on a single machine. This works well when there is not too much manipulation of the raw data in Spark but can be hard to maintain over time. Again, there are several solutions in progress. For example, MLlib can export some models to PMML, a common model interchange format. 手动（或通过其他软件）将您的分布式模型转换为可在单台机器上运行得更快的模型。当 Spark 中的原始数据没有太多操作但随着时间的推移很难维护时，这种方法很有效。同样，有几种解决方案正在进行中。例如，MLlib可以将一些模型导出为PMML，这是一种常见的模型交换格式。 Train your ML algorithm online and use it online. This is possible when used in conjunction with Structured Streaming, but can be complex for some models. 在线训练你的ML算法并在线使用它。当与结构化流结合使用时，这是可能的，但对于某些模型可能很复杂。 While these are some of the options, there are many other ways of performing model deployment and management. This is an area under heavy development and many potential innovations are currently being worked on. 虽然这些是一些选项，但还有许多其他方法可以执行模型部署和管理。这是一个正在蓬勃发展的领域，目前正在开展许多潜在的创新。 Conclusion 结论In this chapter we covered the core concepts behind advanced analytics and MLlib. We also showed you how to use them. The next chapter will discuss preprocessing in depth, including Spark’s tools for feature engineering and data cleaning. Then we’ll move into detailed descriptions of each algorithm available in MLlib along with some tools for graph analytics and deep learning. 在本章中，我们介绍了高级分析和MLlib背后的核心概念。我们还向您展示了如何使用它们。下一章将深入讨论预处理，包括Spark的特征工程和数据清理工具。然后，我们将详细介绍MLlib中可用的每种算法，以及一些用于图形分析和深度学习的工具。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Charpter 19 Performance Tuning]]></title>
    <url>%2F2019%2F08%2F13%2FChapter19_Performance-Tuning(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 19 Performance Tuning 性能调优Chapter 18 covered the Spark user interface (UI) and basic first-aid for your Spark Application. Using the tools outlined in that chapter, you should be able to ensure that your jobs run reliably. However, sometimes you’ll also need them to run faster or more efficiently for a variety of reasons. That’s what this chapter is about. Here, we present a discussion of some of the performance choices that are available to make your jobs run faster. 第18章介绍了Spark用户界面（UI）和 Spark应用程序的基本急救。使用这一章中概述的工具，您应该能够确保工作可靠地运行。但是，有时出于各种原因，您还需要它们更快或更高效地运行。这就是本章的内容。在这里，我们将讨论一些性能选择，这些选择可以使您的工作运行得更快。 Just as with monitoring, there are a number of different levels that you can try to tune at. For instance, if you had an extremely fast network, that would make many of your Spark jobs faster because shuffles are so often one of the costlier steps in a Spark job. Most likely, you won’t have much ability to control such things; therefore, we’re going to discuss the things you can control through code choices or configuration. 正如监视一样，您可以尝试调到许多不同的级别。例如，如果你有一个非常快速的网络，这将使你的许多 Spark 工作更快，因为 shuffle（洗牌）往往是 Spark 工作成本更高的步骤之一。最有可能的是，您没有太多的能力来控制这些事情；因此，我们将讨论通过代码选择或配置可以控制的事情 There are a variety of different parts of Spark jobs that you might want to optimize, and it’s valuable to be specific. Following are some of the areas: Spark作业有许多不同的部分，您可能希望对其进行优化，具体来说很有价值。以下是一些领域： Code-level design choices (e.g., RDDs versus DataFrames) 代码级设计选择（例如，RDD与数据帧） Data at rest 非运行时数据（未流通的数据） Joins 连接 Aggregations 聚合 Data in flight 程序运行中的数据 Individual application properties 单个应用程序属性 Inside of the Java Virtual Machine (JVM) of an executor 执行器的Java虚拟机（JVM）的内部 Worker nodes 工作节点 Cluster and deployment properties 集群和部署属性 This list is by no means exhaustive, but it does at least ground the conversation and the topics that we cover in this chapter. Additionally, there are two ways of trying to achieve the execution characteristics that we would like out of Spark jobs. We can either do so indirectly by setting configuration values or changing the runtime environment. These should improve things across Spark Applications or across Spark jobs. Alternatively, we can try to directly change execution characteristic or design choices at the individual Spark job, stage, or task level. These kinds of fixes are very specific to that one area of our application and therefore have limited overall impact. There are numerous things that lie on both sides of the indirect versus direct divide, and we will draw lines in the sand accordingly. 这个列表决不是详尽的，但它至少为我们在本章中讨论的话题打下了基础。此外，有两种方法可以尝试实现我们希望的无 Spark 作业的执行特性。我们可以通过设置配置值或更改运行时环境来间接地这样做。这些应该可以改善Spark应用程序或Spark作业的性能。或者，我们可以尝试在单个Spark作业、阶段或任务级别直接更改执行特性或设计选择。这些类型的修复程序非常特定于我们应用程序的某个领域，因此总体影响有限。间接与直接之间分水岭的两边有许多东西，我们将区分出来。 One of the best things you can do to figure out how to improve performance is to implement good monitoring and job history tracking. Without this information, it can be difficult to know whether you’re really improving job performance. 要想了解如何提高绩效，最好的方法之一就是实施良好的监控和工作历史跟踪。没有这些信息，很难知道你是否真的在提高工作绩效。 Indirect Performance Enforcement 间接的性能加强As discussed, there are a number of indirect enhancements that you can perform to help your Spark jobs run faster. We’ll skip the obvious ones like “improve your hardware” and focus more on the things within your control. 如前所述，您可以执行一些间接增强，以帮助 Spark 作业运行得更快。我们将跳过“改进硬件”这类明显的问题，更多地关注您控制范围内的事情。 Design Choices 设计选择Although good design choices seem like a somewhat obvious way to optimize performance, we often don’t prioritize this step in the process. When designing your applications, making good design choices is very important because it not only helps you to write better Spark applications but also to get them to run in a more stable and consistent manner over time and in the face of external changes or variations. We’ve already discussed some of these topics earlier in the book, but we’ll summarize some of the fundamental ones again here. 尽管良好的设计选择似乎是优化性能的一种比较明显的方法，但我们通常不会在流程中优先考虑这一步骤。在设计应用程序时，做出良好的设计选择是非常重要的，因为这不仅有助于编写更好的Spark应用程序，而且有助于使它们在一段时间内以更稳定和一致的方式运行，并在面对外部变化运行。我们已经在书的前面讨论了其中的一些主题，但是我们将在这里再次总结一些基本的主题。 Scala versus Java versus Python versus R 语言的选择This question is nearly impossible to answer in the general sense because a lot will depend on your use case. For instance, if you want to perform some single-node machine learning after performing a large ETL job, we might recommend running your Extract, Transform, and Load (ETL) code as SparkR code and then using R’s massive machine learning ecosystem to run your single-node machine learning algorithms. This gives you the best of both worlds and takes advantage of the strength of R as well as the strength of Spark without sacrifices. As we mentioned numerous times, Spark’s Structured APIs are consistent across languages in terms of speed and stability. That means that you should code with whatever language you are most comfortable using or is best suited for your use case. 这个问题几乎不可能从一般情况下回答，因为很大程度上取决于您的使用案例。例如，如果您想在执行大型ETL作业后执行一些单节点机器学习，我们可能建议运行提取、转换和加载（ETL）代码作为 SparkR 代码，然后使用R的大型机器学习生态系统来运行单节点机器学习算法。这给了你两个世界中最好的资源，利用 R 和 Spark 的强大力量且不带（性能上的）牺牲。正如我们多次提到的，Spark的结构化API在速度和稳定性方面跨语言保持一致。这意味着您应该使用最适合您使用或最适合您的用例的任何语言进行编码。 Things do get a bit more complicated when you need to include custom transformations that cannot be created in the Structured APIs. These might manifest themselves as RDD transformations or user defined functions (UDFs). If you’re going to do this, R and Python are not necessarily the best choice simply because of how this is actually executed. It’s also more difficult to provide stricter guarantees of types and manipulations when you’re defining functions that jump across languages. We find that using Python for the majority of the application, and porting some of it to Scala or writing specific UDFs in Scala as your application evolves, is a powerful technique—it allows for a nice balance between overall usability, maintainability, and performance. 当需要包含无法在结构化API中创建的自定义转换时，情况确实会变得更加复杂。这些可能表现为RDD转换或用户自定义函数（UDF）。如果要这样做，R 和 Python不一定是最佳选择，仅仅因为它实际上是如何执行的。在定义跨语言的函数时，更难对类型和操作提供更严格的保证。我们发现，对大多数应用程序使用 Python，随着应用程序的发展，将其中的一部分移植到 Scala 或者在 Scala 中编写特定的 UDF，这是一种强大的技术，它允许在总体可用性、可维护性和性能之间实现良好的平衡。 DataFrames versus SQL versus Datasets versus RDDs 不同层级的API选择This question also comes up frequently. The answer is simple. Across all languages, DataFrames, Datasets, and SQL are equivalent in speed. This means that if you’re using DataFrames in any of these languages, performance is equal. However, if you’re going to be defining UDFs, you’ll take a performance hit writing those in Python or R, and to some extent a lesser performance hit in Java and Scala. If you want to optimize for pure performance, it would behoove you to try and get back to DataFrames and SQL as quickly as possible. Although all DataFrame, SQL, and Dataset code compiles down to RDDs, Spark’s optimization engine will write “better” RDD code than you can manually and certainly do it with orders of magnitude less effort. Additionally, you will lose out on new optimizations that are added to Spark’s SQL engine every release. 这个问题也经常出现。答案很简单。在所有语言中，DataFrames、Datasets 和 SQL 在速度上是等效的。这意味着，如果您使用这些语言中的任何一种，那么性能都是相同的。但是，如果你要定义UDFs，你将在Python或R中进行性能损失，在一定程度上，Java和Scala的性能下降更少一些。如果您希望优化以获得纯粹的性能，那么应该尝试尽快返回到 DataFrames 和 SQL。尽管所有的数据框架、SQL和数据集代码都编译成RDD，但Spark的优化引擎将编写“更好”的RDD代码，这比您可以手动编写的代码要好，而且肯定要花费更少的工作量。此外，在每一个版本中，您都将失去添加到Spark的SQL引擎中的新优化。 Lastly, if you want to use RDDs, we definitely recommend using Scala or Java. If that’s not possible, we recommend that you restrict the “surface area” of RDDs in your application to the bare minimum. That’s because when Python runs RDD code, it’s serializes a lot of data to and from the Python process. This is very expensive to run over very big data and can also decrease stability. 最后，如果您想使用RDDs，我们绝对推荐使用Scala或Java。如果这不可能，我们建议您将应用程序中RDD的使用范围限制为最小值。这是因为当Python运行RDD代码时，它会在Python进程之间序列化大量数据。这是非常昂贵的运行非常大的数据，也可以降低稳定性。 Although it isn’t exactly relevant to performance tuning, it’s important to note that there are also some gaps in what functionality is supported in each of Spark’s languages. We discussed this in Chapter 16. 尽管它与性能调优并不完全相关，但需要注意的是，在Spark的每种语言中，在支持哪些功能方面也存在一些差距。我们在第16章讨论了这一点。 Shuffle Configurations 洗牌的配置Configuring Spark’s external shuffle service (discussed in Chapters 16 and 17) can often increase performance because it allows nodes to read shuffle data from remote machines even when the executors on those machines are busy (e.g., with garbage collection). This does come at the cost of complexity and maintenance, however, so it might not be worth it in your deployment. Beyond configuring this external service, there are also a number of configurations for shuffles, such as the number of concurrent connections per executor, although these usually have good defaults. In addition, for RDD-based jobs, the serialization format has a large impact on shuffle performance—always prefer Kryo over Java serialization, as described in “Object Serialization in RDDs”. 配置Spark的外部shuffle服务（在第16和17章中讨论）通常可以提高性能，因为它允许节点从远程机器读取shuffle数据，即使这些机器上的执行器很忙（例如，垃圾回收）。然而，这是以复杂性和维护为代价的，因此在您的部署中可能不值得这样做。除了配置这个外部服务之外，还有许多配置用于 shuffle，例如每个 executor 的并发连接数，尽管这些配置通常具有良好的默认值。此外，对于基于 RDD 的作业，序列化格式对 shuffle 性能的影响很大，在 Java 序列化中总是首选 Kryo，如“RDDS中的对象序列化”中所描述的。 Furthermore, for all jobs, the number of partitions of a shuffle matters. If you have too few partitions, then too few nodes will be doing work and there may be skew, but if you have too many partitions, there is an overhead to launching each one that may start to dominate. Try to aim for at least a few tens of megabytes of data per output partition in your shuffle. 此外，对于所有作业，shuffle 的分区数都很重要。如果分区太少，那么节点就太少，可能会出现数据倾斜，但是如果分区太多，那么启动每个分区的开销就会开始占主导地位。试着在shuffle时，每个输出分区至少要有几十兆的数据。 Memory Pressure and Garbage Collection 性能压力和垃圾回收During the course of running Spark jobs, the executor or driver machines may struggle to complete their tasks because of a lack of sufficient memory or “memory pressure.” This may occur when an application takes up too much memory during execution or when garbage collection runs too frequently or is slow to run as large numbers of objects are created in the JVM and subsequently garbage collected as they are no longer used. One strategy for easing this issue is to ensure that you’re using the Structured APIs as much as possible. These will not only increase the efficiency with which your Spark jobs will execute, but it will also greatly reduce memory pressure because JVM objects are never realized and Spark SQL simply performs the computation on its internal format. 在运行spark作业的过程中，由于内存不足或“内存压力”，executor 或 driver 机器可能难以完成其任务。当应用程序在执行期间占用太多内存或垃圾回收运行太频繁时，可能会发生这种情况。或者运行起来很慢，因为在JVM中创建了大量的对象，然后由于不再使用这些对象而被垃圾回收。缓解这个问题的一个策略是确保尽可能多地使用结构化API。这些不仅可以提高Spark作业的执行效率，而且还可以极大地降低内存压力，因为JVM对象从未实现，Spark SQL只是对其内部格式执行计算。 The Spark documentation includes some great pointers on tuning garbage collection for RDD andUDF based applications, and we paraphrase the following sections from that information. spark文档包含一些基于RDD和UDF的应用程序优化垃圾回收的重要建议，我们从这些信息中解释了以下部分。 Measuring the impact of garbage collection 衡量垃圾回收的影响The first step in garbage collection tuning is to gather statistics on how frequently garbage collection occurs and the amount of time it takes. You can do this by adding -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps to Spark’s JVM options using the spark.executor.extraJavaOptions configuration parameter. The next time you run your Spark job, you will see messages printed in the worker’s logs each time a garbage collection occurs. These logs will be on your cluster’s worker nodes (in the stdout files in their work directories), not in the driver. GC调优的第一步是垃圾回收发生的频率和 GC 花费的时间。你可以通过配置参数：spark.executor.extraJavaOptions 添加 -verbose：gc -XX:+ PrintGCDetails -XX:+ PrintGCTimeStamps 到 Spark 的 Java 虚拟机选项来完成。下次你运行Spark作业时，每次发生垃圾回收时都会在 worker 的日志中看到消息。请注意，这些日志将位于集群的 worker 节点上（在其工作目录中的 stdout 文件中），而不是在驱动程序上。 译者注：官网例子： 123./bin/spark-submit --name "My app" --master local[4] --conf spark.eventLog.enabled=false--conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" myApp.jar Garbage collection tuning 垃圾回收的调试To further tune garbage collection, you first need to understand some basic information about memory management in the JVM: Java heap space is divided into two regions: Young and Old. The Young generation is meant to hold short-lived objects whereas the Old generation is intended for objects with longer lifetimes. 为了进一步调整垃圾回收，首先需要了解 JVM 中内存管理的一些基本信息：Java 堆空间被分为两个区域：Young 和 Old。年轻一代的目的是持有生命周期短的对象，而老一代的目的在于生命周期较长的对象。 The Young generation is further divided into three regions: Eden, Survivor1, and Survivor2. Here’s a simplified description of the garbage collection procedure: 年轻一代被进一步划分为三个区域：Eden, Survivor1 和 Survivor2。以下是垃圾回收过程的简化描述： When Eden is full, a minor garbage collection is run on Eden and objects that are alive from Eden and Survivor1 are copied to Survivor2. 当 Eden 已满时，会在 Eden 上运行一个小量的垃圾回收，来自 Eden 和 Survivor1 的活动对象会被复制到Survivor2。 The Survivor regions are swapped. Survivor 区域交换。 If an object is old enough or if Survivor2 is full, that object is moved to Old. 如果对象足够旧，或者 Survivor2 已满，则该对象将移动到 Old。 Finally, when Old is close to full, a full garbage collection is invoked. This involves tracing through all the objects on the heap, deleting the unreferenced ones, and moving the others to fill up unused space, so it is generally the slowest garbage collection operation. 最后，当 Old 接近满的时候，将调用完全的垃圾回收 。这涉及到跟踪堆上的所有对象，删除未引用的对象，并移动其他对象以填充未使用的空间，因此它通常是最慢的垃圾回收操作。 The goal of garbage collection tuning in Spark is to ensure that only long-lived cached datasets are stored in the Old generation and that the Young generation is sufficiently sized to store all short-lived objects. This will help avoid full garbage collections to collect temporary objects created during task execution. Here are some steps that might be useful. Spark中垃圾回收调优的目标是确保只有生命周期长的缓存数据集存储在老年代中，并且年轻一代的大小足以存储所有生命周期短的对象。这将有助于避免垃圾回收，以回收在任务执行期间创建的临时对象。 以下是一些可能有用的步骤。 Gather garbage collection statistics to determine whether it is being run too often. If a full garbage collection is invoked multiple times before a task completes, it means that there isn’t enough memory available for executing tasks, so you should decrease the amount of memory Spark uses for caching (spark.memory.fraction). 收集垃圾回收的统计信息以确定它是否运行得太频繁。如果在任务完成之前多次调用完全的垃圾回收，这意味着没有足够的内存用于执行任务，因此应该减少Spark用于缓存的内存量（spark.memory.fraction）。 If there are too many minor collections but not many major garbage collections, allocating more memory for Eden would help. You can set the size of the Eden to be an over-estimate of how much memory each task will need. If the size of Eden is determined to be E, you can set the size of the Young generation using the option -Xmn=4/3*E. (The scaling up by 4/3 is to account for space used by survivor regions, as well.) 如果有太多的小量的垃圾回收，但没有太多大量的垃圾回收，为 Eden 分配更多的内存会有所帮助。您可以将 Eden 的大小设置为对每个任务需要多少内存的过度估计（有冗余空间）。如果 Eden（伊甸园）的大小确定为 E，您可以使用选项 -xmn=4/3*E 设置年轻一代的大小（放大4/3也是为了说明 survivor 区域使用的空间）。 As an example, if your task is reading data from HDFS, the amount of memory used by the task can be estimated by using the size of the data block read from HDFS. Note that the size of a decompressed block is often two or three times the size of the block. So if you want to have three or four tasks’ worth of working space, and the HDFS block size is 128 MB, we can estimate size of Eden to be $43128$ MB. 例如，如果您的任务是从 HDFS 读取数据，那么可以使用从 HDFS 读取的数据块的大小来估计任务使用的内存量。请注意，解压块的大小通常是块大小的两到三倍。因此，如果您想要三个或四个任务的工作空间，并且 HDFS块大小为 $128$ MB，我们可以估计 Eden 的大小为 $4 3 128$ MB。 Try the G1GC garbage collector with -XX:+UseG1GC. It can improve performance in some situations in which garbage collection is a bottleneck and you don’t have a way to reduce it further by sizing the generations. Note that with large executor heap sizes, it can be important to increase the G1 region size with -XX:G1HeapRegionSize . 使用 -xx:+useg1gc 尝试 G1GC 垃圾回收器。在垃圾回收是一个瓶颈的情况下，并且您没有办法通过调整代的大小进一步降低它，这样做可以提高性能。请注意，拥有大的 executor 堆大小，使用 -xx:g1HeapRegionSize 增加G1区大小可能很重要。 Monitor how the frequency and time taken by garbage collection changes with the new settings. Our experience suggests that the effect of garbage collection tuning depends on your application and the amount of memory available. There are many more tuning options described online, but at a high level, managing how frequently full garbage collection takes place can help in reducing the overhead. You can specify garbage collection tuning flags for executors by setting spark.executor.extraJavaOptions in a job’s configuration. 监视垃圾回收所用的频率和时间如何随新设置的变化而变化。我们的经验表明，垃圾回收调优的效果取决于您的应用程序和可用内存量。线上描述了更多的调优选项，但从较高的层次来说，管理完全的垃圾回收的频率有助于减少开销。通过在作业配置中设置 spark.executor.extraJavaOptions，可以为 executor 指定垃圾回收的优化标志。 Direct Performance Enhancements 直接性能增强In the previous section, we touched on some general performance enhancements that apply to all jobs. Be sure to skim the previous couple of pages before jumping to this section and the solutions here. These solutions here are intended as “band-aids” of sorts for issues with specific stages or jobs, but they require inspecting and optimizing each stage or job separately. 在前一节中，我们讨论了一些适用于所有作业的通用的性能增强。在跳到本节和这里的解决方案之前，一定要浏览前面的几页。这里的这些解决方案是针对特定阶段或工作的各种问题的“创可贴”，但它们需要分别检查和优化每个阶段或工作。 Parallelism 并行主义The first thing you should do whenever trying to speed up a specific stage is to increase the degree of parallelism. In general, we recommend having at least two or three tasks per CPU core in your cluster if the stage processes a large amount of data. You can set this via the spark.default.parallelism property as well as tuning the spark.sql.shuffle.partitions according to the number of cores in your cluster. 当您试图加速某个特定阶段时，首先应该做的是提高并行度。通常，如果阶段处理大量数据，我们建议集群中每个CPU核心至少有两到三个任务。您可以通过spark.default.parallelism属性进行设置，也可以根据集群中核心的数量来调整 spark.sql.shuffle.partitions。 Improved Filtering 改进过滤Another frequent source of performance enhancements is moving filters to the earliest part of your Spark job that you can. Sometimes, these filters can be pushed into the data sources themselves and this means that you can avoid reading and working with data that is irrelevant to your end result. Enabling partitioning and bucketing also helps achieve this. Always look to be filtering as much data as you can early on, and you’ll find that your Spark jobs will almost always run faster. 性能增强的另一个常见来源是将过滤器移动到Spark作业的最早部分。有时，这些过滤器可以被推入数据源本身，这意味着您可以避免读取和处理与最终结果无关的数据。启用分区和bucketing也有助于实现这一点。 总是尽可能早地过滤数据，你会发现你的Spark作业几乎总是运行得更快。 Repartitioning and Coalescing 重分区和联合Repartition calls can incur a shuffle. However, doing some can optimize the overall execution of a job by balancing data across the cluster, so they can be worth it. In general, you should try to shuffle the least amount of data possible. For this reason, if you’re reducing the number of overall partitions in a DataFrame or RDD, first try coalesce method, which will not perform a shuffle but rather merge partitions on the same node into one partition. The slower repartition method will also shuffle data across the network to achieve even load balancing. Repartitions can be particularly helpful when performing joins or prior to a cache call. Remember that repartitioning is not free, but it can improve overall application performance and parallelism of your jobs. 重新分区调用可能导致混乱。但是，通过在集群中平衡数据，可以优化作业的整体执行，因此它们是值得的。一般来说，您应该尽量减少数据量。因此，如果要减少 DataFrame 或 RDD 中的整体分区数，请首先尝试 coalesce 方法，它不会执行洗牌（shuffle），而是将同一节点上的分区合并到一个分区中。较慢的重新分区方法还将在网络上洗牌数据，以实现均匀的负载平衡。重新分区在执行连接或在缓存调用之前特别有用。记住，重新分区不是免费的，但它可以提高应用程序的整体性能和作业的并行性。 Custom partitioning 自定义分区器If your jobs are still slow or unstable, you might want to explore performing custom partitioning at the RDD level. This allows you to define a custom partition function that will organize the data across the cluster to a finer level of precision than is available at the DataFrame level. This is very rarely necessary, but it is an option. For more information, see Part III. 如果您的作业仍然很慢或不稳定，您可能希望探索在RDD级别执行自定义分区。这允许您定义一个自定义分区函数，该函数将跨集群组织数据，使其达到比 DataFrame 级别更高的精度级别。这是很少必要的，但它是一种选择。更多信息，见第三部分。 User-Defined Functions (UDFs) 用户定义函数（UDF）In general, avoiding UDFs is a good optimization opportunity. UDFs are expensive because they force representing data as objects in the JVM and sometimes do this multiple times per record in a query. You should try to use the Structured APIs as much as possible to perform your manipulations simply because they are going to perform the transformations in a much more efficient manner than you can do in a high-level language. There is also ongoing work to make data available to UDFs in batches, such as the Vectorized UDF extension for Python that gives your code multiple records at once using a Pandas data frame. We discussed UDFs and their costs in Chapter 18. 一般来说，避免UDF是一个很好的优化机会。UDF之所以昂贵，是因为它们强制将数据表示为JVM中的对象，并且有时在查询中对每条记录执行多次这样的操作。您应该尽可能多地使用结构化API来执行操作，因为它们将以比高级语言更高效的方式执行转换。还有一些正在进行的工作是批量向UDF提供数据，例如针对Python的矢量化UDF扩展，它使用PANDAS数据帧一次为代码提供多个记录。我们在第18章讨论了UDF及其成本。 Temporary Data Storage (Caching) 临时数据存储In applications that reuse the same datasets over and over, one of the most useful optimizations is caching. Caching will place a DataFrame, table, or RDD into temporary storage (either memory or disk) across the executors in your cluster, and make subsequent reads faster. Although caching might sound like something we should do all the time, it’s not always a good thing to do. That’s because caching data incurs a serialization, deserialization, and storage cost. For example, if you are only going to process a dataset once (in a later transformation), caching it will only slow you down. The use case for caching is simple: as you work with data in Spark, either within an interactive session or a standalone application, you will often want to reuse a certain dataset (e.g., a DataFrame or RDD). For example, in an interactive data science session, you might load and clean your data and then reuse it to try multiple statistical models. Or in a standalone application, you might run an iterative algorithm that reuses the same dataset. You can tell Spark to cache a dataset using the cache method on DataFrames or RDDs. 在反复重用相同数据集的应用程序中，最有用的优化之一是缓存。缓存将把一个DataFrame、表或RDD放到集群中执行器的临时存储器（内存或磁盘）中，并使后续的读取速度更快。尽管缓存听起来像是我们一直应该做的事情，但这并不总是一件好事。这是因为缓存数据会导致序列化、反序列化和存储成本。 例如，如果您只处理一次数据集（在以后的转换中），缓存它只会减慢您的速度。缓存的用例很简单：当您在Spark中处理数据时，无论是在交互会话中还是在独立的应用程序中，您通常都希望重用某个数据集（例如，DataFrame 或 RDD）。 例如，在交互式数据科学会话中，您可以加载和清理数据，然后重新使用它来尝试多个统计模型。或者在独立的应用程序中，您可以运行一个重复使用相同数据集的迭代算法。您可以告诉Spark在 DataFrame 或 RDD 上使用cache方法缓存数据集。 Caching is a lazy operation, meaning that things will be cached only as they are accessed. The RDD API and the Structured API differ in how they actually perform caching, so let’s review the gory details before going over the storage levels. When we cache an RDD, we cache the actual, physical data (i.e., the bits). The bits. When this data is accessed again, Spark returns the proper data. This is done through the RDD reference. However, in the Structured API, caching is done based on the physical plan. This means that we effectively store the physical plan as our key (as opposed to the object reference) and perform a lookup prior to the execution of a Structured job. This can cause confusion because sometimes you might be expecting to access raw data but because someone else already cached the data, you’re actually accessing their cached version. Keep that in mind when using this feature. 缓存是一个懒惰的操作，这意味着只有数据被访问时才会对其进行缓存。RDD API和结构化API在实际执行缓存的方式上有所不同， 因此，在讨论存储级别之前，让我们先回顾一下详细信息。当我们缓存一个RDD时，我们缓存实际的物理数据（也就是：比特）。当再次访问此数据时，spark返回正确的数据。这是通过RDD引用完成的。但是，在结构化API中，缓存是基于物理计划完成的。这意味着我们有效地将物理计划存储为键（而不是对象引用），并在执行结构化作业之前执行查找。这可能会导致混淆，因为有时您可能希望访问原始数据，但因为其他人已经缓存了数据，所以实际上您正在访问他们的缓存版本。使用此功能时请记住这一点。 There are different storage levels that you can use to cache your data, specifying what type of storage to use. Table 19-1 lists the levels. 您可以使用不同的存储级别来缓存数据，指定要使用的存储类型。表19-1列出了各等级。 Table 19-1. Data cache storage levels 数据缓存级别 Storage level Meaning Meaning MEMORY_ONLY Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they’re needed. This is the default level. 将RDD存储为JVM中的反序列化Java对象。如果RDD不在内存中，那么某些分区将不会被缓存，并且将在每次需要时即时重新计算。这是默认级别。 MEMORY_AND_DISK Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don’t fit on disk, and read them from there when they’re needed.将RDD存储为JVM中的反序列化Java对象。如果RDD不适合内存，请将不适合的分区存储在磁盘上，并在需要时从磁盘上读取它们。 MEMORY_ONLY_SER(Java and Scala) Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.将RDD存储为序列化的Java对象（每个分区的一个字节数组）。这通常比反序列化对象更节省空间，尤其是在使用快速序列化程序时，但读取时CPU占用更大。 DISK_ONLY Store the RDD partitions only on disk.仅将RDD分区存储在磁盘上。 MEMORY_ONLY_2,MEMORY_AND_DISK_2,etc. Same as the previous levels, but replicate each partition on two cluster nodes.与前面的级别相同，但在两个集群节点上复制每个分区。 OFF_HEAP(experimental) Similar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to be enabled.类似于只存储数据，但将数据存储在堆外内存中。这需要启用堆外内存。 For more information on these options, take a look at “Configuring Memory Management”. 有关这些选项的详细信息，请参阅“配置内存管理”。 Figure 19-1 presents a simple illustrations of the process. 图19-1给出了该过程的简单说明。 We load an initial DataFrame from a CSV file and then derive some new DataFrames from it using transformations. We can avoid having to recompute the original DataFrame (i.e., load and parse the CSV file) many times by adding a line to cache it along the way. 我们从csv文件加载一个初始数据帧，然后使用转换从中派生一些新的数据帧。我们可以避免多次重新计算原始数据帧（即加载和解析csv文件），方法是添加一行缓存它。 Now let’s walk through the code: 现在让我们完整地学习一个代码： 123456789# in Python# Original loading code that does *not* cache DataFrameDF1 = spark.read.format("csv")\.option("inferSchema", "true")\.option("header", "true")\.load("/data/flight-data/csv/2015-summary.csv")DF2 = DF1.groupBy("DEST_COUNTRY_NAME").count().collect()DF3 = DF1.groupBy("ORIGIN_COUNTRY_NAME").count().collect()DF4 = DF1.groupBy("count").count().collect() You’ll see here that we have our “lazily” created DataFrame (DF1), along with three other DataFrames that access data in DF1. All of our downstream DataFrames share that common parent (DF1) and will repeat the same work when we perform the preceding code. In this case, it’s just reading and parsing the raw CSV data, but that can be a fairly intensive process, especially for large datasets. 在这里，您将看到我们的“惰性”创建的数据帧（df1），以及其他三个访问df1中数据的数据帧。我们所有的下游数据帧都共享这个公共父级（DF1），并且在执行前面的代码时将重复相同的工作。在这种情况下，它只是读取和解析原始的csv数据，但这可能是一个相当密集的过程，特别是对于大型数据集。 On my machine, those commands take a second or two to run. Luckily caching can help speed things up. When we ask for a DataFrame to be cached, Spark will save the data in memory or on disk the first time it computes it. Then, when any other queries come along, they’ll just refer to the one stored in memory as opposed to the original file. You do this using the DataFrame’s cache method: 在我的机器上，这些命令需要一两秒钟才能运行。幸运的是，缓存可以帮助加快速度。当我们请求缓存一个 DataFrame 时，spark将在第一次计算数据时将数据保存在内存或磁盘上。然后，当出现任何其他查询时，它们只引用存储在内存中的查询，而不是原始文件。使用 DataFrame 的缓存方法执行此操作： 12DF1.cache()DF1.count() We used the count above to eagerly cache the data (basically perform an action to force Spark to store it in memory), because caching itself is lazy—the data is cached only on the first time you run an action on the DataFrame. Now that the data is cached, the previous commands will be faster, as we can see by running the following code: 我们使用上面的计数来急切地缓存数据（基本上是执行一个操作来强制 spark 将其存储在内存中），因为缓存本身是懒惰的，数据只在您第一次在 DataFrame 上运行一个操作时缓存。现在缓存了数据，前面的命令将更快，我们可以通过运行以下代码看到这一点： 1234# in PythonDF2 = DF1.groupBy("DEST_COUNTRY_NAME").count().collect()DF3 = DF1.groupBy("ORIGIN_COUNTRY_NAME").count().collect()DF4 = DF1.groupBy("count").count().collect() When we ran this code, it cut the time by more than half! This might not seem that wild, but picture a large dataset or one that requires a lot of computation to create (not just reading in a file). The savings can be immense. It’s also great for iterative machine learning workloads because they’ll often need to access the same data a number of times, which we’ll see shortly. 当我们运行这个代码时，它将时间缩短了一半以上！这看起来并不是那么疯狂，但想象一下一个大型数据集或需要大量计算才能创建的数据集（不仅仅是在文件中读取）。节省的钱可能是巨大的。这对于迭代机器学习工作负载也很好，因为它们通常需要多次访问相同的数据，稍后我们将看到。 The cache command in Spark always places data in memory by default, caching only part of the dataset if the cluster’s total memory is full. For more control, there is also a persist method that takes a StorageLevel object to specify where to cache the data: in memory, on disk, or both. Spark中的cache命令在默认情况下总是将数据放在内存中，如果集群的总内存已满，则只缓存数据集的一部分。为了获得更多的控制权，还有一个持久化方法，它使用一个 StorageLevel 对象来指定数据的缓存位置：内存中、磁盘上，或者两者兼而有之。 Joins 连接Joins are a common area for optimization. The biggest weapon you have when it comes to optimizing joins is simply educating yourself about what each join does and how it’s performed. This will help you the most. Additionally, equi-joins are the easiest for Spark to optimize at this point and therefore should be preferred wherever possible. Beyond that, simple things like trying to use the filtering ability of inner joins by changing join ordering can yield large speedups. Additionally, using broadcast join hints can help Spark make intelligent planning decisions when it comes to creating query plans, as described in Chapter 8. Avoiding Cartesian joins or even full outer joins is often low-hanging fruit for stability and optimizations because these can often be optimized into different filtering style joins when you look at the entire data flow instead of just that one particular job area. 连接是优化的一个常见领域。在优化连接时，您拥有的最大武器就是简单地向自己介绍每个连接的作用和执行方式。这对你的帮助最大。此外，equi-joins 对于spark来说是最容易在这一点上进行优化的，因此在可能的情况下应首选。除此之外，通过改变连接顺序来尝试使用内部连接的过滤能力等简单的事情可以产生很大的加速。此外，使用广播连接提示可以帮助Spark在创建查询计划时做出智能规划决策，如第8章所述。避免笛卡尔连接，甚至是完全的外部连接，对于稳定性和优化来说通常都是容易获得的成果，因为当您查看整个数据流而不仅仅是一个特定的工作区域时，这些连接常常可以优化为不同的过滤类型的连接。 Lastly, following some of the other sections in this chapter can have a significant effect on joins. For example, collecting statistics on tables prior to a join will help Spark make intelligent join decisions. Additionally, bucketing your data appropriately can also help Spark avoid large shuffles when joins are performed. 最后，遵循本章中的一些其他部分可以对连接产生显著的影响。例如，在连接之前收集表的统计信息将有助于Spark 做出智能连接决策。此外，适当地将数据进行分桶还可以帮助 Spark 在执行连接时避免大的洗牌（shuffle）。 Aggregations 聚合For the most part, there are not too many ways that you can optimize specific aggregations beyond filtering data before the aggregation having a sufficiently high number of partitions. However, if you’re using RDDs, controlling exactly how these aggregations are performed ( e.g., using reduceByKeywhen possible over groupByKey ) can be very helpful and improve the speed and stability of your code. 在大多数情况下，除了在聚合具有足够多的分区之前过滤数据之外，没有太多方法可以优化特定聚合。但是，如果您使用的是 RDD，那么准确地控制这些聚合的执行方式（例如，在可能的情况下使用 reduceByKey 而不是groupByKey）会非常有帮助，并且可以提高代码的速度和稳定性。 Broadcast Variables 广播变量We touched on broadcast joins and variables in previous chapters, and these are a good option for optimization. The basic premise is that if some large piece of data will be used across multiple UDF calls in your program, you can broadcast it to save just a single read-only copy on each node and avoid re-sending this data with each job. For example, broadcast variables may be useful to save a lookup table or a machine learning model. You can also broadcast arbitrary objects by creating broadcast variables using your SparkContext, and then simply refer to those variables in your tasks, as we discussed in Chapter 14. 我们在前面的章节中讨论了广播连接和变量，这些是一个很好的优化选择。基本前提是，如果在程序中的多个UDF调用之间使用一些大的数据块，您可以广播它以在每个节点上只保存一个只读副本，并避免在每个作业中重新发送这些数据。例如，广播变量对于保存查找表或机器学习模型可能很有用。您还可以通过使用 SparkContext 创建广播变量来广播任意对象，然后在任务中简单地引用这些变量，如我们在第14章中所讨论的。 Conclusion 结论There are many different ways to optimize the performance of your Spark Applications and make them run faster and at a lower cost. In general, the main things you’ll want to prioritize are (1) reading as little data as possible through partitioning and efficient binary formats, (2) making sure there is sufficient parallellism and no data skew on the cluster using partitioning, and (3) using high-level APIs such as the Structured APIs as much as possible to take already optimized code. As with any other software optimization work, you should also make sure you are optimizing the right operations for your job: the Spark monitoring tools described in Chapter 18 will let you see which stages are taking the longest time and focus your efforts on those. Once you have identified the work that you believe can be optimized, the tools in this chapter will cover the most important performance optimization opportunities for the majority of users. 有许多不同的方法来优化Spark应用程序的性能，使其以更低的成本更快地运行。一般来说，您要优先考虑的主要事情是 （1）通过分区和有效的二进制格式读取尽可能少的数据 （2）确保在使用分区的集群上有足够的并行性和没有数据倾斜 （3）尽可能多地使用 high-level APIs去采用已经优化过的代码，例如：结构化的API 与任何其他软件优化工作一样，您还应该确保为您的工作优化了正确的操作：第18章中描述的 Spark 监控工具将让您了解哪些阶段花费的时间最长，并将您的精力集中在这些阶段上。一旦确定了您认为可以优化的工作，本章中的工具将为大多数用户提供最重要的性能优化机会。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter18_Monitoring-and-Debugging]]></title>
    <url>%2F2019%2F08%2F10%2FChapter18_Monitoring-and-Debugging(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 18 Monitoring and Debugging 监控与调试This chapter covers the key details you need to monitor and debug your Spark Applications. To do this, we will walk through the Spark UI with an example query designed to help you understand how to trace your own jobs through the execution life cycle. The example we’ll look at will also help you understand how to debug your jobs and where errors are likely to occur. 本章介绍了监视和调试Spark应用程序所需的关键详细信息。为此，我们将使用一个示例查询来浏览Spark UI，该查询旨在帮助您了解如何在执行生命周期中跟踪自己的作业。我们将看到的示例还将帮助您了解如何调试作业以及可能发生错误的位置。 The Monitoring Landscape 监控的宏观图At some point, you’ll need to monitor your Spark jobs to understand where issues are occuring in them. It’s worth reviewing the different things that we can actually monitor and outlining some of the options for doing so. Let’s review the components we can monitor (see Figure 18-1). 在某些时候，您需要监视您的Spark作业，以了解其中发生问题的位置。值得回顾一下我们可以实际监控的不同内容，并概述了这样做的一些选项。让我们回顾一下我们可以监控的组件（参见图18-1，在下文）。 Spark Applications and Jobs Spark应用程序和作业 The first thing you’ll want to begin monitoring when either debugging or just understanding better how your application executes against the cluster is the Spark UI and the Spark logs. These report information about the applications currently running at the level of concepts in Spark, such as RDDs and query plans. We talk in detail about how to use these Spark monitoring tools throughout this chapter. 在调试或只是更好地了解在集群背景下应用程序执行的方式时，您首先想要开始监视的是Spark UI和Spark日志。这些报告有关当前在Spark中概念级别运行的应用程序的信息，例如RDD和查询计划。我们将在本章中详细讨论如何使用这些Spark监视工具。 JVM Spark runs the executors in individual Java Virtual Machines (JVMs). Therefore, the next level of detail would be to monitor the individual virtual machines (VMs) to better understand how your code is running. JVM utilities such as jstack for providing stack traces, jmap for creating heapdumps, jstat for reporting time–series statistics, and jconsole for visually exploring various JVM properties are useful for those comfortable with JVM internals. You can also use a tool like jvisualvm to help profile Spark jobs. Some of this information is provided in the Spark UI, but for very low-level debugging, the aforementioned tools can come in handy. Spark 在各个 Java 虚拟机（JVM）中运行执行程序（executor）。因此，下一级详细信息将是监视单个虚拟机（VM）以更好地了解代码的运行方式。 JVM实用程序（如用于提供堆栈跟踪的 jstack ，用于创建 heapdumps 的 jmap，用于报告时间序列统计信息的 jstat）以及用于可视化地探索各种 JVM 属性的 jconsole， 这些对于那些熟悉 JVM 内部的人来说非常有用。您还可以使用 jvisualvm 之类的工具来帮助分析 Spark 作业。其中一些信息在 Spark UI 中提供，但对于非常低级的调试，上述工具可以派上用场。 OS/Machine The JVMs run on a host operating system (OS) and it’s important to monitor the state of those machines ensure that they are healthy. This includes monitoring things like CPU, network, and I/O. These are often reported in cluster-level monitoring solutions; however, there are more specific tools that you can use, including dstat, iostat, and iotop. JVM 在主机操作系统（OS）上运行，监控这些机器的状态以确保它们是健康的非常重要。这包括监视 CPU，网络和 I/O 等内容。这些通常在集群级监控解决方案中报告；但是，您可以使用更多特定工具，包括 dstat，iostat 和iotop。 Cluster Naturally, you can monitor the cluster on which your Spark Application(s) will run. This might be a YARN, Mesos, or standalone cluster. Usually it’s important to have some sort of monitoring solution here because, somewhat obviously, if your cluster is not working, you should probably know pretty quickly. Some popular cluster-level monitoring tools include Ganglia and Prometheus. 当然，您可以监视运行 Spark 应用程序的集群。这可能是 YARN，Mesos 或独立集群。通常，在这里使用某种监控解决方案很重要，因为很明显，如果您的集群不工作，您应该很快就会知道。一些流行的集群级监控工具包括Ganglia 和 Prometheus。 What to Monitor 要监控什么After that brief tour of the monitoring landscape, let’s discuss how we can go about monitoring and debugging our Spark Applications. There are two main things you will want to monitor: the processes running your application (at the level of CPU usage, memory usage, etc.), and the query execution inside it (e.g., jobs and tasks). 在简要介绍了监控环境之后，让我们讨论如何监控和调试我们的 Spark 应用程序。您需要监视两个主要内容：运行应用程序的进程（在 CPU 使用情况，内存使用情况的等级）以及在其中执行的查询（例如，作业和任务）。 Driver and Executor Processes 驱动程序和执行程序进程When you’re monitoring a Spark application, you’re definitely going to want to keep an eye on the driver. This is where all of the state of your application lives, and you’ll need to be sure it’s running in a stable manner. If you could monitor only one machine or a single JVM, it would definitely be the driver. With that being said, understanding the state of the executors is also extremely important for monitoring individual Spark jobs. To help with this challenge, Spark has a configurable metrics system based on the Dropwizard Metrics Library. The metrics system is configured via a configuration file that Spark expects to be present at $SPARK_HOME/conf/metrics.properties. A custom file location can be specified by changing the spark.metrics.conf configuration property. These metrics can be output to a variety of different sinks, including cluster monitoring solutions like Ganglia. 当您监控Spark应用程序时，您肯定会想要关注驱动程序（driver）。这是您的应用程序的所有状态所在的位置，您需要确保它以稳定的方式运行。如果您只能监控一台机器或一台 JVM，那肯定是驱动程序（driver）。话虽如此，了解执行程序（executor）的状态对于监视各个 Spark 作业也非常重要。为了应对这一挑战，Spark 拥有一个基于 Dropwizard Metrics 库的可配置衡量系统。衡量系统通过 Spark 预期出现在 $SPARK_HOME/conf/metrics.properties 中的配置文件进行配置。可以通过更改 spark.metrics.conf 配置属性来指定自定义文件位置。这些指标可以输出到各种不同的接收器，包括像 Ganglia 这样的集群监控解决方案。 Queries, Jobs, Stages, and Tasks 查询，作业，阶段和任务Although the driver and executor processes are important to monitor, sometimes you need to debug what’s going on at the level of a specific query. Spark provides the ability to dive into queries, jobs, stages, and tasks. (We learned about these in Chapter 15.) This information allows you to know exactly what’s running on the cluster at a given time. When looking for performance tuning or debugging, this is where you are most likely to start. Now that we know what we want to monitor, let’s look at the two most common ways of doing so: the Spark logs and the Spark UI. 虽然驱动程序（driver）和执行程序（executor）进程对于监视很重要，但有时您需要调试特定查询级别的进程。 Spark 提供了深入查询，工作，阶段和任务的能力。 （我们在第15章中了解了这些内容。）此信息可让您准确了解在给定时间情况下集群上正在运行的内容。在寻找性能调优或调试时，这是您最有可能开始的地方。现在我们知道了我们想要监控的内容，让我们看看这两种最常见的方式：Spark 日志和 Spark UI 。 Spark Logs Spark日志One of the most detailed ways to monitor Spark is through its log files. Naturally, strange events in Spark’s logs, or in the logging that you added to your Spark Application, can help you take note of exactly where jobs are failing or what is causing that failure. If you use the application template provided with the book, the logging framework we set up in the template will allow your application logs to show up along Spark’s own logs, making them very easy to correlate. One challenge, however, is that Python won’t be able to integrate directly with Spark’s Java-based logging library. Using Python’s logging module or even simple print statements will still print the results to standard error, however, and make them easy to find. 监视 Spark 的最详细方法之一是通过其日志文件。当然，Spark 的日志中或您添加到 Spark 应用程序的日志记录中的奇怪事件可以帮助您准确记录作业失败的原因或导致失败的原因。如果您使用本书提供的应用程序模板，我们在模板中设置的日志记录框架将允许您的应用程序日志显示在 Spark 自己的日志中，使它们非常容易关联。然而，一个挑战是 Python 无法直接与 Spark 的基于 Java 的日志库集成。但是，使用 Python 的日志记录模块甚至简单的打印语句仍然会将结果打印到标准错误，并使它们易于查找。 To change Spark’s log level, simply run the following command : 要更改 Spark 的日志级别，只需运行以下命令 : spark.sparkContext.setLogLevel(&quot;INFO&quot;) This will allow you to read the logs, and if you use our application template, you can log your own relevant information along with these logs, allowing you to inspect both your own application and Spark. The logs themselves will be printed to standard error when running a local mode application, or saved to files by your cluster manager when running Spark on a cluster. Refer to each cluster manager’s documentation about how to find them—typically, they are available through the cluster manager’s web UI. 这将允许您阅读日志，如果您使用我们的应用程序模板，您可以记录您自己的相关信息以及这些日志，允许您检查自己的应用程序和 Spark。运行本地模式应用程序时，日志本身将打印为标准错误，或者在集群上运行 Spark 时由集群管理器保存到文件。请参阅每个集群管理器的文档，了解如何查找它们——通常，它们可通过集群管理器的Web UI 获得。 You won’t always find the answer you need simply by searching logs, but it can help you pinpoint the given problem that you’re encountering and possibly add new log statements in your application to better understand it. It’s also convenient to collect logs over time in order to reference them in the future. For instance, if your application crashes, you’ll want to debug why, without access to the now crashed application. You may also want to ship logs off the machine they were written on to hold onto them if a machine crashes or gets shut down (e.g., if running in the cloud). 您不会总是通过搜索日志找到所需的答案，但它可以帮助您查明您遇到的给定问题，并可能在您的应用程序中添加新的日志语句以更好地理解它。随着时间的推移收集日志以便将来引用它们也很方便。例如，如果您的应用程序崩溃，您将需要调试原因，而无需访问现在崩溃的应用程序。如果计算机崩溃或关闭（例如，如果在云中运行），您可能还希望将日志从他们写入的计算机上发送到其上。 The Spark UIThe Spark UI provides a visual way to monitor applications while they are running as well as metrics about your Spark workload, at the Spark and JVM level. Every SparkContext running launches a web UI, by default on port 4040, that displays useful information about the application. When you run Spark in local mode, for example, just navigate to http://localhost:4040 to see the UI when running a Spark Application on your local machine. If you’re running multiple applications, they will launch web UIs on increasing port numbers (4041, 4042, …). Cluster managers will also link to each application’s web UI from their own UI. Spark UI 提供了一种可视化方式，用于在运行时监视应用程序，以及 Spark 和 JVM 级别的 Spark 工作负载指标。每个运行的 SparkContext 都会在端口 4040 上默认启动 Web UI，该UI显示有关应用程序的有用信息。例如，在本地模式下运行 Spark 时，只需导航到 http://localhost:4040，即可在本地计算机上运行 Spark 应用程序时查看UI 。如果您正在运行多个应用程序，他们将在增加端口号（4041,4042，…）时启动Web UI。集群管理器还将从其自己的 UI 链接到每个应用程序的 Web UI。 Figure 18-2 shows all of the tabs available in the Spark UI. 图18-2 显示了 Spark UI 中可用的所有选项卡。 These tabs are accessible for each of the things that we’d like to monitor. For the most part, each of these should be self-explanatory : 这些选项卡可供我们要监控的每个事项访问。在大多数情况下，每一个都应该是不言自明的： The Jobs tab refers to Spark jobs. “作业”选项卡指的是Spark作业。 The Stages tab pertains to individual stages (and their relevant tasks). 阶段选项卡适用于各个阶段（及其相关任务）。 The Storage tab includes information and the data that is currently cached in our Spark Application. “存储”选项卡包含当前在我们的 Spark 应用程序中缓存的信息和数据。 The Environment tab contains relevant information about the configurations and current settings of the Spark application. “环境”选项卡包含有关 Spark 应用程序的配置和当前设置的相关信息。 The SQL tab refers to our Structured API queries (including SQL and DataFrames). SQL选项卡引用我们的结构化API查询（包括 SQL 和 DataFrames）。 The Executors tab provides detailed information about each executor running our application. Executors选项卡提供有关运行我们的应用程序的每个执行程序（executor）的详细信息。 Let’s walk through an example of how you can drill down into a given query. Open a new Spark shell, run the following code, and we will trace its execution through the Spark UI: 让我们来看一个如何深入查看给定查询的示例。打开一个新的 Spark shell，运行以下代码，我们将通过 Spark UI 跟踪它的执行： 123456789# in Pythonspark.read\.option("header", "true")\.csv("/data/retail-data/all/online-retail-dataset.csv")\.repartition(2)\.selectExpr("instr(Description, 'GLASS') &gt;= 1 as is_glass")\.groupBy("is_glass")\.count()\.collect() This results in three rows of various values. The code kicks off a SQL query, so let’s navigate to the SQL tab, where you should see something similar to Figure 18-3. 这导致不同值的三行。 代码开始启动 SQL 查询，所以让我们导航到 SQL 选项卡，在那里你应该看到类似于图 18-3 的内容。 The first thing you see is aggregate statistics about this query: 您看到的第一件事是关于此查询的汇总统计信息： Submitted Time: 2017/04/08 16:24:41Duration: 2 sSucceeded Jobs: 2 These will become important in a minute, but first let’s take a look at the Directed Acyclic Graph (DAG) of Spark stages. Each blue box in these tabs represent a stage of Spark tasks. The entire group of these stages represent our Spark job. Let’s take a look at each stage in detail so that we can better understand what is going on at each level, starting with Figure 18-4. 这些将马上变得重要，但首先让我们来看看 Spark 阶段的有向无环图（DAG）。 这些选项卡中的每个蓝色框表示 Spark 任务的一个阶段。 这些阶段的整个组代表我们的 Spark 工作。 让我们详细了解每个阶段，以便我们可以更好地了解每个级别的情况，从图 18-4 开始。 The box on top, labeled WholeStateCodegen, represents a full scan of the CSV file. The box below that represents a shuffle that we forced when we called repartition. This turned our original dataset (of a yet to be specified number of partitions) into two partitions. 标记为 WholeStateCodegen 的顶部框表示 CSV 文件的完整扫描。 下面的框表示我们在调用重新分区时强制进行的随机洗牌(shuffle)。 这将我们的原始数据集（尚未指定的分区数）转换为两个分区。 The next step is our projection (selecting/adding/filtering columns) and the aggregation. Notice that in Figure 18-5 the number of output rows is six. This conveniently lines up with the number of output rows multiplied by the number of partitions at aggregation time. This is because Spark performs an aggregation for each partition (in this case a hash-based aggregation) before shuffling the data around in preparation for the final stage. 下一步是我们的投影（选择/添加/过滤列）和聚合。 请注意，在图18-5中，输出行数为6（final output * 2 = 6）。 这方便与输出行的数量乘以聚合时的分区数对齐。 这是因为 Spark 在为最终阶段做准备之前对数据进行洗牌(shuffle)之前，为每个分区执行聚合（在这种情况下是基于散列（hash-based） 的聚合）。 The last stage is the aggregation of the subaggregations that we saw happen on a per-partition basis in the previous stage. We combine those two partitions in the final three rows that are the output of our total query (Figure 18-6)。 最后一个阶段是我们在前一阶段基于每个分区发生的子聚合的聚合。 我们将这两个分区组合在最后三行中，这三行是我们总查询的输出（图18-6）。 Let’s look further into the job’s execution. On the Jobs tab, next to Succeeded Jobs, click 2. As Figure 18-7 demonstrates, our job breaks down into three stages (which corresponds to what we saw on the SQL tab). 让我们进一步了解作业的执行情况。 在 Jobs 选项卡上，单击 Succeeded Jobs，单击2. 如图18-7所示，我们的工作分为三个阶段（与我们在SQL选项卡上看到的相对应）。 These stages have more or less the same information as what’s shown in Figure 18-6, but clicking the label for one of them will show the details for a given stage. In this example, three stages ran, with eight, two, and then two hundred tasks each. Before diving into the stage detail, let’s review why this is the case. 这些阶段或多或少与图18-6中显示的信息相同，但单击其中一个阶段的标签将显示给定阶段的详细信息。 在这个例子中，运行了三个阶段，每个阶段分别有八个，两个，然后是两百个任务。 在深入了解阶段细节之前，让我们回顾一下为什么会这样。 The first stage has eight tasks. CSV files are splittable, and Spark broke up the work to be distributed relatively evenly between the different cores on the machine. This happens at the cluster level and points to an important optimization: how you store your files. The following stage has two tasks because we explicitly called a repartition to move the data into two partitions. The last stage has 200 tasks because the default shuffle partitions value is 200. 第一阶段有八个任务。 CSV 文件是可拆分的，Spark 分解了工作使其相对均匀分布在机器上不同核心之间。 这发生在集群级别，并指向一个重要的优化：如何存储文件。 下一个阶段有两个任务，因为我们显式调用了重新分区以将数据移动到两个分区中。 最后一个阶段有200个任务，因为默认的 shuffle 分区值为 200。 Now that we reviewed how we got here, click the stage with eight tasks to see the next level of detail, as shown in Figure 18-8. 现在我们回顾了我们如何到达这里，单击具有八个任务的阶段以查看下一个详细级别，如图18-8所示。 Spark provides a lot of detail about what this job did when it ran. Toward the top, notice the Summary Metrics section. This provides a synopsis of statistics regarding various metrics. What you want to be on the lookout for is uneven distributions of the values (we touch on this in Chapter 19). In this case, everything looks very consistent; there are no wide swings in the distribution of values. In the table at the bottom, we can also examine on a per-executor basis (one for every core on this particular machine, in this case). This can help identify whether a particular executor is struggling with its workload. Spark 提供了很多关于这项工作在运行时所做些什么的细节。在顶部，请注意“摘要衡量标准（Summary Metrics）”部分。这提供了有关各种指标的统计数据的概要。你想要注意的是值的不均匀分布（我们在第19章中讨论）。在这种情况下，一切看起来都非常一致; 值的分布没有大幅波动。在底部的表中，我们还可以基于每个执行程序（executor）进行检查（在这种情况下，该特定计算机上的每个核心都有一个）。这有助于确定特定执行程序（executor）是否在努力应对其工作量。 Spark also makes available a set of more detailed metrics, as shown in Figure 18-8, which are probably not relevant to the large majority of users. To view those, click Show Additional Metrics, and then either choose (De)select All or select individual metrics, depending on what you want to see. Spark还提供了一组更详细的指标，如图18-8所示，这些指标可能与绝大多数用户无关。要查看这些，请单击“显示其他衡量标准”，然后选择（取消）选择“全部”或选择单个衡量标准，具体取决于您要查看的内容。 You can repeat this basic analysis for each stage that you want to analyze. We leave that as an exercise for the reader. 您可以为要分析的每个阶段重复此基本分析。我们把它作为读者的练习。 Other Spark UI tabs 其他Spark UI选项卡The remaining Spark tabs, Storage, Environment, and Executors, are fairly self-explanatory. The Storage tab shows information about the cached RDDs/DataFrames on the cluster. This can help you see if certain data has been evicted from the cache over time. The Environment tab shows you information about the Runtime Environment, including information about Scala and Java as well as the various Spark Properties that you configured on your cluster. 其余的 Spark 选项卡，存储，环境和执行程序（executor），都是不言自明的。 “存储”选项卡显示有关集群上缓存的 RDD / DataFrame 的信息。这可以帮助您查看某些数据是否随着时间的推移从缓存中逐出。 “环境”选项卡显示有关运行时环境的信息，包括有关 Scala 和 Java 的信息以及您在集群上配置的各种Spark属性。 Configuring the Spark user interface 配置Spark用户界面There are a number of configurations that you can set regarding the Spark UI. Many of them are networking configurations such as enabling access control. Others let you configure how the Spark UI will behave (e.g., how many jobs, stages, and tasks are stored). Due to space limitations, we cannot include the entire configuration set here. Consult the relevant table on Spark UI Configurations in the Spark documentation. 您可以设置有关 Spark UI 的许多配置。其中许多是网络配置，例如启用访问控制。其他允许您配置 Spark UI 的行为方式（例如，存储了多少个作业，阶段和任务）。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中 Spark UI配置 的相关表。 Spark REST APIIn addition to the Spark UI, you can also access Spark’s status and metrics via a REST API. This is is available at http://localhost:4040/api/v1 and is a way of building visualizations and monitoring tools on top of Spark itself. For the most part this API exposes the same information presented in the web UI, except that it doesn’t include any of the SQL-related information. This can be a useful tool if you would like to build your own reporting solution based on the information available in the Spark UI. Due to space limitations, we cannot include the list of API endpoints here. Consult the relevant table on REST API Endpoints in the Spark documentation. 除了 Spark UI，您还可以通过 REST API 访问Spark的状态和指标。这可以在 http://localhost:4040/api/v1 上获得，它是一种在Spark本身之上构建可视化和监视工具的方法。在大多数情况下，此API公开Web UI中显示的相同信息，但它不包含任何与SQL相关的信息。如果您希望根据Spark UI中提供的信息构建自己的报告解决方案，这可能是一个有用的工具。由于篇幅限制，我们无法在此处包含API端点列表。请参阅Spark文档中有关REST API端点的相关表。 Spark UI History Server SparkUI历史记录服务器Normally, the Spark UI is only available while a SparkContext is running, so how can you get to it after your application crashes or ends? To do this, Spark includes a tool called the Spark History Server that allows you to reconstruct the Spark UI and REST API, provided that the application was configured to save an event log. You can find up-to-date information about how to use this tool in the Spark documentation. 通常，Spark UI 仅在 SparkContext 运行时可用，因此在应用程序崩溃或结束后如何才能访问它？为此，Spark包含一个名为 Spark History Server 的工具，允许您重建 Spark UI 和 REST API，前提是应用程序已配置为保存事件日志。您可以在Spark文档中找到有关如何使用此工具的最新信息。 To use the history server, you first need to configure your application to store event logs to a certain location. You can do this by by enabling and the event log location with the configuration spark.eventLog.dir. Then, once you have stored the events, you can run the history server as a standalone application, and it will automatically reconstruct the web UI based on these logs. Some cluster managers and cloud services also configure logging automatically and run a history server by default. 要使用历史记录服务器，首先需要配置应用程序以将事件日志存储到特定位置。您可以通过启用spark.eventLog.enabled 和配置 spark.eventLog.dir 的事件日志位置来完成此操作。然后，一旦存储了事件，就可以将历史服务器作为独立应用程序运行，它将根据这些日志自动重建Web UI。某些集群管理器和云服务还会自动配置日志记录并默认运行历史记录服务器。 There are a number of other configurations for the history server. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Spark History Server Configurations in the Spark documentation. 历史服务器还有许多其他配置。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中有关Spark History Server配置的相关表。 Debugging and Spark First Aid 调试和Spark急救The previous sections defined some core “vital signs”—that is, things that we can monitor to check the health of a Spark Application. For the remainder of the chapter we’re going to take a “first aid” approach to Spark debugging: We’ll review some signs and symptoms of problems in your Spark jobs, including signs that you might observe (e.g., slow tasks) as well as symptoms from Spark itself (e.g., OutOfMemoryError). There are many issues that may affect Spark jobs, so it’s impossible to cover everything. But we will discuss some of the more common Spark issues you may encounter. In addition to the signs and symptoms, we’ll also look at some potential treatments for these issues. 前面的部分定义了一些核心的“生命体征” ——也就是说，我们可以监视以检查Spark应用程序运行状况的事情。对于本章的其余部分，我们将对Spark调试采取“急救”方法： 我们将审查Spark作业中的一些问题迹象和症状，包括您可能观察到的迹象（例如，缓慢的任务）以及来自Spark本身的症状（例如，OutOfMemoryError）。有许多问题可能会影响Spark作业，因此无法涵盖所有内容。但我们将讨论您可能遇到的一些更常见的Spark问题。除了症状和体征外，我们还将研究这些问题的一些潜在治疗方法。 Most of the recommendations about fixing issues refer to the configuration tools discussed in Chapter 16. 有关修复问题的大多数建议都参考了第16章中讨论的配置工具。 Spark Jobs Not Starting Spark工作没有开始This issue can arise frequently, especially when you’re just getting started with a fresh deployment or environment. 这个问题可能经常出现，特别是当您刚刚开始全新部署或环境。 Signs and symptoms 迹象和症状 Spark jobs don’t start. Spark工作无法启动。 The Spark UI doesn’t show any nodes on the cluster except the driver. 除驱动程序（driver）外，Spark UI不显示集群上的任何节点。 The Spark UI seems to be reporting incorrect information. Spark UI 似乎报告了错误的信息。 Potential treatments 可能的疗法This mostly occurs when your cluster or your application’s resource demands are not configured properly. Spark, in a distributed setting, does make some assumptions about networks, file systems, and other resources. During the process of setting up the cluster, you likely configured something incorrectly, and now the node that runs the driver cannot talk to the executors. This might be because you didn’t specify what IP and port is open or didn’t open the correct one. This is most likely a cluster level, machine, or configuration issue. Another option is that your application requested more resources per executor than your cluster manager currently has free, in which case the driver will be waiting forever for executors to be launched. 当您的集群或应用程序的资源需求未正确配置时，通常会发生这种情况。 Spark 在分布式设置中确实对网络，文件系统和其他资源做出了一些假设。在设置集群的过程中，您可能错误地配置了某些内容，现在运行驱动程序（driver）的节点无法与执行程序（executor）通信。这可能是因为您未指定打开的IP和端口或未打开正确的IP和端口。这很可能是集群级别，计算机或配置问题。另一个选择是，您的应用程序为每个执行程序（executor）请求的资源比您的集群管理器当前有空的资源要多，在这种情况下，驱动程序（driver）将永远等待执行程序（executor）启动。 Ensure that machines can communicate with one another on the ports that you expect. Ideally, you should open up all ports between the worker nodes unless you have more stringent security constraints. 确保计算机可以在您期望的端口上相互通信。理想情况下，您应该打开工作节点之间的所有端口，除非您有更严格的安全约束。 Ensure that your Spark resource configurations are correct and that your cluster manager is properly set up for Spark. Try running a simple application first to see if that works. One common issue may be that you requested more memory per executor than the cluster manager has free to allocate, so check how much it is reporting free (in its UI) and your sparksubmit memory configuration. 确保Spark资源配置正确并且已正确设置集群管理器以用于 Spark。尝试先运行一个简单的应用程序，看看是否有效。一个常见问题可能是您为每个执行程序（executor）请求的内存多于集群管理器可以自由分配的内存，因此请检查它是报告空闲的多少（在其 UI 中）和 sparksubmit 内存配置。 Errors Before Execution 执行前的错误This can happen when you’re developing a new application and have previously run code on this cluster, but now some new code won’t work. 当您开发新应用程序并且之前在此集群上运行代码时，可能会发生这种情况，但现在某些新代码将无法运行。 Signs and symptoms 迹象和症状 Commands don’t run at all and output large error messages. 命令根本不运行并输出大的错误消息。 You check the Spark UI and no jobs, stages, or tasks seem to run. 您检查Spark UI并且似乎没有任何作业，阶段或任务运行。 Potential treatments 潜在的治疗方法After checking and confirming that the Spark UI environment tab shows the correct information for your application, it’s worth double-checking your code. Many times, there might be a simple typo or incorrect column name that is preventing the Spark job from compiling into its underlying Spark plan (when using the DataFrame API). 在检查并确认 Spark UI 环境选项卡显示应用程序的正确信息后，值得仔细检查您的代码。很多时候，可能会出现一个简单的拼写错误或不正确的列名，导致Spark作业无法编译到其基础 Spark 计划中（使用 DataFrame API 时）。 You should take a look at the error returned by Spark to confirm that there isn’t an issue in your code, such as providing the wrong input file path or field name. Double-check to verify that the cluster has the network connectivity that you expect between your driver, your workers, and the storage system you are using. 您应该查看Spark返回的错误，以确认代码中没有问题，例如提供错误的输入文件路径或字段名称。仔细检查以验证集群是否具有您期望的驱动程序（driver），工作人员和正在使用的存储系统之间的网络连接。 There might be issues with libraries or classpaths that are causing the wrong version of a library to be loaded for accessing storage. Try simplifying your application until you get a smaller version that reproduces the issue (e.g., just reading one dataset). 库或类路径可能存在导致加载库的错误版本以访问存储的问题。尝试简化您的应用程序，直到您获得重现问题的较小版本（例如，只读取一个数据集）。 Errors During Execution 执行期间的错误 This kind of issue occurs when you already are working on a cluster or parts of your Spark 当您已经在集群或部分Spark上工作时，会出现此类问题。 Application run before you encounter an error. This can be a part of a scheduled job that runs at some interval or a part of some interactive exploration that seems to fail after some time. 在遇到错误之前运行应用程序。这可以是某个时间间隔运行的已经安排作业的一部分，也可能是某些时间后似乎失败的某些交互式探索的一部分。 Signs and symptoms 迹象和症状One Spark job runs successfully on the entire cluster but the next one fails. 一个 Spark 作业在整个集群上成功运行，但下一个失败。 A step in a multistep query fails. 多步查询中的步骤失败。 A scheduled job that ran yesterday is failing today. 昨天运行的预定工作今天失败了。 Difficult to parse error message. 难以解析错误消息。 Potential treatments 可能的疗法 Check to see if your data exists or is in the format that you expect. This can change over time or some upstream change may have had unintended consequences on your application. If an error quickly pops up when you run a query (i.e., before tasks are launched), it is most likely an analysis error while planning the query. This means that you likely misspelled a column name referenced in the query or that a column, view, or table you referenced does not exist. 检查您的数据是否存在或是否符合您的预期格式。这可能会随着时间的推移而改变，或者某些上游更改可能会对您的应用程序产生意外后果。如果在运行查询时（即，在启动任务之前）快速弹出错误，则在计划查询时很可能是分析错误。这意味着您可能拼错了查询中引用的列名称，或者您引用的列，视图或表不存在&gt;。 Read through the stack trace to try to find clues about what components are involved (e.g., what operator and stage it was running in). Try to isolate the issue by progressively double-checking input data and ensuring the data conforms to your expectations. Also try removing logic until you can isolate the problem in a smaller version of your application. 读取堆栈跟踪以尝试查找涉及哪些组件的线索（例如，运行的算子和阶段）。尝试通过逐步检查输入数据并确保数据符合您的期望来隔离问题。还可以尝试删除逻辑，直到您可以在较小版本的应用程序中隔离问题。 If a job runs tasks for some time and then fails, it could be due to a problem with the input data itself, wherein the schema might be specified incorrectly or a particular row does not conform to the expected schema. For instance, sometimes your schema might specify that the data contains no nulls but your data does actually contain nulls, which can cause certain transformations to fail. 如果作业运行任务一段时间然后失败，则可能是由于输入数据本身存在问题，其中可能未正确指定模式或特定行不符合预期模式。例如，有时您的模式可能指定数据不包含空值，但您的数据确实包含空值，这可能导致某些转换失败。 It’s also possible that your own code for processing the data is crashing, in which case Spark will show you the exception thrown by your code. In this case, you will see a task marked as “failed” on the Spark UI, and you can also view the logs on that machine to understand what it was doing when it failed. Try adding more logs inside your code to figure out which data record was being processed. 您自己的处理数据的代码也可能崩溃，在这种情况下，Spark会向您显示代码抛出的异常。在这种情况下，您将在Spark UI上看到标记为“失败”的任务，您还可以查看该计算机上的日志以了解失败时正在执行的操作。尝试在代码中添加更多日志，以确定正在处理哪些数据记录。 Slow Tasks or Stragglers 缓慢的任务或StragglersThis issue is quite common when optimizing applications, and can occur either due to work not being evenly distributed across your machines (“skew”), or due to one of your machines being slower than the others (e.g., due to a hardware problem). 在优化应用程序时，此问题非常常见，并且可能由于工作不均匀分布在您的计算机上（“倾斜”），或者由于您的某台计算机比其他计算机慢（例如，由于硬件问题）而发生。 Signs and symptoms 迹象和症状Any of the following are appropriate symptoms of the issue : 以下任何一种都是该问题的适当症状： Spark stages seem to execute until there are only a handful of tasks left. Those tasks then take a long time. Spark阶段似乎执行，直到只剩下少数任务。那些任务需要很长时间。 These slow tasks show up in the Spark UI and occur consistently on the same dataset(s). 这些缓慢的任务显示在 Spark UI 中，并在相同的数据集上一致地发生。 These occur in stages, one after the other. 这些是分阶段发生的，一个接一个。 Scaling up the number of machines given to the Spark Application doesn’t really help—some tasks still take much longer than others. 扩大提供给 Spark 应用程序的机器数量并没有多大帮助——某些任务仍然需要比其他任务更长的时间。 In the Spark metrics, certain executors are reading and writing much more data than others. 在Spark指标中，某些执行程序（executor）正在读取和写入比其他数据更多的数据。 Potential treatments 可能的治疗方法 Slow tasks are often called “stragglers.” There are many reasons they may occur, but most often the source of this issue is that your data is partitioned unevenly into DataFrame or RDD partitions. When this happens, some executors might need to work on much larger amounts of work than others. One particularly common case is that you use a group-by-key operation and one of the keys just has more data than others. In this case, when you look at the Spark UI, you might see that the shuffle data for some nodes is much larger than for others. 缓慢的任务通常被称为“落后者”。它们可能出现的原因很多，但大多数情况下，这个问题的根源是您的数据被不均匀地划分为 DataFrame 或 RDD 分区。当发生这种情况时，一些执行程序（executor）可能需要处理比其他工作量大得多的工作。一个特别常见的情况是您使用逐个键操作，其中一个键只是比其他键更多的数据。在这种情况下，当您查看 Spark UI 时，您可能会看到某些节点的 shuffle 数据比其他节点大得多。 Try increasing the number of partitions to have less data per partition. 尝试增加分区数，以使每个分区的数据更少。 Try repartitioning by another combination of columns. For example, stragglers can come up when you partition by a skewed ID column, or a column where many values are null. In the latter case, it might make sense to first filter out the null values. Try increasing the memory allocated to your executors if possible. 尝试通过另一个列组合重新分区。例如，当您通过倾斜的 ID 列或许多值为 null 的列进行分区时，straggler 可能会出现。在后一种情况下，首先过滤掉空值可能是有意义的。如果可能，尝试增加分配给执行程序（executor）的内存。 Monitor the executor that is having trouble and see if it is the same machine across jobs; you might also have an unhealthy executor or machine in your cluster—for example, one whose disk is nearly full. If this issue is associated with a join or an aggregation, see “Slow Joins” or “Slow Aggregations”. 监视有问题的执行程序（executor），看看它是否是跨不同作业的同一台机器；您的集群中可能还有一个不健康的执行程序（executor）或计算机——例如，磁盘几乎已满的计算机。如果此问题与连接或聚合相关联，请参阅“慢速连接”或“慢速聚合”。 Check whether your user-defined functions (UDFs) are wasteful in their object allocation or business logic. Try to convert them to DataFrame code if possible. Ensure that your UDFs or User-Defined Aggregate Functions (UDAFs) are running on a small enough batch of data. Oftentimes an aggregation can pull a lot of data into memory for a common key, leading to that executor having to do a lot more work than others. 检查用户定义的函数（UDF）在对象分配或业务逻辑中是否浪费。如果可能，尝试将它们转换为 DataFrame 代码。确保您的 UDF 或用户定义的聚合函数（UDAF）在足够小的数据批量上运行。通常，聚合可以将大量数据拉入内存以用于普通的键，从而导致执行程序（executor）必须比其他人执行更多的工作。 Turning on speculation, which we discuss in “Slow Reads and Writes”, will have Spark run a second copy of tasks that are extremely slow. This can be helpful if the issue is due to a faulty node because the task will get to run on a faster one. Speculation does come at a cost, however, because it consumes additional resources. In addition, for some storage systems that use eventual consistency, you could end up with duplicate output data if your writes are not idempotent . (We discussed speculation configurations in Chapter 17.) 打开我们在“慢速读取和写入”中讨论的推测（speculation），将使 Spark 运行极其缓慢的第二个任务副本。如果问题是由于故障节点引起的，这可能会有所帮助，因为任务将以更快的速度运行。然而，推测（speculation）确实需要付出代价，因为它消耗了额外的资源。此外，对于某些使用最终一致性的存储系统，如果写入不是幂等的，则最终可能会出现重复的输出数据。 （我们在第17章讨论了推测(speculation)配置） Another common issue can arise when you’re working with Datasets. Because Datasets perform a lot of object instantiation to convert records to Java objects for UDFs, they can cause a lot of garbage collection. If you’re using Datasets, look at the garbage collection metrics in the Spark UI to see if they’re consistent with the slow tasks. 当您使用 Datasets 时，可能会出现另一个常见问题。由于 Datasets 执行大量对象实例化以将记录转换为UDF 的 Java 对象，因此它们可能导致大量垃圾回收。如果您正在使用 Datasets，请查看Spark UI中的垃圾收集指标，以查看它们是否与缓慢的任务一致。 Stragglers can be one of the most difficult issues to debug, simply because there are so many possible causes. However, in all likelihood, the cause will be some kind of data skew, so definitely begin by checking the Spark UI for imbalanced amountsimbalanced amounts of data across tasks. Stragglers可能是最难调试的问题之一，因为有很多可能的原因。但是，很有可能，原因将是某种数据偏差，因此必须首先检查Spark UI以查找跨任务的不平衡数据量。 Slow Aggregations 慢的聚合If you have a slow aggregation, start by reviewing the issues in the “Slow Tasks” section before proceeding. Having tried those, you might continue to see the same problem. 如果您的聚合速度较慢，请先继续查看“慢速任务”部分中的问题，然后再继续。尝试过这些后，您可能会继续看到同样的问题。 Signs and symptoms 迹象和症状 Slow tasks during a groupBy call. 在 groupBy 调用期间缓慢执行任务。 Jobs after the aggregation are slow, as well. 聚合后的工作也很慢。 Potential treatments 可能的疗法 Unfortunately, this issue can’t always be solved. Sometimes, the data in your job just has some skewed keys, and the operation you want to run on them needs to be slow. Increasing the number of partitions, prior to an aggregation, might help by reducing the number of different keys processed in each task. 不幸的是，这个问题并不总能解决。有时，作业中的数据只有一些倾斜的键，您想要在它们上运行的操作需要很慢。在聚合之前增加分区数可能有助于减少每个任务中处理的不同键的数量。 Increasing executor memory can help alleviate this issue, as well. If a single key has lots of data, this will allow its executor to spill to disk less often and finish faster, although it may still be much slower than executors processing other keys. 增加执行程序（executor）内存也有助于缓解此问题。如果单个键有大量数据，这将允许其执行程序（executor）更少地溢出到磁盘并更快地完成，尽管它可能仍然比处理其他键的执行程序（executor）慢得多。 If you find that tasks after the aggregation are also slow, this means that your dataset might have remained unbalanced after the aggregation. Try inserting a repartition call to partition it randomly. 如果您发现聚合后的任务也很慢，这意味着聚合后您的数据集可能仍然不平衡。尝试插入重新分区调用以随机分区。 Ensuring that all filters and SELECT statements that can be are above the aggregation can help to ensure that you’re working only on the data that you need to be working on and nothing else. Spark’s query optimizer will automatically do this for the structured APIs. 确保可以在聚合之上的所有过滤器和 SELECT 语句可以帮助确保您仅处理您需要处理的数据而不是其他任何内容。 Spark的查询优化器将自动为结构化API执行此操作。 Ensure null values are represented correctly (using Spark’s concept of null) and not as some default value like “ “ or “EMPTY”. Spark often optimizes for skipping nulls early in the job when possible, but it can’t do so for your own placeholder values. 确保正确表示空值（使用Spark的null概念）而不是像“”或“EMPTY”那样的默认值。 Spark通常会尽可能优化在作业的早期跳过空值，但是对于您自己的占位符值，它不能这样做。 Some aggregation functions are also just inherently slower than others. For instance, collect_list and collect_set are very slow aggregation functions because they must return all the matching objects to the driver, and should be avoided in performance-critical code. 某些聚合函数本身也比其他函数慢。例如，collect_list和collect_set是非常慢的聚合函数，因为它们必须将所有匹配的对象返回给驱动程序（driver），并且应该在性能关键代码中避免使用。 Slow Joins 慢加入Joins and aggregations are both shuffles, so they share some of the same general symptoms as well as treatments. 连接和聚合都是随机洗牌(shuffle)，因此它们共享一些相同的症状和应对方法。 Signs and symptoms 迹象和症状 A join stage seems to be taking a long time. This can be one task or many tasks. l连接阶段似乎需要很长时间。这可以是一个任务或许多任务。 Stages before and after the join seem to be operating normally. 连接之前和之后的阶段似乎正常运行。 Potential treatments 可能的疗法 Many joins can be optimized (manually or automatically) to other types of joins. We covered how to select different join types in Chapter 8. 许多连接可以优化（手动或自动）到其他类型的连接。我们在第8章介绍了如何选择不同的连接类型。 Experimenting with different join orderings can really help speed up jobs, especially if some of those joins filter out a large amount of data; do those first. 尝试不同的连接顺序可以真正帮助加快工作，特别是如果其中一些连接过滤掉大量数据; 先做那些。 Partitioning a dataset prior to joining can be very helpful for reducing data movement across the cluster, especially if the same dataset will be used in multiple join operations. It’s worth experimenting with different prejoin partitioning. Keep in mind, again, that this isn’t “free” and does come at the cost of a shuffle. 在连接之前对数据集进行分区对于减少集群中的数据移动非常有用，尤其是在多个连接操作中将使用相同的数据集时。值得尝试不同的预连接（prejoin）分区。请记住，这不是“免费”，而是以洗牌(shuffle)为代价。 Slow joins can also be caused by data skew. There’s not always a lot you can do here, but sizing up the Spark application and/or increasing the size of executors can help, as described in earlier sections. 数据倾斜也可能导致慢连接。你可以在这里做很多事情，但是调整 Spark 应用程序和/或增加执行程序（executor）的数量可以提供帮助，如前面部分所述。 Ensuring that all filters and select statements that can be are above the join can help to ensure that you’re working only on the data that you need for the join. 确保可以在连接之上的所有筛选器和选择语句可以帮助确保您仅处理连接所需的数据。 Ensure that null values are handled correctly (that you’re using null) and not some default value like “ “ or “EMPTY”, as with aggregations. 确保正确处理空值（您使用的是null），而不是像聚合一样处理某些默认值，如“”或“EMPTY”。 Sometimes Spark can’t properly plan for a broadcast join if it doesn’t know any statistics about the input DataFrame or table. If you know that one of the tables that you are joining is small, you can try to force a broadcast (as discussed in Chapter 8), or use Spark’s statistics collection commands to let it analyze the table. 如果 Spark 不知道有关输入DataFrame或表的任何统计信息，Spark有时无法正确规划广播连接（broadcast join）。如果您知道要加入的其中一个表很小，则可以尝试强制广播（如第8章中所述），或使用Spark的统计信息收集命令让它分析表。 Slow Reads and Writes 慢的读和写Slow I/O can be difficult to diagnose, especially with networked file systems. 慢速 I/O 可能难以诊断，尤其是对于网络文件系统。 Signs and symptoms 迹象和症状Slow reading of data from a distributed file system or external system. Slow writes from network file systems or Blob storage. 从分布式文件系统或外部系统缓慢读取数据。从网络文件系统或 Blob 存储缓慢写入。 Potential treatments 潜在的治疗Turning on speculation (set spark.speculation to true) can help with slow reads and writes. This will launch additional tasks with the same operation in an attempt to see whether it’s just some transient issue in the first task. Speculation is a powerful tool and works well with consistent file systems. However, it can cause duplicate data writes with some eventually consistent cloud services, such as Amazon S3, so check whether it is supported by the storage system connector you are using. 打开推测（将 spark.speculation 设置为 true）可以帮助减慢读取和写入。这将使用相同的操作启动其他任务，以尝试查看它是否只是第一个任务中的一些短暂问题。推测(speculation)是一种功能强大的工具，适用于一致的文件系统。但是，它可能导致重复数据写入与一些最终一致的云服务（如Amazon S3），因此请检查您使用的存储系统连接器是否支持它。 Ensuring sufficient network connectivity can be important—your Spark cluster may simply not have enough total network bandwidth to get to your storage system. 确保足够的网络连接非常重要——您的 Spark 集群可能根本没有足够的总网络带宽来访问您的存储系统。 For distributed file systems such as HDFS running on the same nodes as Spark, make sure Spark sees the same hostnames for nodes as the file system. This will enable Spark to do locality-aware scheduling, which you will be able to see in the “locality” column in the Spark UI. We’ll talk about locality a bit more in the next chapter. 对于与Spark在相同节点上运行的分布式文件系统（如HDFS），请确保Spark看到与文件系统相同的节点主机名。这将使Spark能够进行关注局部性的调度，您可以在Spark UI的“locality”列中看到该调度。我们将在下一章中讨论一下局部性。 Driver OutOfMemoryError or Driver Unresponsive 驱动程序OutOfMemoryError或驱动程序无响应This is usually a pretty serious issue because it will crash your Spark Application. It often happens due to collecting too much data back to the driver, making it run out of memory. 这通常是一个相当严重的问题，因为它会使您的Spark应用程序崩溃。它经常发生，因为收集了太多的数据回到驱动程序，使其耗尽内存。 Signs and symptoms 迹象和症状 Spark Application is unresponsive or crashed. OutOfMemoryErrors or garbage collection messages in the driver logs. Spark应用程序无响应或崩溃。驱动程序日志中的OutOfMemoryErrors或垃圾回收消息。 Commands take a very long time to run or don’t run at all.命令需要很长时间才能运行或根本不运行。 Interactivity is very low or non-existent.交互性很低或根本不存在。 Memory usage is high for the driver JVM.驱动程序JVM的内存使用率很高。 Potential treatments 可能的治疗方法There are a variety of potential reasons for this happening, and diagnosis is not always straightforward. 这种情况有多种可能的原因，诊断并不总是直截了当的。 Your code might have tried to collect an overly large dataset to the driver node using operations such as collect. 您的代码可能尝试使用诸如collect之类的操作将过大的数据集收集到驱动程序节点。 You might be using a broadcast join where the data to be broadcast is too big. Use Spark’s maximum broadcast join configuration to better control the size it will broadcast. 您可能正在使用广播连接，其中要广播的数据太大。使用Spark的最大广播连接配置可以更好地控制它将广播的大小。 A long-running application generated a large number of objects on the driver and is unable to release them. Java’s jmap tool can be useful to see what objects are filling most of the memory of your driver JVM by printing a histogram of the heap. However, take note that jmap will pause that JVM while running. Increase the driver’s memory allocation if possible to let it work with more data. 长时间运行的应用程序在驱动程序上生成了大量对象，无法释放它们。 Java 的 jmap 工具可以通过打印堆的直方图来查看哪些对象填充了驱动程序 JVM 的大部分内存。但请注意，jmap 会在运行时暂停该JVM。如果可能的话，增加驱动程序的内存分配，让它可以处理更多数据。 Issues with JVMs running out of memory can happen if you are using another language binding, such as Python, due to data conversion between the two requiring too much memory in the JVM. Try to see whether your issue is specific to your chosen language and bring back less data to the driver node, or write it to a file instead of bringing it back as in-memory objects. 如果您使用其他语言绑定（如Python），JVM内存不足会出现问题，因为两者之间的数据转换需要JVM中的内存过多。尝试查看您的问题是否特定于您选择的语言，并将较少的数据带回驱动程序节点，或将其写入文件而不是将其作为内存中对象重新引入。 If you are sharing a SparkContext with other users (e.g., through the SQL JDBC server and some notebook environments), ensure that people aren’t trying to do something that might be causing large amounts of memory allocation in the driver (like working overly large arrays in their code or collecting large datasets). 如果您与其他用户共享 SparkContext（例如，通过SQL JDBC服务器和某些 notebook 环境），请确保人们不会尝试执行可能导致驱动程序中大量内存分配的操作（例如，过度工作代码中的大型数组或收集大型数据集）。 Executor OutOfMemoryError or Executor Unresponsive Executor OutOfMemoryError或Executor无响应Spark applications can sometimes recover from this automatically, depending on the true underlyingissue. Spark应用程序有时可以自动从中恢复，具体取决于真正的底层问题。 Signs and symptoms 迹象和症状 OutOfMemoryErrors or garbage collection messages in the executor logs. You can find these in the Spark UI. 执行程序(executor)日志中的OutOfMemoryErrors或垃圾回收消息。您可以在Spark UI中找到它们。 Executors that crash or become unresponsive. 崩溃或无响应的执行程序（executor）。 Slow tasks on certain nodes that never seem to recover. 某些节点上的缓慢任务似乎永远无法恢复。 Potential treatments 潜在的疗法 Try increasing the memory available to executors and the number of executors. 尝试增加执行程序(executor)可用的内存和执行程序(executor)的数量。 Try increasing PySpark worker size via the relevant Python configurations. 尝试通过相关的Python配置增加PySpark工作者大小。 Look for garbage collection error messages in the executor logs. Some of the tasks that are running, especially if you’re using UDFs, can be creating lots of objects that need to be garbage collected. Repartition your data to increase parallelism, reduce the amount of records per task, and ensure that all executors are getting the same amount of work. 在执行程序(executor)日志中查找垃圾收集错误消息。正在运行的某些任务（尤其是在使用UDF时）可能会创建大量需要进行垃圾回收的对象。重新分区数据以增加并行度，减少每个任务的记录数量，并确保所有执行程序(executor)获得相同的工作量。 Ensure that null values are handled correctly (that you’re using null) and not some default value like “ “ or “EMPTY”, as we discussed earlier. 确保正确处理空值（您正在使用null）而不是像我们之前讨论的那样的默认值，如“”或“EMPTY”。 This is more likely to happen with RDDs or with Datasets because of object instantiations. 由于对象实例化，这更有可能发生在 RDD 或 Datasets 中。 Try using fewer UDFs and more of Spark’s structured operations when possible. 尽可能尝试使用更少的UDF和更多Spark的结构化操作。 Use Java monitoring tools such as jmap to get a histogram of heap memory usage on your executors, and see which classes are taking up the most space. 使用 jmap 等Java监视工具获取执行程序(executor)堆内存使用情况的直方图，并查看哪些类占用的空间最多。 If executors are being placed on nodes that also have other workloads running on them, such as a key-value store, try to isolate your Spark jobs from other jobs. 如果将执行程序(executor)放置在也运行其他工作负载的节点上（例如键值存储），请尝试将Spark作业与其他作业隔离开来。 Unexpected Nulls in Results 结果中出现意外空白Signs and symptoms 迹象和症状 Unexpected null values after transformations. 转换后出现意外的空值。 Scheduled production jobs that used to work no longer work, or no longer produce the right results. 过去工作安排的生产作业不再起作用，或者不再产生正确的结果。 Potential treatments 潜在的治疗 It’s possible that your data format has changed without adjusting your business logic. This means that code that worked before is no longer valid. 您的数据格式可能已更改，而无需调整业务逻辑。这意味着以前工作的代码不再有效。 Use an accumulator to try to count records or certain types, as well as parsing or processing errors where you skip a record. This can be helpful because you might think that you’re parsing data of a certain format, but some of the data doesn’t. Most often, users will place the accumulator in a UDF when they are parsing their raw data into a more controlled format and perform the counts there. This allows you to count valid and invalid records and then operate accordingly after the fact. 使用累加器尝试计算记录或某些类型，以及解析或处理跳过记录的错误。这可能很有用，因为您可能认为您正在解析某种格式的数据，但有些数据却没有。大多数情况下，用户在将原始数据解析为更受控制的格式并在那里执行计数时，会将累加器放在 UDF 中。这允许您计算有效和无效的记录，然后在事后进行相应的操作。 Ensure that your transformations actually result in valid query plans. Spark SQL sometimes does implicit type coercions that can cause confusing results. For instance, the SQL expression SELECT 5“23” results in 115 because the string “25” converts to an the value 25 as an integer, but the expression SELECT 5 “ “ results in null because casting the empty string to an integer gives null. Make sure that your intermediate datasets have the schema you expect them to (try using printSchema on them), and look for any CAST operations in the final query plan. 确保您的转换实际上产生有效的查询计划。 Spark SQL有时会执行隐式类型强制，这可能会导致混乱的结果。例如，SQL表达式SELECT 5 “23”导致115，因为字符串“25”将整数转换为值25，但表达式SELECT 5 “”导致null，因为将空字符串转换为整数给出null。确保您的中间数据集具有您期望的模式（尝试对它们使用printSchema），并在最终查询计划中查找任何CAST操作。 ​ No Space Left on Disk Errors 磁盘错误没有剩余空间Signs and symptoms 迹象和症状 You see “no space left on disk” errors and your jobs fail. 您看到“磁盘上没有剩余空间”错误，您的作业失败。 Potential treatments 潜在的治疗 The easiest way to alleviate this, of course, is to add more disk space. You can do this by sizing up the nodes that you’re working on or attaching external storage in a cloud environment. 当然，减轻这种情况的最简单方法是添加更多磁盘空间。您可以通过调整正在处理的节点或在云环境中连接外部存储来实现此目的。 If you have a cluster with limited storage space, some nodes may run out first due to skew. 如果您的集群存储空间有限，则某些节点可能会由于数据倾斜而首先耗尽。 Repartitioning the data as described earlier may help here. 如前所述重新分区数据可能对此有所帮助。 There are also a number of storage configurations with which you can experiment. Some of these determine how long logs should be kept on the machine before being removed. For more information, see the Spark executor logs rolling configurations in Chapter 16. 您还可以使用许多存储配置进行试验。其中一些决定了在移除之前应该在机器上保留多长时间的日志。有关更多信息，请参阅第16章中的Spark执行程序(executor)日志滚动配置。 Try manually removing some old log files or old shuffle files from the machine(s) in question. This can help alleviate some of the issue although obviously it’s not a permanent fix. 尝试从相关机器手动删除一些旧的日志文件或旧的随机洗牌文件。这可以帮助减轻一些问题，虽然显然它不是永久性的修复。 Serialization Errors 序列化错误Signs and symptoms 迹象和症状 You see serialization errors and your jobs fail.您看到序列化错误，您的作业失败。 Potential treatments潜在的治疗方法 This is very uncommon when working with the Structured APIs, but you might be trying to perform some custom logic on executors with UDFs or RDDs and either the task that you’re trying to serialize to these executors or the data you are trying to share cannot be serialized. This often happens when you’re working with either some code or data that cannot be serialized into a UDF or function, or if you’re working with strange data types that cannot be serialized. If you are using (or intend to be using Kryo serialization), verify that you’re actually registering your classes so that they are indeed serialized. 这在使用结构化API时非常罕见，但您可能尝试使用UDF或RDD在执行程序(executor)上执行某些自定义逻辑，以及您尝试序列化到这些执行程序(executor)的任务或您尝试共享的数据无法序列化。当您使用某些无法序列化为UDF或函数的代码或数据时，或者您正在使用无法序列化的奇怪数据类型时，通常会发生这种情况。如果您正在使用（或打算使用Kryo序列化），请验证您实际上是在注册类，以便它们确实是序列化的。 Try not to refer to any fields of the enclosing object in your UDFs when creating UDFs inside a Java or Scala class. This can cause Spark to try to serialize the whole enclosing object, which may not be possible. Instead, copy the relevant fields to local variables in the same scope as closure and use those. 在Java或Scala类中创建UDF时，尽量不要引用UDF中封闭对象的任何字段。这可能导致Spark尝试序列化整个封闭对象，这可能是不可能的。相反，将相关字段复制到与闭包相同的范围内的局部变量并使用它们。 Conclusion 结论This chapter covered some of the main tools that you can use to monitor and debug your Spark jobs and applications, as well as the most common issues we see and their resolutions. As with debugging any complex software, we recommend taking a principled, step-by-step approach to debug issues. Add logging statements to figure out where your job is crashing and what type of data arrives at each stage, try to isolate the problem to the smallest piece of code possible, and work up from there. For data skew issues, which are unique to parallel computing, use Spark’s UI to get a quick overview of how much work each task is doing. In Chapter 19, we discuss performance tuning in particular and various tools you can use for that. 本章介绍了一些可用于监视和调试Spark作业和应用程序的主要工具，以及我们看到的最常见问题及其解决方案。与调试任何复杂软件一样，我们建议采用有原则的逐步方法来调试问题。添加日志记录语句以确定作业崩溃的位置以及每个阶段到达的数据类型，尝试将问题隔离到可能的最小代码段，并从那里开始工作。对于并行计算所特有的数据偏差问题，请使用Spark的UI快速了解每项任务的工作量。在第19章中，我们特别讨论了性能调优以及可以使用的各种工具。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 23 Structured Streaming in Production]]></title>
    <url>%2F2019%2F08%2F10%2FChapter23_StructuredStreamingInProduction(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 23 Structured Streaming in Production 生产环境中的结构化流The previous chapters of this part of the book have covered Structured Streaming from a user’s perspective. Naturally this is the core of your application. This chapter covers some of the operational tools needed to run Structured Streaming robustly in production after you’ve developed an application. 本书这一部分的前几章从用户的角度介绍了结构化流。当然，这是应用程序的核心。本章介绍在开发应用程序之后，在生产环境中可靠地运行结构化流所需的一些操作工具。 Structured Streaming was marked as production-ready in Apache Spark 2.2.0, meaning that this release has all the features required for production use and stabilizes the API. Many organizations are already using the system in production because, frankly, it’s not much different from running other production Spark applications. Indeed, through features such as transactional sources/sinks and exactly-once processing, the Structured Streaming designers sought to make it as easy to operate as possible. This chapter will walk you through some of the key operational tasks specific to Structured Streaming. This should supplement everything we saw and learned about Spark operations in Part II. 结构化流在 Apache Spark 2.2.0中被标记为生产就绪，这意味着该版本具有生产使用所需的所有功能，并稳定了API。许多组织已经在生产中使用该系统，因为坦率地说，它与运行其他生产Spark应用程序没有太大区别。事实上，通过事务性源/接收器和一次性处理等功能，结构化流设计人员力求使其尽可能易于操作。本章将引导您完成特定于结构化流的一些关键操作任务。这应该补充我们在第二部分中看到和学到的关于 Spark 操作的所有知识。 Fault Tolerance and Checkpointing 容错和检查点The most important operational concern for a streaming application is failure recovery. Faults are inevitable: you’re going to lose a machine in the cluster, a schema will change by accident without a proper migration, or you may even intentionally restart the cluster or application. In any of these cases, Structured Streaming allows you to recover an application by just restarting it. To do this, you must configure the application to use checkpointing and write-ahead logs, both of which are handled automatically by the engine. Specifically, you must configure a query to write to a checkpoint location on a reliable file system (e.g., HDFS, S3, or any compatible filesystem). Structured Streaming will then periodically save all relevant progress information (for instance, the range of offsets processed in a given trigger) as well as the current intermediate state values to the checkpoint location. In a failure scenario, you simply need to restart your application, making sure to point to the same checkpoint location, and it will automatically recover its state and start processing data where it left off. You do not have to manually manage this state on behalf of the application—Structured Streaming does it for you. 流应用程序最重要的操作问题是故障恢复。错误是不可避免的：您将失去集群中的一台机器，一个模式将在没有适当迁移的情况下意外更改，或者您甚至可能有意重新启动集群或应用程序。在这些情况下，结构化流允许您通过重新启动应用程序来恢复应用程序。为此，必须将应用程序配置为使用检查点和提前写入日志，这两个日志都由引擎自动处理。具体来说，您必须配置一个查询，以写入可靠文件系统（例如，HDFS、S3或任何兼容的文件系统）上的检查点位置。结构化流将定期将所有相关的进度信息（例如，在给定触发器中处理的偏移范围）以及当前中间状态值保存到检查点位置。在失败的情况下，只需重新启动应用程序，确保指向相同的检查点位置，它将自动恢复其状态，并在停止的地方开始处理数据。您不必代表应用程序手动管理此状态，结构化流为您做到了这一点。 To use checkpointing, specify your checkpoint location before starting your application through the checkpointLocation option on writeStream. You can do this as follows: 要使用检查点，请在通过 writeStream 上的检查点位置选项启动应用程序之前指定检查点位置。您可以这样做： 123456789101112131415// in Scalaval static = spark.read.json("/data/activity-data")val streaming = spark.readStream.schema(static.schema).option("maxFilesPerTrigger", 10).json("/data/activity-data").groupBy("gt").count()val query = streaming.writeStream.outputMode("complete").option("checkpointLocation", "/some/location/").queryName("test_stream").format("memory").start() 12345678910111213141516# in Pythonstatic = spark.read.json("/data/activity-data")streaming = spark\.readStream\.schema(static.schema)\.option("maxFilesPerTrigger", 10)\.json("/data/activity-data")\.groupBy("gt")\.count()query = streaming\.writeStream\.outputMode("complete")\.option("checkpointLocation", "/some/python/location/")\.queryName("test_python_stream")\.format("memory")\.start() If you lose your checkpoint directory or the information inside of it, your application will not be able to recover from failures and you will have to restart your stream from scratch. 如果丢失了检查点目录或其中的信息，应用程序将无法从失败中恢复，您必须从头开始重新启动流。 Updating Your Application 更新应用程序 Checkpointing is probably the most important thing to enable in order to run your applications in production. This is because the checkpoint will store all of the information about what your stream has processed thus far and what the intermediate state it may be storing is. However, checkpointing does come with a small catch—you’re going to have to reason about your old checkpoint data when you update your streaming application. When you update your application, you’re going to have to ensure that your update is not a breaking change. Let’s cover these in detail when we review the two types of updates: either an update to your application code or running a new Spark version. 为了在生产环境中运行应用程序，检查点可能是最重要的。这是因为检查点将存储到目前为止您的流处理的内容以及它可能存储的中间状态的所有信息。但是，检查点的出现只是一个小问题，当您更新流应用程序时，您必须考虑旧的检查点数据。当你更新你的应用程序时，你必须确保你的更新不是一个突破性的改变。当我们回顾这两种类型的更新时，让我们详细介绍一下这些：应用程序代码的更新或者运行一个新的 Spark 版本。 Updating Your Streaming Application Code 更新流应用程序代码Structured Streaming is designed to allow certain types of changes to the application code between application restarts. Most importantly, you are allowed to change user-defined functions (UDFs) as long as they have the same type signature. This feature can be very useful for bug fixes. For example, imagine that your application starts receiving a new type of data, and one of the data parsing functions in your current logic crashes. With Structured Streaming, you can recompile the application with a new version of that function and pick up at the same point in the stream where it crashed earlier. While small adjustments like adding a new column or changing a UDF are not breaking changes and do not require a new checkpoint directory, there are larger changes that do require an entirely new checkpoint directory. For example, if you update your streaming application to add a new aggregation key or fundamentally change the query itself, Spark cannot construct the required state for the new query from an old checkpoint directory. In these cases, Structured Streaming will throw an exception saying it cannot begin from a checkpoint directory, and you must start from scratch with a new (empty) directory as your checkpoint location. 结构化流的设计允许在应用程序重新启动之间对应用程序代码进行某些类型的更改。最重要的是，您可以更改用户定义函数（UDF），只要它们具有相同的类型签名。这个特性对于错误修复非常有用。例如，假设应用程序开始接收新类型的数据，并且当前逻辑崩溃时的一个数据解析函数。使用结构化流，您可以使用该函数的新版本重新编译应用程序，并在流中之前崩溃的同一点上继续进行。虽然诸如添加新列或更改UDF之类的小调整不是突破性的改变，也不需要新的检查点目录，但仍有较大的更改需要全新的检查点目录。例如，如果更新流应用程序以添加新的聚合键或从根本上更改查询本身，Spark将无法从旧的检查点目录构造新查询所需的状态。在这些情况下，结构化流将抛出一个异常，说明它不能从检查点目录开始，并且必须从头开始，使用一个新的（空）目录作为检查点位置。 Updating Your Spark Version 更新Spark版本Structured Streaming applications should be able to restart from an old checkpoint directory across patch version updates to Spark (e.g., moving from Spark 2.2.0 to 2.2.1 to 2.2.2). The checkpoint format is designed to be forward-compatible, so the only way it may be broken is due to critical bug fixes. If a Spark release cannot recover from old checkpoints, this will be clearly documented in its release notes. The Structured Streaming developers also aim to keep the format compatible across minor version updates (e.g., Spark 2.2.x to 2.3.x), but you should check the release notes to see whether this is supported for each upgrade. In either case, if you cannot start from a checkpoint, you will need to start your application again using a new checkpoint directory. 结构化流应用程序应该能够从旧的检查点目录跨补丁版本更新重新启动到 Spark（例如，从 Spark 2.2.0迁移到2.2.1到2.2.2）。检查点格式设计为向前兼容，因此唯一可能被破坏的方法是修复关键的错误。如果Spark发行版不能从旧的检查点恢复，那么它的发行说明中会清楚地记录这一点。结构化流式开发人员还致力于保持格式在次要版本更新（例如spark 2.2.x到2.3.x）之间的兼容性，但是您应该检查发行说明，以查看是否支持每次升级。在这两种情况下，如果无法从检查点启动，则需要使用新的检查点目录重新启动应用程序。 Sizing and Rescaling Your Application 调整应用程序的大小和重新缩放In general, the size of your cluster should be able to comfortably handle bursts above your data rate. The key metrics you should be monitoring in your application and cluster are discussed as follows. In general, if you see that your input rate is much higher than your processing rate (elaborated upon momentarily), it’s time to scale up your cluster or application. Depending on your resource manager and deployment, you may just be able to dynamically add executors to your application. When it comes time, you can scale-down your application in the same way—remove executors (potentially through your cloud provider) or restart your application with lower resource counts. These changes will likely incur some processing delay (as data is recomputed or partitions are shuffled around when executors are removed). In the end, it’s a business decision as to whether it’s worthwhile to create a system with more sophisticated resource management capabilities. 一般来说，集群的大小应该能够轻松地处理高于数据速率的突发事件。您应该在应用程序和集群中监控的关键指标讨论如下。一般来说，如果您看到您的输入速率远远高于您的处理速率（马上详细描述），那么是时候扩展集群或应用程序了。根据您的资源管理器和部署，您可能只能动态地向应用程序添加执行器。当遇到这种情况时，您可以用同样的方法缩小应用程序的规模，删除执行者（可能通过云提供商）或以较低的资源计数重新启动应用程序。这些更改可能会导致一些处理延迟（当执行器被删除时，数据会重新计算或分区会四处移动）。最后，对于是否值得创建一个具有更复杂资源管理功能的系统，这是一个业务决策。 While making underlying infrastructure changes to the cluster or application are sometimes necessary, other times a change may only require a restart of the application or stream with a new configuration. For instance, changing spark.sql.shuffle.partitions is not supported while a stream is currently running (it won’t actually change the number of shuffle partitions). This requires restarting the actual stream, not necessarily the entire application. Heavier weight changes, like changing arbitrary Spark application configurations, will likely require an application restart. 虽然有时需要对集群或应用程序进行基础结构更改，但在其他情况下，更改可能只需要用新配置重新启动应用程序或流。例如，当流当前正在运行时，不支持更改 spark.sql.shuffle.partitions（它实际上不会更改shuffle分区的数目）。这需要重新启动实际流，而不一定是整个应用程序。更重的重量变化，如改变任意的 Spark 应用程序配置，可能需要重新启动应用程序。 Metrics and Monitoring 量化指标和监控Metrics and monitoring in streaming applications is largely the same as for general Spark applications using the tools described in Chapter 18. However, Structured Streaming does add several more specifics in order to help you better understand the state of your application. There are two key APIs you can leverage to query the status of a streaming query and see its recent execution progress. With these two APIs, you can get a sense of whether or not your stream is behaving as expected. 流应用程序中的量化指标和监控与使用第18章中描述的工具的一般 Spark 应用程序基本相同。但是，结构化流确实添加了更多的细节，以帮助您更好地了解应用程序的状态。您可以利用两个关键API来查询流式查询的状态并查看其最近的执行进度。通过这两个API，您可以了解流是否按预期运行。 Query StatusThe query status is the most basic monitoring API, so it’s a good starting point. It aims to answer the question, “What processing is my stream performing right now?” This information is reported in the status field of the query object returned by startStream. For example, you might have a simple counts stream that provides counts of IOT devices defined by the following query (here we’re just using the same query from the previous chapter without the initialization code) : 查询状态是最基本的监控API，所以它是一个很好的起点。它的目的是回答这个问题，“我的流现在正在执行什么处理？”“此信息在 startStream 返回的查询对象的 status 字段中报告。例如，您可能有一个简单的计数流，它提供由以下查询定义的物联网设备计数（这里我们只使用上一章中的相同查询，而不使用初始化代码）： 1query.status To get the status of a given query, simply running the command query.status will return the current status of the stream. This gives us details about what is happening at that point in time in the stream. Here’s a sample of what you’ll get back when querying this status: 要获取给定查询的状态，只需运行命令 query.status 即可返回流的当前状态。这为我们提供了有关流中那个时间点发生的事情的详细信息。以下是查询此状态时将返回的示例： 12345&#123; &quot;message&quot; : &quot;Getting offsets from ...&quot;, &quot;isDataAvailable&quot; : true, &quot;isTriggerActive&quot; : true&#125; The above snippet describes getting the offsets from a Structured Streaming data source (hence the message describing getting offsets). There are a variety of messages to describe the stream’s status. 上面的代码段描述了从结构化流数据源获取偏移量（因此描述获取偏移量的消息）。有各种各样的消息来描述流的状态。 NOTE We have shown the status command inline here the way you would call it in a Spark shell. However, for a standalone application, you may not have a shell attached to run arbitrary code inside your process. In that case, you can expose its status by implementing a monitoring server, such as a small HTTP server that listens on a port and returns query.status when it gets a request. Alternatively, you can use the richer StreamingQueryListener API described later to listen to more events. 我们已经在这里显示了 status 命令，您可以在 Spark shell中调用它。但是，对于独立的应用程序，可能没有附加 shell 到进程内运行任意代码。在这种情况下，您可以通过实现监控服务器来公开其状态，例如在端口上侦听并在收到请求时返回 query.status 的小型 HTTP 服务器。或者，您可以使用后面描述的更丰富的 streamingQueryListener API来监听更多的事件。 Recent ProgressWhile the query’s current status is useful to see, equally important is an ability to view the query’s progress. The progress API allows us to answer questions like “At what rate am I processing tuples?” or “How fast are tuples arriving from the source?” By running query.recentProgress, you’ll get access to more time-based information like the processing rate and batch durations. The streaming query progress also includes information about the input sources and output sinks behind your stream. 虽然查询的当前状态很有用，但查看查询进度的能力同样重要。progress API 允许我们回答“我以什么速率处理元组（tuples）？”或者“元组（tuples）从源文件到达的速度有多快？”“通过运行 query.recentProgress，您可以访问更多基于时间的信息，如处理速率和批处理持续时间。流查询进度还包括有关流后面的输入源和输出接收器的信息。 1query.recentProgress Here’s the result of the Scala version after we ran the code from before; the Python one will be similar: 下面是 Scala 版本在运行之前的代码之后的结果；Python 版本将类似： 1234567891011121314151617181920212223242526272829303132Array(&#123; &quot;id&quot; : &quot;d9b5eac5-2b27-4655-8dd3-4be626b1b59b&quot;, &quot;runId&quot; : &quot;f8da8bc7-5d0a-4554-880d-d21fe43b983d&quot;, &quot;name&quot; : &quot;test_stream&quot;, &quot;timestamp&quot; : &quot;2017-08-06T21:11:21.141Z&quot;, &quot;numInputRows&quot; : 780119, &quot;processedRowsPerSecond&quot; : 19779.89350912779, &quot;durationMs&quot; : &#123; &quot;addBatch&quot; : 38179, &quot;getBatch&quot; : 235, &quot;getOffset&quot; : 518, &quot;queryPlanning&quot; : 138, &quot;triggerExecution&quot; : 39440, &quot;walCommit&quot; : 312 &#125;, &quot;stateOperators&quot; : [ &#123; &quot;numRowsTotal&quot; : 7, &quot;numRowsUpdated&quot; : 7 &#125; ], &quot;sources&quot; : [ &#123; &quot;description&quot; : &quot;FileStreamSource[/some/stream/source/]&quot;, &quot;startOffset&quot; : null, &quot;endOffset&quot; : &#123; &quot;logOffset&quot; : 0 &#125;, &quot;numInputRows&quot; : 780119, &quot;processedRowsPerSecond&quot; : 19779.89350912779 &#125; ], &quot;sink&quot; : &#123; &quot;description&quot; : &quot;MemorySink&quot; &#125;&#125;) As you can see from the output just shown, this includes a number of details about the state of the stream. It is important to note that this is a snapshot in time (according to when we asked for the query progress). In order to consistently get output about the state of the stream, you’ll need to query this API for the updated state repeatedly. The majority of the fields in the previous output should be selfexplanatory. However, let’s review some of the more consequential fields in detail. 正如您从刚刚显示的输出中看到的那样，这包括一些关于流状态的详细信息。需要注意的是，这是一个及时的快照（根据我们何时请求查询进度）。为了一致地获得有关流状态的输出，您需要反复查询此API以获取更新状态。上一个输出中的大多数字段都应该是一目了然的。但是，让我们详细回顾一些更重要的字段。 Input rate and processing rate 输入速率和处理速率The input rate specifies how much data is flowing into Structured Streaming from our input source. The processing rate is how quickly the application is able to analyze that data. In the ideal case, the input and processing rates should vary together. Another case might be when the input rate is much greater than the processing rate. When this happens, the stream is falling behind and you will need to scale the cluster up to handle the larger load. 输入速率指定从输入源流入结构化流的数据量。处理速度是应用程序分析数据的速度。在理想情况下，输入和处理速率应该同时变化。另一种情况可能是输入速率远远大于处理速率。当这种情况发生时，流将落在后面，您需要向上扩展集群以处理更大的负载。 Batch duration 批处理持续时间Nearly all streaming systems utilize batching to operate at any reasonable throughput (some have an option of high latency in exchange for lower throughput). Structured Streaming achieves both. As it operates on the data, you will likely see batch duration oscillate as Structured Streaming processes varying numbers of events over time. Naturally, this metric will have little to no relevance when the continuous processing engine is made an execution option. 几乎所有的流系统都利用批处理以任何合理的吞吐量运行（有些系统可以选择高延迟，以换取较低的吞吐量）。结构化流实现了这两个目标。当它对数据进行操作时，您可能会看到批处理持续时间随着结构化流处理时间的变化而波动。当然，当连续处理引擎成为一个执行选项时，这个量化指标几乎没有相关性。 TIP 提示 Generally it’s a best practice to visualize the changes in batch duration and input and processing rates. It’s much more helpful than simply reporting changes over time. 一般来说，将批处理持续时间、输入和处理速率的变化可视化是最佳实践。它比简单地报告随时间变化更有用。 Spark UI Spark用户界面The Spark web UI, covered in detail in Chapter 18, also shows tasks, jobs, and data processing metrics for Structured Streaming applications. On the Spark UI, each streaming application will appear as a sequence of short jobs, one for each trigger. However, you can use the same UI to see metrics, query plans, task durations, and logs from your application. One departure of note from the DStream API is that the Streaming Tab is not used by Structured Streaming. 第18章详细介绍了Spark Web 用户界面，它还显示了结构化流应用程序的任务、作业和数据处理指标。在Spark用户界面上，每个流式应用程序将显示为一系列短作业，每个触发器一个。但是，您可以使用同一个UI查看来自应用程序的量化指标、查询计划、任务工期和日志。与 DStream API 不同的一点是，结构化流不使用流选项卡。 Alerting 警告Understanding and looking at the metrics for your Structured Streaming queries is an important first step. However, this involves constantly watching a dashboard or the metrics in order to discover potential issues. You’re going to need robust automatic alerting to notify you when your jobs are failing or not keeping up with the input data rate without monitoring them manually. There are several ways to integrate existing alerting tools with Spark, generally building on the recent progress API we covered before. For example, you may directly feed the metrics to a monitoring system such as the open source Coda Hale Metrics library or Prometheus, or you may simply log them and use a log aggregation system like Splunk. In addition to monitoring and alerting on queries, you’re also going to want to monitor and alert on the state of the cluster and the overall application (if you’re running multiple queries together). 了解和查看结构化流式查询的指标是重要的第一步。但是，这需要不断观察仪表盘或指标，以发现潜在的问题。当你的工作失败或者没有手动监控就不能跟上输入数据速率时，你需要强大的自动警报来通知你。有几种方法可以将现有的警报工具与Spark集成在一起，通常基于我们之前介绍的新近发展的API。例如，您可以直接将量化指标输入监控系统，如开源 Coda Hale Metrics 库或 Prometheus ，也可以简单地将其记录并使用日志聚合系统，如Splunk。除了对查询进行监控和警报之外，您还需要对集群和整个应用程序的状态进行监控和发出警报（如果您一起运行多个查询）。 Advanced Monitoring with the Streaming Listener 使用流式侦听器进行高级监控We already touched on some of the high-level monitoring tools in Structured Streaming. With a bit of glue logic, you can use the status and queryProgress APIs to output monitoring events into your organization’s monitoring platform of choice (e.g., a log aggregation system or Prometheus dashboard). Beyond these approaches, there is also a lower-level but more powerful way to observe an application’s execution: the StreamingQueryListener class. 我们已经讨论了结构化流中的一些高级监控工具。使用一些粘合逻辑，您可以使用状态和 queryProgress API将监控事件输出到组织的监控平台（例如，日志聚合系统或 Prometheus 仪表板）。除了这些方法之外，还有一种更低阶但更强大的方法来观察应用程序的执行：StreamingQueryListener 类。 The StreamingQueryListener class will allow you to receive asynchronous updates from the streaming query in order to automatically output this information to other systems and implement robust monitoring and alerting mechanisms. You start by developing your own object to extend StreamingQueryListener, then attach it to a running SparkSession. Once you attach your custom listener with sparkSession.streams.addListener(), your class will receive notifications when a query is started or stopped, or progress is made on an active query. Here’s a simple example of a listener from the Structured Streaming documentation: StreamingQueryListener 类将允许您从流查询接收异步更新，以便自动将此信息输出到其他系统，并实现可靠的监控和警报机制。首先开发自己的对象来扩展 StreamingQueryListener，然后将其附加到正在运行的SparkSession。使用 sparkSession.streams.addListener（）附加自定义侦听器后，当查询启动或停止，或在活动查询上取得进展时，类将收到通知。以下是结构化流文档中侦听器的简单示例： 123456789101112131415val spark: SparkSession = ... spark.streams.addListener(new StreamingQueryListener() &#123; override def onQueryStarted(queryStarted: QueryStartedEvent): Unit = &#123; println("Query started: " + queryStarted.id) &#125; override def onQueryTerminated(queryTerminated: QueryTerminatedEvent): Unit = &#123; println("Query terminated: " + queryTerminated.id) &#125; override def onQueryProgress(queryProgress: QueryProgressEvent): Unit = &#123; println("Query made progress: " + queryProgress.progress) &#125;&#125;) Streaming listeners allow you to process each progress update or status change using custom code and pass it to external systems. For example, the following code for a StreamingQueryListener that will forward all query progress information to Kafka. You’ll have to parse this JSON string once you read data from Kafka in order to access the actual metrics: 流式侦听器（streaming listeners）允许您使用自定义代码处理每个进度更新或状态更改，并将其传递给外部系统。例如，下面的代码用于将所有查询进度信息转发到 Kafka 的 StreamingQueryListener。从Kafka读取数据后，必须解析这个JSON字符串，才能访问实际的量化指标： 12345678910111213141516171819class KafkaMetrics(servers: String) extends StreamingQueryListener &#123; val kafkaProperties = new Properties() kafkaProperties.put("bootstrap.servers", servers) kafkaProperties.put( "key.serializer", "kafkashaded.org.apache.kafka.common.serialization.StringSerializer") kafkaProperties.put( "value.serializer", "kafkashaded.org.apache.kafka.common.serialization.StringSerializer") val producer = new KafkaProducer[String, String](kafkaProperties) import org.apache.spark.sql.streaming.StreamingQueryListener import org.apache.kafka.clients.producer.KafkaProduceroverride def onQueryProgress(event : StreamingQueryListener.QueryProgressEvent): Unit = &#123; producer.send(new ProducerRecord("streaming-metrics", event.progress.json)) &#125; override def onQueryStarted(event: StreamingQueryListener.QueryStartedEvent) : Unit = &#123;&#125; override def onQueryTerminated(event: StreamingQueryListener.QueryTerminatedEvent) : Unit = &#123;&#125;&#125; Using the StreamingQueryListener interface, you can even monitor Structured Streaming applications on one cluster by running a Structured Streaming application on that same (or another) cluster. You could also manage multiple streams in this way. 使用streamingquerylistener接口，您甚至可以通过在同一个（或另一个）集群上运行结构化流应用程序来监控一个集群上的结构化流应用程序。您还可以用这种方式管理多个流。 Conclusion 结论In this chapter, we covered the main tools needed to run Structured Streaming in production: checkpoints for fault tolerance and various monitoring APIs that let you observe how your application is running. Lucky for you, if you’re running Spark in production already, many of the concepts and tools are similar, so you should be able to reuse a lot of your existing knowledge. Be sure to check Part IV to see some other helpful tools for monitoring Spark Applications. 在本章中，我们介绍了在生产环境中运行结构化流所需的主要工具：容错检查点和各种监控API，这些API允许您观察应用程序的运行情况。幸运的是，如果您已经在生产中运行了Spark，那么许多概念和工具都是类似的，因此您应该能够重用大量现有的知识。一定要检查第四部分，看看其他一些有助于监测 Spark 应用的工具。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 17 Deploying Spark]]></title>
    <url>%2F2019%2F08%2F07%2FChapter17_Deploying-Spark(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 17 Deploying SparkThis chapter explores the infrastructure you need in place for you and your team to be able to run Spark Applications: 本章将探讨您和您的团队能够运行Spark应用程序所需的基础架构： Cluster deployment choices 集群部署选择 Spark’s different cluster managers Spark的不同集群管理器 Deployment considerations and configuring deployments 部署考虑事项和配置部署 For the most, part Spark should work similarly with all the supported cluster managers ; however, customizing the setup means understanding the intricacies of each of the cluster management systems. The hard part is deciding on the cluster manager (or choosing a managed service). Although we would be happy to include all the minute details about how you can configure different cluster with different cluster managers, it’s simply impossible for this book to provide hyper-specific details for every situation in every single environment. The goal of this chapter, therefore, is not to discuss each of the cluster managers in full detail, but rather to look at their fundamental differences and to provide a reference for a lot of the material already available on the Spark website. Unfortunately, there is no easy answer to “which is the easiest cluster manager to run” because it varies so much by use case, experience, and resources. The Spark documentation site offers a lot of detail about deploying Spark with actionable examples. We do our best to discuss the most relevant points. 对于大多数人来说，Spark应该与所有受支持的集群管理器类似地工作；但是，自定义设置意味着要了解每个集群管理系统的复杂性。困难的部分是决定集群管理器（或选择托管服务）。虽然我们很乐意提供有关如何使用不同集群管理器配置不同集群的所有细节，但本书根本不可能为每个环境中的每种情况提供超特定的详细信息。因此，本章的目标不是详细讨论每个集群管理器，而是要查看它们的基本差异，并为Spark网站上已有的许多资料提供参考。不幸的是，“哪个是最容易运行的集群管理器”没有简单的答案，因为它因用例，经验和资源而变化很大。 Spark文档站点提供了有关使用可操作示例部署 Spark 的大量详细信息。我们会尽力讨论最相关的观点。As of this writing, Spark has three officially supported cluster managers : 在开始撰写本文时，Spark有三个官方支持的集群管理器： Standalone mode 独立模式 Hadoop YARN Apache Mesos These cluster managers maintain a set of machines onto which you can deploy Spark Applications. Naturally, each of these cluster managers has an opinionated view toward management, and so there are trade-offs and semantics that you will need to keep in mind. However, they all run Spark applications the same way (as covered in Chapter 16). Let’s begin with the first point: where to deploy your cluster. 这些集群管理器维护一组可以部署Spark应用程序的计算机。当然，这些集群管理者中的每一个都对管理有一种自以为是的观点，因此需要记住权衡和语义。但是，它们都以相同的方式运行 Spark 应用程序（如第16章所述）。让我们从第一点开始：部署集群的位置。 Where to Deploy Your Cluster to Run Spark Applications 在何处部署集群以运行Spark应用程序There are two high-level options for where to deploy Spark clusters: deploy in an on-premises cluster or in the public cloud. This choice is consequential and is therefore worth discussing. 在哪里部署Spark集群有两个高级选项：在内部部署集群或公共云中部署。这种选择是重要的，因此值得讨论。 On-Premises Cluster Deployments 内部部署集群部署Deploying Spark to an on-premises cluster is sometimes a reasonable option, especially for organizations that already manage their own datacenters. As with everything else, there are trade-offs to this approach. An on-premises cluster gives you full control over the hardware used, meaning you can optimize performance for your specific workload. However, it also introduces some challenges, especially when it comes to data analytics workloads like Spark. First, with on-premises deployment, your cluster is fixed in size, whereas the resource demands of data analytics workloads are often elastic. If you make your cluster too small, it will be hard to launch the occasional very large analytics query or training job for a new machine learning model, whereas if you make it large, you will have resources sitting idle. Second, for on-premises clusters, you need to select and operate your own storage system, such as a Hadoop file system or scalable key-value store. This includes setting up georeplication and disaster recovery if required. 将 Spark 部署到内部部署集群有时是一种合理的选择，特别是对于已经管理自己的数据中心的组织。与其他一切一样，这种方法存在权衡取舍。内部部署集群使您可以完全控制所使用的硬件，这意味着您可以针对特定工作负载优化性能。但是，它也带来了一些挑战，特别是在Spark等数据分析工作负载方面。首先，通过内部部署，您的集群的大小是固定的，而数据分析工作负载的资源需求通常是弹性的。如果您的集群太小，则很难为新的机器学习模型启动偶尔的非常大的分析查询或训练工作，而如果您将其扩大，则会使资源闲置。其次，对于内部部署集群，您需要选择并运行自己的存储系统，例如 Hadoop 文件系统或可伸缩键值存储。这包括在需要时设置地理复制和灾难恢复。If you are going to deploy on-premises, the best way to combat the resource utilization problem is to use a cluster manager that allows you to run many Spark applications and dynamically reassign resources between them, or even allows non-Spark applications on the same cluster. All of Spark’s supported cluster managers allow multiple concurrent applications, but YARN and Mesos have better support for dynamic sharing and also additionally support non-Spark workloads. Handling on-premisesresource sharing is likely going to be the biggest difference your users see day to day with Spark on-premises versus in the cloud: in public clouds, it’s easy to give each application its own cluster of exactly the required size for just the duration of that job. 如果要部署内部部署，解决资源利用率问题的最佳方法是使用集群管理器，它允许您运行许多 Spark 应用程序并在它们之间动态重新分配资源，甚至允许在相同集群上使用非 Spark 应用程序。所有 Spark 支持的集群管理器都允许多个并发应用程序，但 YARN 和 Mesos 可以更好地支持动态共享，还可以支持非Spark工作负载。处理资源共享可能是您的用户每天使用Spark内部部署与云中看到的最大差异：在公共云中，很容易为每个应用程序提供自己的集群，其中包含完全所需的大小那份工作。For storage, you have several different options, but covering all the trade-offs and operational details in depth would probably require its own book. The most common storage systems used for Spark are distributed file systems such as Hadoop’s HDFS and key-value stores such as Apache Cassandra. Streaming message bus systems such as Apache Kafka are also often used for ingesting data. All these systems have varying degrees of support for management, backup, and georeplication, sometimes built into the system and sometimes only through third-party commercial tools. Before choosing a storage option, we recommend evaluating the performance of its Spark connector and evaluating the available management tools. 对于存储，您有几种不同的选择，但是深入讨论所有权衡和操作细节可能需要它自己的书。用于 Spark 的最常见存储系统是分布式文件系统，例如 Hadoop 的 HDFS 和键值存储，例如 Apache Cassandra。诸如 Apache Kafka之类的流式消息总线系统也经常用于摄取数据。所有这些系统都对管理，备份和地理复制有不同程度的支持，有时内置于系统中，有时仅通过第三方商业工具。在选择存储选项之前，我们建议您评估其Spark连接器的性能并评估可用的管理工具。 Spark in the Cloud 云端SparkWhile early big data systems were designed for on-premises deployment, the cloud is now an increasingly common platform for deploying Spark. The public cloud has several advantages when it comes to big data workloads. First, resources can be launched and shut down elastically, so you can run that occasional “monster” job that takes hundreds of machines for a few hours without having to pay for them all the time. Even for normal operation, you can choose a different type of machine and cluster size for each application to optimize its cost performance—for example, launch machines with Graphics Processing Units (GPUs) just for your deep learning jobs. Second, public clouds include low-cost, georeplicated storage that makes it easier to manage large amounts of data. 虽然早期的大数据系统是为内部部署而设计的，但云现在是部署Spark的日益普遍的平台。在涉及大数据工作负载时，公共云有几个优点。首先，资源可以弹性地启动和关闭，因此您可以运行偶尔的“怪物”工作，这需要数百台机器几个小时，而无需一直为它们付费。即使是正常操作，您也可以为每个应用程序选择不同类型的计算机和群集大小，以优化其性价比——例如，仅为您的深度学习作业启动具有图形处理单元（GPU）的计算机。其次，公共云包括低成本，地理复制的存储，可以更轻松地管理大量数据。Many companies looking to migrate to the cloud imagine they’ll run their applications in the same way that they run their on-premises clusters. All the major cloud providers (Amazon Web Services [AWS], Microsoft Azure, Google Cloud Platform [GCP], and IBM Bluemix) include managed Hadoop clusters for their customers, which provide HDFS for storage as well as Apache Spark. This is actually not a great way to run Spark in the cloud, however, because by using a fixed-size cluster and file system, you are not going to be able to take advantage of elasticity. Instead, it is generally a better idea to use global storage systems that are decoupled from a specific cluster, such as Amazon S3, Azure Blob Storage, or Google Cloud Storage and spin up machines dynamically for each Spark workload. With decoupled compute and storage, you will be able to pay for computing resources only when needed, scale them up dynamically, and mix different hardware types. Basically, keep in mind that running Spark in the cloud need not mean migrating an on-premises installation to virtual machines: you can run Spark natively against cloud storage to take full advantage of the cloud’s elasticity, cost-saving benefit, and management tools without having to manage an on-premise computing stack within your cloud environment. 许多希望迁移到云的公司想象他们将以运行其内部部署集群的方式运行其应用程序。所有主要云提供商（Amazon Web Services [AWS]，Microsoft Azure，Google Cloud Platform [GCP]和IBM Bluemix）都为其客户提供托管Hadoop集群，这些集群为存储和Apache Spark提供HDFS。然而，这实际上不是在云中运行Spark的好方法，因为通过使用固定大小的集群和文件系统，您将无法利用弹性。而是，通常最好使用与特定集群（例如Amazon S3，Azure Blob存储或Google云存储）分离的全局存储系统，并为每个Spark工作负载动态启动计算机。通过分离计算和存储，您将能够仅在需要时为计算资源付费，动态扩展计算资源，并混合使用不同的硬件类型。基本上，请记住，在云中运行Spark并不意味着将本地安装迁移到虚拟机：您可以针对云存储本地运行Spark以充分利用云的弹性，成本节约优势和管理工具，而无需必须在云环境中管理内部部署计算堆栈。Several companies provide “cloud-native” Spark-based services, and all installations of Apache Spark can of course connect to cloud storage. Databricks, the company started by the Spark team from UC Berkeley, is one example of a service provider built specifically for Spark in the cloud. Databricks provides a simple way to run Spark workloads without the heavy baggage of a Hadoop installation. The company provides a number of features for running Spark more efficiently in the cloud, such as auto-scaling, auto-termination of clusters, and optimized connectors to cloud storage, as well as a collaborative environment for working on notebooks and standalone jobs. The company also provides a free Community Edition for learning Spark where you can run notebooks on a small cluster and share them live with others. A fun fact is that this entire book was written using the free Community Edition of Databricks, because we found the integrated Spark notebooks, live collaboration, and cluster management the easiest way to produce and test this content. 有几家公司提供“基于云原生”的基于 Spark 的服务，Apache Spark的所有安装当然都可以连接到云存储。由加州大学伯克利分校的 Spark 团队发起的公司 Databricks 是专门为云中的 Spark 构建的服务提供商的一个例子。 Databricks 提供了一种运行Spark工作负载的简单方法，而无需承担 Hadoop 安装的沉重负担。该公司提供了许多功能，可以在云中更有效地运行Spark，例如自动扩展，集群自动终止，云存储的优化连接器，以及用于处理 notebook 和独立作业的协作环境。该公司还提供免费的社区版学习 Spark，您可以在小型集群上运行 notebook 并与他人分享。一个有趣的事实是，整本书是使用免费的 Databricks 社区版编写的，因为我们发现集成的 Spark notebook，实时协作和集群管理是生成和测试此内容的最简单方法。If you run Spark in the cloud, much of the content in this chapter might not be relevant because you can often create a separate, short-lived Spark cluster for each job you execute. In that case, the standalone cluster manager is likely the easiest to use. However, you may still want to read this content if you’d like to share a longer-lived cluster among many applications, or to install Spark on virtual machines yourself. 如果您在云中运行Spark，本章中的大部分内容可能都不相关，因为您通常可以为您执行的每个作业创建一个单独的，短期的Spark集群。在这种情况下，独立集群管理器可能是最容易使用的。但是，如果您希望在许多应用程序之间共享一个较长期的集群，或者您自己在虚拟机上安装Spark，则可能仍希望阅读此内容。 Cluster Managers 集群管理器Unless you are using a high-level managed service, you will have to decide on the cluster manager to use for Spark. Spark supports three aforementioned cluster managers: standalone clusters, Hadoop YARN, and Mesos. Let’s review each of these. 除非您使用的是高级托管服务，否则您必须决定要用于Spark的集群管理器。Spark 支持三个上述集群管理器：独立集群（standalone clusters），Hadoop YARN 和 Mesos。让我们回顾一下这些。 Standalone Mode 独立模式Spark’s standalone cluster manager is a lightweight platform built specifically for Apache Spark workloads. Using it, you can run multiple Spark Applications on the same cluster. It also provides simple interfaces for doing so but can scale to large Spark workloads. The main disadvantage of the standalone mode is that it’s more limited than the other cluster managers—in particular, your cluster can only run Spark. It’s probably the best starting point if you just want to quickly get Spark running on a cluster, however, and you do not have experience using YARN or Mesos. Spark 的独立集群管理器是专为 Apache Spark 工作负载构建的轻量级平台。使用它，您可以在同一个集群上运行多个 Spark 应用程序。它还提供了简单的界面，但可以扩展到大型 Spark 工作负载。独立模式的主要缺点是它比其他集群管理器更有限——特别是，您的集群只能运行 Spark。如果您只想快速让 Spark 在群集上运行，那么这可能是最好的起点，但是您没有使用 YARN 或 Mesos 的经验。 Starting a standalone cluster 启动独立集群Starting a standalone cluster requires provisioning the machines for doing so. That means starting them up, ensuring that they can talk to one another over the network, and getting the version of Spark you would like to run on those sets of machines. After that, there are two ways to start the cluster: by hand or using built-in launch scripts. 启动独立群集需要配置计算机。这意味着启动它们，确保它们可以通过网络相互通信，并获得您希望在这些机器上运行的 Spark 版本。之后，有两种方法可以启动集群：手动或使用内置启动脚本。 Let’s first launch a cluster by hand. The first step is to start the master process on the machine that we want that to run on, using the following command : 让我们首先手动启动一个集群。第一步是使用以下命令在我们希望运行的机器上启动主进程： 1$SPARK_HOME/sbin/start-master.sh When we run this command, the cluster manager master process will start up on that machine. Once started, the master prints out a spark://HOST:PORT URI. You use this when you start each of the worker nodes of the cluster, and you can use it as the master argument to your SparkSession on application initialization. You can also find this URI on the master’s web UI, which is http://masterip-address:8080 by default. With that URI, start the worker nodes by logging in to each machine and running the following script using the URI you just received from the master node. The master machine must be available on the network of the worker nodes you are using, and the port must be open on the master node, as well: 运行此命令时，集群管理器主进程将在该计算机上启动。一旦启动，master 就打印出一个 spark://HOST:PORT URI。在启动集群的每个工作节点时使用它，并且可以在应用程序初始化时将其用作 SparkSession 的主参数。您还可以在 master 的 Web UI 上找到此 URI，默认情况下为 http://masterip-address:8080 。使用该 URI，通过登录到每台计算机并使用刚从主节点收到的 URI 运行以下脚本来启动工作节点。主机必须在您正在使用的工作节点的网络上可用，并且端口必须在主节点上打开，以及 ： 1$SPARK_HOME/sbin/start-slave.sh &lt;master-spark-URI&gt; As soon as you’ve run that on another machine, you have a Spark cluster running! This process is naturally a bit manual; thankfully there are scripts that can help to automate this process. 只要你在另一台机器上运行它，就会运行一个Spark集群！这个过程自然需要一点手动; 谢天谢地，有些脚本可以帮助自动化这个过程。 Cluster launch scripts 集群启动脚本You can configure cluster launch scripts that can automate the launch of standalone clusters. To do this, create a file called conf/slaves in your Spark directory that will contain the hostnames of all the machines on which you intend to start Spark workers, one per line. If this file does not exist, everything will launch locally. When you go to actually start the cluster, the master machine will access each of the worker machines via Secure Shell (SSH). By default, SSH is run in parallel and requires that you configure password-less (using a private key) access. If you do not have a password-less setup, you can set the environment variable SPARK_SSH_FOREGROUND and serially provide a password for each worker. 您可以配置可以自动启动独立集群的集群启动脚本。为此，在Spark目录中创建一个名为 conf/slaves 的文件，该文件将包含要在其上启动 Spark 工作的所有计算机的主机名，每行一个。如果此文件不存在，则所有内容都将在本地启动。当您真正启动集群时，主计算机将通过 Secure Shell(SSH) 访问每个工作计算机。默认情况下，SSH 是并行运行的，需要您配置无密码（使用私钥）访问。如果您没有无密码设置，则可以设置环境变量SPARK_SSH_FOREGROUND 并为每个工作人员连续提供密码。 After you set up this file, you can launch or stop your cluster by using the following shell scripts, based on Hadoop’s deploy scripts, and available in $SPARK_HOME/sbin: 设置此文件后，可以使用以下基于 Hadoop 部署脚本的 shell 脚本启动或停止集群，并在 $SPARK_HOME/sbin 中提供： $SPARK_HOME/sbin/start-master.sh Starts a master instance on the machine on which the script is executed. 在执行脚本的计算机上启动主实例。 $SPARK_HOME/sbin/start-slaves.sh Starts a slave instance on each machine specified in the conf/slaves file. 在conf / slaves文件中指定的每台计算机上启动从属实例。 $SPARK_HOME/sbin/start-slave.sh Starts a slave instance on the machine on which the script is executed. 在执行脚本的计算机上启动从属实例。 $SPARK_HOME/sbin/start-all.sh Starts both a master and a number of slaves as described earlier. 如前所述，启动主服务器和多个从服务器。 $SPARK_HOME/sbin/stop-master.sh Stops the master that was started via the bin/start-master.sh script. 停止通过bin / start-master.sh脚本启动的主服务器。 $SPARK_HOME/sbin/stop-slaves.sh Stops all slave instances on the machines specified in the conf/slaves file. 停止conf / slaves文件中指定的计算机上的所有从属实例。 $SPARK_HOME/sbin/stop-all.sh Stops both the master and the slaves as described earlier. 如前所述，停止主站和从站。 Standalone cluster configurations 独立集群配置Standalone clusters have a number of configurations that you can use to tune your application. These control everything from what happens to old files on each worker for terminated applications to the worker’s core and memory resources. These are controlled via environment variables or via application properties. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Standalone Environment Variables in the Spark documentation. 独立集群具有许多可用于调整应用程序的配置。它们控制从终止应用程序的每个工作程序上的旧文件发生到工作程序的核心和内存资源的所有内容。这些是通过环境变量或应用程序属性控制的。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中有关独立环境变量的相关表。 Submitting applications 提交申请After you create the cluster, you can submit applications to it using the spark://URI of the master. You can do this either on the master node itself or another machine using spark-submit. There are some specific command-line arguments for standalone mode, which we covered in “Launching Applications”. 创建集群后，可以使用主服务器的 spark://URI 向其提交应用程序。您可以在主节点本身或使用 spark-submit 的其他计算机上执行此操作。独立模式有一些特定的命令行参数，我们在“启动应用程序”中介绍了这些参数。 Spark on YARN 运行在YARN上SparkHadoop YARN is a framework for job scheduling and cluster resource management. Even though Spark is often (mis)classified as a part of the “Hadoop Ecosystem,” in reality, Spark has little to do with Hadoop. Spark does natively support the Hadoop YARN cluster manager but it requires nothing from Hadoop itself. Hadoop YARN 是一个用于作业调度和集群资源管理的框架。尽管 Spark 经常（错误地）归类为 “Hadoop生态系统” 的一部分，但事实上，Spark与Hadoop几乎没有关系。 Spark 本身支持 Hadoop YARN 集群管理器，但它不需要Hadoop本身。 You can run your Spark jobs on Hadoop YARN by specifying the master as YARN in the spark-submit command-line arguments. Just like with standalone mode, there are a number of knobs that you are able to tune according to what you would like the cluster to do. The number of knobs is naturally larger than that of Spark’s standalone mode because Hadoop YARN is a generic scheduler for a large number of different execution frameworks. 您可以通过在 spark-submit 命令行参数中将 master 指定为 YARN 来在 Hadoop YARN 上运行 Spark 作业。就像独立模式一样，有许多旋钮可以根据您希望集群执行的操作进行调整。旋钮的数量自然大于 Spark 的独立模式，因为 Hadoop YARN 是大量不同执行框架的通用调度程序。 Setting up a YARN cluster is beyond the scope of this book, but there are some great books on the topic as well as managed services that can simplify this experience. 设置 YARN 集群超出了本书的范围，但是有一些关于该主题的优秀书籍以及可以简化此体验的托管服务。 Submitting applications 提交申请When submitting applications to YARN, the core difference from other deployments is that --master will become yarn as opposed the master node IP, as it is in standalone mode. Instead, Spark will find the YARN configuration files using the environment variable HADOOP_CONF_DIR or YARN_CONF_DIR. Once you have set those environment variables to your Hadoop installation’s configuration directory, you can just run spark-submit like we saw in Chapter 16. 在向 YARN 提交申请时，与其他部署的核心区别在于 --master 将成为 YARN 而不是主节点 IP，正如它处于独立模式的那样。相反，Spark 将使用环境变量 HADOOP_CONF_DIR 或 YARN_CONF_DIR 找到 YARN 配置文件。一旦将这些环境变量设置到 Hadoop 安装的配置目录中，就可以像我们在第16章中看到的那样运行 spark-submit。 NOTE 注意There are two deployment modes that you can use to launch Spark on YARN. As discussed in previous chapters, cluster mode has the spark driver as a process managed by the YARN cluster, and the client can exit after creating the application. In client mode, the driver will run in the client process and therefore YARN will be responsible only for granting executor resources to the application, not maintaining the master node. Also of note is that in cluster mode, Spark doesn’t necessarily run on the same machine on which you’re executing. Therefore libraries and external jars must be distributed manually or through the --jars command-line argument. 您可以使用两种部署模式在 YARN 上启动 Spark。如前几章所述，集群模式将 Spark 驱动程序作为 YARN 集群管理的进程，客户端可以在创建应用程序后退出。在客户端模式下，驱动程序将在客户端进程中运行，因此 YARN 仅负责向应用程序授予执行器（executor）资源，而不是维护主节点。另外值得注意的是，在集群模式下，Spark不一定在您正在执行的同一台机器上运行。因此，必须手动或通过 --jars 命令行参数分发库和外部jar。 There are a few YARN-specific properties that you can set by using spark-submit. These allow you to control priority queues and things like keytabs for security. We covered these in “Launching Applications” in Chapter 16. 您可以使用 spark-submit 设置一些特定于 YARN 的属性。这些允许您控制优先级队列和诸如 keytabs 之类的东西以确保安全性。我们在第16章的“启动应用程序”中介绍了这些内容。 Configuring Spark on YARN Applications 在YARN应用程序上配置SparkDeploying Spark as YARN applications requires you to understand the variety of different configurations and their implications for your Spark applications. This section covers some best practices for basic configurations and includes references to some of the important configuration for running your Spark applications. 将Spark部署为YARN应用程序需要您了解各种不同的配置及其对 Spark 应用程序的影响。本节介绍了一些基本配置的最佳实践，并包括对运行Spark应用程序的一些重要配置的引用。 Hadoop configurations Hadoop配置If you plan to read and write from HDFS using Spark, you need to include two Hadoop configuration files on Spark’s classpath: hdfs-site.xml, which provides default behaviors for the HDFS client; and core-site.xml, which sets the default file system name. The location of these configuration files varies across Hadoop versions, but a common location is inside of /etc/hadoop/conf. Some tools create these configurations on the fly, as well, so it’s important to understand how your managed service might be deploying these, as well. 如果您计划使用 Spark 从 HDFS 读取和写入，则需要在 Spark 的类路径中包含两个Hadoop配置文件：hdfs-site.xml，它为 HDFS 客户端提供默认行为; 和 core-site.xml，它设置默认文件系统名称。这些配置文件的位置因 Hadoop 版本而异，但常见位置在 /etc/hadoop/conf 中。有些工具也可以动态创建这些配置，因此了解托管服务如何部署这些配置也很重要。 To make these files visible to Spark, set HADOOP_CONF_DIR in $SPARK_HOME/spark-env.sh to a location containing the configuration files or as an environment variable when you go to spark–submit your application. 要使这些文件对 Spark 可见，请将 $SPARK_HOME/spark-env.sh 中的 HADOOP_CONF_DIR 设置为包含配置文件的位置，或者当您转到 spark-submit 应用程序时将其设置为环境变量。 Application properties for YARN YARN的应用程序属性There are a number of Hadoop-related configurations and things that come up that largely don’t have much to do with Spark, just running or securing YARN in a way that influences how Spark runs. Due to space limitations, we cannot include the configuration set here. Refer to the relevant table on YARN Configurations in the Spark documentation. 有许多与 Hadoop 相关的配置和出现的东西很大程度上与 Spark 没什么关系，只是以影响 Spark 运行或保护 YARN 的方式。 由于空间限制，我们不能在此处包含配置集。 请参阅 Spark文档 中有关 YARN配置的相关表。 Spark on Mesos 在Mesos运行的SparkApache Mesos is another clustering system that Spark can run on. A fun fact about Mesos is that the project was also started by many of the original authors of Spark, including one of the authors of this book. In the Mesos project’s own words : Apache Mesos 是 Spark 可以运行的另一个集群系统。关于 Mesos 的一个有趣的事实是该项目也是由Spark的许多原作者创建的，包括本书的作者之一。在Mesos项目中用自己的话来说 ： Apache Mesos abstracts CPU, memory, storage, and other compute resources away from machines (physical or virtual), enabling fault-tolerant and elastic distributed systems to easily be built and run effectively. Apache Mesos将CPU，内存，存储和其他计算资源从机器（物理或虚拟）中抽象出来，使容错和弹性分布式系统能够轻松构建并有效运行。For the most part, Mesos intends to be a datacenter scale-cluster manager that manages not just short-lived applications like Spark, but long-running applications like web applications or other resource interfaces. Mesos is the heaviest-weight cluster manager, simply because you might choose this cluster manager only if your organization already has a large-scale deployment of Mesos, but it makes for a good cluster manager nonetheless. 在大多数情况下，Mesos打算成为一个数据中心规模集群管理器，它不仅管理像Spark这样的短期应用程序，而且管理长期运行的应用程序，如Web应用程序或其他资源接口。 Mesos是最重的集群管理器，仅仅因为您可能只在您的组织已经大规模部署Mesos时才选择此集群管理器，但它仍然是一个优秀的集群管理器。Mesos is a large piece of infrastructure, and unfortunately there’s simply too much information for us to cover how to deploy and maintain Mesos clusters. There are many great books on the subject for that, including Dipa Dubhashi and Akhil Das’s Mastering Mesos (O’Reilly, 2016). The goal here is to bring up some of the considerations that you’ll need to think about when running Spark Applications on Mesos. Mesos 是一个很大的基础架构，不幸的是，我们有太多的信息来介绍如何部署和维护 Mesos 集群。有很多关于这个主题的好书，包括 Dipa Dubhashi 和 Akhil Das 的 《Mastering Mesos》（O’Reilly，2016）。这里的目标是提出在 Mesos 上运行 Spark 应用程序时需要考虑的一些注意事项。 For instance, one common thing you will hear about Spark on Mesos is fine-grained versus coarse-grained mode. Historically Mesos supported a variety of different modes (fine-grained and coarse-grained), but at this point, it supports only coarse-grained scheduling (fine-grained has been deprecated). Coarse-grained mode means that each Spark executor runs as a single Mesos task. Spark executors are sized according to the following application properties : 例如，你会听到关于 Spark on Mesos 的一个常见的事情是细粒度和粗粒度模式。从历史上看，Mesos支持各种不同的模式（细粒度和粗粒度），但此时，它仅支持粗粒度调度（细粒度已被弃用）。粗粒度模式意味着每个Spark执行程序作为单个Mesos任务运行。 Spark执行程序根据以下应用程序属性调整大小： 123spark.executor.memoryspark.executor.coresspark.cores.max/spark.executor.cores Submitting applicationsSubmitting applications to a Mesos cluster is similar to doing so for Spark’s other cluster managers. For the most part you should favor cluster mode when using Mesos. Client mode requires some extra configuration on your part, especially with regard to distributing resources around the cluster. For instance, in client mode, the driver needs extra configuration information in spark-env.sh to work with Mesos. 将应用程序提交到 Mesos 集群与 Spark 的其他集群管理器类似。 在大多数情况下，您应该在使用 Mesos 时使用集群模式。 客户端模式需要您进行一些额外配置，尤其是在集群上分配资源方面。 例如，在客户端模式下，驱动程序需要 spark-env.sh 中的额外配置信息才能使用Mesos。In spark-env.sh set some environment variables : 在spark-env.sh中设置一些环境变量： 1234export MESOS_NATIVE_JAVA_LIBRARY=&lt;path to libmesos.so&gt;This path is typically &lt;prefix&gt;/lib/libmesos.so where the prefix is /usr/local by default. On Mac OSX, the library is called libmesos.dylib instead of libmesos.so:export SPARK_EXECUTOR_URI=&lt;URL of spark-2.2.0.tar.gz uploaded above&gt; Finally, set the Spark Application property spark.executor.uri to . Now, when starting a Spark application against the cluster, pass a mesos://URL as the master when creating a SparkContex , and set that property as a parameter in your SparkConf variable or the initialization of a SparkSession : 最后，将 Spark Application 属性 spark.executor.uri 设置为 。 现在，在针对集群启动 Spark 应用程序时，在创建 SparkContext 时将 mesos://URL 作为 master 传递，并将该属性设置为 SparkConf 变量中的参数或 SparkSession 的初始化： 1234567// in Scalaimport org.apache.spark.sql.SparkSessionval spark = SparkSession.builder.master("mesos://HOST:5050").appName("my app").config("spark.executor.uri", "&lt;path to spark-2.2.0.tar.gz uploaded above&gt;").getOrCreate() Submitting cluster mode applications is fairly straightforward and follows the same spark-submit structure you read about before. We covered these in “Launching Applications”. 提交集群模式应用程序非常简单，并遵循您之前阅读的相同的 spark-submit 结构。 我们在“启动应用程序”中介绍了这些内容。 Configuring Mesos 配置MesosJust like any other cluster manager, there are a number of ways that we can configure our Spark Applications when they’re running on Mesos. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Mesos Configurations in the Spark documentation. 就像任何其他集群管理器一样，我们可以通过多种方式在 Spark 应用程序运行时配置它们。 由于篇幅限制，我们无法在此处包含整个配置集。 请参阅 Spark文档 中有关 Mesos配置的相关表 。 Secure Deployment Configurations 安全部署配置Spark also provides some low-level ability to make your applications run more securely, especially in untrusted environments. Note that the majority of this setup will happen outside of Spark. These configurations are primarily network-based to help Spark run in a more secure manner. This means authentication, network encryption, and setting TLS and SSL configurations. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Security Configurations in the Spark documentation. Spark 还提供了一些低阶功能，使您的应用程序运行更安全，尤其是在不受信任的环境中。请注意，此设置的大部分将在 Spark 之外发生。这些配置主要基于网络，以帮助 Spark 以更安全的方式运行。这意味着身份验证，网络加密以及设置 TLS 和 SSL 配置。由于篇幅限制，我们无法在此处包含整个配置集。请参阅 Spark文档 中有关 安全配置的相关表 。 Cluster Networking Configurations 集群网络配置Just as shuffles are important, there can be some things worth tuning on the network. This can also be helpful when performing custom deployment configurations for your Spark clusters when you need to use proxies in between certain nodes. If you’re looking to increase Spark’s performance, these should not be the first configurations you go to tune, but may come up in custom deployment scenarios. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Networking Configurations in the Spark documentation. 正如洗牌（shuffle）很重要，网络上可能会有一些值得调整的东西。当您需要在某些节点之间使用代理时，在为Spark 集群执行自定义部署配置时，这也很有用。如果您希望提高Spark的性能，这些不应该是您要调整的第一个配置，但可能会出现在自定义部署方案中。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档 中的网络配置相关表。 Application Scheduling 应用程序调度Spark has several facilities for scheduling resources between computations. First, recall that, as described earlier in the book, each Spark Application runs an independent set of executor processes. Cluster managers provide the facilities for scheduling across Spark applications. Second, within each Spark application, multiple jobs (i.e., Spark actions) may be running concurrently if they were submitted by different threads. This is common if your application is serving requests over the network. Spark includes a fair scheduler to schedule resources within each application. We introduced this topic in the previous chapter. Spark 有几种用于在计算之间调度资源的工具。首先，回想一下，如本书前面所述，每个 Spark 应用程序都运行一组独立的执行器（executor）进程。集群管理器提供跨Spark应用程序进行调度的工具。其次，在每个Spark应用程序中，如果它们是由不同的线程提交的，则多个作业（即 Spark actions ）可以同时运行。如果您的应用程序通过网络提供请求，这种情况很常见。 Spark包含一个公平的调度程序（fair scheduler）来安排每个应用程序中的资源。我们在前一章介绍了这个主题。 If multiple users need to share your cluster and run different Spark Applications, there are different options to manage allocation, depending on the cluster manager. The simplest option, available on all cluster managers, is static partitioning of resources. With this approach, each application is given a maximum amount of resources that it can use, and holds onto those resources for the entire duration. In spark-submit there are a number of properties that you can set to control the resource allocation of a particular application. Refer to Chapter 16 for more information. In addition, dynamic allocation (described next) can be turned on to let applications scale up and down dynamically based on their current number of pending tasks. If, instead, you want users to be able to share memory and executor resources in a fine-grained manner, you can launch a single Spark Application and use thread scheduling within it to serve multiple requests in parallel. 如果多个用户需要共享您的集群并运行不同的Spark应用程序，则可以使用不同的选项来管理分配，具体取决于集群管理器。所有集群管理器上都可以使用的最简单的选项是资源的静态分区（static partitioning of resources）。通过这种方法，每个应用程序都可以使用最多的资源，并在整个持续时间内保留这些资源。在spark-submit中，您可以设置许多属性来控制特定应用程序的资源分配。有关更多信息，请参阅第16章。此外，可以打开动态分配（下面描述），让应用程序根据当前挂起的任务数量动态扩展和缩小。相反，如果您希望用户能够以细粒度的方式共享内存和执行程序资源，则可以启动单个Spark应用程序并在其中使用线程调度来并行处理多个请求。 Dynamic allocation 动态分配If you would like to run multiple Spark Applications on the same cluster, Spark provides a mechanism to dynamically adjust the resources your application occupies based on the workload. This means that your application can give resources back to the cluster if they are no longer used, and request them again later when there is demand. This feature is particularly useful if multiple applications share resources in your Spark cluster. 如果您希望在同一个集群上运行多个Spark应用程序，Spark会提供一种机制，根据工作负载动态调整应用程序占用的资源。这意味着如果不再使用，您的应用程序可以将资源提供给集群，并在需要时稍后再次请求它们。如果多个应用程序共享Spark集群中的资源，则此功能特别有用。This feature is disabled by default and available on all coarse-grained cluster managers; that is, standalone mode, YARN mode, and Mesos coarse-grained mode. There are two requirements for using this feature. First, your application must set spark.dynamicAllocation.enabled to true. Second, you must set up an external shuffle service on each worker node in the same cluster and set spark.shuffle.service.enabled to true in your application. The purpose of the external shuffle service is to allow executors to be removed without deleting shuffle files written by them. This is set up differently for each cluster manager and is described in the job scheduling configuration. Due to space limitations, we cannot include the configuration set for dynamic allocation. Refer to the relevant table on Dynamic Allocation Configurations. 默认情况下禁用此功能，并且所有粗粒度集群管理器均可使用此功能; 也就是独立模式，YARN模式和Mesos粗粒度模式。使用此功能有两个要求。首先，您的应用程序必须将 spark.dynamicAllocation.enabled 设置为true。其次，必须在同一群集中的每个工作节点上设置外部洗牌（shuffle） 服务，并在应用程序中将 spark.shuffle.service.enabled 设置为 true。外部 shuffle 服务的目的是允许删除执行程序而不删除它们写入的shuffle文件。对于每个集群管理器，此设置不同，并在作业调度配置中进行了描述。由于空间限制，我们不能包含动态分配的配置集。请参阅动态分配配置的相关表。 Miscellaneous Considerations 各种各样的考虑因素There several other topics to consider when deploying Spark applications that may affect your choice of cluster manager and its setup. These are just things that you should think about when comparing different deployment options. 在部署可能影响您选择的集群管理器及其设置的 Spark 应用程序时，还需要考虑其他几个主题。在比较不同的部署选项时，您应该考虑这些事项。 One of the more important considerations is the number and type of applications you intend to be running. For instance, YARN is great for HDFS-based applications but is not commonly used for much else. Additionally, it’s not well designed to support the cloud, because it expects information to be available on HDFS. Also, compute and storage is largely coupled together, meaning that scaling your cluster involves scaling both storage and compute instead of just one or the other. Mesos does improve on this a bit conceptually, and it supports a wide range of application types, but it still requires pre-provisioning machines and, in some sense, requires buy-in at a much larger scale. For instance, it doesn’t really make sense to have a Mesos cluster for only running Spark Applications. Spark standalone mode is the lightest-weight cluster manager and is relatively simple to understand and take advantage of, but then you’re going to be building more application management infrastructure that you could get much more easily by using YARN or Mesos. 其中一个更重要的考虑因素是您打算运行的应用程序的数量和类型。例如，YARN 非常适合基于 HDFS 的应用程序，但并不常用于其他许多应用程序。此外，它还没有很好地支持云，因为它希望在 HDFS 上提供信息。此外，计算和存储在很大程度上是耦合在一起的，这意味着扩展集群涉及扩展存储和计算，而不仅仅是一个或另一个。 Mesos 在概念上确实有所改进，它支持多种应用类型，但它仍然需要预配置机器，从某种意义上说，需要更大规模的支持。例如，只运行 Spark 应用程序的 Mesos 集群没有真正发挥它的价值。 Spark 独立模式是最轻量级的集群管理器，并且相对易于理解和利用，但随后您将构建更多应用程序管理基础架构，使用 YARN 或 Mesos 可以更轻松地获得这些基础架构。Another challenge is managing different Spark versions. Your hands are largely tied if you want to try to run a variety of different applications running different Spark versions, and unless you use a well-managed service, you’re going to need to spend a fair amount of time either managing different setup scripts for different Spark services or removing the ability for your users to use a variety of different Spark applications. 另一个挑战是管理不同的 Spark 版本。如果你想尝试运行运行不同 Spark 版本的各种不同应用程序，你的手很大程度上是捆绑在一起的，除非你使用管理良好的服务，否则你需要花费相当多的时间来管理不同的设置脚本用于不同的 Spark 服务或删除用户使用各种不同 Spark 应用程序的能力。Regardless of the cluster manager that you choose, you’re going to want to consider how you’re going to set up logging, store logs for future reference, and allow end users to debug their applications. These are more “out of the box” for YARN or Mesos and might need some tweaking if you’re using standalone. 无论您选择哪个集群管理器，您都会想要考虑如何设置日志记录，存储日志以供将来参考，以及允许最终用户调试其应用程序。对于 YARN 或 Mesos 来说，这些更“开箱即用”，如果您使用独立的话，可能需要进行一些调整。One thing you might want to consider—or that might influence your decision making—is maintaining a metastore in order to maintain metadata about your stored datasets, such as a table catalog. We saw how this comes up in Spark SQL when we are creating and maintaining tables. Maintaining an Apache Hive metastore, a topic beyond the scope of this book, might be something that’s worth doing to facilitate more productive, cross-application referencing to the same datasets. Depending on your workload, it might be worth considering using Spark’s external shuffle service. Typically Spark stores shuffle blocks (shuffle output) on a local disk on that particular node. An external shuffle service allows for storing those shuffle blocks so that they are available to all executors, meaning that you can arbitrarily kill executors and still have their shuffle outputs available to other applications. 您可能想要考虑的一件事——或者可能影响您决策的一件事——是维护一个 Metastore，以维护有关您存储的数据集的元数据，例如表目录。我们在创建和维护表时看到了如何在 Spark SQL 中出现这种情况。维护 Apache Hive Metastore 是一个超出本书范围的主题，可能值得做些什么来促进对同一数据集的更高效，跨应用程序的引用。根据您的工作量，可能值得考虑使用 Spark 的外部 shuffle 服务。通常，Spark 将 shuffle 块（ shuffle 输出）存储在该特定节点上的本地磁盘上。外部 shuffle 服务允许存储这些 shuffle 块，以便它们可供所有执行程序使用，这意味着您可以任意杀死执行程序，并且仍然可以将其 shuffle 输出提供给其他应用程序。Finally, you’re going to need to configure at least some basic monitoring solution and help users debug their Spark jobs running on their clusters. This is going to vary across cluster management options and we touch on some of the things that you might want to set up in Chapter 18. 最后，您将需要至少配置一些基本监视解决方案，并帮助用户调试在其集群上运行的 Spark 作业。这在集群管理选项中会有所不同，我们会讨论您可能希望在第18章中设置的一些内容。 Conclusion 结论This chapter looked at the world of configuration options that you have when choosing how to deploy Spark. Although most of the information is irrelevant to the majority of users, it is worth mentioning if you’re performing more advanced use cases. It might seem fallacious, but there are other configurations that we have omitted that control even lower-level behavior. You can find these in the Spark documentation or in the Spark source code. Chapter 18 talks about some of the options that we have when monitoring Spark Applications. 本章介绍了在选择部署 Spark 时所具有的配置选项的世界。虽然大多数信息与大多数用户无关，但如果您正在执行更高级的用户案例，则值得一提。它可能看起来很荒谬，但是我们已经省略了其他配置来控制甚至更低阶的行为。您可以在 Spark 文档或 Spark 源代码中找到它们。第18章讨论了监视Spark应用程序时的一些选项。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 8 Joins]]></title>
    <url>%2F2019%2F08%2F05%2FChapter8-Joins(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 8. Joins 连接 译者：https://snaildove.github.ioChapter 7 covered aggregating single datasets, which is helpful, but more often than not, your Spark applications are going to bring together a large number of different datasets. For this reason, joins are an essential part of nearly all Spark workloads. Spark’s ability to talk to different data means that you gain the ability to tap into a variety of data sources across your company. This chapter covers not just what joins exist in Spark and how to use them, but some of the basic internals so that you can think about how Spark actually goes about executing the join on the cluster. This basic knowledge can help you avoid running out of memory and tackle problems that you could not solve before. 第7章介绍了聚合单个数据集的方法，这很有用，但通常，您的Spark应用程序将把大量不同的数据集组合在一起。因此，连接几乎是所有Spark工作负载中必不可少的一部分。 Spark能够处理不同数据的能力意味着您可以利用公司中的各种数据源。本章不仅涵盖Spark中存在的连接及其使用方式，还涵盖了一些基本的内部原理，以便您可以考虑Spark实际如何在集群上执行连接。这些基础知识可以帮助您避免内存不足，并解决以前无法解决的问题。 Join Expressions 连接表达式A join brings together two sets of data, the left and the right, by comparing the value of one or more keys of the left and right and evaluating the result of a join expression that determines whether Spark should bring together the left set of data with the right set of data. The most common join expression, an equi-join, compares whether the specified keys in your left and right datasets are equal. If they are equal, Spark will combine the left and right datasets. The opposite is true for keys that do not match; Spark discards the rows that do not have matching keys. Spark also allows for much more sophsticated join policies in addition to equi-joins. We can even use complex types and perform something like checking whether a key exists within an array when you perform a join. 连接通过比较左右键中一个或多个键的值并评估连接表达式的结果来确定左右两个数据集，该连接表达式确定Spark是否应将左数据集与右数据集放在一起。最常见的连接表达式，equi-join，比较您左右数据集中的指定键是否相等。如果它们相等，Spark将合并左右数据集。对于不匹配的键则相反。 Spark丢弃没有匹配键的行。除 equi-join 外，Spark还允许更多复杂的连接策略。我们甚至可以使用复杂的类型并执行类似的操作，例如在执行连接时检查数组中是否存在键。 Join Types 连接类型Whereas the join expression determines whether two rows should join, the join type determines what should be in the result set. There are a variety of different join types available in Spark for you to use: 连接表达式确定是否应连接两行，连接类型确定结果集中应包含的内容。 Spark提供了多种不同的连接类型供您使用： Inner joins (keep rows with keys that exist in the left and right datasets)内部连接（保持行包含在左右数据集中的键） Outer joins (keep rows with keys in either the left or right datasets)外部连接（在左侧或右侧数据集中保留带有键的行） Left outer joins (keep rows with keys in the left dataset)左外部连接（在左数据集中保留带有键的行） Right outer joins (keep rows with keys in the right dataset)右外部连接（在右数据集中保留带有键的行） Left semi joins (keep the rows in the left, and only the left, dataset where the key appears in the right dataset)左半连接（保留键在右侧数据集中出现的左侧数据集，并且仅保留左侧数据集） Left anti joins (keep the rows in the left, and only the left, dataset where they do not appear in the right dataset)左反连接（保留左侧的行，并且仅保留左侧的数据集，而这些行未出现在右侧的数据集中） Natural joins (perform a join by implicitly matching the columns between the two datasets with the same names)自然连接（通过隐式匹配两个具有相同名称的数据集之间的列来执行连接） Cross (or Cartesian) joins (match every row in the left dataset with every row in the right dataset)交叉（或笛卡尔）连接（将左数据集中的每一行与右数据集中的每一行匹配） If you have ever interacted with a relational database system, or even an Excel spreadsheet, the concept of joining different datasets together should not be too abstract. Let’s move on to showing examples of each join type. This will make it easy to understand exactly how you can apply these to your own problems. To do this, let’s create some simple datasets that we can use in our examples: 如果您曾经与关系数据库系统甚至是Excel电子表格进行过交互，那么将不同数据集连接在一起的概念就不会太抽象。让我们继续展示每种连接类型的示例。这将使您容易准确地理解如何将其应用于自己的问题。为此，让我们创建一些可在示例中使用的简单数据集：12345678910111213141516// in Scalaval person = Seq((0, "Bill Chambers", 0, Seq(100)),(1, "Matei Zaharia", 1, Seq(500, 250, 100)),(2, "Michael Armbrust", 1, Seq(250, 100))).toDF("id", "name", "graduate_program", "spark_status")val graduateProgram = Seq((0, "Masters", "School of Information", "UC Berkeley"),(2, "Masters", "EECS", "UC Berkeley"),(1, "Ph.D.", "EECS", "UC Berkeley")).toDF("id", "degree", "department", "school")val sparkStatus = Seq((500, "Vice President"),(250, "PMC Member"),(100, "Contributor")).toDF("id", "status") 12345678910111213141516# in Pythonperson = spark.createDataFrame([(0, "Bill Chambers", 0, [100]),(1, "Matei Zaharia", 1, [500, 250, 100]),(2, "Michael Armbrust", 1, [250, 100])])\.toDF("id", "name", "graduate_program", "spark_status")graduateProgram = spark.createDataFrame([(0, "Masters", "School of Information", "UC Berkeley"),(2, "Masters", "EECS", "UC Berkeley"),(1, "Ph.D.", "EECS", "UC Berkeley")])\.toDF("id", "degree", "department", "school")sparkStatus = spark.createDataFrame([(500, "Vice President"),(250, "PMC Member"),(100, "Contributor")])\.toDF("id", "status") Next, let’s register these as tables so that we use them throughout the chapter: 接下来，让我们将它们注册为表格，以便在本章中使用它们： 123person.createOrReplaceTempView("person")graduateProgram.createOrReplaceTempView("graduateProgram")sparkStatus.createOrReplaceTempView("sparkStatus") Inner Joins 内连Inner joins evaluate the keys in both of the DataFrames or tables and include (and join together) only the rows that evaluate to true. In the following example, we join the graduateProgram DataFrame with the person DataFrame to create a new DataFrame: 内部连接评估两个DataFrame或表中的键，并且仅包括（并连接）评估结果为true的行。 在以下示例中，我们将 graduateProgram DataFrame与 person DataFrame一起创建一个新的DataFrame： 12// in Scalaval joinExpression = person.col("graduate_program") === graduateProgram.col("id") 12# in PythonjoinExpression = person["graduate_program"] == graduateProgram['id'] Keys that do not exist in both DataFrames will not show in the resulting DataFrame. For example, the following expression would result in zero values in the resulting DataFrame: 两个 DataFrame 中都不存在的键将不会显示在结果 Dataframe 中。 例如，以下表达式将在结果 DataFrame 中导致零值： 12// in Scalaval wrongJoinExpression = person.col("name") === graduateProgram.col("school") 12# in PythonwrongJoinExpression = person["name"] == graduateProgram["school"] Inner joins are the default join, so we just need to specify our left DataFrame and join the right in the JOIN expression: 内部连接是默认连接，因此我们只需要指定左侧的DataFrame并在JOIN表达式中连接右侧： 1person.join(graduateProgram, joinExpression).show() 123-- in SQLSELECT * FROM person JOIN graduateProgramON person.graduate_program = graduateProgram.id We can also specify this explicitly by passing in a third parameter, the joinType: 我们还可以通过传入第三个参数 joinType 来明确指定此名称： 12// in Scalavar joinType = "inner" 123# in PythonjoinType = "inner"person.join(graduateProgram, joinExpression, joinType).show() 123-- in SQLSELECT * FROM person INNER JOIN graduateProgramON person.graduate_program = graduateProgram.id Outer Joins 外连接Outer joins evaluate the keys in both of the DataFrames or tables and includes (and joins together) the rows that evaluate to true or false. If there is no equivalent row in either the left or right DataFrame, Spark will insert null: 外部连接评估两个DataFrames或表中的键，并包括（并连接在一起）评估为true或false的行。 如果左侧或右侧DataFrame中没有等效行，Spark将插入null： 12joinType = "outer"person.join(graduateProgram, joinExpression, joinType).show() 123-- in SQLSELECT * FROM person FULL OUTER JOIN graduateProgramON graduate_program = graduateProgram.id Left Outer Joins 左外连接Left outer joins evaluate the keys in both of the DataFrames or tables and includes all rows from the left DataFrame as well as any rows in the right DataFrame that have a match in the left DataFrame. If there is no equivalent row in the right DataFrame, Spark will insert null: 左外部连接会评估两个DataFrame或表中的键，并包括左DataFrame中的所有行以及右DataFrame中与左DataFrame中具有匹配项的所有行。 如果右侧DataFrame中没有等效的行，Spark将插入null：12joinType = "left_outer"graduateProgram.join(person, joinExpression, joinType).show() 123-- in SQLSELECT * FROM graduateProgram LEFT OUTER JOIN personON person.graduate_program = graduateProgram.id Right Outer Joins 右外连接Right outer joins evaluate the keys in both of the DataFrames or tables and includes all rows from the right DataFrame as well as any rows in the left DataFrame that have a match in the right DataFrame. If there is no equivalent row in the left DataFrame, Spark will insert null: 右外部连接会评估两个DataFrame或表中的键，并包括右DataFrame中的所有行以及左DataFrame中与右DataFrame中具有匹配项的所有行。如果左侧DataFrame中没有等效的行，Spark将插入null： 12joinType = "right_outer"person.join(graduateProgram, joinExpression, joinType).show() 123-- in SQLSELECT * FROM person RIGHT OUTER JOIN graduateProgramON person.graduate_program = graduateProgram.id Left Semi Joins 左半连接Semi joins are a bit of a departure from the other joins. They do not actually include any values from the right DataFrame. They only compare values to see if the value exists in the second DataFrame. If the value does exist, those rows will be kept in the result, even if there are duplicate keys in the left DataFrame. Think of left semi joins as filters on a DataFrame, as opposed to the function of a conventional join: 半连接与其他连接有些偏离。它们实际上并不包含来自右DataFrame的任何值。他们仅比较值以查看该值是否存在于第二个DataFrame中。如果该值确实存在，则即使左侧DataFrame中存在重复的键，这些行也将保留在结果中。可以将左半连接视为DataFrame上的过滤器，这与常规连接的功能相反： 12joinType = "left_semi"graduateProgram.join(person, joinExpression, joinType).show() 1234// in Scalaval gradProgram2 = graduateProgram.union(Seq((0, "Masters", "Duplicated Row", "Duplicated School")).toDF())gradProgram2.createOrReplaceTempView("gradProgram2") 12345# in PythongradProgram2 = graduateProgram.union(spark.createDataFrame([(0, "Masters", "Duplicated Row", "Duplicated School")]))gradProgram2.createOrReplaceTempView("gradProgram2")gradProgram2.join(person, joinExpression, joinType).show() 123-- in SQLSELECT * FROM gradProgram2 LEFT SEMI JOIN personON gradProgram2.id = person.graduate_program Left Anti Joins 左反连接Left anti joins are the opposite of left semi joins. Like left semi joins, they do not actually include any values from the right DataFrame. They only compare values to see if the value exists in the second DataFrame. However, rather than keeping the values that exist in the second DataFrame, they keep only the values that do not have a corresponding key in the second DataFrame. Think of anti joins as a NOT IN SQL-style filter: 左反连接与左半连接相反。像左半连接一样，它们实际上不包括右DataFrame中的任何值。他们仅比较值以查看该值是否存在于第二个DataFrame中。但是，与其保留第二个DataFrame中存在的值，不如保留第二个DataFrame中没有相应键的值。将反连接视为NOT IN SQL样式的过滤器： 12joinType = "left_anti"graduateProgram.join(person, joinExpression, joinType).show() 123-- in SQLSELECT * FROM graduateProgram LEFT ANTI JOIN personON graduateProgram.id = person.graduate_program Natural Joins 自然连接Natural joins make implicit guesses at the columns on which you would like to join. It finds matching columns and returns the results. Left, right, and outer natural joins are all supported. 自然连接对要连接的列进行隐式猜测。它找到匹配的列并返回结果。左，右和外部的自然连接均受到这样的支持。 WARNING 警告Implicit is always dangerous! The following query will give us incorrect results because the two DataFrames/tables share a column name (id), but it means different things in the datasets. You should always use this join with caution. 隐式总是危险的！以下查询将为我们提供不正确的结果，因为两个DataFrame或表共享一个列名（id），但这意味着数据集中的内容有所不同。您应始终谨慎使用此连接。 12-- in SQLSELECT * FROM graduateProgram NATURAL JOIN person Cross (Cartesian) Joins 交叉（笛卡尔）连接The last of our joins are cross-joins or cartesian products. Cross-joins in simplest terms are inner joins that do not specify a predicate. Cross joins will join every single row in the left DataFrame to ever single row in the right DataFrame. This will cause an absolute explosion in the number of rows contained in the resulting DataFrame. If you have 1,000 rows in each DataFrame, the cross-join of these will result in 1,000,000 (1,000 x 1,000) rows. For this reason, you must very explicitly state that you want a cross-join by using the cross join keyword : 我们的最后一个连接是交叉连接或笛卡尔积。 用最简单的术语来说，交叉连接是不指定谓词的内部连接。 交叉连接会将左侧DataFrame中的每一行连接到右侧DataFrame中的每一行。 这将导致结果DataFrame中包含的行数发生绝对爆炸。 如果每个DataFrame中有1,000行，则这些交叉连接将导致1,000,000（1,000 x 1,000）行。 因此，您必须使用cross join关键字非常显示地声明要进行交叉连接： 12joinType = "cross"graduateProgram.join(person, joinExpression, joinType).show() 123-- in SQLSELECT * FROM graduateProgram CROSS JOIN personON graduateProgram.id = person.graduate_program If you truly intend to have a cross-join, you can call that out explicitly : 如果您确实打算进行交叉连接，则可以显示调用： 1person.crossJoin(graduateProgram).show() 12-- in SQLSELECT * FROM graduateProgram CROSS JOIN person WARNING 警告You should use cross-joins only if you are absolutely, 100 percent sure that this is the join you need. There is a reason why you need to be explicit when defining a cross-join in Spark. They’re dangerous! Advanced users can set the session-level configuration spark.sql.crossJoin.enable to true in order to allow cross-joins without warnings or without Spark trying to perform another join for you. 仅在绝对必要时才应使用交叉连接，100％确保这是您需要的连接。 有一个原因为什么在Spark中定义交叉连接时需要明确： 他们很危险！ 高级用户可以将会话级别的配置 spark.sql.crossJoin.enable 设置为true，以允许交叉连接而不会发出警告或Spark不会尝试为您执行另一个连接。 Challenges When Using Joins 使用连接时的挑战When performing joins, there are some specific challenges and some common questions that arise. The rest of the chapter will provide answers to these common questions and then explain how, at a high level, Spark performs joins. This will hint at some of the optimizations that we are going to cover in later parts of this book. 执行连接时，会出现一些特定的挑战和一些常见的问题。 本章的其余部分将提供对这些常见问题的解答，然后从较高的角度解释Spark如何执行连接。 这将暗示我们将在本书的后面部分中介绍的一些优化。 Joins on Complex Types 连接复杂类型Even though this might seem like a challenge, it’s actually not. Any expression is a valid join expression, assuming that it returns a Boolean: 尽管这似乎是一个挑战，但实际上并非如此。 假定它返回一个布尔值，则任何表达式都是有效的连接表达式： 123import org.apache.spark.sql.functions.exprperson.withColumnRenamed("id", "personId").join(sparkStatus, expr("array_contains(spark_status, id)")).show() 1234# in Pythonfrom pyspark.sql.functions import exprperson.withColumnRenamed("id", "personId")\.join(sparkStatus, expr("array_contains(spark_status, id)")).show() 1234-- in SQLSELECT * FROM(select id as personId, name, graduate_program, spark_status FROM person)INNER JOIN sparkStatus ON array_contains(spark_status, id) Handling Duplicate Column Names 处理重复的列名One of the tricky things that come up in joins is dealing with duplicate column names in your results DataFrame. In a DataFrame, each column has a unique ID within Spark’s SQL Engine, Catalyst. This unique ID is purely internal and not something that you can directly reference. This makes it quite difficult to refer to a specific column when you have a DataFrame with duplicate column names. 连接中棘手的事情之一是处理结果DataFrame中的重复列名。 在DataFrame中，Spark的SQL引擎Catalyst中的每一列都有唯一的ID。 此唯一ID纯粹是内部的，不能直接引用。 当您的DataFrame具有重复的列名时，这使得引用特定的列变得非常困难。 This can occur in two distinct situations: 这可能在两种不同的情况下发生： The join expression that you specify does not remove one key from one of the input DataFrames and the keys have the same column name您指定的连接表达式不会从输入DataFrame之一中删除一个键，并且这些键具有相同的列名。 Two columns on which you are not performing the join have the same name没有执行连接的两列拥有相同的名称。 Let’s create a problem dataset that we can use to illustrate these problems: 让我们创建一个问题数据集，以用来说明这些问题： 123val gradProgramDupe = graduateProgram.withColumnRenamed("id", "graduate_program")val joinExpr = gradProgramDupe.col("graduate_program") === person.col("graduate_program") Note that there are now two graduate_program columns, even though we joined on that key: 请注意，即使我们加入了该键，现在也有两个Graduate_program列： 1person.join(gradProgramDupe, joinExpr).show() The challenge arises when we refer to one of these columns: 当我们引用这些列之一时，挑战就出现了： 1person.join(gradProgramDupe, joinExpr).select("graduate_program").show() Given the previous code snippet, we will receive an error. In this particular example, Spark generates this message: 给定前面的代码片段，我们将收到一个错误。 在此特定示例中，Spark生成以下消息： 12org.apache.spark.sql.AnalysisException: Reference 'graduate_program' isambiguous, could be: graduate_program#40, graduate_program#1079.; Approach 1: Different join expression 方法1：不同的连接表达式When you have two keys that have the same name, probably the easiest fix is to change the join expression from a Boolean expression to a string or sequence. This automatically removes one of the columns for you during the join: 当您有两个名称相同的键时，最简单的解决方法可能是将连接表达式从布尔表达式更改为字符串或序列。 这会在连接过程中自动为您删除其中一列： 1person.join(gradProgramDupe,"graduate_program").select("graduate_program").show() Approach 2: Dropping the column after the join 方法2：加入后删除列Another approach is to drop the offending column after the join. When doing this, we need to refer to the column via the original source DataFrame. We can do this if the join uses the same key names or if the source DataFrames have columns that simply have the same name : 另一种方法是在连接后删除有问题的列。这样做时，我们需要通过源DataFrame引用该列。如果连接使用相同的键名，或者源DataFrame的列仅具有相同的名称，则可以执行以下操作： 12345person.join(gradProgramDupe, joinExpr).drop(person.col("graduate_program")).select("graduate_program").show()val joinExpr = person.col("graduate_program") === graduateProgram.col("id")person.join(graduateProgram, joinExpr).drop(graduateProgram.col("id")).show() This is an artifact of Spark’s SQL analysis process in which an explicitly referenced column will pass analysis because Spark has no need to resolve the column. Notice how the column uses the .col method instead of a column function. That allows us to implicitly specify that column by its specific ID. 这是Spark的SQL分析过程的产物，其中显示引用的列将传递到分析过程，因为Spark不需要解析该列。 注意该列如何使用 .col 方法而不是列函数。 这使我们可以通过其特定ID隐式指定该列。 Approach 3: Renaming a column before the join 方法3：在连接之前重命名列We can avoid this issue altogether if we rename one of our columns before the join: 如果我们在连接之前重命名其中一个列，则可以完全避免此问题： 123val gradProgram3 = graduateProgram.withColumnRenamed("id", "grad_id")val joinExpr = person.col("graduate_program") === gradProgram3.col("grad_id")person.join(gradProgram3, joinExpr).show() How Spark Performs Joins Spark如何执行连接To understand how Spark performs joins, you need to understand the two core resources at play: the node-to-node communication strategy and per node computation strategy. These internals are likely irrelevant to your business problem. However, comprehending how Spark performs joins can mean the difference between a job that completes quickly and one that never completes at all. 要了解Spark如何执行连接，您需要了解两个核心资源：节点对节点通信策略和每个节点计算策略。这些内部因素可能与您的业务问题无关。但是，了解Spark如何执行连接可能意味着快速完成的工作与根本没有完成的工作之间的区别。 Communication Strategies 通信策略Spark approaches cluster communication in two different ways during joins. It either incurs a shuffle join, which results in an all-to-all communication or a broadcast join. Keep in mind that there is a lot more detail than we’re letting on at this point, and that’s intentional. Some of these internal optimizations are likely to change over time with new improvements to the cost-based optimizer and improved communication strategies. For this reason, we’re going to focus on the high-level examples to help you understand exactly what’s going on in some of the more common scenarios, and let you take advantage of some of the low-hanging fruit that you can use right away to try to speed up some of your workloads. 在连接期间，Spark以两种不同的方式处理集群通信。它会导致洗牌连接（shuffle join），从而导致进行所有节点之间（all-to-all）相互通信或广播连接（broadcast join）。请记住，这时我们要提供的细节比我们要多得多，这是故意的。随着基于成本的优化器的新改进和改进的通信策略，其中一些内部优化可能会随时间而变化。因此，我们将集中在高阶示例上，以帮助您准确了解某些较常见的情况下发生的事情，并让您充分利用一些可以直接用的容易的方法去尝试加快一些工作量。 The core foundation of our simplified view of joins is that in Spark you will have either a big table or a small table. Although this is obviously a spectrum (and things do happen differently if you have a “medium-sized table”), it can help to be binary about the distinction for the sake of this explanation. 我们简化的连接视图的核心基础是，在Spark中，您将有一个大表或一个小表。尽管这显然是一个范围（如果您有“中型表”，事情的发生会有所不同），但是为了便于说明，将其区分成2个范围可能会有所帮助。 Big table–to–big table 大表与大表的连接When you join a big table to another big table, you end up with a shuffle join, such as that illustrates in Figure 8-1. 当将一个大表连接到另一个大表时，最终将进行随机组合，如图8-1所示。 In a shuffle join, every node talks to every other node and they share data according to which node has a certain key or set of keys (on which you are joining). These joins are expensive because the network can become congested with traffic, especially if your data is not partitioned well. 在洗牌连接中，每个节点都与其他每个节点通信，并根据哪个节点具有某个键或一组键（要加入的键）共享数据。这些连接代价非常高，因为网络可能会变得拥塞，特别是如果您的数据没有很好地分区的时候。 This join describes taking a big table of data and joining it to another big table of data. An example of this might be a company that receives billions of messages every day from the Internet of Things, and needs to identify the day-over-day changes that have occurred. The way to do this is by joining on deviceId, messageType, and date in one column, and date - 1 day in the other column. In Figure 8-1, DataFrame 1 and DataFrame 2 are both large DataFrames. This means that all worker nodes (and potentially every partition) will need to communicate with one another during the entire join process (with no intelligent partitioning of data). 此连接描述获取一个大数据表并将其连接到另一个大数据表。例如，一家公司每天从物联网接收数十亿条消息，并且需要确定每天发生的变化，这就是一个例子。要做到这一点，方法是在一列中加入deviceId，messageType和date，在另一列中加入date-1天。在图8-1中，DataFrame 1和DataFrame 2都是大型DataFrame。这意味着所有工作节点（以及可能的每个分区）在整个连接过程中都需要相互通信（没有智能分区数据）。 Big table–to–small table 大表连接小表When the table is small enough to fit into the memory of a single worker node, with some breathing room of course, we can optimize our join. Although we can use a big table–to–big table communication strategy, it can often be more efficient to use a broadcast join. What this means is that we will replicate our small DataFrame onto every worker node in the cluster (be it located on one machine or many). Now this sounds expensive. However, what this does is prevent us from performing the all-to-all communication during the entire join process. Instead, we perform it only once at the beginning and then let each individual worker node perform the work without having to wait or communicate with any other worker node, as is depicted in Figure 8-2. 当表足够小到融入单个工作节点的内存时，当然还有一些喘息的空间，我们可以优化连接。尽管我们可以使用大表对大表的通信策略，但使用广播连接通常会更有效。这意味着我们将把小型DataFrame复制到集群中的每个工作节点上（无论它位于一台计算机上还是多台计算机上）。现在听起来代价很高。但是，这样做会阻止我们在整个连接过程中执行所有节点之间相互的通信。相反，我们仅在开始时执行一次，然后让每个单独的工作程序节点执行工作，而不必等待或与任何其他工作程序节点通信，如图8-2所示。 At the beginning of this join will be a large communication, just like in the previous type of join. However, immediately after that first, there will be no further communication between nodes. This means that joins will be performed on every single node individually, making CPU the biggest bottleneck. For our current set of data, we can see that Spark has automatically set this up as a broadcast join by looking at the explain plan: 与以前的连接类型一样，此连接的开始将进行大量通信。但是，紧接在那之后，节点之间将不再有进一步的通信。这意味着连接将在每个单个节点上单独执行，这使CPU成为最大的瓶颈。对于我们当前的数据集，我们可以通过查看解释计划来看到Spark已自动将其设置为广播连接： 12val joinExpr = person.col("graduate_program") === graduateProgram.col("id")person.join(graduateProgram, joinExpr).explain() 1234== Physical Plan ==*BroadcastHashJoin [graduate_program#40], [id#5....:- LocalTableScan [id#38, name#39, graduate_progr...+- BroadcastExchange HashedRelationBroadcastMode(.... +- LocalTableScan [id#56, degree#57, departmen.... With the DataFrame API, we can also explicitly give the optimizer a hint that we would like to use a broadcast join by using the correct function around the small DataFrame in question. In this example, these result in the same plan we just saw; however, this is not always the case: 借助DataFrame API，我们还可以通过对所讨论的小型DataFrame使用正确的函数，为优化程序明确提示我们要使用广播连接。在此示例中，这些结果与我们刚刚看到的计划相同；然而，这并非总是如此： 123import org.apache.spark.sql.functions.broadcastval joinExpr = person.col("graduate_program") === graduateProgram.col("id")person.join(broadcast(graduateProgram), joinExpr).explain() The SQL interface also includes the ability to provide hints to perform joins. These are not enforced, however, so the optimizer might choose to ignore them. You can set one of these hints by using a special comment syntax. MAPJOIN, BROADCAST, and BROADCASTJOIN all do the same thing and are all supported : SQL接口还提供执行连接提示的功能。但是，这些功能不是强制性的，因此优化程序可能会选择忽略它们。您可以使用特殊的注释语法设置这些提示之一。 MAPJOIN，BROADCAST和 BROADCASTJOIN 都做相同的事情，并且都受支持： 123-- in SQLSELECT /*+ MAPJOIN(graduateProgram) */ * FROM person JOIN graduateProgramON person.graduate_program = graduateProgram.id This doesn’t come for free either: if you try to broadcast something too large, you can crash your driver node (because that collect is expensive). This is likely an area for optimization in the future. 这也不是免费的：如果您尝试广播太大的内容，可能会导致驱动程序节点崩溃（因为收集代价很高）。这可能是将来需要优化的领域。 Little table–to–little table 小表连接小表When performing joins with small tables, it’s usually best to let Spark decide how to join them. You can always force a broadcast join if you’re noticing strange behavior. 在执行小表之间的连接时，通常最好让Spark决定如何连接它们。如果您发现异常行为，可以随时强制加入广播连接。 Conclusion 结论In this chapter, we discussed joins, probably one of the most common use cases. One thing we did not mention but is important to consider is if you partition your data correctly prior to a join, you can end up with much more efficient execution because even if a shuffle is planned, if data from two different DataFrames is already located on the same machine, Spark can avoid the shuffle. Experiment with some of your data and try partitioning beforehand to see if you can notice the increase in speed when performing those joins. In Chapter 9, we will discuss Spark’s data source APIs. There are additional implications when you decide what order joins should occur in. Because some joins act as filters, this can be a low-hanging improvement in your workloads, as you are guaranteed to reduce data exchanged over the network. 在本章中，我们讨论了连接（可能是最常见的用例之一）。我们没有提到但要考虑的一件事是，如果在连接之前正确地对数据进行了分区，则可以提高执行效率，因为即使洗牌是有计划得，如果来自两个不同DataFrames的数据位于同一台机器上，Spark可以避免洗牌。试用一些数据，然后尝试进行分区，以查看执行这些连接时是否可以注意到速度的提高。 在第9章中，我们将讨论Spark的数据源API。当您决定应按什么顺序进行连接时，还存在其他含义。由于某些连接充当过滤器，因此可以保证在网络上交换的数据减少，这对您的工作负载而言是微不足道的改进。 The next chapter will depart from user manipulation, as we’ve seen in the last several chapters, and touch on reading and writing data using the Structured APIs. 正如我们在前几章中所看到的那样，下一章将脱离用户操作，并介绍使用结构化API读写数据。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 16 Developing Spark Applications]]></title>
    <url>%2F2019%2F08%2F05%2FChapter16_DevelopingSparkApplications(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 16 Developing Spark ApplicationsTesting Spark Applications 测试Spark应用程序You now know what it takes to write and run a Spark Application, so let’s move on to a less exciting but still very important topic: testing. Testing Spark Applications relies on a couple of key principles and tactics that you should keep in mind as you’re writing your applications. 您现在知道编写和运行Spark应用程序需要什么，所以让我们继续讨论一个不太令人兴奋但仍然非常重要的主题：测试。测试Spark应用程序依赖于在编写应用程序时应该记住的几个关键原则和策略。 Strategic Principles 战略原则Testing your data pipelines and Spark Applications is just as important as actually writing them. This is because you want to ensure that they are resilient to future change, in data, logic, and output. In this section, we’ll first discuss what you might want to test in a typical Spark Application, then discuss how to organize your code for easy testing. 测试数据管道和 Spark 应用程序与实际编写它们一样重要。这是因为您希望确保它们能够适应未来的变化，包括数据，逻辑和输出。在本节中，我们将首先讨论您可能希望在典型的 Spark 应用程序中测试的内容，然后讨论如何组织代码以便于轻松测试。 Input data resilience 输入数据弹性Being resilient to different kinds of input data is something that is quite fundamental to how you write your data pipelines. The data will change because the business needs will change. Therefore your Spark Applications and pipelines should be resilient to at least some degree of change in the input data or otherwise ensure that these failures are handled in a graceful and resilient way. For the most part this means being smart about writing your tests to handle those edge cases of different inputs and making sure that the pager only goes off when it’s something that is truly important. 对不同类型的输入数据具有弹性对于编写数据管道非常重要。数据将发生变化，因为业务需求将发生变化。因此，您的Spark应用程序和管道应该能够适应输入数据的至少某种程度的变化，或者确保以优雅和弹性的方式处理这些故障。在大多数情况下，这意味着要聪明地编写测试来处理不同输入的边缘情况（极端情况），并确保报警器仅在真正重要的事情发生时才会发出警报。 Business logic resilience and evolution 业务逻辑弹性和演变The business logic in your pipelines will likely change as well as the input data. Even more importantly, you want to be sure that what you’re deducing from the raw data is what you actually think that you’re deducing. This means that you’ll need to do robust logical testing with realistic data to ensure that you’re actually getting what you want out of it. One thing to be wary of here is trying to write a bunch of “Spark Unit Tests” that just test Spark’s functionality. You don’t want to be doing that; instead, you want to be testing your business logic and ensuring that the complex business pipeline that you set up is actually doing what you think it should be doing. 管道中的业务逻辑可能会随输入数据而变化。更重要的是，您希望确保从原始数据中推断出的内容是您实际认为正在推导的内容。这意味着您需要使用真实数据进行强大的逻辑测试，以确保您实际上从中获得了您想要的内容。需要注意的一件事是尝试编写一组只测试Spark功能的“Spark Unit Tests”。你不想这样做;相反，您希望测试业务逻辑并确保您设置的复杂业务管道实际上正在执行您认为应该执行的操作。 Resilience in output and atomicity 输出的弹性和原子性Assuming that you’re prepared for departures in the structure of input data and that your business logic is well tested, you now want to ensure that your output structure is what you expect. This means you will need to gracefully handle output schema resolution. It’s not often that data is simply dumped in some location, never to be read again—most of your Spark pipelines are probably feeding other Spark pipelines. For this reason you’re going to want to make certain that your downstream consumers understand the “state” of the data—this could mean how frequently it’s updated as well as whether the data is “complete” (e.g., there is no late data) or that there won’t be any last-minute corrections to the data. All of the aforementioned issues are principles that you should be thinking about as you build your data pipelines (actually, regardless of whether you’re using Spark). This strategic thinking is important for laying down the foundation for the system that you would like to build. 假设您已准备好在输入数据结构中离开，并且您的业务逻辑已经过充分测试，那么您现在需要确保您的输出结构符合您的预期。这意味着您需要优雅地处理输出模式解析。通常不会将数据简单地转储到某个位置，永远不会再次读取——大多数Spark管道可能正在为其他 Spark 管道提供服务。因此，您需要确保下游消费者了解数据的“状态”——这可能意味着更新频率以及数据是否“完整”（例如，没有后期数据）或者不会对数据进行任何最后修正。所有上述问题都是您在构建数据管道时应该考虑的原则（实际上，无论您是否使用Spark）。这种战略思维对于为您希望构建的系统奠定基础非常重要。 Tactical Takeaways 战术外卖Although strategic thinking is important, let’s talk a bit more in detail about some of the tactics that you can actually use to make your application easy to test. The highest value approach is to verify that your business logic is correct by employing proper unit testing and to ensure that you’re resilient to changing input data or have structured it so that schema evolution will not become unwielding in the future. The decision for how to do this largely falls on you as the developer because it will vary according to your business domain and domain expertise. 虽然战略思维很重要，但让我们更详细地谈谈您可以实际使用的一些策略，以使您的应用程序易于测试。最有价值的方法是通过采用适当的单元测试来验证您的业务逻辑是否正确，并确保您能够适应不断变化的输入数据，或者对其进行结构化，以便模式演变在未来不会变得无法使用。关于如何做到这一点的决定很大程度上取决于您作为开发人员，因为它将根据您的业务领域和领域专业知识而有所不同。 Managing SparkSessions 管理SparkSessionsTesting your Spark code using a unit test framework like JUnit or ScalaTest is relatively easy because of Spark’s local mode—just create a local mode SparkSession as part of your test harness to run it. However, to make this work well, you should try to perform dependency injection as much as possible when managing SparkSessions in your code. That is, initialize the SparkSession only once and pass it around to relevant functions and classes at runtime in a way that makes it easy to substitute during testing. This makes it much easier to test each individual function with a dummy SparkSession in unit tests. 使用单元测试框架（如 JUnit 或 ScalaTest ）测试Spark代码相对容易，因为Spark的本地模式——只需创建一个本地模式 SparkSession 作为测试工具的一部分来运行它。但是，为了使这项工作更好，您应该在代码中管理SparkSessions 时尽可能多地执行依赖项注入。也就是说，只将 SparkSession 初始化一次并在运行时将其传递给相关的函数和类，以便在测试期间轻松替换。这使得在单元测试中使用仿真的SparkSession测试每个单独的函数变得更加容易。 Which Spark API to Use? 使用哪种Spark API？Spark offers several choices of APIs, ranging from SQL to DataFrames and Datasets, and each of these can have different impacts for maintainability and testability of your application. To be perfectly honest, the right API depends on your team and its needs: some teams and projects will need the less strict SQL and DataFrame APIs for speed of development, while others will want to use type-safe Datasets or RDDs. Spark提供了多种API选择，从 SQL 到 DataFrames 和 Datasets，每种API都会对应用程序的可维护性和可测试性产生不同的影响。说实话，正确的API取决于您的团队及其需求：一些团队和项目需要不太严格的SQL和DataFrame API来提高开发速度，而其他团队和项目则需要使用类型安全的 DataSet 或 RDD 。 In general, we recommend documenting and testing the input and output types of each function regardless of which API you use. The type-safe API automatically enforces a minimal contract for your function that makes it easy for other code to build on it. If your team prefers to use DataFrames or SQL, then spend some time to document and test what each function returns and what types of inputs it accepts to avoid surprises later, as in any dynamically typed programming language. While the lower-level RDD API is also statically typed, we recommend going into it only if you need low-level features such as partitioning that are not present in Datasets, which should not be very common; the Dataset API allows more performance optimizations and is likely to provide even more of them in the future. 通常，我们建议记录和测试每个函数的输入和输出类型，无论您使用哪个API。类型安全的API会自动为您的函数强制执行最小的约定（可以理解为：可使用的条款少，灵活度低），以便在其上构建其他代码。如果您的团队更喜欢使用DataFrames或SQL，那么花一些时间来记录和测试每个函数返回的内容以及它接受哪些类型的输入以避免以后出现意外，就像在任何动态类型编程语言中一样。虽然较低阶的RDD API也是静态类型的，但我们建议只有在需要低阶功能（例如数据集中不存在的分区）时才进入它，这应该不常见; DataSet API允许更多性能优化，并且可能在将来提供更多性能优化。 A similar set of considerations applies to which programming language to use for your application: there certainly is no right answer for every team, but depending on your needs, each language will provide different benefits. We generally recommend using statically typed languages like Scala and Java for larger applications or those where you want to be able to drop into low-level code to fully control performance, but Python and R may be significantly better in other cases—for example, if you need to use some of their other libraries. Spark code should easily be testable in the standard unit testing frameworks in every language. 一组类似的注意事项适用于您的应用程序使用哪种编程语言：每个团队肯定没有正确的答案，但根据您的需求，每种语言都会提供不同的好处。我们通常建议对大型应用程序使用静态类型语言（如Scala和Java），或者希望能够放入低阶代码以完全控制性能的语言，但在其他情况下，Python和R可能会明显更好——例如，如果你需要使用他们的一些其他库。 Spark代码应该可以在每种语言的标准单元测试框架中轻松测试。 Connecting to Unit Testing Frameworks 连接到单元测试框架To unit test your code, we recommend using the standard frameworks in your langage (e.g., JUnit or ScalaTest ), and setting up your test harnesses to create and clean up a SparkSession for each test. Different frameworks offer different mechanisms to do this, such as “before” and “after” methods. We have included some sample unit testing code in the application templates for this chapter. 要对代码进行单元测试，我们建议您使用标准框架（例如，JUnit 或 ScalaTest），并设置测试工具来为每个测试创建和清理 SparkSession。不同的框架提供了不同的机制来实现这一点，例如“之前”和“之后”方法。我们在本章的应用程序模板中包含了一些示例单元测试代码。 Connecting to Data Sources 连接到数据源As much as possible, you should make sure your testing code does not connect to production data sources, so that developers can easily run it in isolation if these data sources change. One easy way to make this happen is to have all your business logic functions take DataFrames or Datasets as input instead of directly connecting to various sources; after all, subsequent code will work the same way no matter what the data source was. If you are using the structured APIs in Spark, another way to make this happen is named tables: you can simply register some dummy datasets (e.g., loaded from small text file or from in-memory objects) as various table names and go from there. 您应尽可能确保测试代码不会连接到生产数据源，以便开发人员可以在这些数据源发生更改时轻松地单独运行它。实现这一目标的一种简单方法是让所有业务逻辑功能将DataFrames或Datasets作为输入，而不是直接连接到各种源; 毕竟，无论数据源是什么，后续代码都将以相同的方式工作。如果您在Spark中使用结构化API，另一种实现此目的的方法是命名表：您只需将一些仿真数据集（例如，从小文本文件或内存中对象加载）注册为各种表名，然后从那里开始。 The Development Process 开发过程The development process with Spark Applications is similar to development workflows that you have probably already used. First, you might maintain a scratch space, such as an interactive notebook or some equivalent thereof, and then as you build key components and algorithms, you move them to a more permanent location like a library or package. The notebook experience is one that we often recommend (and are using to write this book) because of its simplicity in experimentation. There are also some tools, such as Databricks, that allow you to run notebooks as production applications as well. Spark应用程序的开发过程类似于您可能已经使用过的开发工作流程。首先，您可以维护一个临时空间，例如交互式笔记本或其等效物，然后在构建关键组件和算法时，将它们移动到更永久的位置，如库或包。notebook（与jupter notebook类似）的体验是我们经常推荐的（并且正在用来编写本书），因为它的实验非常简单。还有一些工具，如 Databricks，允许您将笔记本作为生产应用程序运行。 When running on your local machine, the spark-shell and its various language-specific implementations are probably the best way to develop applications. For the most part, the shell is for interactive applications, whereas spark-submit is for production applications on your Spark cluster. You can use the shell to interactively run Spark, just as we showed you at the beginning of this book. This is the mode with which you will run PySpark, Spark SQL, and SparkR. In the bin folder, when you download Spark, you will find the various ways of starting these shells. Simply run sparkshell(for Scala), spark-sql, pyspark, and sparkR. After you’ve finished your application and created a package or script to run, spark-submit will become your best friend to submit this job to a cluster. 在本地计算机上运行时，spark-shell 及其各种特定于语言的实现可能是开发应用程序的最佳方式。在大多数情况下，shell用于交互式应用程序，而 spark-submit 用于Spark集群上的生产应用程序。您可以使用shell以交互方式运行Spark，就像我们在本书开头部分向您展示的那样。这是运行 PySpark，Spark SQL 和 SparkR 的模式。在bin文件夹中，当您下载 Spark 时，您将找到启动这些 shell 的各种方法。只需运行 sparkshell（适用于Scala），spark-sql，pyspark 和 sparkR。在您完成应用程序并创建要运行的包或脚本后，spark-submit 将成为您将此作业提交到群集的最佳朋友。 Launching Applications 启动应用程序The most common way for running Spark Applications is through spark-submit. Previously in this chapter, we showed you how to run spark-submit; you simply specify your options, the application JAR or script, and the relevant arguments: 运行Spark应用程序的最常用方法是通过spark-submit。 在本章的前面，我们向您展示了如何运行spark-submit; 您只需指定选项，应用程序 JAR 或脚本以及相关参数： 12345678./bin/spark-submit \--class &lt;main-class&gt; \--master &lt;master-url&gt; \--deploy-mode &lt;deploy-mode&gt; \--conf &lt;key&gt;=&lt;value&gt; \... # other options&lt;application-jar-or-script&gt; \[application-arguments] You can always specify whether to run in client or cluster mode when you submit a Spark job with spark-submit. However, you should almost always favor running in cluster mode (or in client mode on the cluster itself) to reduce latency between the executors and the driver.当您使用 spark-submit 提交 Spark 作业时，您始终可以指定是以客户端还是群集模式运行。 但是，您几乎总是倾向于在群集模式下运行（或在集群本身的客户端模式下）以减少执行程序和驱动程序之间的延迟。When submitting applciations, pass a .py file in the place of a .jar, and add Python .zip, .egg, or .py to the search path with –py-files.提交 applciations 时，在 .jar 的位置传递 .py 文件，并使用 –py-files 将 Python .zip，.egg 或 .py 添加到搜索路径。For reference, Table 16-1 lists all of the available spark-submit options, including those that are particular to some cluster managers. To enumerate all these options yourself, run spark-submit with –help.作为参考，表16-1列出了所有可用的 spark-submit 选项，包括某些集群管理器特有的选项。 要自己枚举所有这些选项，请使用 –help 运行 spark-submit。Table 16-1. Spark submit help text| Parameter | Description || —————————- | ———————————————————— || –masterMASTER_URL | spark://host:port, mesos://host:port, yarn, or localSpark 连接的资源管理器 || –deploymodeDEPLOY_MODE | Whether to launch the driver program locally (“client”) or on one of the worker machines inside the cluster (“cluster”) (Default: client)是在本地（“client”）还是在集群内（“cluster”）的某个工作机器上启动驱动程序（默认值：client） || –classCLASS_NAME | Your application’s main class (for Java / Scala apps).您的应用程序的主类（适用于Java / Scala应用程序）。 || –name NAME | A name of your application.你的应用程序主类。 || –jars JARS | Comma-separated list of local JARs to include on the driver and executor classpaths.以逗号分隔的本地 JAR 列表，包含在驱动程序和执行程序类路径中。 || –packages | search the local Maven repo, then Maven Central and any additional remote repositories given by –repositories. The format for the coordinates should be groupId:artifactId:version.搜索本地 Maven 仓库，然后搜索 Maven Central 以及 --repositories 给出的任何其他远程仓库。 坐标的格式应为 groupId:artifactId:version。 || –exclude-packages | Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in –packages to avoid dependency conflicts.逗号分隔的 groupId:artifactId 列表，在解析 --packages 中提供的依赖项时排除，以避免依赖性冲突。 || –repositories | Comma-separated list of additional remote repositories to search for the Maven coordinates given with –packages.以逗号分隔的其他远程仓库列表，用于搜索 --packages 给出的Maven坐标 || –py-filesPY_FILES | Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps.以逗号分隔的 .zip，.egg 或 .py 文件列表，放在 Python 应用程序的 PYTHONPATH 上。 || –filesFILES | Comma-separated list of files to be placed in the working directory of each executor.以逗号分隔的文件列表，放在每个执行程序的工作目录中。 || –confPROP=VALUE | Arbitrary Spark configuration property.任意Spark配置属性 || –propertiesfile FILE | Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf.从中加载额外属性的文件的路径。 如果未指定，则会查找 conf/spark-defaults.conf。 || –driver-memory MEM | Memory for driver (e.g., 1000M, 2G) (Default: 1024M).驱动器的内存（默认：1024M）。 || –driver-java-options | Extra Java options to pass to the driver.传递给驱动器的额外 Java 选项。 || –driver-library-path | Extra library path entries to pass to the driver.要传递给驱动程序的额外库路径条目。 || –driver-class-path | Extra class path entries to pass to the driver. Note that JARs added with –jars are automatically included in the classpath.要传递给驱动程序的额外类路径条目。请注意，添加了 --jars 的 JAR 会自动包含在类路径中。 || –executor-memory MEM | Memory per executor (e.g., 1000M, 2G) (Default: 1G).每个执行器的内存（例如，1000M，2G）（默认值：1G）。 || –proxy-user NAME | User to impersonate when submitting the application. This argument does not work with –principal / keytab.用户在提交申请时进行冒充。此参数不适用于 --principal/keytab。 || –help, -h | Show this help message and exit.显示此帮助消息并退出。 || –verbose, -v | Print additional debug output.打印其他调试输出。 || –version | Print the version of current Spark.打印当前Spark的版本。 |There are some deployment-specific configurations as well (see Table 16-2).还有一些特定于部署的配置（参见表16-2）。| Cluster Managers | Modes | Conf | Description || —————- | ——- | ————————— | ———————————————————— || Standalone | Cluster | –driver-cores NUM | Cores for driver (Default: 1)驱动核心（默认值：1） || Standalone/Mesos | Cluster | –supervise | If given, restarts the driver on failure.如果给定，则在失败时重新启动驱动程序。 || Standalone/Mesos | Cluster | –killSUBMISSION_ID | If given, kills the driver specified.如果给定，则杀死指定的驱动程序。 || Standalone/Mesos | Cluster | –statusSUBMISSION_ID | If given, requests the status of the driver specified.如果给定，请求指定的驱动程序的状态。 || Standalone/Mesos | Either | –total-executor-cores NUM | Total cores for all executors.所有执行器（executor）的核心总数。 || Standalone/YARN | Either | –total-executor-cores NUM1 | Number of cores per executor. (Default: 1 in YARN mode or all available cores on the worker in standalone mode)每个执行器（executor）的核心数。 （默认值：YARN模式下为1或独立模式下工作线程上的所有可用核心） || YARN | Either | –driver-cores NUM | Number of cores used by the driver, only in cluster mode (Default: 1).驱动程序使用的核心数，仅在集群模式下（默认值：1）。 || YARN | Either | queue QUEUE_NAME | The YARN queue to submit to (Default: “default”).要提交的YARN队列（默认值：“默认”）。 || YARN | Either | –num-executors NUM | Number of executors to launch (Default: 2). If dynamic allocation is enabled, the initial number of executors will be at least NUM.要启动的执行程序数（默认值：2）。如果启用了动态分配，则执行程序的初始数量将至少为NUM。 || YARN | Either | –archivesARCHIVES | Comma-separated list of archives to be extracted into the working directory of each executor.以逗号分隔的档案列表，提取到每个执行程序的工作目录中。 || YARN | Either | –principal PRINCIPAL | Principal to be used to log in to KDC, while running on secure HDFS.Principal 用于在安全 HDFS 上运行时登录 KDC。 || YARN | Either | –keytabKEYTAB | The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the Application Master via the Secure Distributed Cache, for renewing the login tickets and the delegation tokens periodically.包含上面指定的主体的keytab的文件的完整路径。此密钥表将通过安全分布式缓存复制到运行Application Master的节点，以定期更新登录票证和委派令牌。 |最新的 Spark 应用程序提交帮助文档### Application Launch ExamplesWe already covered some local-mode application examples previously in this chapter, but it’s worth looking at how we use some of the aforementioned options, as well. Spark also includes several examples and demonstration applications in the examples directory that is included when you download Spark. If you’re stuck on how to use certain parameters, simply try them first on your local machine and use the SparkPi class as the main class:我们已经介绍了本章前面的一些本地模式应用程序示例，但是值得一看的是我们如何使用上述一些选项。 Spark还包含下载Spark时包含的示例目录中的几个示例和演示应用程序。 如果你坚持使用某些参数，只需先在本地机器上尝试它们，然后使用SparkPi类作为主类：1234567./bin/spark-submit \--class org.apache.spark.examples.SparkPi \--master spark://207.184.161.138:7077 \--executor-memory 20G \--total-executor-cores 100 \replace/with/path/to/examples.jar \1000The following snippet does the same for Python. You run it from the Spark directory and this will allow you to submit a Python application (all in one script) to the standalone cluster manager. You can also set the same executor limits as in the preceding example:以下代码段对Python也是如此。 您可以从Spark目录运行它，这将允许您将Python应用程序（所有在一个脚本中）提交给独立的集群管理器。 您还可以设置与前面示例中相同的执行器（executor）限制：1234./bin/spark-submit \--master spark://207.184.161.138:7077 \examples/src/main/python/pi.py \1000You can change this to run in local mode as well by setting the master to local or local[] to run on all the cores on your machine. You will also need to change the /path/to/examples.jar to the relevant Scala and Spark versions you are running.您可以将此更改为在本地模式下运行，方法是将主服务器设置为 local 或 local[] 以在计算机上的所有核心上运行。 您还需要将 /path/to/examples.jar 更改为您正在运行的相关Scala和Spark版本。## Configuring Applications 配置应用程序Spark includes a number of different configurations, some of which we covered in Chapter 15. There are many different configurations, depending on what you’re hoping to achieve. This section covers those very details. For the most part, this information is included for reference and is probably worth skimming only, unless you’re looking for something in particular. The majority of configurations fall into the following categories:Spark包含许多不同的配置，其中一些我们在第15章中介绍过。根据您希望实现的目标，有许多不同的配置。本节介绍了这些细节。在大多数情况下，这些信息仅供参考，可能仅值得略读，除非您特别寻找某些内容。大多数配置分为以下几类：- Application properties 应用属性- Runtime environment 运行环境- Shuffle behavior 洗牌行为- Spark UI- Compression and serialization 解压缩- Memory management 内存管理- Execution behavior 执行行为- Networking 网络- Scheduling 调度- Dynamic allocation 动态分配- Security 安全- Encryption 加密- Spark SQL- Spark streaming Spark流- SparkRSpark provides three locations to configure the system:Spark提供三个位置来配置系统：- Spark properties control most application parameters and can be set by using a SparkConf object Spark 属性控制大多数应用程序参数，可以使用 SparkConf 对象进行设置- Java system properties Java系统属性- Hardcoded configuration files 硬编码配置文件There are several templates that you can use, which you can find in the /conf directory available in the root of the Spark home folder. You can set these properties as hardcoded variables in your applications or by specifying them at runtime. You can use environment variables to set per-machine settings, such as the IP address, through the conf/spark-env.sh script on each node. Lastly, you can configure logging through log4j.properties.您可以使用几个模板，您可以在Spark主文件夹的根目录中的 /conf 目录中找到这些模板。您可以在应用程序中将这些属性设置为硬编码变量，也可以在运行时指定它们。您可以使用环境变量通过每个节点上的 conf/spark-env.sh 脚本设置每台计算机设置，例如IP地址。最后，您可以通过 log4j.properties 配置日志记录。### The SparkConfThe SparkConf manages all of our application configurations. You create one via the import statement, as shown in the example that follows. After you create it, the SparkConf is immutable for that specific Spark Application:SparkConf 管理我们的所有应用程序配置。您可以通过 import 语句创建一个，如下面的示例所示。创建它之后，SparkConf 对于特定的Spark应用程序是不可变的：1234// in Scalaimport org.apache.spark.SparkConfval conf = new SparkConf().setMaster("local[2]").setAppName("DefinitiveGuide").set("some.conf", "to.some.value")1234# in Pythonfrom pyspark import SparkConfconf = SparkConf().setMaster("local[2]").setAppName("DefinitiveGuide")\.set("some.conf", "to.some.value")You use the SparkConf to configure individual Spark Applications with Spark properties. These Spark properties control how the Spark Application runs and how the cluster is configured. The example that follows configures the local cluster to have two threads and specifies the application name that shows up in the Spark UI.您可以使用 SparkConf 使用 Spark 属性配置各个 Spark 应用程序。这些 Spark 属性控制 Spark 应用程序的运行方式以及集群的配置方式。以下示例将本地集群配置为具有两个线程，并指定在 Spark UI 中显示的应用程序名称。You can configure these at runtime, as you saw previously in this chapter through command-line arguments. This is helpful when starting a Spark Shell that will automatically include a basic Spark Application for you; for instance:您可以在运行时配置它们，如本章前面通过命令行参数所见。这在启动Spark Shell时非常有用，它将自动包含一个基本的Spark应用程序；例如：1./bin/spark-submit --name "DefinitiveGuide" --master local[4] ...Of note is that when setting time duration-based properties, you should use the following format:值得注意的是，在设置基于持续时间的属性时，您应该使用以下格式：- 25ms (milliseconds 毫秒)- 5s (seconds 秒)- 10m or 10min (minutes 分钟)- 3h (hours 小时)- 5d (days 天)- 1y (years 年)### Application Properties 应用属性 Application properties are those that you set either from spark-submit or when you create your Spark Application. They define basic application metadata as well as some execution characteristics. Table 16-3 presents a list of current application properties. 应用程序属性是您通过 spark-submit 或创建 Spark 应用程序时设置的属性。它们定义了基本的应用程序元数据以及一些执行特性。表 16-3 列出了当前的应用程序属性。 Table 16-3. Application properties Property name 属性名 Default默认值 Meaning 意思 spark.app.name (none) The name of your application. This will appear in the UI and in log data.您的应用程序的名称。 这将显示在UI和日志数据中。 spark.driver.cores 1 Number of cores to use for the driver process, only in cluster mode.仅在集群模式下用于驱动程序进程的核心数。 spark.driver.maxResultSize 1g Limit of total size of serialized results of all partitions for each Spark action (e.g., collect). Should be at least 1M, or 0 for unlimited. Jobs will be aborted if the total size exceeds this limit. Having a high limit can cause OutOfMemoryErrors in the driver (depends on spark.driver.memory and memory overhead of objects in JVM). Setting a proper limit can protect the driver from OutOfMemoryErrors.每个Spark操作的所有分区的序列化结果的总大小限制（例如，collect）。 应至少为1M，或0为无限制。 如果总大小超过此限制，则将中止作业。 具有上限可能会导致驱动程序中的OutOfMemoryErrors（取决于spark.driver.memory和JVM中对象的内存开销）。 设置适当的限制可以保护驱动程序免受OutOfMemoryErrors的影响。 spark.driver.memory 1g Amount of memory to use for the driver process, where SparkContext is initialized. (e.g. 1g, 2g). Note: in client mode, this must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, set this through the –driver-memory command-line option or in your default properties file.用于初始化 SparkContext 的驱动程序进程的内存量。 （例如1g，2g）。 注意：在客户端模式下，不能直接在应用程序中通过 SparkConf 设置，因为驱动程序JVM已在此时启动。 而是通过–driver-memory命令行选项或在默认属性文件中设置它。 spark.executor.memory 1g Amount of memory to use per executor process (e.g., 2g, 8g).每个执行程序进程使用的内存量（例如，2g，8g）。 spark.extraListeners (none) A comma-separated list of classes that implement SparkListener; when initializing SparkContext, instances of these classes will be created and registered with Spark’s listener bus. If a class has a single-argument constructor that accepts a SparkConf, that constructor will be called; otherwise, a zero-argument constructor will be called. If no valid constructor can be found, the SparkContext creation will fail with an exception.以逗号分隔的实现SparkListener的类列表; 在初始化SparkContext时，将创建这些类的实例并使用Spark的侦听器总线进行注册。 如果一个类有一个接受SparkConf的单参数构造函数，那么将调用该构造函数; 否则，将调用零参数构造函数。 如果找不到有效的构造函数，SparkContext创建将失败并出现异常。 spark.logConf (false) Logs the effective SparkConf as INFO when a SparkContext is started.启动 SparkContext 时，将有效的 SparkConf 记录为INFO。 spark.master (none) The cluster manager to connect to. See the list of allowed master URLs.要连接的集群管理器。 请参阅允许的主URL列表。 spark.submit.deployMode (none) The deploy mode of the Spark driver program, either “client” or “cluster,” which means to launch driver program locally (“client”) or remotely (“cluster”) on one of the nodes inside the cluster.Spark驱动程序的部署模式，“客户端”或“集群”，这意味着在集群内的一个节点上本地（“客户端”）或远程（“集群”）启动驱动程序。 spark.log.callerContext (none) Application information that will be written into Yarn RM log/HDFS audit log when running on Yarn/HDFS. Its length depends on the Hadoop configuration hadoop.caller.context.max.size. It should be concise, and typically can have up to 50 characters.在 Yarn/HDFS 上运行时将写入Yarn RM log / HDFS 审核日志的应用程序信息。 它的长度取决于Hadoop配置hadoop.caller.context.max.size。 它应该简洁，通常最多可包含50个字符。 spark.driver.supervise (false) If true, restarts the driver automatically if it fails with a non-zero exit status. Only has effect in Spark standalone mode or Mesos cluster deploy mode.如果为true，则在失败且退出状态为非零时自动重新启动驱动程序。 仅在Spark独立模式或Mesos集群部署模式下有效。 You can ensure that you’ve correctly set these values by checking the application’s web UI on port 4040 of the driver on the “Environment” tab. Only values explicitly specified through sparkdefaults.conf, SparkConf, or the command line will appear. For all other configuration properties, you can assume the default value is used. 您可以通过在“环境”选项卡上检查驱动程序的端口4040上的应用程序的Web UI来确保您正确设置了这些值。仅显示那些通过 sparkdefaults.conf，SparkConf或命令行显式指定的值。对于所有其他配置属性，您可以假设使用默认值。 Runtime Properties 运行时属性Although less common, there are times when you might also need to configure the runtime environment of your application. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on the Runtime Environment in the Spark documentation. These properties allow you to configure extra classpaths and python paths for both drivers and executors, Python worker configurations, as well as miscellaneous logging properties. 虽然不太常见，但有时您可能还需要配置应用程序的运行时环境。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中的 Runtime Environment 上的相关表。这些属性允许您对驱动程序（driver）和执行器（excutor），Python工作程序进行配置以及各种日志记录属性配置额外的类路径和python路径。 Execution Properties 执行属性These configurations are some of the most relevant for you to configure because they give you finer-grained control on actual execution. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Execution Behavior in the Spark documentation. The most common configurations to change are spark.executor.cores (to control the number of available cores) and spark.files.maxPartitionBytes (maximum partition size when reading files). 这些配置与您配置最相关，因为它们可以为您提供对实际执行的精细控制。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中的执行行为相关表。要更改的最常见配置是 spark.executor.cores（用于控制可用内核的数量）和 spark.files.maxPartitionBytes（读取文件时的最大分区大小）。 Configuring Memory Management 配置内存管理There are times when you might need to manually manage the memory options to try and optimize your applications. Many of these are not particularly relevant for end users because they involve a lot of legacy concepts or fine-grained controls that were obviated in Spark 2.X because of automatic memory management. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Memory Management in the Spark documentation. 有时您可能需要手动管理内存选项以尝试和优化您的应用程序。 其中许多与最终用户并不特别相关，因为它们涉及很多遗留概念或由于自动内存管理而在Spark 2.X中避免的细粒度控制。 由于篇幅限制，我们无法在此处包含整个配置集。 请参阅Spark文档中的内存管理相关表。 Configuring Shuffle Behavior 配置洗牌行为We’ve emphasized how shuffles can be a bottleneck in Spark jobs because of their high communication overhead. Therefore there are a number of low-level configurations for controlling shuffle behavior. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Shuffle Behavior in the Spark documentation. 我们已经强调了洗牌如何成为Spark工作的瓶颈，因为它们的通信开销很高。因此，存在许多用于控制洗牌行为的低阶配置。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中有关洗牌行为的相关表。 Environmental Variables 环境变量You can configure certain Spark settings through environment variables, which are read from the conf/spark-env.sh script in the directory where Spark is installed (or conf/spark-env.cmd on Windows). In Standalone and Mesos modes, this file can give machine-specific information such as hostnames. It is also sourced when running local Spark Applications or submission scripts. Note that conf/spark-env.sh does not exist by default when Spark is installed. However, you can copy conf/spark-env.sh.template to create it. Be sure to make the copy executable. 您可以通过环境变量配置某些Spark设置，这些环境变量是从安装Spark的目录中的 conf/spark-env.sh 脚本（或Windows上的 conf/spark-env.cmd）中读取的。 在Standalone和Mesos模式下，此文件可以提供特定于机器的信息，例如主机名。 它还在运行本地Spark应用程序或提交脚本时获取。 请注意，安装Spark时默认情况下不存在conf/spark-env.sh。 但是，您可以复制 conf/spark-env.sh.template 来创建它。 务必使副本可执行。 The following variables can be set in spark-env.sh: 可以在spark-env.sh中设置以下变量： JAVA_HOME ​ Location where Java is installed (if it’s not on your default PATH).​ 安装Java的位置（如果它不在您的默认PATH上）。 PYSPARK_PYTHON ​ Python binary executable to use for PySpark in both driver and workers (default is python2.7 if available; otherwise, python). Property spark.pyspark.python takes precedence if it is set.​ 在驱动程序和工作程序中用于 PySpark 的 Python 二进制可执行文件（如果可用，默认为 python2.7; 否则为python）。 如果设置了属性 spark.pyspark.python，则优先级。 PYSPARK_DRIVER_PYTHON ​ Python binary executable to use for PySpark in driver only (default is PYSPARK_PYTHON). Property spark.pyspark.driver.python takes precedence if it is set.​ Python二进制可执行文件仅用于驱动程序中的PySpark（默认为PYSPARK_PYTHON）。 如果设置了属性spark.pyspark.driver.python，则优先级。 SPARKR_DRIVER_R ​ R binary executable to use for SparkR shell (default is R). Property spark.r.shell.command takes precedence if it is set.​ 用于SparkR shell的R二进制可执行文件（默认为R）。 如果设置了属性spark.r.shell.command优先。 SPARK_LOCAL_IP ​ IP address of the machine to which to bind.​ 要绑定的计算机的IP地址。 SPARK_PUBLIC_DNS ​ Hostname your Spark program will advertise to other machines.​ 您的Spark程序的主机名将通告给其他计算机。 In addition to the variables ust listed, there are also options for setting up the Spark standalone cluster scripts, such as number of cores to use on each machine and maximum memory. Because spark-env.sh is a shell script, you can set some of these programmatically; for example, you might compute SPARK_LOCAL_IP by looking up the IP of a specific network interface. 除了列出的变量之外，还有用于设置Spark独立集群脚本的选项，例如每台计算机上使用的核心数和最大内存。因为spark-env.sh是一个shell脚本，你可以通过编程方式设置其中一些;例如，您可以通过查找特定网络接口的IP来计算SPARK_LOCAL_IP。 NOTE 注意When running Spark on YARN in cluster mode, you need to set environment variables by using the spark.yarn.appMasterEnv.[EnvironmentVariableName] property in your conf/spark-defaults.conf file. Environment variables that are set in spark-env.sh will not be reflected in the YARN Application Master process in cluster mode. See the YARN-related Spark Properties for more information. 在集群模式下在 YARN 上运行Spark时，需要使用 conf/spark-defaults.conf 文件中的spark.yarn.appMasterEnv.[EnvironmentVariableName] 属性设置环境变量。在 spark-env.sh 中设置的环境变量不会在集群模式下反映在 YARN Application Master 进程中。有关更多信息，请参阅与 YARN 相关的 Spark属性。 Job Scheduling Within an Application 应用程序内的作业调度Within a given Spark Application, multiple parallel jobs can run simultaneously if they were submitted from separate threads. By job, in this section, we mean a Spark action and any tasks that need to run to evaluate that action. Spark’s scheduler is fully thread-safe and supports this use case to enable applications that serve multiple requests (e.g., queries for multiple users). By default, Spark’s scheduler runs jobs in FIFO fashion. If the jobs at the head of the queue don’t need to use the entire cluster, later jobs can begin to run right away, but if the jobs at the head of the queue are large, later jobs might be delayed significantly. 在给定的 Spark 应用程序中，如果从不同的的线程提交多个并行作业，则它们可以同时运行。按照作业，在本节中，我们指的是 Spark 操作以及需要运行以评估该操作的任何任务。 Spark 的调度程序是完全线程安全的，并支持此用户案例以支持提供多个请求的应用程序（例如，为多个用户进行查询）。默认情况下，Spark 的调度程序以FIFO方式运行作业。如果队列头部的作业不需要使用整个集群，则以后的作业可以立即开始运行，但如果队列头部的作业很大，则后续作业可能会显着延迟。 It is also possible to configure fair sharing between jobs. Under fair sharing, Spark assigns tasks between jobs in a round-robin fashion so that all jobs get a roughly equal share of cluster resources. This means that short jobs submitted while a long job is running can begin receiving resources right away and still achieve good response times without waiting for the long job to finish. This mode is best for multiuser settings. 也可以在作业之间配置公平共享。在公平共享下，Spark以循环方式在作业之间分配任务，以便所有作业获得大致相等的集群资源份额。这意味着当长期运行的工作正在执行时所提交的短期工作可以立即开始接收资源，并且仍然可以实现良好的响应时间，而无需等待长时间的工作完成。此模式最适合多用户设置。 To enable the fair scheduler, set the spark.scheduler.mode property to FAIR when configuring a SparkContext. 要启用公平调度器，请在配置 SparkContext 时将 spark.scheduler.mode 属性设置为 FAIR。 123val conf = new SparkConf().setMaster(...).setAppName(...)conf.set("spark.scheduler.mode", "FAIR")val sc = new SparkContext(conf) The fair scheduler also supports grouping jobs into pools, and setting different scheduling options, or weights, for each pool. This can be useful to create a high-priority pool for more important jobs or to group the jobs of each user together and give users equal shares regardless of how many concurrent jobs they have instead of giving jobs equal shares. This approach is modeled after the Hadoop Fair Scheduler. 公平调度器还支持将作业分组到池中，并为每个池设置不同的调度选项或权重。这对于为更重要的作业创建高优先级池或将每个用户的作业组合在一起并为用户提供相同的份额非常有用，无论他们有多少并发作业而不是给予作业相等的份额。此方法模拟Hadoop Fair Scheduler。 Without any intervention, newly submitted jobs go into a default pool, but jobs pools can be set by adding the spark.scheduler.pool local property to the SparkContext in the thread that’s submitting them. This is done as follows (assuming sc is your SparkContext )： 在没有任何干预的情况下，新提交的作业将进入默认池，但可以通过将 spark.scheduler.pool 本地属性添加到提交它们的线程中的 SparkContext 来设置作业池。这是完成如下（假设sc是你的 SparkContext )： 1sc.setLocalProperty("spark.scheduler.pool", "pool1") After setting this local property, all jobs submitted within this thread will use this pool name. The setting is per-thread to make it easy to have a thread run multiple jobs on behalf of the same user. If you’d like to clear the pool that a thread is associated with, set it to null. 设置此本地属性后，此线程中提交的所有作业都将使用此池名称。该设置是每个线程，以便让线程代表同一个用户运行多个作业变得容易。如果要清除与线程关联的池，请将其设置为null。 Conclusion 结论This chapter covered a lot about Spark Applications; we learned how to write, test, run, and configure them in all of Spark’s languages. In Chapter 17, we talk about deploying and the cluster management options you have when it comes to running Spark Applications. 本章介绍了 Spark 应用程序；我们学习了如何使用Spark的所有语言编写，测试，运行和配置它们。在第17章中，我们将讨论在运行 Spark 应用程序时的部署和集群管理选项。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 5 Basic Structured Operations]]></title>
    <url>%2F2019%2F08%2F05%2FChapter5_Basic-Structured-Operations(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 5 Basic Structured Operations 基本结构的操作 译者：https://snaildove.github.ioIn Chapter 4, we introduced the core abstractions of the Structured API. This chapter moves away from the architectural concepts and toward the tactical tools you will use to manipulate DataFrames and the data within them. This chapter focuses exclusively on fundamental DataFrame operations and avoids aggregations, window functions, and joins. These are discussed in subsequent chapters. Definitionally, a DataFrame consists of a series of records (like rows in a table), that are of type Row, and a number of columns (like columns in a spreadsheet) that represent a computation expression that can be performed on each individual record in the Dataset. Schemas define the name as well as the type of data in each column. Partitioning of the DataFrame defines the layout of the DataFrame or Dataset’s physical distribution across the cluster. The partitioning scheme defines how that is allocated. You can set this to be based on values in a certain column or nondeterministically. 在第4章中，我们介绍了结构化API的核心抽象。 本章从体系结构概念转向使用战术工具，您将使用这些工具来操纵DataFrame及其中的数据。 本章专门介绍DataFrame的基本操作，并避免聚合，窗口函数和连接。 这些将在后续章节中讨论。 从定义上讲，DataFrame由一系列记录（如表中的行）组成，这些记录的类型为Row，以及许多列（如电子表格中的列），他们可以表示成对Dataset中的每个单独记录执行的计算表达式。 模式定义每列中的名称和数据类型。 DataFrame的分区定义了整个集群中DataFrame或Dataset物理分布的布局。 分区方案定义了如何分配。 您可以将其设置为基于特定列中的值或不确定地。 Let’s create a DataFrame with which we can work: 让我们创建一个可以使用的DataFrame： 123// in Scalaval df = spark.read.format("json").load("/data/flight-data/json/2015-summary.json") 12# in Pythondf = spark.read.format("json").load("/data/flight-data/json/2015-summary.json") We discussed that a DataFame will have columns, and we use a schema to define them. Let’s take a look at the schema on our current DataFrame: 我们讨论了DataFame将具有列，并使用一种模式来定义它们。让我们看一下当前DataFrame上的模式： 1df.printSchema() Schemas tie everything together, so they’re worth belaboring. 模式将所有内容捆绑在一起，因此值得反复强调。 Schemas 模式A schema defines the column names and types of a DataFrame. We can either let a data source define the schema (called schema-on-read) or we can define it explicitly ourselves. 模式定义了DataFrame的列名和类型。我们可以让数据源定义模式（称为读取时的模式schema-on-read），也可以自己显示定义。 WARNING 警告Deciding whether you need to define a schema prior to reading in your data depends on your use case. For ad hoc analysis, schema-on-read usually works just fine (although at times it can be a bit slow with plain-text file formats like CSV or JSON). However, this can also lead to precision issues like a long type incorrectly set as an integer when reading in a file. When using Spark for production Extract, Transform, and Load (ETL), it is often a good idea to define your schemas manually, especially when working with untyped data sources like CSV and JSON because schema inference can vary depending on the type of data that you read in. 在读取数据之前决定是否需要定义模式取决于您的用例。对于临时分析，读取时的模式（schema-on-read）通常效果很好（尽管有时使用CSV或JSON等纯文本文件格式可能会有点慢）。但是，这也可能导致精度问题，例如在读取文件时将long类型错误地设置为整数。当使用Spark进行生产提取，转换和加载（ETL）时，手动定义模式通常是一个好主意，尤其是在使用CSV和JSON等无类型数据源时，因为模式推断会根据你所读取的数据类型的不同而有所不同。 Let’s begin with a simple file, which we saw in Chapter 4, and let the semi-structured nature of line delimited JSON define the structure. This is flight data from the United States Bureau of Transportation statistics: 让我们从第4章中看到的简单文件开始，让以行分隔JSON的半结构化性质定义结构。这是来自美国运输局统计数据的航班数据： 1234567// in Scalaspark.read.format("json").load("/data/flight-data/json/2015-summary.json").schemaScala returns the following:org.apache.spark.sql.types.StructType = ...StructType(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)) 123456# in Pythonspark.read.format("json").load("/data/flight-data/json/2015-summary.json").schemaPython returns the following:StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true))) A schema is a StructType made up of a number of fields, StructFields, that have a name, type, a Boolean flag which specifies whether that column can contain missing or null values, and, finally, users can optionally specify associated metadata with that column. The metadata is a way of storing information about this column (Spark uses this in its machine learning library). 模式是一种 StructType，由许多字段，StructFields 组成，这些字段具有名称，类型，布尔值标志，用于指定该列可以包含缺失值还是null值，最后，用户可以选择指定与该列关联的元数据 。 元数据是一种存储有关此列的信息的方式（Spark在其机器学习库中使用此信息）。 Schemas can contain other StructTypes (Spark’s complex types). We will see this in Chapter 6 when we discuss working with complex types. If the types in the data (at runtime) do not match the schema, Spark will throw an error. The example that follows shows how to create and enforce a specific schema on a DataFrame. 模式可以包含其他 StructType（Spark的复杂类型）。当我们讨论使用复杂类型时，我们将在第6章中看到这一点。如果数据中的类型（在运行时）与模式不匹配，Spark将引发错误。下面的示例演示如何在DataFrame上创建和实施特定的模式。 123456789101112// in Scalaimport org.apache.spark.sql.types.&#123;StructField, StructType, StringType, LongType&#125;import org.apache.spark.sql.types.Metadataval myManualSchema = StructType(Array(StructField("DEST_COUNTRY_NAME", StringType, true),StructField("ORIGIN_COUNTRY_NAME", StringType, true),StructField("count", LongType, false,Metadata.fromJson("&#123;"hello":"world"&#125;"))))val df = spark.read.format("json").schema(myManualSchema).load("/data/flight-data/json/2015-summary.json") Here’s how to do the same in Python： 在Python中执行以下操作的方法如下： 12345678910# in Pythonfrom pyspark.sql.types import StructField, StructType, StringType, LongTypemyManualSchema = StructType([StructField("DEST_COUNTRY_NAME", StringType(), True),StructField("ORIGIN_COUNTRY_NAME", StringType(), True),StructField("count", LongType(), False, metadata=&#123;"hello":"world"&#125;)])df = spark.read.format("json").schema(myManualSchema)\.load("/data/flight-data/json/2015-summary.json") As discussed in Chapter 4, we cannot simply set types via the per-language types because Spark maintains its own type information. Let’s now discuss what schemas define: columns. 如第4章所述，我们不能简单地通过每种语言类型设置类型，因为Spark维护自己的类型信息。现在让我们讨论一下模式定义：列。 Columns and ExpressionsColumns in Spark are similar to columns in a spreadsheet, R dataframe, or pandas DataFrame. You can select, manipulate, and remove columns from DataFrames and these operations are represented as expressions. Spark中的列与电子表格，R dataframe 或 pandas DataFrame 中的列相似。您可以从DataFrames中选择，操作和删除列，并且这些操作表示为表达式。 To Spark, columns are logical constructions that simply represent a value computed on a per-record basis by means of an expression. This means that to have a real value for a column, we need to have a row; and to have a row, we need to have a DataFrame. You cannot manipulate an individual column outside the context of a DataFrame; you must use Spark transformations within a DataFrame to modify the contents of a column. 对Spark而言，列是逻辑构造，仅表示通过表达式基于每个记录计算的值。这意味着要有一个列的实际值，我们需要有一行；要有一行，我们需要有一个DataFrame。您不能在DataFrame上下文之外操作单个列；您必须在DataFrame中使用Spark转换来修改列的内容。 1234// in Scalaimport org.apache.spark.sql.functions.&#123;col, column&#125;col("someColumnName")column("someColumnName") 1234# in Pythonfrom pyspark.sql.functions import col, columncol("someColumnName")column("someColumnName") We will stick to using col throughout this book. As mentioned, this column might or might not exist in our DataFrames. Columns are not resolved until we compare the column names with those we are maintaining in the catalog. Column and table resolution happens in the analyzer phase, as discussed in Chapter 4. 在本书中，我们将坚持使用col。如前所述，此列可能存在也可能不存在于我们的DataFrame中。在将列名与目录中维护的列名进行比较之前，不会解析列。列和表的解析发生在分析器阶段，如第4章所述。 NOTE 注意We just mentioned two different ways of referring to columns. Scala has some unique language features that allow for more shorthand ways of referring to columns. The following bits of syntactic sugar perform the exact same thing, namely creating a column, but provide no performance improvement: 我们刚刚提到了两种不同的列引用方式。 Scala具有一些独特的语言功能，允许使用更简便的方式引用列。语法糖的以下几部分执行的操作完全相同，即创建一个列，但没有改善性能： 123// in Scala$"myColumn"'myColumn The allows us to designate a string as a special string that should refer to an expression. The tick mark (‘) is a special thing called a symbol; this is a Scala-specific construct of referring to some identifier. They both perform the same thing and are shorthand ways of referring to columns by name. You’ll likely see all of the aforementioned references when you read different people’s Spark code. We leave it to you to use whatever is most comfortable and maintainable for you and those with whom you work. 允许我们将字符串指定为应引用表达式的特殊字符串。记号（’）是一种特殊的东西，称为符号；这是引用某些标识符的Scala专门构建的一种方式。它们都执行相同的操作，并且是通过名称引用列的简便方法。阅读其他人的Spark代码时，您可能会看到所有上述参考。我们留给您，使用对您和与您一起工作的人来说最舒适和可维护的任何一种方式。 Explicit column references 显示的列引用If you need to refer to a specific DataFrame’s column, you can use the col method on the specific DataFrame. This can be useful when you are performing a join and need to refer to a specific column in one DataFrame that might share a name with another column in the joined DataFrame. We will see this in Chapter 8. As an added benefit, Spark does not need to resolve this column itself (during the analyzer phase) because we did that for Spark: 如果您需要引用特定DataFrame的列，则可以在特定DataFrame上使用 col 方法。当您执行连接并需要引用一个DataFrame中的特定列，而该列可能与连接的DataFrame中的另一列共享名称时，此功能很有用。我们将在第8章中看到这一点。作为一个额外的好处，Spark不需要自己解决此列（在分析器阶段），因为我们为Spark做到了： 1df.col("count") Expressions 表达式We mentioned earlier that columns are expressions, but what is an expression? An expression is a set of transformations on one or more values in a record in a DataFrame. Think of it like a function that takes as input one or more column names, resolves them, and then potentially applies more expressions to create a single value for each record in the dataset. Importantly, this “single value” can actually be a complex type like a Map or Array. We’ll see more of the complex types in Chapter 6. In the simplest case, an expression, created via the expr function, is just a DataFrame column reference. In the simplest case, expr(&quot;someCol&quot;) is equivalent to col(&quot;someCol&quot;). 前面我们提到列是表达式，但是什么是表达式？表达式是对DataFrame中记录中一个或多个值的一组转换。可以将其视为一个函数，该函数将一个或多个列名作为输入，进行解析，然后可能应用更多表达式为数据集中的每个记录创建单个值。重要的是，此“单个值”实际上可以是诸如Map或Array之类的复杂类型。我们将在第6章中看到更多复杂类型。在最简单的情况下，通过expr函数创建的表达式仅是DataFrame列引用。在最简单的情况下，expr(&quot;someCol&quot;)等同于col(&quot;someCol&quot;)。 Columns as expressions 列作为表达式Columns provide a subset of expression functionality. If you use col() and want to perform transformations on that column, you must perform those on that column reference. When using an expression, the expr function can actually parse transformations and column references from a string and can subsequently be passed into further transformations. Let’s look at some examples. expr(&quot;someCol - 5&quot;) is the same transformation as performing col(&quot;someCol&quot;) - 5, or even expr(&quot;someCol&quot;) - 5. 列提供了表达式功能的子集。如果使用 col() 并想在该列上执行转换，则必须在该列引用上执行那些转换。使用表达式时，expr函数实际上可以解析字符串中的转换和列引用，并且可以随后将其传递到其他转换中。让我们看一些例子。expr(&quot;someCol - 5&quot;) 是与执行 col(&quot;someCol&quot;) - 5 相同的转换，或者甚至与：expr(&quot;someCol&quot;) - 5 相同。 That’s because Spark compiles these to a logical tree specifying the order of operations. This might be a bit confusing at first, but remember a couple of key points: 这是因为Spark将这些内容编译为指定操作顺序的逻辑树。刚开始时这可能有点令人困惑，但请记住几个要点： Columns are just expressions. 列只是表达式。 Columns and transformations of those columns compile to the same logical plan as parsed expressions. 列和这些列的转换编译为与经过解析的表达式拥有相同的逻辑计划。 Let’s ground this with an example: 让我们以一个示例为基础： 1(((col("someCol") + 5) * 200) - 6) &lt; col("otherCol") Figure 5-1 shows an overview of that logical tree. 图 5-1 展示一个逻辑树的整体概述。 This might look familiar because it’s a directed acyclic graph. This graph is represented equivalently by the following code: 这可能看起来很熟悉，因为它是有向无环图。此图由以下代码等效表示： 123// in Scalaimport org.apache.spark.sql.functions.exprexpr("(((someCol + 5) * 200) - 6) &lt; otherCol") 123# in Pythonfrom pyspark.sql.functions import exprexpr("(((someCol + 5) * 200) - 6) &lt; otherCol") This is an extremely important point to reinforce. Notice how the previous expression is actually valid SQL code, as well, just like you might put in a SELECT statement? That’s because this SQL expression and the previous DataFrame code compile to the same underlying logical tree prior to execution. This means that you can write your expressions as DataFrame code or as SQL expressions and get the exact same performance characteristics. This is discussed in Chapter 4. 这是必须加强的极为重要的一点。注意前面的表达式实际上是多么有效的SQL代码，就像您可能在SELECT语句中一样吗？这是因为该SQL表达式和之前的DataFrame代码在执行之前会编译为相同的基础逻辑树。这意味着您可以将表达式编写为DataFrame代码或SQL表达式，并获得完全相同的性能特性。这将在第4章中讨论。 Accessing a DataFrame’s columns 访问DataFrame的列Sometimes, you’ll need to see a DataFrame’s columns, which you can do by using something like printSchema; however, if you want to programmatically access columns, you can use the columns property to see all columns on a DataFrame: 有时，您需要查看DataFrame的列，您可以使用诸如printSchema之类的方法来完成；但是，如果要以编程方式访问列，则可以使用columns属性查看DataFrame上的所有列： 12spark.read.format("json").load("/data/flight-data/json/2015-summary.json").columns Records and Rows 记录和行In Spark, each row in a DataFrame is a single record. Spark represents this record as an object of type Row. Spark manipulates Row objects using column expressions in order to produce usable values. Row objects internally represent arrays of bytes. The byte array interface is never shown to users because we only use column expressions to manipulate them. 在Spark中，DataFrame中的每一行都是一条记录。 Spark将此记录表示为Row类型的对象。 Spark使用列表达式操纵Row对象，以产生可用的值。 行对象在内部表示字节数组。 字节数组接口从未显示给用户，因为我们仅使用列表达式来操作它们。 You’ll notice commands that return individual rows to the driver will always return one or more Row types when we are working with DataFrames. 您会注意到，当我们使用DataFrames时，将单个行返回给驱动程序的命令将始终返回一种或多种行类型。 NOTE 注意We use lowercase “row” and “record” interchangeably in this chapter, with a focus on the latter. A capitalized Row refers to the Row object. 在本章中，我们将小写的“行”和“记录”互换使用，重点是后者。大写的行是指Row对象。 Let’s see a row by calling first on our DataFrame: 让我们先在DataFrame上调用以下行： 1df.first() Creating Rows 创建行You can create rows by manually instantiating a Row object with the values that belong in each column. It’s important to note that only DataFrames have schemas. Rows themselves do not have schemas. This means that if you create a Row manually, you must specify the values in the same order as the schema of the DataFrame to which they might be appended (we will see this when we discuss creating DataFrames): 您可以通过手动实例化具有每个列中的值的Row对象来创建行。 请务必注意，只有 DataFrame 具有模式。 行本身没有模式。 这意味着，如果您手动创建Row，则必须以与可能被附加的DataFrame模式相同的顺序指定值（在讨论创建DataFrame时将看到此值）： 123// in Scalaimport org.apache.spark.sql.Rowval myRow = Row("Hello", null, 1, false) 123# in Pythonfrom pyspark.sql import RowmyRow = Row("Hello", None, 1, False) Accessing data in rows is equally as easy: you just specify the position that you would like. In Scala or Java, you must either use the helper methods or explicitly coerce the values. However, in Python or R, the value will automatically be coerced into the correct type : 访问行中的数据同样容易：只需指定所需的位置即可。 在Scala或Java中，必须使用辅助方法或显式强制值。 但是，在Python或R中，该值将自动强制为正确的类型： 12345// in ScalamyRow(0) // type AnymyRow(0).asInstanceOf[String] // StringmyRow.getString(0) // StringmyRow.getInt(2) // Int 123# in PythonmyRow[0]myRow[2] You can also explicitly return a set of Data in the corresponding Java Virtual Machine (JVM) objects by using the Dataset APIs. This is covered in Chapter 11. 您还可以使用Dataset API在相应的Java虚拟机（JVM）对象中显式返回一组数据。 这将在第11章中介绍。 DataFrame Transformations DataFrame转换Now that we briefly defined the core parts of a DataFrame, we will move onto manipulating DataFrames. When working with individual DataFrames there are some fundamental objectives. These break down into several core operations, as depicted in Figure 5-2: 现在，我们简要定义了DataFrame的核心部分，我们将继续操作DataFrame。使用单个DataFrame时，有一些基本目标。这些细分为几个核心操作，如图5-2所示： We can add rows or columns我们可以添加行或列 We can remove rows or columns我们可以删除行或列 We can transform a row into a column (or vice versa)我们可以将一行转换成一列（反之亦然） We can change the order of rows based on the values in columns我们可以根据列中的值更改行的顺序 Luckily, we can translate all of these into simple transformations, the most common being those that take one column, change it row by row, and then return our results. 幸运的是，我们可以将所有这些转换为简单的转换，最常见的转换是采用一列，逐行更改然后返回结果的转换。 Creating DataFrames 创建数据框As we saw previously, we can create DataFrames from raw data sources. This is covered extensively in Chapter 9; however, we will use them now to create an example DataFrame (for illustration purposes later in this chapter, we will also register this as a temporary view so that we can query it with SQL and show off basic transformations in SQL, as well) : 如前所述，我们可以从原始数据源创建DataFrame。第9章对此进行了广泛讨论。但是，我们现在将使用它们创建一个示例DataFrame（出于本章稍后的说明目的，我们还将其注册为一个临时视图，以便我们可以使用SQL查询它，并展示SQL中的基本转换）: 1234// in Scalaval df = spark.read.format("json").load("/data/flight-data/json/2015-summary.json")df.createOrReplaceTempView("dfTable") 123# in Pythondf = spark.read.format("json").load("/data/flight-data/json/2015-summary.json")df.createOrReplaceTempView("dfTable") We can also create DataFrames on the fly by taking a set of rows and converting them to a DataFrame. 我们还可以通过获取一组行并将其转换为 DataFrame 来动态（程序运行时）创建 DataFrame。 1234567891011// in Scalaimport org.apache.spark.sql.Rowimport org.apache.spark.sql.types.&#123;StructField, StructType, StringType, LongType&#125;val myManualSchema = new StructType(Array(new StructField("some", StringType, true),new StructField("col", StringType, true),new StructField("names", LongType, false)))val myRows = Seq(Row("Hello", null, 1L))val myRDD = spark.sparkContext.parallelize(myRows)val myDf = spark.createDataFrame(myRDD, myManualSchema)myDf.show() NOTE 注意In Scala, we can also take advantage of Spark’s implicits in the console (and if you import them in your JAR code) by running toDF on a Seq type. This does not play well with null types, so it’s not necessarily recommended for production use cases. 在 Scala 中，我们还可以通过在 Seq 类型上运行 toDF 来利用控制台中 Spark 的隐式内容（如果您将其导入JAR代码中）。 此方法不适用于null类型，因此不一定建议在生产用例中使用。 12// in Scalaval myDF = Seq(("Hello", 2, 1L)).toDF("col1", "col2", "col3") 12345678910111213# in Pythonfrom pyspark.sql import Rowfrom pyspark.sql.types import StructField, StructType, StringType, LongTypemyManualSchema = StructType([StructField("some", StringType(), True),StructField("col", StringType(), True),StructField("names", LongType(), False)])myRow = Row("Hello", None, 1)myDf = spark.createDataFrame([myRow], myManualSchema)myDf.show() Giving an output of: 提供以下输出： 12345+-----+----+-----+| some| col|names|+-----+----+-----+|Hello|null| 1 |+-----+----+-----+ Now that you know how to create DataFrames, let’s take a look at their most useful methods that you’re going to be using: the select method when you’re working with columns or expressions, and the selectExpr method when you’re working with expressions in strings. Naturally some transformations are not specified as methods on columns; therefore, there exists a group of functions found in the org.apache.spark.sql.functions package. 现在您已经知道如何创建 DataFrames，下面让我们看一下它们将要使用的最有用的方法：使用列或表达式时的select 方法，以及当你处理在字符串中的表达式的时候用 selectExpr 方法。自然，某些转换未指定为列上的方法；因此，在org.apache.spark.sql.functions 包中可以找到一组函数。 With these three tools, you should be able to solve the vast majority of transformation challenges that you might encounter in DataFrames. 使用这三个工具，您应该能够解决DataFrame中可能遇到的绝大多数转换问题。 select and selectExprselect and selectExpr allow you to do the DataFrame equivalent of SQL queries on a table of data: select 和 selectExpr 允许您与数据表上执行SQL查询等效的 DataFrame 操作： 1234-- in SQLSELECT * FROM dataFrameTableSELECT columnName FROM dataFrameTableSELECT columnName * 10, otherColumn, someOtherCol as c FROM dataFrameTable In the simplest possible terms, you can use them to manipulate columns in your DataFrames. Let’s walk through some examples on DataFrames to talk about some of the different ways of approaching this problem. The easiest way is just to use the select method and pass in the column names as strings with which you would like to work : 用最简单的术语来说，您可以使用它们来操作 DataFrame 中的列。 让我们来看一下 DataFrame 上的一些示例，以讨论解决此问题的一些不同方法。 最简单的方法是使用 select 方法并将列名作为您要使用的字符串传递： 12// in Scaladf.select("DEST_COUNTRY_NAME").show(2) 12# in Pythondf.select("DEST_COUNTRY_NAME").show(2) 12-- in SQLSELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2 Giving an output of : 提供以下输出： 123456+-----------------+|DEST_COUNTRY_NAME|+-----------------+| United States || United States |+-----------------+ You can select multiple columns by using the same style of query, just add more column name strings to your select method call: 您可以使用相同的查询样式选择多个列，只需将更多列名称字符串添加到 select 方法调用中即可： 12// in Scaladf.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME").show(2) 12# in Pythondf.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME").show(2) 12-- in SQLSELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME FROM dfTable LIMIT 2 Giving an output of: 提供以下输出： 123456+-----------------+-------------------+|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|+-----------------+-------------------+| United States | Romania || United States | Croatia |+-----------------+-------------------+ As discussed in “Columns and Expressions”, you can refer to columns in a number of different ways; all you need to keep in mind is that you can use them interchangeably: 如“列和表达式”中所述，您可以通过多种不同的方式引用列。 您需要记住的是可以互换使用它们： 1234567891011// in Scalaimport org.apache.spark.sql.functions.&#123;expr, col, column&#125;df.select(df.col("DEST_COUNTRY_NAME"),col("DEST_COUNTRY_NAME"),column("DEST_COUNTRY_NAME"),'DEST_COUNTRY_NAME,$"DEST_COUNTRY_NAME",expr("DEST_COUNTRY_NAME")).show(2) 12345678# in Pythonfrom pyspark.sql.functions import expr, col, columndf.select(expr("DEST_COUNTRY_NAME"),col("DEST_COUNTRY_NAME"),column("DEST_COUNTRY_NAME")).show(2) One common error is attempting to mix Column objects and strings. For example, the following code will result in a compiler error: 一种常见的错误是尝试混合Column对象和字符串。 例如，以下代码将导致编译器错误： 1df.select(col("DEST_COUNTRY_NAME"), "DEST_COUNTRY_NAME")) As we’ve seen thus far, expr is the most flexible reference that we can use. It can refer to a plain column or a string manipulation of a column. To illustrate, let’s change the column name, and then change it back by using the AS keyword and then the alias method on the column: 到目前为止，我们已经看到，expr是我们可以使用的最灵活的引用。它可以引用普通列或列的字符串操作。为了说明这一点，让我们更改列名，然后使用AS关键字然后在该列上使用alias方法将其更改回： 12// in Scaladf.select(expr("DEST_COUNTRY_NAME AS destination")).show(2) 12# in Pythondf.select(expr("DEST_COUNTRY_NAME AS destination")).show(2) 12-- in SQLSELECT DEST_COUNTRY_NAME as destination FROM dfTable LIMIT 2 This changes the column name to “destination.” You can further manipulate the result of your expression as another expression: 这会将列名更改为“ destination”。您可以进一步将表达式的结果作为另一个表达式来处理： 123// in Scaladf.select(expr("DEST_COUNTRY_NAME as destination").alias("DEST_COUNTRY_NAME")).show(2) 123# in Pythondf.select(expr("DEST_COUNTRY_NAME as destination").alias("DEST_COUNTRY_NAME"))\.show(2) The preceding operation changes the column name back to its original name. Because select followed by a series of expr is such a common pattern, Spark has a shorthand for doing this efficiently: selectExpr. This is probably the most convenient interface for everyday use: 前面的操作将列名称更改回其原始名称。因为select后跟一系列的expr是一种常见的模式，所以Spark有一个有效执行此操作的简写：selectExpr。这可能是日常使用中最方便的界面： 12// in Scaladf.selectExpr("DEST_COUNTRY_NAME as newColumnName", "DEST_COUNTRY_NAME").show(2) 12# in Pythondf.selectExpr("DEST_COUNTRY_NAME as newColumnName", "DEST_COUNTRY_NAME").show(2) This opens up the true power of Spark. We can treat selectExpr as a simple way to build up complex expressions that create new DataFrames. In fact, we can add any valid non-aggregating SQL statement, and as long as the columns resolve, it will be valid! Here’s a simple example that adds a new column withinCountry to our DataFrame that specifies whether the destination and origin are the same: 这打开了Spark的真正力量。我们可以将 selectExpr 视为构建可创建新DataFrame的复杂表达式的简单方法。实际上，我们可以添加任何有效的非聚合SQL语句，并且只要这些列能够解析，它就会有效！这是一个简单的示例，在Country中向我们的DataFrame添加了一个新列，用于指定目的地和起点是否相同： 12345// in Scaladf.selectExpr("*", // include all original columns"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry").show(2) 12345# in Pythondf.selectExpr("*", # all original columns"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry").show(2) 12-- in SQLSELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry FROM dfTable LIMIT 2 Giving an output of: 提供以下输出： 123456+-----------------+-------------------+-----+-------------+|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|+-----------------+-------------------+-----+-------------+| United States | Romania | 15 | false || United States | Croatia | 1 | false |+-----------------+-------------------+-----+-------------+ With select expression, we can also specify aggregations over the entire DataFrame by taking advantage of the functions that we have. These look just like what we have been showing so far: 使用select表达式，我们还可以利用我们拥有的函数在整个DataFrame上指定聚合，这些看起来就像我们到目前为止所展示的： 12// in Scaladf.selectExpr("avg(count)", "count(distinct(DEST_COUNTRY_NAME))").show(2) 12# in Pythondf.selectExpr("avg(count)", "count(distinct(DEST_COUNTRY_NAME))").show(2) 12-- in SQLSELECT avg(count), count(distinct(DEST_COUNTRY_NAME)) FROM dfTable LIMIT 2 Giving an output of : 提供以下输出： 12345+-----------+---------------------------------+| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|+-----------+---------------------------------+|1770.765625| 132 |+-----------+---------------------------------+ Converting to Spark Types (Literals) 转换为Spark类型（字面量）Sometimes, we need to pass explicit values into Spark that are just a value (rather than a new column). This might be a constant value or something we’ll need to compare to later on. The way we do this is through literals. This is basically a translation from a given programming language’s literal value to one that Spark understands. Literals are expressions and you can use them in the same way: 有时，我们需要将仅作为值（而不是新列）的显式值传递给Spark。这可能是一个恒定值，或者是以后我们需要比较的值。我们这样做的方法是通过文字。这基本上是从给定编程语言的字面值到Spark可以理解的一种转换。文字是表达式，您可以按照相同的方式使用它们： 123// in Scalaimport org.apache.spark.sql.functions.litdf.select(expr("*"), lit(1).as("One")).show(2) 123# in Pythonfrom pyspark.sql.functions import litdf.select(expr("*"), lit(1).alias("One")).show(2) In SQL, literals are just the specific value : 在SQL中，文字只是特定的值： 12-- in SQLSELECT *, 1 as One FROM dfTable LIMIT 2 Giving an output of: 提供以下输出： 123456+-----------------+-------------------+-----+---+|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|+-----------------+-------------------+-----+---+| United States | Romania | 15 | 1 || United States | Croatia | 1 | 1 |+-----------------+-------------------+-----+---+ This will come up when you might need to check whether a value is greater than some constant or other programmatically created variable. 当您可能需要检查某个值是否大于某个常量或其他以编程方式创建的变量时，就会出现这种情况。 Adding Columns 添加列There’s also a more formal way of adding a new column to a DataFrame, and that’s by using the withColumn method on our DataFrame. For example, let’s add a column that just adds the number one as a column: 还有一种更正式的方法，可以在DataFrame中添加新列，即使用DataFrame上的withColumn方法。例如，让我们添加一列，将数字一添加为一列： 12// in Scaladf.withColumn("numberOne", lit(1)).show(2) 12# in Pythondf.withColumn("numberOne", lit(1)).show(2) 12-- in SQLSELECT *, 1 as numberOne FROM dfTable LIMIT 2 Giving an output of : 提供以下输出： 123456+-----------------+-------------------+-----+---------+|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|+-----------------+-------------------+-----+---------+| United States | Romania | 15 | 1 || United States | Croatia | 1 | 1 |+-----------------+-------------------+-----+---------+ Let’s do something a bit more interesting and make it an actual expression. In the next example, we’ll set a Boolean flag for when the origin country is the same as the destination country: 让我们做一些更有趣的事情，并将其变为实际的表达方式。在下一个示例中，我们将为原籍国与目的地国相同时设置一个布尔值标志： 123// in Scaladf.withColumn("withinCountry", expr("ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME")).show(2) 123# in Pythondf.withColumn("withinCountry", expr("ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME"))\.show(2) Notice that the withColumn function takes two arguments: the column name and the expression that will create the value for that given row in the DataFrame. Interestingly, we can also rename a column this way. The SQL syntax is the same as we had previously, so we can omit it in this example: 请注意，withColumn函数带有两个参数：列名和将为DataFrame中给定行创建值的表达式。有趣的是，我们也可以用这种方式重命名列。 SQL语法与之前的语法相同，因此在此示例中可以省略它： 1df.withColumn("Destination", expr("DEST_COUNTRY_NAME")).columns Resulting in: 导致： 1... DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count, Destination Renaming Columns 重命名列Although we can rename a column in the manner that we just described, another alternative is to use the withColumnRenamed method. This will rename the column with the name of the string in the first argument to the string in the second argument: 尽管我们可以按照刚才描述的方式重命名列，但是另一种替代方法是使用 withColumnRenamed 方法。这会将第一个参数中的字符串名称重命名为第二个参数中的字符串名称： 12// in Scaladf.withColumnRenamed("DEST_COUNTRY_NAME", "dest").columns 12# in Pythondf.withColumnRenamed("DEST_COUNTRY_NAME", "dest").columns 1... dest, ORIGIN_COUNTRY_NAME, count Reserved Characters and Keywords 保留字符和关键字One thing that you might come across is reserved characters like spaces or dashes in column names. Handling these means escaping column names appropriately. In Spark, we do this by using backtick (`) characters. Let’s use withColumn, which you just learned about to create a column with reserved characters. We’ll show two examples—in the one shown here, we don’t need escape characters, but in the next one, we do: 您可能遇到的一件事是保留的字符，例如列名中的空格或破折号。处理这些意味着适当地转义了列名。在Spark中，我们通过使用反引号（`）字符来做到这一点。让我们使用 withColumn，您刚刚学习了如何创建带有保留字符的列。我们将显示两个示例，在这里显示的一个示例中，我们不需要转义符，但是在下一个示例中，我们这样做： 12345// in Scalaimport org.apache.spark.sql.functions.exprval dfWithLongColName = df.withColumn("This Long Column-Name",expr("ORIGIN_COUNTRY_NAME")) 12# in PythondfWithLongColName = df.withColumn("This Long Column-Name", expr("ORIGIN_COUNTRY_NAME")) We don’t need escape characters here because the first argument to withColumn is just a string for the new column name. In this example, however, we need to use backticks because we’re referencing a column in an expression: 我们在这里不需要转义字符，因为 withColumn 的第一个参数只是新列名称的字符串。 但是，在此示例中，我们需要使用反引号，因为我们要引用表达式中的列： 12345// in ScaladfWithLongColName.selectExpr("`This Long Column-Name`","`This Long Column-Name` as `new col`").show(2) 123456# in PythondfWithLongColName.selectExpr("`This Long Column-Name`","`This Long Column-Name` as `new col`")\.show(2)dfWithLongColName.createOrReplaceTempView("dfTableLong") 123-- in SQLSELECT This Long Column-Name, This Long Column-Name as new colFROM dfTableLong LIMIT 2 We can refer to columns with reserved characters (and not escape them) if we’re doing an explicit string-to-column reference, which is interpreted as a literal instead of an expression. We only need to escape expressions that use reserved characters or keywords. The following two examples both result in the same DataFrame: 如果我们正在执行显式的字符串到列引用，则可以引用带有保留字符的列（而不是对它们进行转义），该引用被解释为文字而不是表达式。 我们只需要转义使用保留字符或关键字的表达式。 以下两个示例均导致相同的DataFrame： 12// in ScaladfWithLongColName.select(col("This Long Column-Name")).columns 12# in PythondfWithLongColName.select(expr("`This Long Column-Name`")).columns Case Sensitivity 区分大小写By default Spark is case insensitive; however, you can make Spark case sensitive by setting the configuration: 默认情况下，Spark不区分大小写；但是，可以通过设置配置使Spark区分大小写： 12-- in SQLset spark.sql.caseSensitive true Removing Columns 删除列Now that we’ve created this column, let’s take a look at how we can remove columns from DataFrames. You likely already noticed that we can do this by using select. However, there is also a dedicated method called drop: 现在我们已经创建了此列，让我们看一下如何从DataFrames中删除列。您可能已经注意到我们可以通过使用select来做到这一点。但是，还有一个专用的方法称为drop： 1df.drop("ORIGIN_COUNTRY_NAME").columns We can drop multiple columns by passing in multiple columns as arguments: 我们可以通过传递多个列作为参数来删除多个列： 1dfWithLongColName.drop("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME") Changing a Column’s Type (cast) 更改列的类型（广播）Sometimes, we might need to convert from one type to another; for example, if we have a set of StringType that should be integers. We can convert columns from one type to another by casting the column from one type to another. For instance, let’s convert our count column from an integer to a type Long: 有时，我们可能需要从一种类型转换为另一种类型。例如，如果我们有一组应该为整数的StringType。通过将列从一种类型转换为另一种类型，我们可以将列从一种类型转换为另一种类型。例如，让我们将count列从整数转换为Long类型： 1df.withColumn("count2", col("count").cast("long")) 12-- in SQLSELECT *, cast(count as long) AS count2 FROM dfTable Filtering RowsTo filter rows, we create an expression that evaluates to true or false. You then filter out the rows with an expression that is equal to false. The most common way to do this with DataFrames is to create either an expression as a String or build an expression by using a set of column manipulations. There are two methods to perform this operation: you can use where or filter and they both will perform the same operation and accept the same argument types when used with DataFrames. We will stick to where because of its familiarity to SQL; however, filter is valid as well. 为了过滤行，我们创建一个表达式，其结果为true或false。 然后，使用等于false的表达式过滤掉行。 使用DataFrames执行此操作的最常见方法是将表达式创建为String或通过使用一组列操作来构建表达式。 有两种方法可以执行此操作：您可以使用where或filter，它们与DataFrames一起使用时将执行相同的操作并接受相同的参数类型。 由于对SQL的熟悉，我们将坚持到底。 但是，过滤器也有效。 NOTE 注意When using the Dataset API from either Scala or Java, filter also accepts an arbitrary function that Spark will apply to each record in the Dataset. See Chapter 11 for more information. 当从Scala或Java使用Dataset API时，filter还接受Spark应用于 Dataset 中每个记录的任意函数。有关更多信息，请参见第11章。 The following filters are equivalent, and the results are the same in Scala and Python: 以下过滤器是等效的，并且在Scala和Python中结果是相同的： 12df.filter(col("count") &lt; 2).show(2)df.where("count &lt; 2").show(2) 12-- in SQLSELECT * FROM dfTable WHERE count &lt; 2 LIMIT 2 Giving an output of: 给出以下输出： 123456+-----------------+-------------------+-----+|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|+-----------------+-------------------+-----+| United States | Croatia | 1 || United States | Singapore | 1 |+-----------------+-------------------+-----+ Instinctually, you might want to put multiple filters into the same expression. Although this is possible, it is not always useful, because Spark automatically performs all filtering operations at the same time regardless of the filter ordering. This means that if you want to specify multiple AND filters, just chain them sequentially and let Spark handle the rest: 本能地，您可能希望将多个过滤器放入同一表达式中。 尽管这是可行的，但并不总是有用的，因为Spark会自动同时同时执行所有过滤操作，而不考虑过滤器的顺序。 这意味着，如果您要指定多个AND过滤器，只需按顺序将它们链接起来，然后让Spark处理其余的过滤器： 123// in Scaladf.where(col("count") &lt; 2).where(col("ORIGIN_COUNTRY_NAME") =!= "Croatia").show(2) 123# in Pythondf.where(col("count") &lt; 2).where(col("ORIGIN_COUNTRY_NAME") != "Croatia")\.show(2) 123-- in SQLSELECT * FROM dfTable WHERE count &lt; 2 AND ORIGIN_COUNTRY_NAME != "Croatia"LIMIT 2 Giving an output of: 给出以下输出： 123456+-----------------+-------------------+-----+|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|+-----------------+-------------------+-----+| United States | Singapore | 1 || Moldova | United States | 1 |+-----------------+-------------------+-----+ Getting Unique Rows 获取非重复的行A very common use case is to extract the unique or distinct values in a DataFrame. These values can be in one or more columns. The way we do this is by using the distinct method on a DataFrame, which allows us to deduplicate any rows that are in that DataFrame. For instance, let’s get the unique origins in our dataset. This, of course, is a transformation that will return a new DataFrame with only unique rows: 一个非常常见的用例是在DataFrame中提取唯一或不同的值。 这些值可以在一列或多列中。 我们这样做的方法是对DataFrame使用不同的方法，该方法使我们能够对DataFrame中的所有行进行重复数据删除。 例如，让我们获取数据集中的唯一来源。 当然，这是一个转换，将返回仅具有唯一行的新DataFrame： 12// in Scaladf.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME").distinct().count() 12# in Pythondf.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME").distinct().count() 12-- in SQLSELECT COUNT(DISTINCT(ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)) FROM dfTable Results in 256. 结果是256。 12// in Scaladf.select("ORIGIN_COUNTRY_NAME").distinct().count() 12# in Pythondf.select("ORIGIN_COUNTRY_NAME").distinct().count() 12-- in SQLSELECT COUNT(DISTINCT ORIGIN_COUNTRY_NAME) FROM dfTable Results in 125. 结果是125。 Random Samples 随机抽样Sometimes, you might just want to sample some random records from your DataFrame. You can do this by using the sample method on a DataFrame, which makes it possible for you to specify a fraction of rows to extract from a DataFrame and whether you’d like to sample with or without replacement: 有时，您可能只想从DataFrame中抽取一些随机记录。 您可以通过在DataFrame上使用sample方法来执行此操作，这使您可以指定要从DataFrame提取的行的一部分，以及是否要替换或不替换地进行采样： 1234val seed = 5val withReplacement = falseval fraction = 0.5df.sample(withReplacement, fraction, seed).count() 12345# in Pythonseed = 5withReplacement = Falsefraction = 0.5df.sample(withReplacement, fraction, seed).count() Giving an output of 126. 给出输出：126。 Random Splits 随机分割Random splits can be helpful when you need to break up your DataFrame into a random “splits” of the original DataFrame. This is often used with machine learning algorithms to create training, validation, and test sets. In this next example, we’ll split our DataFrame into two different DataFrames by setting the weights by which we will split the DataFrame (these are the arguments to the function). Because this method is designed to be randomized, we will also specify a seed (just replace seed with a number of your choosing in the code block). It’s important to note that if you don’t specify a proportion for each DataFrame that adds up to one, they will be normalized so that they do: 当您需要将DataFrame分解成原始DataFrame的随机“拆分”时，随机拆分会很有帮助。这通常与机器学习算法一起使用以创建训练，验证和测试集。在下一个示例中，我们将通过设置将DataFrame分割的权重（这些是函数的参数），将DataFrame分为两个不同的DataFrame。因为此方法是随机设计的，所以我们还将指定一个种子（只需在代码块中用您选择的数量替换种子）。重要的是要注意，如果您没有为每个总计为1的DataFrame指定比例，则将它们标准化，这样就可以了： 123// in Scalaval dataFrames = df.randomSplit(Array(0.25, 0.75), seed)dataFrames(0).count() &gt; dataFrames(1).count() // False 123# in PythondataFrames = df.randomSplit([0.25, 0.75], seed)dataFrames[0].count() &gt; dataFrames[1].count() # False Concatenating and Appending Rows (Union) 串联和附加行（联合）As you learned in the previous section, DataFrames are immutable. This means users cannot append to DataFrames because that would be changing it. To append to a DataFrame, you must union the original DataFrame along with the new DataFrame. This just concatenates the two DataFrames. To union two DataFrames, you must be sure that they have the same schema and number of columns; otherwise, the union will fail. 如上一节所述，DataFrame是不可变的。这意味着用户无法附加到DataFrame，因为这将对其进行更改。要附加到DataFrame，必须将原始DataFrame与新DataFrame合并在一起。这只是连接两个DataFrame。要合并两个DataFrame，必须确保它们具有相同的模式和列数。否则，联合将失败。 WARNING 警告Unions are currently performed based on location, not on the schema. This means that columns will not automatically line up the way you think they might. 当前，联合是基于位置而不是基于模式执行的。这意味着列将不会自动按照您认为的方式排列。 123456789101112// in Scalaimport org.apache.spark.sql.Rowval schema = df.schemaval newRows = Seq(Row("New Country", "Other Country", 5L),Row("New Country 2", "Other Country 3", 1L)) val parallelizedRows = spark.sparkContext.parallelize(newRows)val newDF = spark.createDataFrame(parallelizedRows, schema)df.union(newDF).where("count = 1").where($"ORIGIN_COUNTRY_NAME" =!= "United States").show() // get all of them and we'll see our new rows at the end 12345678910111213# in Pythonfrom pyspark.sql import Rowschema = df.schemanewRows = [Row("New Country", "Other Country", 5L),Row("New Country 2", "Other Country 3", 1L)] parallelizedRows = spark.sparkContext.parallelize(newRows)newDF = spark.createDataFrame(parallelizedRows, schema)df.union(newDF)\.where("count = 1")\.where(col("ORIGIN_COUNTRY_NAME") != "United States")\.show() In Scala, you must use the operator so that you don’t just compare the unevaluated column expression to a string but instead to the evaluated one: 在Scala中，您必须使用 =!= 运算符，以便您不只是将未求值的列表达式与字符串进行比较，而是与已求值的表达式进行比较： Giving the output of: 给出输出： 12345+-----------------+-------------------+-----+|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|+-----------------+-------------------+-----+| United States | Croatia | 1 |+-----------------+-------------------+-----+ As expected, you’ll need to use this new DataFrame reference in order to refer to the DataFrame with the newly appended rows. A common way to do this is to make the DataFrame into a view or register it as a table so that you can reference it more dynamically in your code. 如预期的那样，您将需要使用这个新的 DataFrame 引用，以引用带有新添加的行的 DataFrame。 一种常见的方法是将 DataFrame 放入视图或将其注册为表，以便您可以在代码中更动态地引用它。 Sorting Rows 排序行When we sort the values in a DataFrame, we always want to sort with either the largest or smallest values at the top of a DataFrame. There are two equivalent operations to do this sort and orderBy that work the exact same way. They accept both column expressions and strings as well as multiple columns. The default is to sort in ascending order: 当我们对一个DataFrame中的值进行排序时，我们总是希望使用DataFrame顶部的最大值或最小值进行排序。 有两种等效的操作可以执行完全相同的排序和 orderBy 操作。 它们接受列表达式和字符串以及多列。 默认是按升序排序： 1234// in Scaladf.sort("count").show(5)df.orderBy("count", "DEST_COUNTRY_NAME").show(5)df.orderBy(col("count"), col("DEST_COUNTRY_NAME")).show(5) 1234# in Pythondf.sort("count").show(5)df.orderBy("count", "DEST_COUNTRY_NAME").show(5)df.orderBy(col("count"), col("DEST_COUNTRY_NAME")).show(5) To more explicitly specify sort direction, you need to use the asc and desc functions if operating on a column. These allow you to specify the order in which a given column should be sorted: 要更明确地指定排序方向，如果对列进行操作，则需要使用 asc 和 desc 函数。 这些允许您指定给定列的排序顺序： 1234// in Scalaimport org.apache.spark.sql.functions.&#123;desc, asc&#125;df.orderBy(expr("count desc")).show(2)df.orderBy(desc("count"), asc("DEST_COUNTRY_NAME")).show(2) 1234# in Pythonfrom pyspark.sql.functions import desc, ascdf.orderBy(expr("count desc")).show(2)df.orderBy(col("count").desc(), col("DEST_COUNTRY_NAME").asc()).show(2) 12-- in SQLSELECT * FROM dfTable ORDER BY count DESC, DEST_COUNTRY_NAME ASC LIMIT 2 An advanced tip is to use asc_nulls_first, desc_nulls_first, asc_nulls_last, or desc_nulls_last to specify where you would like your null values to appear in an ordered DataFrame. 一个高级技巧是使用 asc_nulls_first，desc_nulls_first，asc_nulls_last 或 desc_nulls_last 指定您希望空值出现在有序DataFrame中的位置。 For optimization purposes, it’s sometimes advisable to sort within each partition before another set of transformations. You can use the sortWithinPartitions method to do this: 出于优化目的，有时建议在每个分区内进行另一组转换之前进行排序。 您可以使用 sortWithinPartitions 方法执行此操作： 123// in Scalaspark.read.format("json").load("/data/flight-data/json/*-summary.json").sortWithinPartitions("count") 123# in Pythonspark.read.format("json").load("/data/flight-data/json/*-summary.json")\.sortWithinPartitions("count") We will discuss this more when we look at tuning and optimization in Part III. 当我们在第三部分中讨论调优和优化时，我们将对此进行更多讨论。 Limit 限制Oftentimes, you might want to restrict what you extract from a DataFrame; for example, you might want just the top ten of some DataFrame. You can do this by using the limit method: 通常，您可能希望限制从DataFrame中提取的内容； 例如，您可能只需要某些DataFrame的前十名。 您可以通过使用limit方法来做到这一点： 12// in Scaladf.limit(5).show() 12# in Pythondf.limit(5).show() 12-- in SQLSELECT * FROM dfTable LIMIT 6 12// in Scaladf.orderBy(expr("count desc")).limit(6).show() 12# in Pythondf.orderBy(expr("count desc")).limit(6).show() 12-- in SQLSELECT * FROM dfTable ORDER BY count desc LIMIT 6 Repartition and Coalesce 分区与合并Another important optimization opportunity is to partition the data according to some frequently filtered columns, which control the physical layout of data across the cluster including the partitioning scheme and the number of partitions. Repartition will incur a full shuffle of the data, regardless of whether one is necessary. This means that you should typically only repartition when the future number of partitions is greater than your current number of partitions or when you are looking to partition by a set of columns: 另一个重要的优化机会是根据一些频繁过滤的列对数据进行分区，这些列控制整个群集中数据的物理布局，包括分区方案和分区数。 无论是否需要重新分区，重新分区都会导致数据的完全随机洗牌（shuffle）。 这意味着您通常仅应在将来的分区数大于当前的分区数时或在按一组列进行分区时重新分区： 12// in Scaladf.rdd.getNumPartitions // 1 12# in Pythondf.rdd.getNumPartitions() # 1 12// in Scaladf.repartition(5) 12# in Pythondf.repartition(5) If you know that you’re going to be filtering by a certain column often, it can be worth repartitioning based on that column: 如果您知道经常要按某个列进行过滤，则值得根据该列进行重新分区： 12// in Scaladf.repartition(col("DEST_COUNTRY_NAME")) 12# in Pythondf.repartition(col("DEST_COUNTRY_NAME")) You can optionally specify the number of partitions you would like, too: 您也可以选择指定所需的分区数： 12// in Scaladf.repartition(5, col("DEST_COUNTRY_NAME")) 12# in Pythondf.repartition(5, col("DEST_COUNTRY_NAME")) Coalesce, on the other hand, will not incur a full shuffle and will try to combine partitions. This operation will shuffle your data into five partitions based on the destination country name, and then coalesce them (without a full shuffle): 另一方面，合并将不会引起全量随机洗牌，并会尝试合并分区。 此操作将根据目标国家/地区名称将数据随机分为五个分区，然后将它们合并（不进行全量随机洗牌）： 12// in Scaladf.repartition(5, col("DEST_COUNTRY_NAME")).coalesce(2) 12# in Pythondf.repartition(5, col("DEST_COUNTRY_NAME")).coalesce(2) Collecting Rows to the Driver 将行收集到驱动程序中As discussed in previous chapters, Spark maintains the state of the cluster in the driver. There are times when you’ll want to collect some of your data to the driver in order to manipulate it on your local machine. 如前几章所述，Spark在驱动程序中维护集群的状态。有时候，您希望将一些数据收集到驱动程序以便在本地计算机上进行操作。 Thus far, we did not explicitly define this operation. However, we used several different methods for doing so that are effectively all the same. collect gets all data from the entire DataFrame, take selects the first N rows, and show prints out a number of rows nicely. 到目前为止，我们尚未明确定义此操作。但是，我们使用了几种不同的方法来进行操作，这些方法实际上都是相同的。 collect从整个DataFrame中获取所有数据，take选择前N行，然后show很好地打印出多行。 123456// in Scalaval collectDF = df.limit(10)collectDF.take(5) // take works with an Integer countcollectDF.show() // this prints it out nicelycollectDF.show(5, false)collectDF.collect() 123456# in PythoncollectDF = df.limit(10)collectDF.take(5) # take works with an Integer countcollectDF.show() # this prints it out nicelycollectDF.show(5, False)collectDF.collect() There’s an additional way of collecting rows to the driver in order to iterate over the entire dataset. The method toLocalIterator collects partitions to the driver as an iterator. This method allows you to iterate over the entire dataset partition-by-partition in a serial manner: 还有另一种收集行到驱动程序的方法，以便遍历整个数据集。方法 toLocalIterator 将分区作为迭代器收集到驱动程序。此方法允许您以串行方式逐分区遍历整个数据集： 1collectDF.toLocalIterator() WARNING 警告Any collection of data to the driver can be a very expensive operation! If you have a large dataset and call collect, you can crash the driver. If you use toLocalIterator and have very large partitions, you can easily crash the driver node and lose the state of your application. This is also expensive because we can operate on a one-by-one basis, instead of running computation in parallel. 向驱动程序收集任何数据都是非常昂贵的操作！如果您有一个很大的数据集并调用 collect，则可能会使驱动程序崩溃。如果使用 toLocalIterator 并具有很大的分区，则很容易使驱动程序节点崩溃并失去应用程序的状态。这也很昂贵，因为我们可以一对一地操作，而不是并行运行计算。 Conclusion 总结This chapter covered basic operations on DataFrames. You learned the simple concepts and tools that you will need to be successful with Spark DataFrames. Chapter 6 covers in much greater detail all of the different ways in which you can manipulate the data in those DataFrames. 本章介绍了DataFrame的基本操作。您了解了Spark DataFrame成功所需的简单概念和工具。第6章更加详细地介绍了可以在这些DataFrame中操作数据的所有不同方式。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 7. Aggregations]]></title>
    <url>%2F2019%2F08%2F05%2FChapter7_Aggregations(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 7 Aggregations 聚合 译者：https://snaildove.github.ioAggregating is the act of collecting something together and is a cornerstone of big data analytics. In an aggregation, you will specify a key or grouping and an aggregation function that specifies how you should transform one or more columns. This function must produce one result for each group, given multiple input values. Spark’s aggregation capabilities are sophisticated and mature, with a variety of different use cases and possibilities. In general, you use aggregations to summarize numerical data usually by means of some grouping. This might be a summation, a product, or simple counting. Also, with Spark you can aggregate any kind of value into an array, list, or map, as we will see in “Aggregating to Complex Types”. 聚合是将某物收集在一起的行为，是大数据分析的基石。在聚合中，您将指定一个键或分组以及一个聚合函数，该函数指定应如何转换一个或多个列。给定多个输入值，此函数必须为每个组产生一个结果。 Spark的聚合功能是复杂巧妙且成熟的，具有各种不同的用例和可能性。通常，通过分组使用聚合去汇总数值型数据。这可能是求和，乘积或简单的计数。另外，使用Spark可以将任何类型的值聚合到数组，列表或映射中，如我们在“聚合为复杂类型”中所见。 In addition to working with any type of values, Spark also allows us to create the following groupings types: 除了使用任何类型的值外，Sp​​ark还允许我们创建以下分组类型： The simplest grouping is to just summarize a complete DataFrame by performing an aggregation in a select statement.最简单的分组是通过在select语句中执行聚合来汇总一个完整的DataFrame。 A “group by” allows you to specify one or more keys as well as one or more aggregation functions to transform the value columns.“分组依据”允许您指定一个或多个键以及一个或多个聚合函数来转换值列。 A “window” gives you the ability to specify one or more keys as well as one or more aggregation functions to transform the value columns. However, the rows input to the function are somehow related to the current row.“窗口”使您能够指定一个或多个键以及一个或多个聚合函数来转换值列。但是，输入到函数的行以某种方式与当前行相关。 A “grouping set,” which you can use to aggregate at multiple different levels. Grouping sets are available as a primitive in SQL and via rollups and cubes in DataFrames.一个“分组集”，可用于在多个不同级别进行汇总。分组集可作为SQL中的原语以及通过DataFrames中的 rollup 和 cube 使用。 A “rollup” makes it possible for you to specify one or more keys as well as one or more aggregation functions to transform the value columns, which will be summarized hierarchically.“rollup”使您可以指定一个或多个键以及一个或多个聚合函数来转换值列，这些列将按层次进行汇总。 A “cube” allows you to specify one or more keys as well as one or more aggregation functions to transform the value columns, which will be summarized across all combinations of columns.“cube”允许您指定一个或多个键以及一个或多个聚合函数来转换值列，这些列将在所有列的组合中汇总。 Each grouping returns a RelationalGroupedDataset on which we specify our aggregations. 每个分组都返回一个 RelationalGroupedDataset，在上面我们指定聚合。 NOTE 注意An important thing to consider is how exact you need an answer to be. When performing calculations over big data, it can be quite expensive to get an exact answer to a question, and it’s often much cheaper to simply request an approximate to a reasonable degree of accuracy. You’ll note that we mention some approximation functions throughout the book and oftentimes this is a good opportunity to improve the speed and execution of your Spark jobs, especially for interactive and ad hoc analysis. 要考虑的重要事项是您需要答案的精确程度。 在对大数据进行计算时，获得问题的准确答案可能会非常昂贵，而简单地请求一个近似的准确度通常会便宜得多。 您会注意到，我们在整本书中都提到了一些近似函数，通常这是一个提高Spark作业的速度和执行速度的好机会，尤其是对于交互式和临时安排的分析而言。 Let’s begin by reading in our data on purchases, repartitioning the data to have far fewer partitions (because we know it’s a small volume of data stored in a lot of small files), and caching the results for rapid access: 首先，我们读取购买数据，将数据重新分区为更少的分区（因为我们知道存储在许多小文件中的数据量很小），并缓存结果以进行快速访问： 12345678// in Scalaval df = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/data/retail-data/all/*.csv").coalesce(5)df.cache()df.createOrReplaceTempView("dfTable") 12345678# in Pythondf = spark.read.format("csv")\.option("header", "true")\.option("inferSchema", "true")\.load("/data/retail-data/all/*.csv")\.coalesce(5)df.cache()df.createOrReplaceTempView("dfTable") Here’s a sample of the data so that you can reference the output of some of the functions: 这是数据示例，因此您可以引用某些函数的输出： 123456789+---------+---------+--------------------+--------+--------------+---------+-----|InvoiceNo|StockCode| Description |Quantity| InvoiceDate |UnitPrice|Cu...+---------+---------+--------------------+--------+--------------+---------+-----| 536365 | 85123A | WHITE HANGING... | 6 |12/1/2010 8:26| 2.55 | ...| 536365 | 71053 | WHITE METAL... | 6 |12/1/2010 8:26| 3.39 | ......| 536367 | 21755 |LOVE BUILDING BLO...| 3 |12/1/2010 8:34| 5.95 | ...| 536367 | 21777 |RECIPE BOX WITH M...| 4 |12/1/2010 8:34| 7.95 | ...+---------+---------+--------------------+--------+--------------+---------+----- As mentioned, basic aggregations apply to an entire DataFrame. The simplest example is the count method: 如前所述，基本聚合适用于整个DataFrame。 最简单的示例是count方法： 1df.count() == 541909 If you’ve been reading this book chapter by chapter, you know that count is actually an action as opposed to a transformation, and so it returns immediately. You can use count to get an idea of the total size of your dataset but another common pattern is to use it to cache an entire DataFrame in memory, just like we did in this example. 如果您已经逐章阅读过本书，那么您就会知道，计数实际上是一种动作（action），而不是一种转换（transformation），因此它会立即返回。您可以使用count来了解数据集的总大小，但是另一个常见的模式是使用它来将整个DataFrame缓存在内存中，就像我们在本示例中所做的那样。 Now, this method is a bit of an outlier because it exists as a method (in this case) as opposed to a function and is eagerly evaluated instead of a lazy transformation. In the next section, we will see count used as a lazy function, as well. 现在，此方法有点离群值，因为它作为一种方法存在（在这种情况下）而不是函数，并且急切地被求值而不是延迟转换。在下一节中，我们还将看到count也用作惰性函数。 Aggregation Functions 聚合函数All aggregations are available as functions, in addition to the special cases that can appear on DataFrames or via .stat , like we saw in Chapter 6. You can find most aggregation functions in the org.apache.spark.sql.functions package. 除了可以在DataFrames上或通过 .stat 出现的特殊情况（如我们在第6章中看到的）之外，所有聚合都可用作函数。您可以在 org.apache.spark.sql.functions 包中找到大多数聚合函数。 NOTE 注意There are some gaps between the available SQL functions and the functions that we can import in Scala and Python. This changes every release, so it’s impossible to include a definitive list. This section covers the most common functions. 可用的SQL函数与我们可以在Scala和Python中导入的函数之间存在一些差距。每个发行版都会更改，因此不可能包含确定的列表。本节介绍最常用的功能。 count 计数The first function worth going over is count, except in this example it will perform as a transformation instead of an action. In this case, we can do one of two things: specify a specific column to count, or all the columns by using count(*) or count(1) to represent that we want to count every row as the literal one, as shown in this example: 值得复习的第一个功能是计数，除了在此示例中，它将作为转换而不是动作执行。 在这种情况下，我们可以执行以下两项操作之一：指定要计数的特定列，或者使用 count(*) 或 count(1) 表示要将每一行都视为文字行来指定所有列，如图所示 在此示例中： 123// in Scalaimport org.apache.spark.sql.functions.countdf.select(count("StockCode")).show() // 541909 123# in Pythonfrom pyspark.sql.functions import countdf.select(count("StockCode")).show() # 541909 12-- in SQLSELECT COUNT(*) FROM dfTable NOTE 注意There are a number of gotchas when it comes to null values and counting. For instance, when performing a count(*), Spark will count null values (including rows containing all nulls). However, when counting an individual column, Spark will not count the null values. 当涉及到空值和计数时，有很多陷阱。 例如，当执行 count(*) 时，Spark将计算空值（包括包含所有空值的行）。 但是，当计算单个列时，Spark将不计算空值。 countDistinctSometimes, the total number is not relevant; rather, it’s the number of unique groups that you want. To get this number, you can use the countDistinct function. This is a bit more relevant for individual columns: 有时，总数并不重要； 而是您想要的不重复的组的数目。 要获得此数字，可以使用 countDistinct 函数。 这与各个列更相关： 123// in Scalaimport org.apache.spark.sql.functions.countDistinctdf.select(countDistinct("StockCode")).show() // 4070 123# in Pythonfrom pyspark.sql.functions import countDistinctdf.select(countDistinct("StockCode")).show() # 4070 12-- in SQLSELECT COUNT(DISTINCT *) FROM DFTABLE approx_count_distinctOften, we find ourselves working with large datasets and the exact distinct count is irrelevant. There are times when an approximation to a certain degree of accuracy will work just fine, and for that, you can use the approx_count_distinct function: 通常，我们发现自己正在处理大型数据集，而确切的不同数量无关紧要。 有时，达到某种程度的精确度就可以了，为此，您可以使用 approx_count_distinct 函数： 123// in Scalaimport org.apache.spark.sql.functions.approx_count_distinctdf.select(approx_count_distinct("StockCode", 0.1)).show() // 3364 123# in Pythonfrom pyspark.sql.functions import approx_count_distinctdf.select(approx_count_distinct("StockCode", 0.1)).show() # 3364 12-- in SQLSELECT approx_count_distinct(StockCode, 0.1) FROM DFTABLE You will notice that approx_count_distinct took another parameter with which you can specify the maximum estimation error allowed. In this case, we specified a rather large error and thus receive an answer that is quite far off but does complete more quickly than countDistinct. You will see much greater performance gains with larger datasets. 您会注意到，approx_count_distinct 使用了另一个参数，您可以使用该参数指定允许的最大估计误差。 在这种情况下，我们指定了一个相当大的错误，因此得到的答案相差很远，但是比 countDistinct 完成得更快。 使用更大的数据集，您将看到更大的性能提升。 first and lastYou can get the first and last values from a DataFrame by using these two obviously named functions. This will be based on the rows in the DataFrame, not on the values in the DataFrame: 您可以通过使用这两个明显的命名函数得到从 DataFrame 的第一和最后一个值。这将基于在 DataFrame 中的行，而不是在 DataFrame 的值： 123// in Scalaimport org.apache.spark.sql.functions.&#123;first, last&#125;df.select(first("StockCode"), last("StockCode")).show() 123# in Pythonfrom pyspark.sql.functions import first, lastdf.select(first("StockCode"), last("StockCode")).show() 12-- in SQLSELECT first(StockCode), last(StockCode) FROM dfTable 12345+-----------------------+----------------------+|first(StockCode, false)|last(StockCode, false)|+-----------------------+----------------------+| 85123A | 22138 |+-----------------------+----------------------+ min and maxTo extract the minimum and maximum values from a DataFrame, use the min and max functions: 要从DataFrame中提取最小值和最大值，请使用min和max函数： 123// in Scalaimport org.apache.spark.sql.functions.&#123;min, max&#125;df.select(min("Quantity"), max("Quantity")).show() 123# in Pythonfrom pyspark.sql.functions import min, maxdf.select(min("Quantity"), max("Quantity")).show() 12-- in SQLSELECT min(Quantity), max(Quantity) FROM dfTable 12345+-------------+-------------+|min(Quantity)|max(Quantity)|+-------------+-------------+| -80995 | 80995 |+-------------+-------------+ sumAnother simple task is to add all the values in a row using the sum function: 另一个简单的任务是使用sum函数将所有值连续添加： 123// in Scalaimport org.apache.spark.sql.functions.sumdf.select(sum("Quantity")).show() // 5176450 123# in Pythonfrom pyspark.sql.functions import sumdf.select(sum("Quantity")).show() # 5176450 12-- in SQLSELECT sum(Quantity) FROM dfTable sumDistinctIn addition to summing a total, you also can sum a distinct set of values by using the sumDistinct function: 除了求和外，还可以使用 sumDistinct 函数对一组不同的值求和： 123// in Scalaimport org.apache.spark.sql.functions.sumDistinctdf.select(sumDistinct("Quantity")).show() // 29310 123# in Pythonfrom pyspark.sql.functions import sumDistinctdf.select(sumDistinct("Quantity")).show() # 29310 12-- in SQLSELECT SUM(Quantity) FROM dfTable -- 29310 avgAlthough you can calculate average by dividing sum by count, Spark provides an easier way to get that value via the avg or mean functions. In this example, we use alias in order to more easily reuse these columns later: 尽管您可以通过将总和除以计数来计算平均值，但是Spark提供了一种更简单的方法，可以通过 avg 或 mean 函数获取该值。 在此示例中，我们使用 alias，以便以后更轻松地重用这些列： 1234567891011// in Scalaimport org.apache.spark.sql.functions.&#123;sum, count, avg, expr&#125;df.select(count("Quantity").alias("total_transactions"),sum("Quantity").alias("total_purchases"),avg("Quantity").alias("avg_purchases"),expr("mean(Quantity)").alias("mean_purchases")).selectExpr("total_purchases/total_transactions","avg_purchases","mean_purchases").show() 1234567891011# in Pythonfrom pyspark.sql.functions import sum, count, avg, exprdf.select(count("Quantity").alias("total_transactions"),sum("Quantity").alias("total_purchases"),avg("Quantity").alias("avg_purchases"),expr("mean(Quantity)").alias("mean_purchases"))\.selectExpr("total_purchases/total_transactions","avg_purchases","mean_purchases").show() 12345+--------------------------------------+----------------+----------------+|(total_purchases / total_transactions)| avg_purchases | mean_purchases |+--------------------------------------+----------------+----------------+| 9.55224954743324 |9.55224954743324|9.55224954743324|+--------------------------------------+----------------+----------------+ NOTE 注意You can also average all the distinct values by specifying distinct. In fact, most aggregate functions support doing so only on distinct values. 您还可以通过指定distinct将所有非重复值取平均值。 实际上，大多数聚合函数仅在不同的值上支持这样做。 Variance and Standard Deviation 方差与标准差Calculating the mean naturally brings up questions about the variance and standard deviation. These are both measures of the spread of the data around the mean. The variance is the average of the squared differences from the mean, and the standard deviation is the square root of the variance. You can calculate these in Spark by using their respective functions. However, something to note is that Spark has both the formula for the sample standard deviation as well as the formula for the population standard deviation. These are fundamentally different statistical formulae, and we need to differentiate between them. By default, Spark performs the formula for the sample standard deviation or variance if you use the variance or stddev functions. 计算平均值自然会引起有关方差和标准偏差的问题。 这些都是衡量数据均值分布的方法。 方差是与平均值的平方差的平均值，标准差是方差的平方根。 您可以使用各自的功能在Spark中计算这些值。 但是，需要注意的是，Spark同时具有样本标准偏差的公式和总体标准偏差的公式。 这些是根本不同的统计公式，我们需要对其进行区分。 默认情况下，如果您使用方差或 stddev 函数，Spark将为样本标准差或方差执行公式。 You can also specify these explicitly or refer to the population standard deviation or variance: 您还可以明确指定这些内容，或参考总体标准差或方差：12345// in Scalaimport org.apache.spark.sql.functions.&#123;var_pop, stddev_pop&#125;import org.apache.spark.sql.functions.&#123;var_samp, stddev_samp&#125;df.select(var_pop("Quantity"), var_samp("Quantity"),stddev_pop("Quantity"), stddev_samp("Quantity")).show() 12345# in Pythonfrom pyspark.sql.functions import var_pop, stddev_popfrom pyspark.sql.functions import var_samp, stddev_sampdf.select(var_pop("Quantity"), var_samp("Quantity"),stddev_pop("Quantity"), stddev_samp("Quantity")).show() 1234-- in SQLSELECT var_pop(Quantity), var_samp(Quantity),stddev_pop(Quantity), stddev_samp(Quantity)FROM dfTable 12345+------------------+------------------+--------------------+-------------------+| var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quan...|+------------------+------------------+--------------------+-------------------+|47559.303646609056|47559.391409298754| 218.08095663447796 | 218.081157850... |+------------------+------------------+--------------------+-------------------+ skewness and kurtosis 偏度和峰度Skewness and kurtosis are both measurements of extreme points in your data. Skewness measures the asymmetry of the values in your data around the mean, whereas kurtosis is a measure of the tail of data. These are both relevant specifically when modeling your data as a probability distribution of a random variable. Although here we won’t go into the math behind these specifically, you can look up definitions quite easily on the internet. You can calculate these by using the functions: 偏度和峰度都是数据中极端值的度量。 偏斜度测量数据中均值前后的不对称性，而峰度则是对数据尾部的度量。 当将数据建模为随机变量的概率分布时，这两个都特别相关。 尽管这里我们不专门讨论这些内容，但您可以在因特网上轻松查找定义。 您可以使用以下函数来计算这些： 12import org.apache.spark.sql.functions.&#123;skewness, kurtosis&#125;df.select(skewness("Quantity"), kurtosis("Quantity")).show() 123# in Pythonfrom pyspark.sql.functions import skewness, kurtosisdf.select(skewness("Quantity"), kurtosis("Quantity")).show() 12-- in SQLSELECT skewness(Quantity), kurtosis(Quantity) FROM dfTable 12345+-------------------+------------------+| skewness(Quantity)|kurtosis(Quantity)|+-------------------+------------------+|-0.2640755761052562|119768.05495536952|+-------------------+------------------+ Covariance and Correlation 协方差和相关性We discussed single column aggregations, but some functions compare the interactions of the values in two difference columns together. Two of these functions are cov and corr, for covariance and correlation, respectively. Correlation measures the Pearson correlation coefficient, which is scaled between –1 and +1. The covariance is scaled according to the inputs in the data. 我们讨论了单列聚合，但是某些函数将两个不同列中的值的相互作用进行了比较。 其中两个函数分别是 cov 和 corr，分别用于协方差和相关性。 相关测量皮尔森相关系数，该系数在–1和+1之间缩放。 协方差根据数据中的输入进行缩放。 Like the var function, covariance can be calculated either as the sample covariance or the population covariance. Therefore it can be important to specify which formula you want to use. Correlation has no notion of this and therefore does not have calculations for population or sample. Here’s how they work: 像 var 函数一样，可以将协方差计算为样本协方差或总体协方差。因此，指定要使用的公式可能很重要。相关性不具有此概念，因此不具有总体或样本的计算。这里是他们是如何工作： 1234// in Scalaimport org.apache.spark.sql.functions.&#123;corr, covar_pop, covar_samp&#125;df.select(corr("InvoiceNo", "Quantity"), covar_samp("InvoiceNo", "Quantity"),covar_pop("InvoiceNo", "Quantity")).show() 1234# in Pythonfrom pyspark.sql.functions import corr, covar_pop, covar_sampdf.select(corr("InvoiceNo", "Quantity"), covar_samp("InvoiceNo", "Quantity"),covar_pop("InvoiceNo", "Quantity")).show() 1234-- in SQLSELECT corr(InvoiceNo, Quantity), covar_samp(InvoiceNo, Quantity),covar_pop(InvoiceNo, Quantity)FROM dfTable 12345+-------------------------+-------------------------------+---------------------+|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceN...|+-------------------------+-------------------------------+---------------------+| 4.912186085635685E-4 | 1052.7280543902734 | 1052.7... |+-------------------------+-------------------------------+---------------------+ Aggregating to Complex Types 聚合为复杂类型In Spark, you can perform aggregations not just of numerical values using formulas, you can also perform them on complex types. For example, we can collect a list of values present in a given column or only the unique values by collecting to a set. 在Spark中，您不仅可以使用公式对数值进行汇总，还可以对复杂类型进行汇总。 例如，我们可以通过收集到一个集合来收集给定列中存在的值列表或仅收集唯一值。 You can use this to carry out some more programmatic access later on in the pipeline or pass the entire collection in a user-defined function (UDF): 您可以使用它在以后的管道中执行更多的编程访问，或者将整个集合传递给用户定义的函数（UDF）： 123// in Scalaimport org.apache.spark.sql.functions.&#123;collect_set, collect_list&#125;df.agg(collect_set("Country"), collect_list("Country")).show() 123# in Pythonfrom pyspark.sql.functions import collect_set, collect_listdf.agg(collect_set("Country"), collect_list("Country")).show() 12-- in SQLSELECT collect_set(Country), collect_set(Country) FROM dfTable 12345+--------------------+---------------------+|collect_set(Country)|collect_list(Country)|+--------------------+---------------------+|[Portugal, Italy,...| [United Kingdom, ...|+--------------------+---------------------+ Grouping 分组Thus far, we have performed only DataFrame-level aggregations. A more common task is to perform calculations based on groups in the data. This is typically done on categorical data for which we group our data on one column and perform some calculations on the other columns that end up in that group. 到目前为止，我们仅执行了DataFrame级的聚合。 一个更常见的任务是基于数据中的组执行计算。 这通常是针对分类数据完成的，对于这些分类数据，我们将数据分组在一个列上，然后对最终归入该组的其他列执行一些计算。 The best way to explain this is to begin performing some groupings. The first will be a count, just as we did before. We will group by each unique invoice number and get the count of items on that invoice. Note that this returns another DataFrame and is lazily performed. 解释此问题的最佳方法是开始执行一些分组。就像我们之前所做的那样，第一个是计数。我们将按每个唯一的发票编号分组，并获取该发票上的项目数。请注意，这将返回另一个DataFrame并被延迟执行。 We do this grouping in two phases. First we specify the column(s) on which we would like to group, and then we specify the aggregation(s). The first step returns a RelationalGroupedDataset, and the second step returns a DataFrame. 我们分两个阶段进行分组。首先，我们指定要分组的列，然后指定聚合。第一步返回一个 RelationalGroupedDataset，第二步返回一个DataFrame。 As mentioned, we can specify any number of columns on which we want to group: 如前所述，我们可以指定要分组的任意数量的列： 1df.groupBy("InvoiceNo", "CustomerId").count().show() 12-- in SQLSELECT count(*) FROM dfTable GROUP BY InvoiceNo, CustomerId 1234567+---------+----------+-----+|InvoiceNo|CustomerId|count|+---------+----------+-----+| 536846 | 14573 | 76 |...| C544318 | 12989 | 1 |+---------+----------+-----+ Grouping with Expressions 用表达式分组As we saw earlier, counting is a bit of a special case because it exists as a method. For this, usually we prefer to use the count function. Rather than passing that function as an expression into a select statement, we specify it as within agg. This makes it possible for you to pass-in arbitrary expressions that just need to have some aggregation specified. You can even do things like alias a column after transforming it for later use in your data flow: 如前所述，计数是一种特殊情况，因为它作为一种方法存在。 为此，通常我们更喜欢使用 count 函数。 与其将函数作为表达式传递到 select 语句中，不如在 agg 中指定它。 这使您可以传入只需要特定的一些聚合函数的任意表达式。 您甚至可以在对列进行转换后进行别名处理，以供以后在数据流中使用： 12345// in Scalaimport org.apache.spark.sql.functions.countdf.groupBy("InvoiceNo").agg( count("Quantity").alias("quan"), expr("count(Quantity)")).show() 12345# in Pythonfrom pyspark.sql.functions import countdf.groupBy("InvoiceNo").agg( count("Quantity").alias("quan"), expr("count(Quantity)")).show() 1234567+---------+----+---------------+|InvoiceNo|quan|count(Quantity)|+---------+----+---------------+| 536596 | 6 | 6 |...| C542604 | 8 | 8 |+---------+----+---------------+ Grouping with Maps 用映射分组Sometimes, it can be easier to specify your transformations as a series of Maps for which the key is the column, and the value is the aggregation function (as a string) that you would like to perform. You can reuse multiple column names if you specify them inline, as well: 有时，将转换指定为一系列 Map（以键为列，值是您要执行的聚合函数（作为字符串））会更容易。 如果您内联地（inline）指定了多个列名，则可以重用它们： 12// in Scaladf.groupBy("InvoiceNo").agg("Quantity"-&gt;"avg", "Quantity"-&gt;"stddev_pop").show() 123# in Pythondf.groupBy("InvoiceNo").agg(expr("avg(Quantity)"),expr("stddev_pop(Quantity)"))\.show() 123-- in SQLSELECT avg(Quantity), stddev_pop(Quantity), InvoiceNo FROM dfTableGROUP BY InvoiceNo 1234567+---------+------------------+--------------------+|InvoiceNo| avg(Quantity) |stddev_pop(Quantity)|+---------+------------------+--------------------+| 536596 | 1.5 | 1.1180339887498947 |...| C542604 | -8.0 | 15.173990905493518 |+---------+------------------+--------------------+ Window Functions 窗口函数You can also use window functions to carry out some unique aggregations by either computing some aggregation on a specific “window” of data, which you define by using a reference to the current data. This window specification determines which rows will be passed in to this function. Now this is a bit abstract and probably similar to a standard group-by, so let’s differentiate them a bit more. 您还可以使用窗口函数来执行某些唯一的聚合，方法是在特定的数据“窗口”上计算某些聚合，您可以使用对当前数据的引用来定义这些聚合。此窗口规范确定哪些行将传递到此函数。现在，这有点抽象，可能类似于标准分组方式，因此让我们对其进行更多区分。 A group-by takes data, and every row can go only into one grouping. A window function calculates a return value for every input row of a table based on a group of rows, called a frame. Each row can fall into one or more frames. A common use case is to take a look at a rolling average of some value for which each row represents one day. If you were to do this, each row would end up in seven different frames. We cover defining frames a little later, but for your reference, Spark supports three kinds of window functions: ranking functions, analytic functions, and aggregate functions. Figure 7-1 illustrates how a given row can fall into multiple frames. 分组依据可以获取数据，并且每一行只能分为一组。对于由一组行（称为帧）组成的表，窗口函数根据每个输入行计算返回值。每一行可以分为一个或多个帧。一个常见的用例是查看某个值的滚动平均值，该值的每一行代表一天。如果要这样做，每一行将以七个不同的帧结束。稍后我们将介绍定义帧的步骤，但仅供参考，Spark支持三种窗口函数：排名函数，分析函数和聚合函数。图7-1说明了给定的行如何分成多个帧。 To demonstrate, we will add a date column that will convert our invoice date into a column that contains only date information (not time information, too): 为了说明这一点，我们将添加一个日期列，该列会将发票日期转换为仅包含日期信息（也不包含时间信息）的列： 1234// in Scalaimport org.apache.spark.sql.functions.&#123;col, to_date&#125;val dfWithDate = df.withColumn("date", to_date(col("InvoiceDate"), "MM/d/yyyy H:mm"))dfWithDate.createOrReplaceTempView("dfWithDate") 1234# in Pythonfrom pyspark.sql.functions import col, to_datedfWithDate = df.withColumn("date", to_date(col("InvoiceDate"), "MM/d/yyyy H:mm"))dfWithDate.createOrReplaceTempView("dfWithDate") The first step to a window function is to create a window specification. Note that the partition by is unrelated to the partitioning scheme concept that we have covered thus far. It’s just a similar concept that describes how we will be breaking up our group. The ordering determines the ordering within a given partition, and, finally, the frame specification (the rowsBetween statement) states which rows will be included in the frame based on its reference to the current input row. In the following example, we look at all previous rows up to the current row: 窗口函数的第一步是创建窗口规范。 注意，partition by 与到目前为止我们所讨论的分区方案概念无关。 这只是一个类似的概念，描述了我们将如何拆分小组。 排序确定给定分区内的排序，最后，帧规范（rowsBetween语句）根据对当前输入行的引用，说明哪些行将包含在帧中。 在以下示例中，我们查看直到当前行的所有先前行： 1234567// in Scalaimport org.apache.spark.sql.expressions.Windowimport org.apache.spark.sql.functions.colval windowSpec = Window.partitionBy("CustomerId", "date").orderBy(col("Quantity").desc).rowsBetween(Window.unboundedPreceding, Window.currentRow) 1234567# in Pythonfrom pyspark.sql.window import Windowfrom pyspark.sql.functions import descwindowSpec = Window\.partitionBy("CustomerId", "date")\.orderBy(desc("Quantity"))\.rowsBetween(Window.unboundedPreceding, Window.currentRow) Now we want to use an aggregation function to learn more about each specific customer. An example might be establishing the maximum purchase quantity over all time. To answer this, we use the same aggregation functions that we saw earlier by passing a column name or expression. In addition, we indicate the window specification that defines to which frames of data this function will apply: 现在，我们要使用聚合函数来了解有关每个特定客户的更多信息。 一个示例可能是一直以来的最大购买数量。 为了回答这个问题，我们使用相同的聚合函数。 另外，我们指出了窗口规范，该规范定义了此功能将应用哪些数据帧： 12import org.apache.spark.sql.functions.maxval maxPurchaseQuantity = max(col("Quantity")).over(windowSpec) 123# in Pythonfrom pyspark.sql.functions import maxmaxPurchaseQuantity = max(col("Quantity")).over(windowSpec) You will notice that this returns a column (or expressions). We can now use this in a DataFrame select statement. Before doing so, though, we will create the purchase quantity rank. To do that we use the dense_rank function to determine which date had the maximum purchase quantity for every customer. We use dense_rank as opposed to rank to avoid gaps in the ranking sequence when there are tied values (or in our case, duplicate rows): 您会注意到，这将返回一个列（或表达式）。 现在，我们可以在DataFrame select语句中使用它。 但是，在此之前，我们将创建采购数量等级。 为此，我们使用 dense_rank 函数来确定哪个日期的每个客户的购买数量最多。 当存在绑定值（或在我们的示例中为重复的行）时，我们使用 dense_rank 而不是 rank 来避免排名序列中的空白： 1234// in Scalaimport org.apache.spark.sql.functions.&#123;dense_rank, rank&#125;val purchaseDenseRank = dense_rank().over(windowSpec)val purchaseRank = rank().over(windowSpec) 1234# in Pythonfrom pyspark.sql.functions import dense_rank, rankpurchaseDenseRank = dense_rank().over(windowSpec)purchaseRank = rank().over(windowSpec) This also returns a column that we can use in select statements. Now we can perform a select to view the calculated window values: 这还会返回一个可在select语句中使用的列。 现在，我们可以执行选择以查看计算出的窗口值： 12345678910// in Scalaimport org.apache.spark.sql.functions.coldfWithDate.where("CustomerId IS NOT NULL").orderBy("CustomerId").select(col("CustomerId"),col("date"),col("Quantity"),purchaseRank.alias("quantityRank"),purchaseDenseRank.alias("quantityDenseRank"),maxPurchaseQuantity.alias("maxPurchaseQuantity")).show() 12345678910# in Pythonfrom pyspark.sql.functions import coldfWithDate.where("CustomerId IS NOT NULL").orderBy("CustomerId")\.select(col("CustomerId"),col("date"),col("Quantity"),purchaseRank.alias("quantityRank"),purchaseDenseRank.alias("quantityDenseRank"),maxPurchaseQuantity.alias("maxPurchaseQuantity")).show() 123456789101112131415161718-- in SQLSELECT CustomerId, date, Quantity, rank(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as rank, dense_rank(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as dRank, max(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as maxPurchaseFROM dfWithDate WHERE CustomerId IS NOT NULL ORDER BY CustomerId 123456789101112+----------+----------+--------+------------+-----------------+---------------+|CustomerId| date |Quantity|quantityRank|quantityDenseRank|maxP...Quantity|+----------+----------+--------+------------+-----------------+---------------+| 12346 |2011-01-18| 74215 | 1 | 1 | 74215 || 12346 |2011-01-18| -74215 | 2 | 2 | 74215 || 12347 |2010-12-07| 36 | 1 | 1 | 36 || 12347 |2010-12-07| 30 | 2 | 2 | 36 |...| 12347 |2010-12-07| 12 | 4 | 4 | 36 || 12347 |2010-12-07| 6 | 17 | 5 | 36 || 12347 |2010-12-07| 6 | 17 | 5 | 36 |+----------+----------+--------+------------+-----------------+---------------+ Grouping Sets 分组集Thus far in this chapter, we’ve seen simple group-by expressions that we can use to aggregate on a set of columns with the values in those columns. However, sometimes we want something a bit more complete—an aggregation across multiple groups. We achieve this by using grouping sets. Grouping sets are a low-level tool for combining sets of aggregations together. They give you the ability to create arbitrary aggregation in their group-by statements. 到目前为止，在本章中，我们已经看到了简单的分组表达式，可用于将一组列中的值聚合在一起。但是，有时我们想要一些更完整的东西——跨多个组的汇总。我们通过使用分组集来实现。分组集是用于将聚合集组合在一起的低阶工具。它们使您能够在group-by语句中创建任意聚合。 Let’s work through an example to gain a better understanding. Here, we would like to get the total quantity of all stock codes and customers. To do so, we’ll use the following SQL expression: 让我们通过一个例子来获得更好的理解。在这里，我们想获得所有股票代码和客户的总数。为此，我们将使用以下SQL表达式： 123// in Scalaval dfNoNull = dfWithDate.drop()dfNoNull.createOrReplaceTempView("dfNoNull") 123# in PythondfNoNull = dfWithDate.drop()dfNoNull.createOrReplaceTempView("dfNoNull") 1234-- in SQLSELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNullGROUP BY customerId, stockCodeORDER BY CustomerId DESC, stockCode DESC You can do the exact same thing by using a grouping set: 您可以使用分组集来做完全相同的事情： 1234-- in SQLSELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNullGROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode))ORDER BY CustomerId DESC, stockCode DESC 123456789+----------+---------+-------------+|CustomerId|stockCode|sum(Quantity)|+----------+---------+-------------+| 18287 | 85173 | 48 || 18287 | 85040A | 48 || 18287 | 85039B | 120 |...| 18287 | 23269 | 36 |+----------+---------+-------------+ WARNING 警告Grouping sets depend on null values for aggregation levels. If you do not filter-out null values, you will get incorrect results. 分组集取决于聚合级别的空值。 如果不筛选出空值，则会得到不正确的结果。 This applies to cubes, rollups, and grouping sets. Simple enough, but what if you also want to include the total number of items, regardless of customer or stock code? With a conventional group-by statement, this would be impossible. But, it’s simple with grouping sets: we simply specify that we would like to aggregate at that level, as well, in our grouping set. This is, effectively, the union of several different groupings together: 这适用于 cube，rollup 和分组集。 很简单，但是如果您还想包括项目总数，而不管客户或库存代码如何呢？ 使用常规的分组声明，这将是不可能的。 但是，使用分组集很简单：我们只需在分组集中指定要在该级别进行汇总。 实际上，这是几个不同分组的结合： 1234-- in SQLSELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNullGROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode),())ORDER BY CustomerId DESC, stockCode DESC 123456789+----------+---------+-------------+|customerId|stockCode|sum(Quantity)|+----------+---------+-------------+| 18287 | 85173 | 48 || 18287 | 85040A | 48 || 18287 | 85039B | 120 |...| 18287 | 23269 | 36 |+----------+---------+-------------+ The GROUPING SETS operator is only available in SQL. To perform the same in DataFrames, you use the rollup and cube operators—which allow us to get the same results. Let’s go through those. GROUPING SETS 运算符仅在 SQL 中可用。 要在DataFrames中执行相同的操作，请使用 rollup 和 cube 运算符——允许我们获得相同的结果。 让我们来看看这些。 Rollups 汇总Thus far, we’ve been looking at explicit groupings. When we set our grouping keys of multiple columns, Spark looks at those as well as the actual combinations that are visible in the dataset. A rollup is a multidimensional aggregation that performs a variety of group-by style calculations for us. Let’s create a rollup that looks across time (with our new Date column) and space (with the Country column) and creates a new DataFrame that includes the grand total over all dates, the grand total for each date in the DataFrame, and the subtotal for each country on each date in the DataFrame: 到目前为止，我们一直在研究显式分组。设置多列的分组键时，Spark会查看这些键以及数据集中可见的实际组合。rollup 是一个多维聚合，可以为我们执行各种分组样式计算。让我们创建一个跨时间（使用新的Date列）和空间（使用Country列）查看的汇总，并创建一个新的DataFrame，其中包括所有日期的总计，DataFrame中每个日期的总计以及小计对于每个国家/地区在DataFrame中的每个日期： 1234val rolledUpDF = dfNoNull.rollup("Date", "Country").agg(sum("Quantity")).selectExpr("Date", "Country", "`sum(Quantity)` as total_quantity").orderBy("Date")rolledUpDF.show() 12345# in PythonrolledUpDF = dfNoNull.rollup("Date", "Country").agg(sum("Quantity"))\.selectExpr("Date", "Country", "`sum(Quantity)` as total_quantity")\.orderBy("Date")rolledUpDF.show() 123456789101112+----------+--------------+--------------+| Date | Country |total_quantity|+----------+--------------+--------------+| null | null | 5176450 ||2010-12-01|United Kingdom| 23949 ||2010-12-01| Germany | 117 ||2010-12-01| France | 449 |...|2010-12-03| France | 239 ||2010-12-03| Italy | 164 ||2010-12-03| Belgium | 528 |+----------+--------------+--------------+ Now where you see the null values is where you’ll find the grand totals. A null in both rollup columns specifies the grand total across both of those columns: 现在，您可以在其中找到空值的地方找到总计。 两个汇总列中的null指定这两个列的总计： 12rolledUpDF.where("Country IS NULL").show()rolledUpDF.where("Date IS NULL").show() 12345+----+-------+--------------+|Date|Country|total_quantity|+----+-------+--------------+|null| null| 5176450 |+----+-------+--------------+ Cube 多维数据集A cube takes the rollup to a level deeper. Rather than treating elements hierarchically, a cube does the same thing across all dimensions. This means that it won’t just go by date over the entire time period, but also the country. To pose this as a question again, can you make a table that includes the following? cube 将 rollup 扩展到更深的层次。 cube 不是在层次上处理元素，而是在所有维度上执行相同的操作。 这意味着它不仅会在整个时间段内按日期显示，而且还会在整个国家/地区显示。 再次提出这个问题，您可以制作一个包含以下内容的表格吗？ The total across all dates and countries所有日期和国家/地区的总计 The total for each date across all countries所有国家/地区每个日期的总计 The total for each country on each date每个国家/地区在每个日期的总数 The total for each country across all dates所有国家/地区在所有日期的总计 The method call is quite similar, but instead of calling rollup, we call cube: 方法调用非常相似，但是我们不调用 rollup，而是调用 cube： 123// in ScaladfNoNull.cube("Date", "Country").agg(sum(col("Quantity"))).select("Date", "Country", "sum(Quantity)").orderBy("Date").show() 1234# in Pythonfrom pyspark.sql.functions import sumdfNoNull.cube("Date", "Country").agg(sum(col("Quantity")))\.select("Date", "Country", "sum(Quantity)").orderBy("Date").show() 1234567891011121314+----+--------------------+-------------+|Date| Country |sum(Quantity)|+----+--------------------+-------------+|null| Japan | 25218 ||null| Portugal | 16180 ||null| Unspecified | 3300 ||null| null | 5176450 ||null| Australia | 83653 |...|null| Norway | 19247 ||null| Hong Kong | 4769 ||null| Spain | 26824 ||null| Czech Republic| 592 |+----+--------------------+-------------+ This is a quick and easily accessible summary of nearly all of the information in our table, and it’s a great way to create a quick summary table that others can use later on. 这是对我们表中几乎所有信息的快速且易于访问的摘要，也是创建其他人以后可以使用的快速摘要表的好方法。 Grouping Metadata 分组元数据Sometimes when using cubes and rollups, you want to be able to query the aggregation levels so that you can easily filter them down accordingly. We can do this by using the grouping_id, which gives us a column specifying the level of aggregation that we have in our result set. The query in the example that follows returns four distinct grouping IDs: 有时，在使用多维数据集和汇总时，您希望能够查询聚合级别，以便可以轻松地相应地对其进行过滤。 我们可以使用grouping_id做到这一点，它为我们提供了一列，用于指定结果集中的聚合级别。 以下示例中的查询返回四个不同的分组ID： Table 7-1. Purpose of grouping IDs 对ID分组的目的 Grouping ID Description 3 This will appear for the highest-level aggregation, which will gives us the total quantity regardless of customerId and stockCode. 这将显示在最高级别的汇总中，无论 customerId 和 stockCode 如何，都将为我们提供总量。 2 This will appear for all aggregations of individual stock codes. This gives us the total quantity per stock code, regardless of customer. 这将显示在单个股票代码的所有汇总中。 这给了我们每个股票代码的总数量，而与客户无关。 1 This will give us the total quantity on a per-customer basis, regardless of item purchased. 这将为我们提供每个客户的总数量，而与购买的商品无关。 0 This will give us the total quantity for individual customerId and stockCode combinations. 这将为我们提供单个“ customerId”和“ stockCode”组合的总数量。 This is a bit abstract, so it’s well worth trying out to understand the behavior yourself : 这有点抽象，因此值得尝试自己了解一下行为： 12345// in Scalaimport org.apache.spark.sql.functions.&#123;grouping_id, sum, expr&#125;dfNoNull.cube("customerId", "stockCode").agg(grouping_id(), sum("Quantity")).orderBy(expr("grouping_id()").desc).show() 12345678+----------+---------+-------------+-------------+|customerId|stockCode|grouping_id()|sum(Quantity)|+----------+---------+-------------+-------------+| null | null | 3 | 5176450 || null | 23217 | 2 | 1309 || null | 90059E | 2 | 19 |...+----------+---------+-------------+-------------+ Pivot Pivots make it possible for you to convert a row into a column. For example, in our current data we have a Country column. With a pivot, we can aggregate according to some function for each of those given countries and display them in an easy-to-query way: Pivot 使您可以将行转换为列。 例如，在当前数据中，我们有一个“国家”列。 有了枢轴（Pivot），我们可以针对每个给定国家/地区按照某种功能进行汇总，并以易于查询的方式显示它们： 12// in Scalaval pivoted = dfWithDate.groupBy("date").pivot("Country").sum() 12# in Pythonpivoted = dfWithDate.groupBy("date").pivot("Country").sum() This DataFrame will now have a column for every combination of country, numeric variable, and a column specifying the date. For example, for USA we have the following columns: 现在，此 DataFrame 将为国家、数字变量的每种组合提供一列，并为指定日期提供一列。例如，对于美国，我们有以下几列： USA_sum(Quantity), USA_sum(UnitPrice), USA_sum(CustomerID)This represents one for each numeric column in our dataset (because we just performed an aggregation over all of them). 这代表了数据集中每个数字列的一个（因为我们只是对所有它们进行了汇总）。 Here’s an example query and result from this data: 这是查询示例，并根据这些数据得出结果： 1pivoted.where("date &gt; '2011-12-05'").select("date" ,"`USA_sum(Quantity)`").show() 12345678+----------+-----------------+| date |USA_sum(Quantity)|+----------+-----------------+|2011-12-06| null ||2011-12-09| null ||2011-12-08| -196 ||2011-12-07| null |+----------+-----------------+ Now all of the columns can be calculated with single groupings, but the value of a pivot comes down to how you would like to explore the data. It can be useful, if you have low enough cardinality in a certain column to transform it into columns so that users can see the schema and immediately know what to query for. 现在，可以使用单个分组来计算所有列，但是数据透视表的价值取决于您希望如何浏览数据。如果您在某个列中的基数小的足够将其转换为列，以便用户可以看到模式并立即知道要查询的内容，则此方法很有用。 User-Defined Aggregation Functions 用户定义的聚合函数User-defined aggregation functions (UDAFs) are a way for users to define their own aggregation functions based on custom formulae or business rules. You can use UDAFs to compute custom calculations over groups of input data (as opposed to single rows). Spark maintains a single AggregationBuffer to store intermediate results for every group of input data. 用户定义的聚合函数（UDAF）是用户根据自定义公式或业务规则定义自己的聚合函数的一种方式。您可以使用UDAF在输入数据组（而不是单行）上计算自定义计算。 Spark维护一个 AggregationBuffer 来存储每组输入数据的中间结果。 To create a UDAF, you must inherit from the UserDefinedAggregateFunction base class and implement the following methods: 要创建UDAF，您必须继承自基类 UserDefinedAggregateFunction 并实现以下方法： inputSchema represents input arguments as a StructTypeinputSchema 将输入参数表示为 StructType bufferSchema represents intermediate UDAF results as a StructTypebufferSchema 将中间的UDAF结果表示为 StructType dataType represents the return DataTypedataType 表示返回的 DataType deterministic is a Boolean value that specifies whether this UDAF will return the same result for a given inputdeterministic 是一个布尔值，它指定此UDAF对于给定的输入是否将返回相同的结果 initialize allows you to initialize values of an aggregation bufferinitialize 允许您初始化聚合缓冲区的值 update describes how you should update the internal buffer based on a given rowupdate 描述了如何根据给定的行更新内部缓冲区 merge describes how two aggregation buffers should be mergedmerge 描述了如何合并两个聚合缓冲区 evaluate will generate the final result of the aggregationevaluate 将生成聚合的最终结果 The following example implements a BoolAnd, which will inform us whether all the rows (for a given column) are true; if they’re not, it will return false: 下面的示例实现了一个BoolAnd，它将通知我们所有行（对于给定列）是否为true；如果不是，它将返回false： 1234567891011121314151617181920212223242526// in Scalaimport org.apache.spark.sql.expressions.MutableAggregationBufferimport org.apache.spark.sql.expressions.UserDefinedAggregateFunctionimport org.apache.spark.sql.Rowimport org.apache.spark.sql.types._class BoolAnd extends UserDefinedAggregateFunction &#123; def inputSchema: org.apache.spark.sql.types.StructType = StructType(StructField("value", BooleanType) :: Nil) def bufferSchema: StructType = StructType( StructField("result", BooleanType) :: Nil ) def dataType: DataType = BooleanType def deterministic: Boolean = true def initialize(buffer: MutableAggregationBuffer): Unit = &#123; buffer(0) = true &#125; def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; buffer(0) = buffer.getAs[Boolean](0) &amp;&amp; input.getAs[Boolean](0) &#125; def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; buffer1(0) = buffer1.getAs[Boolean](0) &amp;&amp; buffer2.getAs[Boolean](0) &#125; def evaluate(buffer: Row): Any = &#123; buffer(0) &#125;&#125; Now, we simply instantiate our class and/or register it as a function: 现在，我们只需实例化我们的类和/或将其注册为一个函数： 123456789// in Scalaval ba = new BoolAndspark.udf.register("booland", ba)import org.apache.spark.sql.functions._spark.range(1).selectExpr("explode(array(TRUE, TRUE, TRUE)) as t").selectExpr("explode(array(TRUE, FALSE, TRUE)) as f", "t").select(ba(col("t")), expr("booland(f)")).show() 12345+----------+----------+|booland(t)|booland(f)|+----------+----------+| true | false |+----------+----------+ UDAFs are currently available only in Scala or Java. However, in Spark 2.3, you will also be able to call Scala or Java UDFs and UDAFs by registering the function just as we showed in the UDF section in Chapter 6. For more information, go to SPARK-19439. UDAF当前仅在Scala或Java中可用。 但是，在Spark 2.3中，您也可以通过注册函数来调用Scala或Java UDF和UDAF，就像我们在第6章UDF部分中所显示的那样。有关更多信息，请转到SPARK-19439。 Conclusion 结论This chapter walked through the different types and kinds of aggregations that you can perform in Spark. You learned about simple grouping-to window functions as well as rollups and cubes. Chapter 8 discusses how to perform joins to combine different data sources together. 本章介绍了可以在Spark中执行的不同类型的聚合。 您了解了简单的分组到窗口函数以及rollup和cube。 第8章讨论如何执行 join 以将不同的数据源组合在一起。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 4 Structured API Overview]]></title>
    <url>%2F2019%2F08%2F05%2FChapter4_StructuredAPIOverview(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 4 Structured API OverviewThis part of the book will be a deep dive into Spark’s Structured APIs. The Structured APIs are a tool for manipulating all sorts of data, from unstructured log files to semi-structured CSV files and highly structured Parquet files. These APIs refer to three core types of distributed collection APIs: 本书的这一部分将深入探讨Spark的结构化API。结构化API是用于处理各种数据的工具，从非结构化日志文件到半结构化CSV文件以及高度结构化的Parquet文件。这些API指的是分布式集合API的三种核心类型： Datasets DataFrames SQL tables and views SQL表和视图 Although they are distinct parts of the book, the majority of the Structured APIs apply to both batch and streaming computation. This means that when you work with the Structured APIs, it should be simple to migrate from batch to streaming (or vice versa) with little to no effort. We’ll cover streaming in detail in Part V. 尽管它们是本书的不同部分，但大多数结构化API均适用于批处理和流计算。这意味着，当您使用结构化API时，不费吹灰之力就可以轻松地从批处理迁移到流式处理（反之亦然）。我们将在第五部分中详细介绍流式传输。 The Structured APIs are the fundamental abstraction that you will use to write the majority of your data flows. Thus far in this book, we have taken a tutorial-based approach, meandering our way through much of what Spark has to offer. This part offers a more in-depth exploration. In this chapter, we’ll introduce the fundamental concepts that you should understand: the typed and untyped APIs (and their differences); what the core terminology is; and, finally, how Spark actually takes your Structured API data flows and executes it on the cluster. We will then provide more specific task-based information for working with certain types of data or data sources. 结构化API是基本的抽象概念，可用于编写大多数数据流。到目前为止，在本书中，我们采用了基于手册的方法，蜿蜒地浏览过了Spark提供的许多功能。这部分提供了更深入的探索。在本章中，我们将介绍您应该了解的基本概念：类型化API和非类型化API（及其区别）；核心术语是什么；最后，Spark实际如何获取结构化API数据流并在集群上执行它。然后，我们将提供更具体的基于任务的信息，以处理某些类型的数据或数据源。 NOTE 注意 Before proceeding, let’s review the fundamental concepts and definitions that we covered in Part I. Spark is a distributed programming model in which the user specifies transformations. Multiple transformations build up a directed acyclic graph of instructions. An action begins the process of executing that graph of instructions, as a single job, by breaking it down into stages and tasks to execute across the cluster. The logical structures that we manipulate with transformations and actions are DataFrames and Datasets. To create a new DataFrame or Dataset, you call a transformation. To start computation or convert to native language types, you call an action. 在继续之前，让我们回顾一下在第一部分中介绍的基本概念和定义。Spark是一个分布式编程模型，用户可以在其中指定转换。多次转换建立了指令的有向无环图。动作 (action) 通过将指令图分解为要在整个集群中执行的阶段 (stage) 和任务 (task) 来开始，作为单个作业执行该指令图的过程。我们通过转换 (transformation) 和 动作(action)操作的逻辑结构是DataFrame 和 Dataset。要创建新的DataFrame或Dataset，请调用 transformation。要开始计算或转换为本地语言类型，请调用一个 action。 DataFrames and DatasetsPart I discussed DataFrames. Spark has two notions of structured collections: DataFrames and Datasets. We will touch on the (nuanced) differences shortly, but let’s define what they both represent first. 第一部分讨论了 DataFrame。 Spark有两个结构化集合的概念：DataFrames和Datasets。 我们将在短期内探讨（细微的）差异，但让我们先定义它们分别代表什么。 DataFrames and Datasets are (distributed) table-like collections with well-defined rows and columns. Each column must have the same number of rows as all the other columns (although you can use null to specify the absence of a value) and each column has type information that must be consistent for every row in the collection. To Spark, DataFrames and Datasets represent immutable, lazily evaluated plans that specify what operations to apply to data residing at a location to generate some output. When we perform an action on a DataFrame, we instruct Spark to perform the actual transformations and return the result. These represent plans of how to manipulate rows and columns to compute the user’s desired result. DataFrame和Dataset是（分布式的）类表集合，具有明确定义的行和列。 每列必须具有与所有其他列相同的行数（尽管您可以使用null来指定不存在值），并且每一列的类型信息必须与集合中的每一行保持一致。 对于Spark，DataFrame和Dataset表示不可变，惰性求值的计划，这些计划指定对驻留在某个位置的数据进行哪些操作以生成某些输出。 当我们在DataFrame上执行 action 时，我们指示Spark执行实际的转换并返回结果。 这些代表如何操作行和列以计算用户期望结果的计划。 NOTE 注意 Tables and views are basically the same thing as DataFrames. We just execute SQL against them instead o DataFrame code. We cover all of this in Chapter 10, which focuses specifically on Spark SQL. To add a bit more specificity to these definitions, we need to talk about schemas, which are the way you define the types of data you’re storing in this distributed collection. 表和视图与DataFrames基本相同。我们只是针对它们执行 SQL 而不是 DataFrame 代码。我们将在第10章中专门介绍Spark SQL。为了使这些定义更加具体，我们需要讨论模式（schema），这是定义存储在此分布式集合中的数据类型的方式。 Schemas 模式A schema defines the column names and types of a DataFrame. You can define schemas manually or read a schema from a data source (often called schema on read). Schemas consist of types, meaning that you need a way of specifying what lies where. 模式定义了DataFrame的列名和类型。您可以手动定义模式，也可以从数据源读取模式（通常称为读取时模式）。模式由类型组成，这意味着您需要一个方式指定类型（数据列的具体类型）和相应的位置（数据列）。 Overview of Structured Spark Types 结构化Spark类型概述Spark is effectively a programming language of its own. Internally, Spark uses an engine called Catalyst that maintains its own type information through the planning and processing of work. In doing so, this opens up a wide variety of execution optimizations that make significant differences. Spark types map directly to the different language APIs that Spark maintains and there exists a lookup table for each of these in Scala, Java, Python, SQL, and R. Even if we use Spark’s Structured APIs from Python or R, the majority of our manipulations will operate strictly on Spark types, not Python types. For example, the following code does not perform addition in Scala or Python; it actually performs addition purely in Spark: Spark实际上是一种自己的编程语言。在内部，Spark使用一种称为Catalyst的引擎，该引擎通过计划和处理工作来维护自己的类型信息。这样一来，就可以开辟出各种各样的执行优化方案，从而产生显着差异。 Spark类型直接映射到Spark维护的不同语言API，并且在Scala，Java，Python，SQL和R中存在针对每种语言的查找表。即使我们使用来自Python或R的Spark的结构化API，我们大多数操作将严格针对Spark类型而不是Python类型进行操作。例如，以下代码在Scala或Python中不执行加法；它实际上仅在Spark中执行加法： 123// in Scalaval df = spark.range(500).toDF("number")df.select(df.col("number") + 10) 123# in Pythondf = spark.range(500).toDF("number")df.select(df["number"] + 10) This addition operation happens because Spark will convert an expression written in an input language to Spark’s internal Catalyst representation of that same type information. It then will operate on that internal representation. We touch on why this is the case momentarily, but before we can, we need to discuss Datasets. 之所以进行这种加法操作，是因为Spark会将以一种输入语言编写的表达式转换为相同类型信息的Spark内部Catalyst表示形式，然后它将在该内部表示上运行。我们短暂地谈谈为什么会这样，但是在我们这样做之前，我们需要讨论数据集。 DataFrames Versus Datasets DataFrames对比DataSetIn essence, within the Structured APIs, there are two more APIs, the “untyped” DataFrames and the “typed” Datasets. To say that DataFrames are untyped is as lightly in accurate; they have types, but Spark maintains them completely and only checks whether those types line up to those specified in the schema at runtime. Datasets, on the other hand, check whether types conform to the specification at compile time. Datasets are only available to Java Virtual Machine (JVM)–based languages (Scala and Java) and we specify types with case classes or Java beans. 本质上，在结构化API中，还有另外两个API，即“无类型 ”DataFrames 和“有类型” Datasets。说DataFrame是未类型化的，这是不准确的。它们具有类型，但是Spark会完全维护它们，并且仅在运行时检查那些类型是否与模式中指定的类型一致。另一方面，Dataset在编译时检查类型是否符合规范。Dataset仅适用于基于Java虚拟机（JVM）的语言（Scala和Java），并且我们使用案例类或Java Bean指定类型。 For the most part, you’re likely to work with DataFrames. To Spark (in Scala), DataFrames are simply Datasets of Type Row. The “Row” type is Spark’s internal representation of its optimized in memory format for computation. This format makes for highly specialized and efficient computation because rather than using JVM types, which can cause high garbage-collection and object instantiation costs, Spark can operate on its own internal format without incurring any of those costs. To Spark (in Python or R), there is no such thing as a Dataset: everything is a DataFrame and therefore we always operate on that optimized format. 在大多数情况下，您可能会使用DataFrame。对于Spark（在Scala中），DataFrames只是类型为Row的数据集。 “Row”类型是Spark内部优化表示的内部表示形式，用于计算。这种格式可以进行高度专业化和高效的计算，因为Spark可以使用自己的内部格式运行，而不会产生任何这些代价，而不是使用JVM类型（后者可能导致高昂的垃圾收集和对象实例化成本）。对于Spark（在Python或R中），没有诸如Dataset之类的东西：一切都是DataFrame，因此我们始终以优化格式运行。 NOTE 注意 The internal Catalyst format is well covered in numerous Spark presentations. Given that this book is intended for a more general audience, we’ll refrain from going into the implementation. If you’re curious, there are some excellent talks by Josh Rosen and Herman van Hovell, both of Databricks, about their work in the development of Spark’s Catalyst engine. 许多Spark演示都很好地介绍了内部Catalyst格式。 鉴于本书是为更广泛的读者准备的，我们将不着手实施。 如果您感到好奇，Databricks的 Josh Rosen 和 Herman van Hovell 都会就他们在Spark的Catalyst引擎开发方面的工作进行精彩的演讲。 译者附为什么使用结构化API？ 截图来自Herman van Hovell视频的内容，非原书内容。 Understanding DataFrames, Spark Types, and Schemas takes some time to digest. What you need to know is that when you’re using DataFrames, you’re taking advantage of Spark’s optimized internal format. This format applies the same efficiency gains to all of Spark’s language APIs. If you need strict compile-time checking, read Chapter 11 to learn more about it. 了解DataFrame，Spark类型和模式需要一些时间来进行消化。您需要了解的是，在使用DataFrames时，您会利用Spark的优化内部格式。这种格式可将所有Spark语言API的效率提高相同效益。如果需要严格的编译时检查，请阅读第11章以了解更多信息。 Let’s move onto some friendlier and more approachable concepts: columns and rows. 让我们进入一些更友好，更平易近人的概念：列和行。 Columns 列Columns represent a simple type like an integer or string, a complex type like an array or map, or a null value. Spark tracks all of this type information for you and offers a variety of ways, with which you can transform columns. Columns are discussed extensively in Chapter 5, but for the most part you can think about Spark Column types as columns in a table. 列表示简单类型（例如整数或字符串），复杂类型（例如数组或映射）或空值。 Spark会为您跟踪所有此类信息，并提供多种方式来转换列。列在第5章中进行了广泛讨论，但是在大多数情况下，您可以将Spark列类型视为表中的列。 Rows 行A row is nothing more than a record of data. Each record in a DataFrame must be of type Row, as we can see when we collect the following DataFrames. We can create these rows manually from SQL, from Resilient Distributed Datasets (RDDs), from data sources, or manually from scratch. Here, we create one by using a range: 行只不过是数据记录。 DataFrame中的每个记录都必须是Row类型，正如我们在 collect 以下 DataFrame 时所看到的。我们可以从SQL，弹性分布式数据集（RDD），数据源或从头开始手动创建这些行。在这里，我们使用 range 创建一个： 12// in Scalaspark.range(2).toDF().collect() 12# in Pythonspark.range(2).collect() These both result in an array of Row objects. 这些都导致 Row 对象的数组。 Spark TypesWe mentioned earlier that Spark has a large number of internal type representations. We include a handy reference table on the next several pages so that you can most easily reference what type, in your specific language, lines up with the type in Spark. 前面我们提到，Spark具有大量内部类型表示形式。 在接下来的几页中，我们将提供一个方便的参考表，以便您可以最轻松地参考特定语言与Spark中的类型对齐的类型。 Before getting to those tables, let’s talk about how we instantiate, or declare, a column to be of a certain type. 在进入这些表之前，让我们谈谈如何实例化或声明一列属于某种类型。 To work with the correct Scala types, use the following: 要使用正确的Scala类型，请使用以下命令： 12import org.apache.spark.sql.types._val b = ByteType To work with the correct Java types, you should use the factory method in the following package: 要使用正确的Java类型，应使用以下软件包中的工厂方法： 12import org.apache.spark.sql.types.DataTypes;ByteType x = DataTypes.ByteType; Python types at times have certain requirements, which you can see listed in Table 4-1, as do Scala and Java, which you can see listed in Tables 4-2 and 4-3, respectively. To work with the correct Python types, use the following: 有时，Python类型具有某些要求，表4-1中列出了这些要求，而Scala和Java则具有某些要求，分别在表4-2和表4-3中列出了。 要使用正确的Python类型，请使用以下命令： 12from pyspark.sql.types import *b = ByteType() The following tables provide the detailed type information for each of Spark’s language bindings. 下表提供了每种Spark语言绑定的详细类型信息。 Table 4-1. Python type reference Data type Value type in Python API to access or create a data type ByteType int or long. Note: Numbers will be converted to 1-byte signed integer numbers at runtime. Ensure that numbers are within the range of-128 to 127.int或long。注意：数字将在运行时转换为1字节有符号整数。确保数字在-128到127的范围内。 ByteType() ShortType int or long. Note: Numbers will be converted to 2-byte signed integer numbers at runtime. Ensure that numbers are within the range of-32768 to 32767.int或long。注意：数字将在运行时转换为2字节有符号整数。确保数字在-32768到32767的范围内。 ShortType() IntegerType int or long. Note: Python has a lenient definition of “integer.” Numbers that are too large will be rejected by Spark SQL if you use the IntegerType(). It’s best practice to use LongType.int或long。注意：Python的宽泛定义是“整数”。如果您使用IntegerType()，则太大的数字将被Spark SQL拒绝。最佳做法是使用LongType。 Integerlype() LongType long. Note: Numbers will be converted to 8-byte signed integer numbers at runtime. Ensure that numbers are within the range of-9223372036854775808 to 9223372036854775807. Otherwise, convert data to decimaLDecimal and use DecimaFlype.long。注意：数字将在运行时转换为8字节有符号整数。确保数字在-9223372036854775808到9223372036854775807之间。否则，将数据转换为decimaLDecimal并使用DecimaFlype。 Longlype() FloatType float. Note: Numbers will be converted to 4-byte single-precision floating-point numbers at runtime.float。注意：数字将在运行时转换为4字节单精度浮点数。 FloatType() DoubleType float DoubleType() DecimalType decimalDecimal DecimalTypeO StringType string StringType() BinaryType bytearray BinaryType() BooleanType bool BooleanType() llmestamplype datetime.datetime TlmestampTypeO DateType datetime.date DateType() ArrayType list, tuple, or array ArrayType(elementType, [containsNull]). Note: The default value of containsNull is True.注意：containsNull的默认值为True。 MapType diet MapType(keyType, valueType, [valueContainsNull]). Note: The default value of valueContainsNull is True.注意：valueContainsNull的默认值为True。 Structlype list or tuple StructType(fields). Note: fields is a list of StructFields. Also, fields with the same name are not allowed.注意：字段是StructFields的列表。同样，不允许使用具有相同名称的字段。 StructField The value type in Python of the data type of this field (for example, Int for a StructField with the data type IntegerType) StructField(name, datalype, [nullable]) Note: The defaul value of nullable is True.注意：nullable的默认值为True。 Table 4-2. Scala type reference Data type Value type in Scala API to access or create a data type ByteType Byte ByteType ShortType Short ShortType IntegerType Int IntegerType LongType Long LongType FloatType Float FloatType DoubleType Double DoubleType DecimalType java.math.BigDecimal DecimalType StringType String StringType BinaryType Array[Byte] BinaryType BooleanType Boolean BooleanType TimestampType java.sql.Timestamp TimestampType DateType java.sql.Date DateType ArrayType scala.collection.Seq ArrayType(elementType, [containsNull]). Note: The default value of containsNull is true.注意：containsNull的默认值为true。 MapType scala.collection.Map MapType(keyType, valueType, [valueContainsNull]). Note: The default value of valueContainsNull is true.注意：valueContainsNull的默认值为true。 StructType org.apache.spark.sql.Row StructType(fields). Note: fields is an Array of StructFields. Also, fields with the same name are not allowed.注意：字段是StructFields的数组。同样，不允许使用具有相同名称的字段。 StructField The value type in Scala of the data type of this field (for example, Int for a StructField with the data type IntegerType)Scala中此字段的数据类型的值类型（例如，对于数据类型为IntegerType的StructField为Int） StructField(name, dataType, [nullable]). Note: The default value of nullable is true.注意：nullable的默认值为true。 Table 4-3. Java type reference Data type Value type in Java API to access or create a data type ByteType byte or Byte DataTypes. ByteType ShortType short or Short DataTypes. ShortType IntegerType int or Integer DataTypes. IntegerType LongType long or Long DataTypes. LongType FloatType float or Float DataTypes. FloatType DoubleType double or Double DataTypes. DoubleType DecinialType java .math.BigDecimal DataTypes.createDecinialType()DataTypes.createDecinialType(precision, scale). StringType String DataTypes. StringType BmaryType byte[] DataTypes. BinaryType BooleanType boolean or Boolean DataTypes. BooleanType TimestampType java. sqL Timestamp DataTypes.TimestampType DateType java.sqLDate DataTypes. DateType ArrayType java.utiLList DataTypes.createArrayType(elementType). Note: The value of contamsNull will be true.DataTypes.createArrayType(elementType, contamsNull). MapType java.util.Map DataTypes.createMapType(keyType, vahieType). Note: The value of valueContainsNull will be true.注意：valueContainsNull的值将为true。DataTypes.createMapType(keyType, vahieType, vahieContainsNull) StructType org.apache.spark.sql.Row DataTypes.createStructType(fieIds). Note: fields is a List or an array of StructFields. Also, two fields with the same name are not alfowed.注意：字段是StructField的列表或数组。同样，两个同名字段也不被允许。 StructField The value type in Java of the data type of this field (for example, int for a StructField with the data type IntegerType)Java中此字段的数据类型的值类型（例如，数据类型为IntegerType的StructField的int） DataTypes.createStructField(name, dataType, nullable) Overview of Structured API Execution 结构化API执行概述This section will demonstrate how this code is actually executed across a cluster. This will help you understand (and potentially debug) the process of writing and executing code on clusters, so let’s walk through the execution of a single structured API query from user code to executed code. Here’s an overview of the steps: 本节将演示如何在整个集群中实际执行此代码。这将帮助您了解（并可能调试）在集群上编写和执行代码的过程，因此让我们逐步执行从用户代码到执行代码的单个结构化API查询的执行。以下是步骤概述： Write DataFrame/Dataset/SQL Code. 编写DataFrame / Dataset / SQL代码。 If valid code, Spark converts this to a Logical Plan. 如果是有效代码，Spark会将其转换为逻辑计划。 Spark transforms this Logical Plan to a Physical Plan, checking for optimizations along the way. Spark将此逻辑计划转换为物理计划，并按照方式检查优化。 Spark then executes this Physical Plan (RDD manipulations) on the cluster. 然后，Spark在集群上执行此物理计划（RDD操作）。 To execute code, we must write code. This code is then submitted to Spark either through the console or via a submitted job. This code then passes through the Catalyst Optimizer, which decides how the code should be executed and lays out a plan for doing so before, finally, the code is run and the result is returned to the user. Figure 4-1 shows the process. 要执行代码，我们必须编写代码。然后，此代码通过控制台或提交的作业提交给Spark。然后，此代码通过Catalyst Optimizer，后者确定应如何执行代码，并在此之前制定执行计划，最后，代码将运行并将结果返回给用户。流程如图4-1所示。 Logical Planning 逻辑规划The first phase of execution is meant to take user code and convert it into a logical plan. Figure 4-2 illustrates this process. 执行的第一阶段旨在获取用户代码并将其转换为逻辑计划。图4-2说明了此过程。 This logical plan only represents a set of abstract transformations that do not refer to executors or drivers, it’s purely to convert the user’s set of expressions into the most optimized version. It does this by converting user code into an unresolved logical plan. This plan is unresolved because although your code might be valid, the tables or columns that it refers to might or might not exist. Spark uses the catalog, a repository of all table and DataFrame information, to resolve columns and tables in the analyzer. The analyzer might reject the unresolved logical plan if the required table or column name does not exist in the catalog. If the analyzer can resolve it, the result is passed through the Catalyst Optimizer, a collection of rules that attempt to optimize the logical plan by pushing down predicates or selections. Packages can extend the Catalyst to include their own rules for domain specific optimizations. 此逻辑计划仅代表一组抽象转换，这些转换不涉及 executor 或 driver，而仅仅是将用户的表达式集转换为最优化的版本。它通过将用户代码转换为未解析的逻辑计划来实现。该计划尚未解析，因为尽管您的代码可能有效，但它所引用的表或列可能存在或可能不存在。 Spark使用 catalog（所有表和DataFrame信息的存储库）解析分析器中的列和表。如果所需的表或列名称在目录中不存在，则分析器可能会拒绝未解析的逻辑计划。如果分析器可以解析问题，则结果将通过Catalyst Optimizer传递，Catalyst Optimizer是一组规则的集合，这些规则试图通过向下推理谓词（predicates）或选择（selections）来优化逻辑计划。软件包可以扩展Catalyst，以包括其用于特定领域优化的规则。 Physical Planning 物理规划After successfully creating an optimized logical plan, Spark then begins the physical planning process. The physical plan, often called a Spark plan, specifies how the logical plan will execute on the cluster by generating different physical execution strategies and comparing them through a cost model, as depicted in Figure 4-3. An example of the cost comparison might be choosing how to perform a given join by looking at the physical attributes of a given table (how big the table is or how big its partitions are) 成功创建优化的逻辑计划后，Spark然后开始物理计划过程。物理计划通常称为Spark计划，它通过生成不同的物理执行策略并通过成本模型进行比较来指定逻辑计划在集群上的执行方式，如图4-3所示。成本比较的一个示例可能是通过查看给定表的物理属性（表的大小或分区的大小）来选择如何执行给定的连接。 Physical planning results in a series of RDDs and transformations. This result is why you might have heard Spark referred to as a compiler—it takes queries in DataFrames, Datasets, and SQL and compiles them into RDD transformations for you. 物理规划会导致一系列的RDD和转换。这个结果就是为什么您可能听说过Spark称为编译器的原因——它接受DataFrames，Datasets和SQL中的查询，然后将它们编译为RDD转换。 Execution 执行Upon selecting a physical plan, Spark runs all of this code over RDDs, the lower-level programming interface of Spark (which we cover in Part III). Spark performs further optimizations at runtime, generating native Java bytecode that can remove entire tasks or stages during execution. Finally the result is returned to the user. 选择了物理计划后，Spark将在RDD（Spark的较底层编程接口）上运行所有这些代码（我们将在第III部分中介绍）。 Spark在运行时执行进一步的优化，生成本地Java字节码，可以在执行过程中删除整个任务或阶段。最后，结果返回给用户。 Conclusion 结论In this chapter, we covered Spark Structured APIs and how Spark transforms your code into what will physically execute on the cluster. In the chapters that follow, we cover core concepts and how to use the key functionality of the Structured APIs. 在本章中，我们介绍了Spark结构化API，以及Spark如何将您的代码转换为将在集群上实际执行的代码。在接下来的章节中，我们将介绍核心概念以及如何使用结构化API的关键功能。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 29 Unsupervised Learning]]></title>
    <url>%2F2019%2F08%2F05%2FChapter29-Unsupervised-Learning(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 29 Unsupervised Learning 译者：https://snaildove.github.ioThis chapter will cover the details of Spark’s available tools for unsupervised learning, focusing specifically on clustering. Unsupervised learning is, generally speaking, used less often than supervised learning because it’s usually harder to apply and measure success (from an end-result perspective). These challenges can become exacerbated at scale. For instance, clustering in high dimensional space can create odd clusters simply because of the properties of high-dimensional spaces, something referred to as the curse of dimensionality . The curse of dimensionality describes the fact that as a feature space expands in dimensionality, it becomes increasingly sparse. This means that the data needed to fill this space for statistically meaningful results increases rapidly with any increase in dimensionality. Additionally, with high dimensions comes more noise in the data. This, in turn, may cause your model to hone in on noise instead of the true factors causing a particular result or grouping. Therefore in the model scalability table, we include computational limits, as well as a set of statistical recommendations. These are heuristics and should be helpful guides, not requirements. 本章将详细介绍Spark的可用于无监督学习的工具，重点是集群。一般来说，无监督学习的使用频率比有监督学习的频率要低，因为无监督学习通常很难应用和衡量成功（从最终结果的角度来看）。这些挑战可能会在规模上加剧。例如，高维空间中的聚类可以仅仅由于高维空间的特性（称为维数的诅咒）而创建奇数簇。维度的诅咒描述了一个事实，即随着特征空间维度的扩展，它变得越来越稀疏。这意味着，填充该空间以获取具有统计意义的结果所需的数据会随着维度的增加而迅速增加。此外，尺寸越大，数据中的噪声越多。反过来，这可能会导致模型陷入噪音，而不是导致特定结果或分组的真实因素。因此，在模型可伸缩性表中，我们包括计算限制以及一组统计建议。这些是试探法，应该是有用的指南，而不是要求。 At its core, unsupervised learning is trying to discover patterns or derive a concise representation of the underlying structure of a given dataset. 本质上，无监督学习试图发现模式或派生给定数据集的基础结构的简洁表示。 Use Cases 用户案例Here are some potential use cases. At its core, these patterns might reveal topics, anomalies, or groupings in our data that may not have been obvious beforehand: 这里是一些潜在的用例。从根本上讲，这些模式可能会揭示我们数据中可能事先不明显的主题，异常或分组： Finding anomalies in data 在数据中找异常值 If the majority of values in a dataset cluster into a larger group with several small groups on the outside, those groups might warrant further investigation.如果数据集中的大多数值聚集到一个较大的组中，外部又有几个小组，则这些组可能需要进一步调查。 Topic modeling 主题模型 By looking at large bodies of text, it is possible to find topics that exist across those different documents. 通过查看大量的正文，可以找到这些不同文档中存在的主题。 Model Scalability 模型的伸缩性Just like with our other models, it’s important to mention the basic model scalability requirements along with statistical recommendations. 与其他模型一样，重要的是要提及基本模型可扩展性要求以及统计建议。 Table 29-1. Clustering model scalability reference 集群模型可伸缩性参考 Model Statistical recommendation Computation limits Training examples k-means 50 to 100 maximum Features x clusters &lt; 10 million最大特征群小于1000,0000万 No limit Bisecting k-means 50 to 100 maximum Features x clusters &lt; 10 million最大特征群小于1000,0000万 No limit GMM 50 to 100 maximum Features x clusters &lt; 10 million最大特征群小于1000,0000万 No limit LDA An interpretable number可解释的数字 1,000s of topics千数理级的主题 No limit Let’s get started by loading some example numerical data: 让我们从加载一些示例数字数据开始： 12345678910111213// in Scalaimport org.apache.spark.ml.feature.VectorAssemblerval va = new VectorAssembler().setInputCols(Array("Quantity", "UnitPrice")).setOutputCol("features")val sales = va.transform(spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/data/retail-data/by-day/*.csv").limit(50).coalesce(1).where("Description IS NOT NULL"))sales.cache() 12345678910111213# in Pythonfrom pyspark.ml.feature import VectorAssemblerva = VectorAssembler()\.setInputCols(["Quantity", "UnitPrice"])\.setOutputCol("features")sales = va.transform(spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/data/retail-data/by-day/*.csv").limit(50).coalesce(1).where("Description IS NOT NULL"))sales.cache() k-means K均值-means is one of the most popular clustering algorithms. In this algorithm, a user-specified number of clusters () are randomly assigned to different points in the dataset. The unassigned points are then “assigned” to a cluster based on their proximity (measured in Euclidean distance) to the previously assigned point. Once this assignment happens, the center of this cluster (called the centroid) is computed, and the process repeats. All points are assigned to a particular centroid, and a new centroid is computed. We repeat this process for a finite number of iterations or until convergence (i.e., when our centroid locations stop changing). This does not, however, mean that our clusters are always sensical. For instance, a given “logical” cluster of data might be split right down the middle simply because of the starting points of two distinct clusters. Thus, it is often a good idea to perform multiple runs of -means starting with different initializations. -means是最流行的聚类算法之一。在此算法中，将用户指定数量的聚类（）随机分配给数据集中的不同点。然后根据未分配点与先前分配点的接近度（以欧几里德距离测量）将它们“分配”到聚类。一旦发生这种分配，就会计算出该簇的中心（称为质心），然后重复该过程。将所有点分配给特定的质心，并计算一个新的质心。我们将这个过程重复进行有限的迭代或直到收敛为止（即，当我们的质心位置停止更改时）。但是，这并不意味着我们的集群总是明智的。例如，一个给定的“逻辑”数据集群可能仅由于两个不同集群的起点而在中间被拆分。因此，从不同的初始化开始执行多次-means通常是一个好主意。 Choosing the right value for is an extremely important aspect of using this algorithm successfully, as well as a hard task. There’s no real prescription for the number of clusters you need, so you’ll likely have to experiment with different values and consider what you would like the end result to be. For more information on -means, see ISL 10.3 and ESL 14.3. 选择正确的值是成功使用此算法极为重要的方面，也是一项艰巨的任务。对于所需的群集数量并没有真正的规定，因此您可能必须尝试使用不同的值并考虑最终结果是什么。有关-means的更多信息，请参见 ISL 10.3 和 ESL 14.3。 Model Hyperparameters 模型的超参数These are configurations that we specify to determine the basic structure of the model: 我们指定了以下这些配置来确定模型的基本结构： This is the number of clusters that you would like to end up with. 这是您最终希望使用的集群数量。 Training Parameters 训练参数initModeThe initialization mode is the algorithm that determines the starting locations of the centroids. The supported options are random and -means|| (the default). The latter is a parallelized variant of the -means|| method. While the details are not within the scope of this book, the thinking behind the latter method is that rather than simply choosing random initialization locations, the algorithm chooses cluster centers that are already well spread out to generate a better clustering. 初始化模式是确定质心起始位置的算法。支持的选项是random和 -means||。（默认）。后者是 -means|| 的并行变体。方法。尽管细节不在本书的讨论范围之内，但后一种方法的思想是，该算法不仅选择随机初始化位置，还选择分布良好的聚类中心，以生成更好的聚类。 initStepsThe number of steps for -means|| initialization mode. Must be greater than 0. (The default value is 2.) -means|| 的步骤数初始化模式。必须大于0。（默认值为2。） maxIterTotal number of iterations over the data before stopping. Changing this probably won’t change your results a ton, so don’t make this the first parameter you look to adjust. The default is 20. 停止之前，数据上的迭代总数。更改此设置可能不会使您的结果大为改变，因此请不要将其设为您要调整的第一个参数。默认值为20。 tol Specifies a threshold by which changes in centroids show that we optimized our model enough, and can stop iterating early, before maxIter iterations. The default value is 0.0001. 指定一个阈值，通过该阈值质心的变化可以表明我们已经充分优化了模型，并且可以在maxIter迭代之前尽早停止迭代。默认值为0.0001。 This algorithm is generally robust to these parameters, and the main trade-off is that running more initialization steps and iterations may lead to a better clustering at the expense of longer training time: 该算法通常对这些参数具有鲁棒性，并且主要的权衡是，运行更多的初始化步骤和迭代可能会导致更好的聚类，但需要更长的训练时间： Example12345// in Scalaimport org.apache.spark.ml.clustering.KMeansval km = new KMeans().setK(5)println(km.explainParams())val kmModel = km.fit(sales) 12345# in Pythonfrom pyspark.ml.clustering import KMeanskm = KMeans().setK(5)print km.explainParams()kmModel = km.fit(sales) k-means Metrics Summary k-means衡量指标简述-means includes a summary class that we can use to evaluate our model. This class provides some common measures for -means success (whether these apply to your problem set is another question). The -means summary includes information about the clusters created, as well as their relative sizes (number of examples). -means包括一个摘要类，可用于评估模型。此类提供了一些用于-means成功的常用措施（这些措施是否适用于您的问题集是另一个问题）。-means摘要包括有关创建的集群及其相对大小（示例数）的信息。 We can also compute the within set sum of squared errors, which can help measure how close our values are from each cluster centroid, using computeCost. The implicit goal in -means is that we want to minimize the within set sum of squared error, subject to the given number of clusters: 我们还可以计算出平方误差的集合内总和，这可以使用computeCost帮助测量值与每个聚类质心的接近程度。-means中的隐式目标是，在给定数量的簇的情况下，我们希望最小化平方误差的设置和之内： 123456// in Scalaval summary = kmModel.summarysummary.clusterSizes // number of pointskmModel.computeCost(sales)println("Cluster Centers: ")kmModel.clusterCenters.foreach(println) 12345678# in Pythonsummary = kmModel.summaryprint summary.clusterSizes # number of pointskmModel.computeCost(sales)centers = kmModel.clusterCenters()print("Cluster Centers: ")for center in centers:print(center) Bisecting k-means二等分K均值Bisecting -means is a variant of -means. The core difference is that instead of clustering points by starting “bottom-up” and assigning a bunch of different groups in the data, this is a top-down clustering method. This means that it will start by creating a single group and then splitting that groupinto smaller groups in order to end up with the number of clusters specified by the user. This is usually a faster method than -means and will yield different results. 均分-means是-means的变体。核心区别在于，这是一种通过自上而下的聚类方法，而不是通过“自下而上”开始并在数据中分配一堆不同的组来聚类点。这意味着它将首先创建一个组，然后将该组分成较小的组，以得到用户指定的群集数。这通常是比-means更快的方法，并且会产生不同的结果。 Model Hyperparameters 模型参数These are configurations that we specify to determine the basic structure of the model: 我们指定了以下这些配置来确定模型的基本结构： ​ This is the number of clusters that you would like to end up with ​ 这是您最终希望使用的集群数量 Training Parameters 训练参数minDivisibleClusterSizeThe minimum number of points (if greater than or equal to 1.0) or the minimum proportion of points (if less than 1.0) of a divisible cluster. The default is 1.0, meaning that there must be at least one point in each cluster. 可整类的最小点数（如果大于或等于1.0）或最小比例点（如果小于1.0）。默认值为1.0，这意味着每个群集中至少必须有一个点。 maxIterTotal number of iterations over the data before stopping. Changing this probably won’t change your results a ton, so don’t make this the first parameter you look to adjust. The default is 20. 停止之前，数据上的迭代总数。更改此设置可能不会使您的结果大为改变，因此请不要将其设为您要调整的第一个参数。默认值为20。 Most of the parameters in this model should be tuned in order to find the best result. There’s no rule that applies to all datasets. 该模型中的大多数参数都应进行调整以找到最佳结果。没有适用于所有数据集的规则。 Example12345// in Scalaimport org.apache.spark.ml.clustering.BisectingKMeansval bkm = new BisectingKMeans().setK(5).setMaxIter(5)println(bkm.explainParams())val bkmModel = bkm.fit(sales) 1234# in Pythonfrom pyspark.ml.clustering import BisectingKMeansbkm = BisectingKMeans().setK(5).setMaxIter(5)bkmModel = bkm.fit(sales) Bisecting k-means Summary 二等分k-means简述Bisecting -means includes a summary class that we can use to evaluate our model, that is largely the same as the -means summary. This includes information about the clusters created, as well as their relative sizes (number of examples): 二等分-means包括一个摘要类，我们可以使用该类来评估我们的模型，该类与-means摘要大致相同。这包括有关创建的集群及其相对大小（示例数）的信息： 12345// in Scalaval summary = bkmModel.summarysummary.clusterSizes // number of pointskmModel.computeCost(sales)println("Cluster Centers: ")kmModel.clusterCenters.foreach(println) 12345678# in Pythonsummary = bkmModel.summaryprint summary.clusterSizes # number of pointskmModel.computeCost(sales)centers = kmModel.clusterCenters()print("Cluster Centers: ")for center in centers:print(center) Gaussian Mixture ModelsGaussian mixture models (GMM) are another popular clustering algorithm that makes different assumptions than bisecting -means or -means do. Those algorithms try to group data by reducing the sum of squared distances from the center of the cluster. Gaussian mixture models, on the other hand, assume that each cluster produces data based upon random draws from a Gaussian distribution. This means that clusters of data should be less likely to have data at the edge of the cluster (reflected in the Guassian distribution) and much higher probability of having data in the center. Each Gaussian cluster can be of arbitrary size with its own mean and standard deviation (and hence a possibly different, ellipsoid shape). There are still user-specified clusters that will be created during training. 高斯混合模型（GMM）是另一种流行的聚类算法，与将-means或-means分为两等分相比，它做出了不同的假设。这些算法尝试通过减少距群集中心的平方距离之和来对数据进行分组。另一方面，高斯混合模型假设每个聚类基于来自高斯分布的随机抽取生成数据。这意味着数据集群在集群边缘的数据（在高斯分布中反映）的可能性应该较小，而在中心拥有数据的可能性则更高。每个高斯聚类可以具有任意大小，具有自己的均值和标准差（因此可能是不同的椭圆形）。在培训期间仍将创建用户指定的群集。 A simplified way of thinking about Gaussian mixture models is that they’re like a soft version of means. -means creates very rigid clusters—each point is only within one cluster. GMMs allow for a more nuanced cluster associated with probabilities, instead of rigid boundaries. 考虑高斯混合模型的一种简化方法是，它们就像均值的软版本。-means创建非常严格的群集-每个点仅在一个群集内。GMM允许与概率相关的更细微的簇，而不是严格的边界。 For more information, see ISL 14.3. 更多信息，请查看 ISL 14.3. Model Hyperparameters 模型参数These are configurations that we specify to determine the basic structure of the model: 我们指定这些配置来确定模型的基本结构： ​ This is the number of clusters that you would like to end up with. ​ 这是您最终希望使用的集群数量。 Training Parameters 训练参数maxIterTotal number of iterations over the data before stopping. Changing this probably won’t change your results a ton, so don’t make this the first parameter you look to adjust. The default is 100.停止之前，数据上的迭代总数。更改此设置可能不会使您的结果大为改变，因此请不要将其设为您要调整的第一个参数。默认值为100。 tolThis value simply helps us specify a threshold by which changes in parameters show that we optimized our weights enough. A smaller value can lead to higher accuracy at the cost of performing more iterations (although never more than maxIter). The default value is 0.01.该值只是简单地帮助我们指定一个阈值，通过该阈值参数的变化表明我们已经充分优化了权重。较小的值可以以执行更 多迭代为代价提高精度（尽管绝不超过maxIter）。默认值为0.01。 As with our -means model, these training parameters are less likely to have an impact than the number of clusters, . 与我们的-means模型一样，这些训练参数产生影响的可能性要小于聚类的数量。 Example12345// in Scalaimport org.apache.spark.ml.clustering.GaussianMixtureval gmm = new GaussianMixture().setK(5)println(gmm.explainParams())val model = gmm.fit(sales) 12345# in Pythonfrom pyspark.ml.clustering import GaussianMixturegmm = GaussianMixture().setK(5)print gmm.explainParams()model = gmm.fit(sales) Gaussian Mixture Model Summary 高斯混合模型简述Like our other clustering algorithms, Gaussian mixture models include a summary class to help with model evaluation. This includes information about the clusters created, like the weights, the means, and the covariance of the Gaussian mixture, which can help us learn more about the underlying structure inside of our data: 与我们的其他聚类算法一样，高斯混合模型包括摘要类，以帮助模型评估。这包括有关创建的聚类的信息，例如权重，均值和高斯混合的协方差，这些信息可以帮助我们进一步了解数据内部的基础结构： 1234567// in Scalaval summary = model.summarymodel.weightsmodel.gaussiansDF.show()summary.cluster.show()summary.clusterSizessummary.probability.show() 1234567# in Pythonsummary = model.summaryprint model.weightsmodel.gaussiansDF.show()summary.cluster.show()summary.clusterSizessummary.probability.show() Latent Dirichlet Allocation 隐式狄利克雷分布Latent Dirichlet Allocation (LDA) is a hierarchical clustering model typically used to perform topic modelling on text documents. LDA tries to extract high-level topics from a series of documents and keywords associated with those topics. It then interprets each document as having a variable number of contributions from multiple input topics. There are two implementations that you can use: online LDA and expectation maximization. In general, online LDA will work better when there are more examples, and the expectation maximization optimizer will work better when there is a larger input vocabulary. This method is also capable of scaling to hundreds or thousands of topics. 潜在狄利克雷分配（LDA）是一种层次结构的聚类模型，通常用于对文本文档执行主题建模。LDA尝试从一系列与这些主题相关的文档和关键字中提取高级主题。然后，它将每个文档解释为具有来自多个输入主题的不同数量的文稿。您可以使用两种实现：在线LDA和期望最大化。通常，当有更多示例时，在线LDA会更好地工作；而在输入词汇量更大的情况下，期望最大化优化器也将更好地工作。此方法还可以扩展到数百或数千个主题。 To input our text data into LDA, we’re going to have to convert it into a numeric format. You can use the CountVectorizer to achieve this. 要将文本数据输入到LDA中，我们必须将其转换为数字格式。您可以使用CountVectorizer来实现。 Model Hyperparameters 模型参数These are configurations that we specify to determine the basic structure of the model: 我们指定这些配置来确定模型的基本结构： ​ The total number of topics to infer from the data. The default is 10 and must be a positive number. ​ 从数据推断出的主题总数。默认值为10，并且必须为正数。 docConcentrationConcentration parameter (commonly named “alpha”) for the prior placed on documents’ distributions over topics (“theta”). This is the parameter to a Dirichlet distribution, where larger values mean more smoothing (more regularization).优先级的浓度参数（通常称为“ alpha”）放在文档的主题分布（“ theta”）上。这是Dirichlet分布的参数，其中较大的值表示更平滑（更规则化）。 If not set by the user, then docConcentration is set automatically. If set to singleton vector [alpha], then alpha is replicated to a vector of length k in fitting. Otherwise, the docConcentration vector must be length .如果用户未设置，则将自动设置docConcentration。如果设置为单例向量α，则将α复制到拟合中长度为k的向量。否则，docConcentration向量必须为length。 如果用户未设置，则将自动设置docConcentration。如果设置为单例向量α，则将α复制到拟合中长度为k的向量。否则，docConcentration向量必须为length。 topicConcentrationThe concentration parameter (commonly named “beta” or “eta”) for the prior placed on a topic’s distributions over terms. This is the parameter to a symmetric Dirichlet distribution. If not set by the user, then topicConcentration is set automatically.优先事项的浓度参数（通常称为“ beta”或“ eta”），位于主题的各个术语的分布中。这是对称Dirichlet分布的参数。如果用户未设置，则topicConcentration会自动设置。 Training Parameters 训练参数maxIterTotal number of iterations over the data before stopping. Changing this probably won’t change your results a ton, so don’t make this the first parameter you look to adjust. The default is 20.停止之前，数据上的迭代总数。更改此设置可能不会使您的结果大为改变，因此请不要将其设为您要调整的第一个参数。默认值为20。 optimizerThis determines whether to use EM or online training optimization to determine the LDA model. The default is online.这确定是使用EM还是在线培训优化来确定LDA模型。默认为在线。 learningDecayLearning rate, set as an exponential decay rate. This should be between (0.5, 1.0] to guarantee asymptotic convergence. The default is 0.51 and only applies to the online optimizer.学习率，设置为指数衰减率。此值应介于（0.5，1.0]之间以确保渐近收敛。默认值为0.51，仅适用于在线优化器。 learningOffsetA (positive) learning parameter that downweights early iterations. Larger values make early iterations count less. The default is 1,024.0 and only applies to the online optimizer.一个（正）学习参数，可以减轻早期迭代的负担。较大的值使早期迭代的计数减少。默认值为1,024.0，仅适用于在线优化器。 optimizeDocConcentrationIndicates whether the docConcentration (Dirichlet parameter for document-topic distribution) will be optimized during training. The default is true but only applies to the online optimizer. 指示在培训期间是否将优化docConcentration（用于文档主题分发的Dirichlet参数）。默认值为true，但仅适用于在线优化器。 subsamplingRateThe fraction of the corpus to be sampled and used in each iteration of mini-batch gradient descent, in range (0, 1]. The default is 0.5 and only applies to the online optimizer.在小批量梯度下降的每次迭代中要采样和使用的语料库分数，范围为（0，1]。默认值为0.5，仅适用于在线优化器。 seedThis model also supports specifying a random seed for reproducibility.该模型还支持指定可重复性的随机种子。 checkpointIntervalThis is the same checkpoint feature that we saw in Chapter 26. 这是我们在第26章中看到的相同的检查点功能。 Prediction Parameters 预测参数topicDistributionColThe column that will hold the output of the topic mixture distribution for each document.该列将保存每个文档的主题混合分布的输出。 Example12345678910111213// in Scalaimport org.apache.spark.ml.feature.&#123;Tokenizer, CountVectorizer&#125;val tkn = new Tokenizer().setInputCol("Description").setOutputCol("DescOut")val tokenized = tkn.transform(sales.drop("features"))val cv = new CountVectorizer().setInputCol("DescOut").setOutputCol("features").setVocabSize(500).setMinTF(0).setMinDF(0).setBinary(true)val cvFitted = cv.fit(tokenized)val prepped = cvFitted.transform(tokenized) 123456789101112# in Pythonfrom pyspark.ml.feature import Tokenizer, CountVectorizertkn = Tokenizer().setInputCol("Description").setOutputCol("DescOut")tokenized = tkn.transform(sales.drop("features"))cv = CountVectorizer()\.setInputCol("DescOut")\.setOutputCol("features")\.setVocabSize(500)\.setMinTF(0)\.setMinDF(0)\.setBinary(True)cvFitted = cv.fit(tokenized)prepped = cvFitted.transform(tokenized) 12345// in Scalaimport org.apache.spark.ml.clustering.LDAval lda = new LDA().setK(10).setMaxIter(5)println(lda.explainParams())val model = lda.fit(prepped) 12345# in Pythonfrom pyspark.ml.clustering import LDAlda = LDA().setK(10).setMaxIter(5)print lda.explainParams()model = lda.fit(prepped) After we train the model, you will see some of the top topics. This will return the term indices, and we’ll have to look these up using the CountVectorizerModel that we trained in order to find out the true words. For instance, when we trained on the data our top 3 topics were hot, home, and brown after looking them up in our vocabulary: 训练模型后，您将看到一些热门话题。这将返回术语索引，我们必须使用我们训练的CountVectorizerModel来查找这些索引，以便找出真实的单词。例如，当我们对数据进行培训时，在我们的词汇表中查找它们之后，我们的前3个主题是热门，家和棕色： 123// in Scalamodel.describeTopics(3).show()cvFitted.vocabulary 123# in Pythonmodel.describeTopics(3).show()cvFitted.vocabulary These methods result in detailed information about the vocabulary used as well as the emphasis on particular terms. These can be helpful for better understanding the underlying topics. Due to space constraints, we can’t show this output. Using similar APIs, we can get some more technical measures like the log likelihood and perplexity. The goal of these tools is to help you optimize the number of topics, based on your data. When using perplexity in your success criteria, you should apply these metrics to a holdout set to reduce the overall perplexity of the model. Another option is to optimize to increase the log likelihood value on the holdout set. We can calculate each of these by passing a dataset into the following functions: model.logLikelihood and model.logPerplexity. 这些方法可提供有关所用词汇的详细信息以及对特定术语的强调。这些有助于更好地理解基础主题。由于篇幅所限，我们无法显示此输出。使用类似的API，我们可以获得更多技术指标，例如对数可能性和困惑度。这些工具的目的是帮助您根据数据优化主题数。在成功标准中使用困惑度时，应将这些指标应用于保留集，以减少模型的总体困惑度。另一个选择是优化以增加保留集上的对数似然值。我们可以通过将数据集传递给以下函数来计算每个参数：model.logLikelihood和model.logPerplexity。 Conclusion 总结This chapter covered the most popular algorithms that Spark includes for unsupervised learning. The next chapter will bring us out of MLlib and talk about some of the advanced analytics ecosystem that has grown outside of Spark. 本章介绍了Spark包含的用于无监督学习的最受欢迎的算法。下一章将使我们脱离MLlib，并讨论一些Spark以外的高级分析生态系统。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 15 How Spark Runs on a Cluster]]></title>
    <url>%2F2019%2F08%2F05%2FChapter15_HowSparkRuns-on-a-Cluster(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 15 How Spark Runs on a Cluster Spark如何在集群上的运行译者：https://snaildove.github.ioThus far in the book, we focused on Spark’s properties as a programming interface. We have discussed how the structured APIs take a logical operation, break it up into a logical plan, and convert that to a physical plan that actually consists of Resilient Distributed Dataset (RDD) operations that execute across the cluster of machines. This chapter focuses on what happens when Spark goes about executing that code. We discuss this in an implementation-agnostic way—this depends on neither the cluster manager that you’re using nor the code that you’re running. At the end of the day, all Spark code runs the same way. 到目前为止，在书中，我们将重点放在Spark作为编程接口的属性上。我们已经讨论了结构化API如何执行逻辑操作，将其分解为逻辑计划，并将其转换为实际由跨机器集群执行的弹性分布式数据集（RDD）操作组成的物理计划。本章主要讨论 Spark 执行该代码时会发生什么。我们以一种不知实现的方式讨论这个问题，这既不依赖于您正在使用的集群管理器，也不依赖于您正在运行的代码。一天结束时，所有 Spark 代码都以相同的方式运行。 This chapter covers several key topics:本章包括几个关键主题：- The architecture and components of a Spark ApplicationSpark应用程序的体系结构和组件- The life cycle of a Spark Application inside and outside of Spark Spark 内外 Spark 应用的生命周期- Important low-level execution properties, such as pipelining 重要的低级执行属性，如管道- What it takes to run a Spark Application, as a segue into Chapter 16. 运行Spark应用程序需要什么，作为转到第16章的桥接。Let’s begin with the architecture.让我们从架构开始## The Architecture of a Spark Application Spark应用程序的架构In Chapter 2, we discussed some of the high-level components of a Spark Application. Let’s review those again:在第2章中，我们讨论了 Spark 应用程序的一些高级组件。让我们再次回顾一下：The Spark driver Spark驱动器The driver is the process “in the driver seat” of your Spark Application. It is the controller of the execution of a Spark Application and maintains all of the state of the Spark cluster (the state and tasks of the executors). It must interface with the cluster manager in order to actually get physical resources and launch executors. At the end of the day, this is just a process on a physical machine that is responsible for maintaining the state of the application running on the cluster.驱动器是 Spark 应用程序处在“驾驶员席位”的进程。它是Spark应用程序执行的控制器，维护Spark集群的所有状态（执行器的状态和任务）。它必须与群集管理器接口，以便实际获得物理资源和启动执行器。最后，这只是一个物理机器上的进程，负责维护集群上运行的应用程序的状态。The Spark executors Spark执行器Spark executors are the processes that perform the tasks assigned by the Spark driver. Executors have one core responsibility: take the tasks assigned by the driver, run them, and report back their state (success or failure) and results. Each Spark Application has its own separate executor processes.Spark 执行器是执行 Spark 驱动程序分配的任务的进程。执行者有一个核心责任：承担驱动程序分配的任务，运行它们，并报告它们的状态（成功或失败）和结果。每个Spark应用程序都有自己的独立执行器进程。The cluster manager 集群管理员The Spark Driver and Executors do not exist in a void, and this is where the cluster manager comes in. The cluster manager is responsible for maintaining a cluster of machines that will run your Spark Application(s). Somewhat confusingly, a cluster manager will have its own “driver” (sometimes called master) and “worker” abstractions. The core difference is that these are tied to physical machines rather than processes (as they are in Spark). Figure 15-1 shows a basic cluster setup. The machine on the left of the illustration is the Cluster Manager Driver Node. The circles represent daemon processes running on and managing each of the individual worker nodes. There is no Spark Application running as of yet—these are just the processes from the cluster manager.Spark驱动程序和执行器不存在于一个空间，这就是集群管理器所处的位置。集群管理器负责维护运行Spark应用程序的机器集群。有些令人困惑的是，集群管理器将有自己的“驱动程序（driver）”（有时称为master）和“工作者（worker）”的抽象结构。核心区别在于，它们与物理机器而不是进程（如 Spark 中的进程）联系在一起。 图15-1显示了一个基本的集群设置。图左侧的机器是群集管理器驱动程序节点。圆圈表示运行在每个工作节点上并管理每个工作节点的守护进程。到目前为止还没有运行spark应用程序，这些只是来自集群管理器的进程。When it comes time to actually run a Spark Application, we request resources from the cluster manager to run it. Depending on how our application is configured, this can include a place to run the Spark driver or might be just resources for the executors for our Spark Application. Over the course of Spark Application execution, the cluster manager will be responsible for managing the underlying machines that our application is running on.在实际运行Spark应用程序时，我们从集群管理器请求资源来运行它。根据应用程序的配置方式，这可能包括一个运行Spark驱动程序的位置，或者可能只是Spark应用程序的执行者的资源。在Spark应用程序执行过程中，集群管理员将负责管理应用程序运行的底层机器。Spark currently supports three cluster managers: a simple built-in standalone cluster manager, Apache Mesos, and Hadoop YARN. However, this list will continue to grow, so be sure to check the documentation for your favorite cluster manager. Now that we’ve covered the basic components of an application, let’s walk through one of the first choices you will need to make when running your applications: choosing the execution mode.Spark目前支持三个集群管理器：一个简单的内置独立集群管理器、Apache Mesos 和 Hadoop Yarn。但是，这个列表将继续增长，因此一定要检查您最喜欢的集群管理器的文档。既然我们已经介绍了应用程序的基本组件，那么让我们来看看在运行应用程序时需要做的第一个选择：选择执行模式。### Execution Modes 执行模式An execution mode gives you the power to determine where the aforementioned resources are physically located when you go to run your application. You have three modes to choose from:执行模式使您能够在运行应用程序时确定上述资源的物理位置。您有三种模式可供选择：- Cluster mode- Client mode- Local modeWe will walk through each of these in detail using Figure 15-1 as a template. In the following section, rectangles with solid borders represent Spark driver process whereas those with dotted borders represent the executor processes.我们将使用图15-1作为模板详细介绍每种方法。在下面的部分中，带实心边框的矩形表示 Spark 驱动程序进程，而带虚线边框的矩形表示执行程序进程。#### Cluster mode 集群模式Cluster mode is probably the most common way of running Spark Applications. In cluster mode, a user submits a pre-compiled JAR, Python script, or R script to a cluster manager. The cluster manager then launches the driver process on a worker node inside the cluster, in addition to the executor processes. This means that the cluster manager is responsible for maintaining all Spark Application–related processes. Figure 15-2 shows that the cluster manager placed our driver on a worker node and the executors on other worker nodes.集群模式可能是运行Spark应用程序的最常见方式。在集群模式下，用户向集群管理器提交预编译的JAR、Python脚本或R脚本。然后，除了执行器进程之外，集群管理员在集群内的工作节点上启动驱动程序进程。这意味着集群管理员负责维护所有与Spark应用程序相关的流程。图15-2显示集群管理器将我们的驱动程序放在一个工作节点上，而执行器放在其他工作节点上。#### Client mode 客户端模式Client mode is nearly the same as cluster mode except that the Spark driver remains on the client machine that submitted the application. This means that the client machine is responsible for maintaining the Spark driver process, and the cluster manager maintains the executor processses. In Figure 15-3, we are running the Spark Application from a machine that is not colocated on the cluster. These machines are commonly referred to as gateway machines or edge nodes. In Figure 15-3, you can see that the driver is running on a machine outside of the cluster but that the workers are located on machines in the cluster.客户端模式与集群模式几乎相同，只是Spark驱动程序保留在提交应用程序的客户端上。这意味着客户端负责维护Spark 驱动程序进程，集群管理员维护执行器进程。在图15-3中，我们运行的Spark应用程序来自一台未在集群上并置的机器。这些机器通常被称为网关机器或边缘节点。在图15-3中，您可以看到驱动程序（driver）在集群外部的一台机器上运行，但工作人员（worker）位于集群中的机器上。#### Local mode 本地模式Local mode is a significant departure from the previous two modes: it runs the entire Spark Application on a single machine. It achieves parallelism through threads on that single machine. This is a common way to learn Spark, to test your applications, or experiment iteratively with local development. However, we do not recommend using local mode for running production applications.本地模式与前两种模式有很大的不同：它在一台机器上运行整个Spark应用程序。它通过单个机器上的线程实现并行性。这是学习Spark、测试应用程序或使用本地开发进行迭代实验的常用方法。但是，我们不建议在运行生产应用程序时使用本地模式。## The Life Cycle of a Spark Application (Outside Spark) Spark 应用的生命周期（Spark外部）This chapter has thus far covered the vocabulary necessary for discussing Spark Applications. It’s now time to talk about the overall life cycle of Spark Applications from “outside” the actual Spark code. We will do this with an illustrated example of an application run with spark-submit (introduced in Chapter 3). We assume that a cluster is already running with four nodes, a driver (not a Spark driver but cluster manager driver) and three worker nodes. The actual cluster manager does not matter at this point: this section uses the vocabulary from the previous section to walk through a step-by-step Spark Application life cycle from initialization to program exit.本章迄今为止涵盖了讨论 Spark 应用程序所需的词汇。现在是时候从实际的 Spark 代码“外部”来讨论 Spark 应用程序的整个生命周期了。我们将通过一个使用 spark-submit 运行的应用程序的示例（在第3章中介绍）来实现这一点。我们假设一个集群已经运行了四个节点、一个驱动程序（不是 Spark 驱动程序，而是集群管理器驱动程序）和三个工作节点。此时，实际的集群管理器并不重要：本节使用上一节中的词汇表逐步遍历从初始化到程序退出的 Spark 应用程序生命周期。—NOTE 注意This section also makes use of illustrations and follows the same notation that we introduced previously. Additionally, we now introduce lines that represent network communication. Darker arrows represent communication by Spark or Spark related processes, whereas dashed lines represent more general communication (like cluster management communication).本节还使用了插图，并遵循我们前面介绍的相同的符号。此外，我们现在引入表示网络通信的线。较暗的箭头表示通过 Spark 或 Spark 相关进程进行的通信，而虚线表示更一般的通信（如集群管理通信）。—### Client Request 客户端请求The first step is for you to submit an actual application. This will be a pre-compiled JAR or library. At this point, you are executing code on your local machine and you’re going to make a request to the cluster manager driver node (Figure 15-4). Here, we are explicitly asking for resources for the Spark driver process only. We assume that the cluster manager accepts this offer and places the driver onto a node in the cluster. The client process that submitted the original job exits and the application is off and running on the cluster.第一步是提交实际的申请。这将是一个预编译的 JAR 或库。此时，您正在本地计算机上执行代码，并将向集群管理员驱动程序节点发出请求（图15-4）。这里，我们明确地要求只为 Spark 驱动程序进程提供资源。我们假设集群管理员接受这个提议，并将驱动程序放在集群中的一个节点上。提交原始作业的客户端进程退出，应用程序在集群上关闭并运行。To do this, you’ll run something like the following command in your terminal:为此，您将在终端中运行如下命令：12345678./bin/spark-submit \--class &lt;main-class&gt; \--master &lt;master-url&gt; \--deploy-mode cluster \--conf &lt;key&gt;=&lt;value&gt; \... # other options&lt;application-jar&gt; \[application-arguments]### Launch 启动（应用程序）Now that the driver process has been placed on the cluster, it begins running user code (Figure 15-5). This code must include a SparkSession that initializes a Spark cluster (e.g., driver + executors). The SparkSession will subsequently communicate with the cluster manager (the darker line), asking it to launch Spark executor processes across the cluster (the lighter lines). The number of executors and their relevant configurations are set by the user via the command-line arguments in the original spark-submit call.现在驱动程序进程已经放置在集群上，它开始运行用户代码（图15-5）。此代码必须包含初始化 Spark 集群的SparkSession（例如，驱动程序+执行器）。SparkSession 随后将与集群管理器（较暗的线）通信，要求它在集群中启动 Spark executor进程（较亮的线）。执行器（executor）的数量及其相关配置由用户通过原始 spark-submit 调用中的命令行参数设置。The cluster manager responds by launching the executor processes (assuming all goes well) and sends the relevant information about their locations to the driver process. After everything is hooked upcorrectly, we have a “Spark Cluster” as you likely think of it today.集群管理器通过启动执行器进程（假设一切正常）进行响应，并将有关其位置的相关信息发送到驱动程序进程。在所有的东西都连接正确之后，我们就有了一个“Spark 集群”，就像你今天想象的那样。### Execution 执行Now that we have a “Spark Cluster,” Spark goes about its merry way executing code, as shown in Figure 15-6. The driver and the workers communicate among themselves, executing code and moving data around. The driver schedules tasks onto each worker, and each worker responds with the status of those tasks and success or failure. (We cover these details shortly.)既然我们有了一个“Spark 集群”，Spark就会以一种愉快的方式执行代码，如图15-6所示。驱动程序和工作人员（workers ）之间进行通信，执行代码并移动数据。驱动程序将任务调度到每个工作人员（workers ）身上，每个工作人员对这些任务的状态以及成功或失败作出响应。（我们将很快介绍这些细节。）### Completion 完成After a Spark Application completes, the driver processs exits with either success or failure (Figure 15-7). The cluster manager then shuts down the executors in that Spark cluster for the driver. At this point, you can see the success or failure of the Spark Application by asking the cluster manager for this information.Spark应用程序完成后，驱动程序进程以成功或失败退出（图15-7）。然后，集群管理员为驱动程序关闭该 Spark 集群中的执行器。此时，通过向集群管理器询问这些信息，您可以看到 Spark 应用程序的成功或失败。## The Life Cycle of a Spark Application (Inside Spark) Spark应用程序的生命周期（Spark内部）We just examined the life cycle of a Spark Application outside of user code (basically the infrastructure that supports Spark), but it’s arguably more important to talk about what happens within Spark when you run an application. This is “user-code” (the actual code that you write that defines your Spark Application). Each application is made up of one or more Spark jobs. Spark jobs within an application are executed serially (unless you use threading to launch multiple actions in parallel).我们刚刚研究了 Spark 应用程序在用户代码之外的生命周期（基本上是支持 Spark 的基础设施），但是讨论运行应用程序时 Spark 内发生的事情可能更重要。这是“用户代码”（定义 Spark 应用程序的实际代码）。每个应用程序由一个或多个 Spark 作业组成。应用程序中的 Spark 作业是串行执行的（除非使用线程并行启动多个操作）。### The SparkSessionThe first step of any Spark Application is creating a SparkSession. In many interactive modes, this is done for you, but in an application, you must do it manually. Some of your legacy code might use the new SparkContext pattern. This should be avoided in favor of the builder method on the SparkSession, which more robustly instantiates the Spark and SQL Contexts and ensures that there is no context conflict, given that there might be multiple libraries trying to create a session in the same Spark Appication:任何Spark应用程序的第一步都是创建 SparkSession。在许多交互模式中，这是为您完成的，但在应用程序中，您必须手动完成。一些遗留代码可能使用新的 SparkContext 模式。应该避免这样做，因为 SparkSession 上的builder方法更能有力地实例化 Spark 和 SQL 上下文，并确保没有上下文冲突，因为可能有多个库试图在同一Spark应用程序中创建会话：1234// Creating a SparkSession in Scalaimport org.apache.spark.sql.SparkSessionval spark = SparkSession.builder().appName("Databricks Spark Example").config("spark.sql.warehouse.dir", "/user/hive/warehouse").getOrCreate()12345# Creating a SparkSession in Pythonfrom pyspark.sql import SparkSessionspark = SparkSession.builder.master("local").appName("Word Count")\.config("spark.some.config.option", "some-value")\.getOrCreate()After you have a SparkSession, you should be able to run your Spark code. From the SparkSession, you can access all of low-level and legacy contexts and configurations accordingly, as well. Note that the SparkSession class was only added in Spark 2.X. Older code you might find would instead directly create a SparkContext and a SQLContext for the structured APIs.在进行 SparkSession 之后，您应该能够运行spark代码。通过 SparkSession，您还可以相应地访问所有低阶和遗留上下文和配置。请注意，SparkSession 类只添加在 Spark 2.x 中。您可能会发现，较旧的代码将直接为结构化API创建 SparkContext 和 sqlContext。#### The SparkContextA SparkContext object within the SparkSession represents the connection to the Spark cluster. This class is how you communicate with some of Spark’s lower-level APIs, such as RDDs. It is commonly stored as the variable sc in older examples and documentation. Through a SparkContext, you can create RDDs, accumulators, and broadcast variables, and you can run code on the cluster. For the most part, you should not need to explicitly initialize a SparkContext; you should just be able to access it through the SparkSession. If you do want to, you should create it in the most general way, through the getOrCreate method:SparkSession 中的 SparkContext 对象表示与 Spark 群集的连接。这个类是如何与Spark的一些低阶API（如RDD）通信的。在旧的示例和文档中，它通常存储为变量 sc 。通过 SparkContext，您可以创建RDD、累加器（accumulators）和广播（broadcast）变量，并且可以在集群上运行代码。在大多数情况下，您不需要显式初始化 SparkContext；您只需要能够通过 SparkSession 访问它。如果您愿意，您应该通过 getOrCreate 方法以最一般的方式创建它：123// in Scalaimport org.apache.spark.SparkContextval sc = SparkContext.getOrCreate()THE SPARKSESSION, SQLCONTEXT, AND HIVECONTEXT In previous versions of Spark, the SQLContext and HiveContext provided the ability to work with DataFrames and Spark SQL and were commonly stored as the variable sqlContext in examples, documentation, and legacy code. As a historical point, Spark 1.X had effectively two contexts. The SparkContext and the SQLContext. These two each performed different things. The former focused on more fine-grained control of Spark’s central abstractions, whereas the latter focused on the higher-level tools like Spark SQL. In Spark 2.X, the communtiy combined the two APIs into the centralized SparkSession that we have today. However, both of these APIs still exist and you can access them via the SparkSession. It is important to note that you should never need to use the SQLContext and rarely need to use the SparkContext. 在Spark的早期版本中，SQLContext 和 HiveContext提供了使用 DataFrame 和 Spark SQL的能力，并且通常作为变量SQLContext存储在示例、文档和旧代码中。作为一个历史点，spark 1.x实际上有两个上下文。SparkContext和SQLContext。这两个人的表现各不相同。前者侧重于对Spark的中心抽象进行更细粒度的控制，而后者则侧重于更高级的工具，如Spark SQL。在spark 2.x中，社区将这两个API合并到了我们今天使用的集中式 SparkSession 中。但是，这两个API仍然存在，您可以通过SparkSession访问它们。需要注意的是，您不应该需要使用 SQLContext，而且很少需要使用 SparkContext。After you initialize your SparkSession, it’s time to execute some code. As we know from previous chapters, all Spark code compiles down to RDDs. Therefore, in the next section, we will take some logical instructions (a DataFrame job) and walk through, step by step, what happens over time.初始化 SparkSession 之后，该执行一些代码了。正如我们从前面的章节所知道的，所有 Spark 代码都编译成RDD。因此，在下一节中，我们将接受一些逻辑指令（一个 DataFrame 作业）并逐步了解随着时间的推移会发生什么。### Logical Instructions 逻辑指令As you saw in the beginning of the book, Spark code essentially consists of transformations and actions. How you build these is up to you—whether it’s through SQL, low-level RDD manipulation, or machine learning algorithms. Understanding how we take declarative instructions like DataFrames and convert them into physical execution plans is an important step to understanding how Spark runs on a cluster. In this section, be sure to run this in a fresh environment (a new Spark shell) to follow along with the job, stage, and task numbers.正如您在书的开头所看到的，Spark代码基本上由转换（transformation ）和动作（action）组成。无论是通过SQL、低阶的RDD操作还是机器学习算法，如何构建这些都取决于您。了解我们如何使用声明性指令（如DataFrame）并将其转换为物理执行计划是了解Spark如何在集群上运行的重要步骤。在本节中，请确保在新的环境（新的 Spark shell）中运行此程序，以跟踪作业（job）、阶段（stage）和任务（task）编号。### Logical instructions to physical execution 物理执行的逻辑指令We mentioned this in Part II, but it’s worth reiterating so that you can better understand how Spark takes your code and actually runs the commands on the cluster. We will walk through some more code, line by line, explain what’s happening behind the scenes so that you can walk away with a better understanding of your Spark Applications. In later chapters, when we discuss monitoring, we will perform a more detailed tracking of a Spark job through the Spark UI. In this current example, we’ll take a simpler approach. We are going to do a three-step job: using a simple DataFrame, we’ll repartition it, perform a value-by-value manipulation, and then aggregate some values and collect the final result.我们在第二部分中提到了这一点，但是值得重申，这样您就可以更好地理解 Spark 是如何使用代码并在集群上实际运行命令的。我们将一行一行地介绍更多的代码，解释幕后发生的事情，以便您能够更好地了解 Spark 应用程序。在后面的章节中，当我们讨论监控时，我们将通过 Spark UI 对 Spark 作业执行更详细的跟踪。在当前的示例中，我们将采用更简单的方法。我们要做一个三步的工作：使用一个简单的数据框架，我们将对它重新分区，执行一个值一个值的操作，然后聚合一些值并收集最终的结果。—NOTE 注意This code was written and runs with Spark 2.2 in Python (you’ll get the same result in Scala, so we’ve omitted it). The number of jobs is unlikely to change drastically but there might be improvements to Spark’s underlying optimizations that change physical execution strategies.这段代码是用 Python 中的 Spark 2.2 编写和运行的（您将在 Scala 中得到相同的结果，所以我们省略了它）。工作数量不太可能大幅度改变，但 Spark 的底层优化可能会有所改进，从而改变物理执行策略。—123456789# in Pythondf1 = spark.range(2, 10000000, 2)df2 = spark.range(2, 10000000, 4)step1 = df1.repartition(5)step12 = df2.repartition(6)step2 = step1.selectExpr("id * 5 as id")step3 = step2.join(step12, ["id"])step4 = step3.selectExpr("sum(id)")step4.collect() # 2500000000000When you run this code, we can see that your action triggers one complete Spark job. Let’s take a look at the explain plan to ground our understanding of the physical execution plan. We can access this information on the SQL tab (after we actually run a query) in the Spark UI, as well:当您运行此代码时，我们可以看到您的操作触发了一个完整的Spark作业。让我们看一下解释计划，以加深我们对实际执行计划的理解。我们可以在Spark UI中的SQL选项卡（在实际运行查询之后）上访问这些信息，以及：1step4.explain()123456789101112131415== Physical Plan ==*HashAggregate(keys=[], functions=[sum(id#15L)])+- Exchange SinglePartition +- *HashAggregate(keys=[], functions=[partial_sum(id#15L)]) +- *Project [id#15L] +- *SortMergeJoin [id#15L], [id#10L], Inner :- *Sort [id#15L ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(id#15L, 200) : +- *Project [(id#7L * 5) AS id#15L] : +- Exchange RoundRobinPartitioning(5) : +- *Range (2, 10000000, step=2, splits=8) +- *Sort [id#10L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(id#10L, 200) +- Exchange RoundRobinPartitioning(6) +- *Range (2, 10000000, step=4, splits=8)What you have when you call collect (or any action) is the execution of a Spark job that individually consist of stages and tasks. Go to localhost:4040 if you are running this on your local machine to see the Spark UI. We will follow along on the “jobs” tab eventually jumping to stages and tasks as we proceed to further levels of detail.当您调用 collect（或任何操作）时，您所拥有的是 Spark作业的执行，它分别由阶段（stage）和任务（task）组成。如果您在本地机器上运行这个程序，请转到 localhost:4040 查看Spark用户界面。我们将继续关注“jobs”选项卡，最终跳到阶段（stage）和任务（task），继续深入到更详细的层次。### A Spark Job 一个Spark作业In general, there should be one Spark job for one action. Actions always return results. Each job breaks down into a series of stages, the number of which depends on how many shuffle operations need to take place.通常，一个动作（action）应该有一个Spark作业。操作始终返回结果。每项工作分为一系列阶段（stage），其数量取决于需要进行多少次洗牌（shuffle）操作。This job breaks down into the following stages and tasks:这项工作分为以下几个阶段（stage）和任务（task）：- Stage 1 with 8 Tasks第1阶段，8个任务- Stage 2 with 8 Tasks第2阶段，8个任务- Stage 3 with 6 Tasks第3阶段有6个任务- Stage 4 with 5 Tasks第4阶段有5个任务- Stage 5 with 200 Tasks第5阶段，200个任务- Stage 6 with 1 Task第6阶段，1个任务I hope you’re at least somewhat confused about how we got to these numbers so that we can take the time to better understand what is going on!我希望你至少对我们如何得到这些数字感到困惑，以便我们可以花时间更好地了解正在发生的事情！### Stages 阶段Stages in Spark represent groups of tasks that can be executed together to compute the same operation on multiple machines. In general, Spark will try to pack as much work as possible (i.e., as many transformations as possible inside your job) into the same stage, but the engine starts new stages after operations called shuffles. A shuffle represents a physical repartitioning of the data—for example, sorting a DataFrame, or grouping data that was loaded from a file by key (which requires sending records with the same key to the same node). This type of repartitioning requires coordinating across executors to move data around. Spark starts a new stage after each shuffle, and keeps track of what order the stages must run in to compute the final result.Spark中的阶段（stage）表示可以一起执行以在多台计算机上计算相同操作的任务（task）组。一般来说，Spark会尝试将尽可能多的工作（即工作中尽可能多的转换）打包到同一个阶段（stage），但引擎会在称为洗牌（shuffle）的操作后启动新的阶段（stage）。 shuffle 表示数据的物理重新分区——例如，对 DataFrame 进行排序，或者根据键（key）分组从文件加载的数据（这需要将具有相同键的记录发送到同一节点）。这种类型的重新分区需要跨执行器（executor）进行协调以移动数据。 Spark在每次shuffle之后开始一个新阶段（stage），并跟踪阶段（stage）必须运行的顺序以计算最终结果。In the job we looked at earlier, the first two stages correspond to the range that you perform in order to create your DataFrames. By default when you create a DataFrame with range, it has eight partitions. The next step is the repartitioning. This changes the number of partitions by shuffling the data. These DataFrames are shuffled into six partitions and five partitions, corresponding to the number of tasks in stages 3 and 4.在我们之前查看的工作中，前两个阶段（stage）对应于您为创建DataFrame而执行的范围（range）。默认情况下，当您使用范围（range）创建DataFrame时，它有八个分区。下一步是重新分区（repartitioning）。这会通过对数据洗牌（shuffle）来更改分区数。这些DataFrame被洗牌到六个分区和五个分区，对应于阶段3和4中的任务数。Stages 3 and 4 perform on each of those DataFrames and the end of the stage represents the join (a shuffle). Suddenly, we have 200 tasks. This is because of a Spark SQL configuration. The spark.sql.shuffle.partitions default value is 200, which means that when there is a shuffle performed during execution, it outputs 200 shuffle partitions by default. You can change this value, and the number of output partitions will change.阶段（stage）3和4对每个DataFrame执行，阶段（stage）结束表示连接（join）。突然间，我们有200个任务（task）。这是因为Spark SQL配置。 spark.sql.shuffle.partitions 默认值为200，这意味着当执行期间执行了洗牌（shuffle）时，它默认输出200个洗牌（shuffle）分区。您可以更改此值，并且输出分区的数量将更改。—TIP 提示We cover the number of partitions in a bit more detail in Chapter 19 because it’s such an important parameter. This value should be set according to the number of cores in your cluster to ensure efficient execution. Here’s how to set it:我们在第19章中更详细地介绍了分区的数量，因为它是一个非常重要的参数。这个值应该根据集群中核心的数量来设置，以确保高效执行。设置方法如下：1spark.conf.set("spark.sql.shuffle.partitions", 50)—A good rule of thumb is that the number of partitions should be larger than the number of executors on your cluster, potentially by multiple factors depending on the workload. If you are running code on your local machine, it would behoove you to set this value lower because your local machine is unlikely to be able to execute that number of tasks in parallel. This is more of a default for a cluster in which there might be many more executor cores to use. Regardless of the number of partitions, that entire stage is computed in parallel. The final result aggregates those partitions individually, brings them all to a single partition before finally sending the final result to the driver. We’ll see this configuration several times over the course of this part of the book.一个好的经验法则是分区数应该大于集群上执行器（executor）的数量，可能由多个因素决定，具体取决于工作负载。如果您在本地计算机上运行代码，那么您可以将此值设置得更低，因为本地计算机不太可能并行执行该数量的任务。对于可能需要使用更多执行程序核心的集群，这更像是一个默认设置。无论分区数量如何，整个阶段都是并行计算的。最终结果单独聚合这些分区，在最后将最终结果发送给驱动程序之前将它们全部带到一个分区。在本书的这一部分过程中，我们会多次看到这种配置。### Tasks 任务Stages in Spark consist of tasks. Each task corresponds to a combination of blocks of data and a set of transformations that will run on a single executor. If there is one big partition in our dataset, we will have one task. If there are 1,000 little partitions, we will have 1,000 tasks that can be executed in parallel. A task is just a unit of computation applied to a unit of data (the partition). Partitioning your data into a greater number of partitions means that more can be executed in parallel. This is not a panacea, but it is a simple place to begin with optimization.Spark中的阶段（stage）由任务（task）组成。每个任务（task）对应于将在单个执行器（executor）上运行的数据块和一组转换的组合。如果我们的数据集中有一个大分区，我们将有一个任务。如果有1000个小分区，我们将有1,000个可以并行执行的任务。任务只是应用于数据单元（分区）的计算单位。将数据划分为更多数量的分区意味着可以并行执行更多数据。这不是灵丹妙药，但它是一个简单的开始优化的入手之处。## Execution Details 执行细节Tasks and stages in Spark have some important properties that are worth reviewing before we close out this chapter. First, Spark automatically pipelines stages and tasks that can be done together, such as a map operation followed by another map operation. Second, for all shuffle operations, Spark writes the data to stable storage (e.g., disk), and can reuse it across multiple jobs. We’ll discuss these concepts in turn because they will come up when you start inspecting applications through the Spark UI.在我们结束本章之前，Spark中的任务和阶段具有一些值得检查的重要属性。首先，Spark自动管理可以一起完成的阶段和任务，例如映射（map）操作，然后是另一个映射（map）操作。其次，对于所有洗牌（shuffle）操作，Spark将数据写入稳定存储（例如，磁盘），并且可以在多个作业中重复使用它。我们将依次讨论这些概念，因为当您开始通过Spark UI检查应用程序时，它们会出现。### Pipelining 管道化An important part of what makes Spark an “in-memory computation tool” is that unlike the tools that came before it (e.g., MapReduce), Spark performs as many steps as it can at one point in time before writing data to memory or disk. One of the key optimizations that Spark performs is pipelining, which occurs at and below the RDD level. With pipelining, any sequence of operations that feed data directly into each other, without needing to move it across nodes, is collapsed into a single stage of tasks that do all the operations together. For example, if you write an RDD-based program that does a map, then a filter, then another map, these will result in a single stage of tasks that immediately read each input record, pass it through the first map, pass it through the filter, and pass it through the last map function if needed. This pipelined version of the computation is much faster than writing the intermediate results to memory or disk after each step. The same kind of pipelining happens for a DataFrame or SQL computation that does a select, filter, and select.使Spark成为“内存计算工具”的一个重要部分是，与之前的工具（例如，MapReduce）不同，Spark在将数据写入内存或磁盘之前的一个时间点执行尽可能多的步骤。 Spark执行的一个关键优化是流水线操作，它发生在RDD级别和低于RDD级别。通过流水线操作，将数据直接相互馈送而无需跨节点移动的任何操作序列都会折叠为一起完成所有操作的任务。例如，如果你编写一个基于RDD的程序来执行一个映射（map），然后是一个过滤器（filter），然后是另一个映射（map），这些将导致一个阶段的任务立即读取每个输入记录，将其传递通过第一个映射，传递给它过滤器，如果需要，将其传递给最后一个映射（map）函数。这个流水线版的计算比在每个步骤之后将中间结果写入内存或磁盘要快得多。对于执行select，filter和select的DataFrame或SQL计算，会发生相同类型的流水线操作。From a practical point of view, pipelining will be transparent to you as you write an application—the Spark runtime will automatically do it—but you will see it if you ever inspect your application through the Spark UI or through its log files, where you will see that multiple RDD or DataFrame operations were pipelined into a single stage.从实际的角度来看，在编写应用程序时，流水线操作对您来说是透明的——Spark运行时会自动执行——但如果您通过Spark UI或其日志文件检查应用程序，您将看到它将看到多个RDD或DataFrame操作被流水线化为单个阶段。### Shuffle Persistence 洗牌的持久化The second property you’ll sometimes see is shuffle persistence. When Spark needs to run an operation that has to move data across nodes, such as a reduce-by-key operation (where input data for each key needs to first be brought together from many nodes), the engine can’t perform pipelining anymore, and instead it performs a cross-network shuffle. Spark always executes shuffles by first having the “source” tasks (those sending data) write shuffle files to their local disks during their execution stage. Then, the stage that does the grouping and reduction launches and runs tasks that fetch their corresponding records from each shuffle file and performs that computation (e.g., fetches and processes the data for a specific range of keys). Saving the shuffle files to disk lets Spark run this stage later in time than the source stage (e.g., if there are not enough executors to run both at the same time), and also lets the engine re-launch reduce tasks on failure without rerunning all the input tasks.你有时会看到的第二个属性是随机持久性。当 Spark 需要运行必须跨节点移动数据的操作时，例如 reduce-by-key 操作（每个键的输入数据需要首先从许多节点聚集在一起），引擎不能再执行流水线操作了，而是它执行交叉网络随机洗牌（shuffle）。 在执行阶段，Spark总是首先通过让“源”任务（那些发送数据的任务）将洗牌（shuffle）文件写入本地磁盘来执行洗牌（shuffle）操作。然后，执行分组和减少启动项，并运行从每个洗牌文件获取其相应记录的任务并执行该计算（例如，获取和处理特定范围的键的数据）。将洗牌（shuffle）文件保存到磁盘允许Spark比源阶段更晚地运行此阶段（例如，如果没有足够的执行器（executor）同时运行两者），并且还允许引擎重新启动以在故障时且不用重新运行所有输入任务的情况下减少任务。One side effect you’ll see for shuffle persistence is that running a new job over data that’s already been shuffled does not rerun the “source” side of the shuffle. Because the shuffle files were already written to disk earlier, Spark knows that it can use them to run the later stages of the job, and it need not redo the earlier ones. In the Spark UI and logs, you will see the pre-shuffle stages marked as “skipped”. This automatic optimization can save time in a workload that runs multiple jobs over the same data, but of course, for even better performance you can perform your own caching with the DataFrame or RDD cache method, which lets you control exactly which data is saved and where. You’ll quickly grow accustomed to this behavior after you run some Spark actions on aggregated data and inspect them in the UI.您将看到的随机持久性的一个副作用是，对已经被洗牌的数据运行新作业不会重新运行“源”端的洗牌操作。因为洗牌（shuffle）文件早先已经写入磁盘，所以Spark知道它可以使用它们来运行作业的后期阶段，并且它不需要重做早期的那些（任务）。在Spark UI和日志中，您将看到标记为“已跳过”的预洗牌阶段。这种自动优化可以节省在同一数据上运行多个作业的工作负载的时间，但当然，为了获得更好的性能，您可以使用DataFrame或RDD缓存方法执行自己的缓存，这样您就可以精确控制保存的数据和哪里。在对聚合数据运行一些Spark操作并在UI中检查它们之后，您将很快习惯于此行为。## Conclusion 总结In this chapter, we discussed what happens to Spark Applications when we go to execute them on a cluster. This means how the cluster will actually go about running that code as well as what happens within Spark Applications during the process. At this point, you should feel quite comfortable understanding what happens within and outside of a Spark Application. This will give you a starting point for debugging your applications. Chapter 16 will discuss writing Spark Applications and the things you should consider when doing so.在本章中，我们讨论了当我们在集群上执行它们时Spark应用程序会发生什么。 这意味着集群将如何实际运行该代码以及在此过程中Spark应用程序中发生的事情。 此时，您应该非常自如地了解Spark应用程序内部和外部发生的情况。 这将为您调试应用程序提供一个起点。 第16章将讨论编写Spark应用程序以及执行此操作时应考虑的事项。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 6. Working with Different Types of Data]]></title>
    <url>%2F2019%2F08%2F05%2FChapter6_Working-with-Different-Types-of-Data(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 6. Working with Different Types of Data 处理不同类型的数据 译者：https://snaildove.github.ioChapter 5 presented basic DataFrame concepts and abstractions. This chapter covers building expressions, which are the bread and butter of Spark’s structured operations. We also review working with a variety of different kinds of data, including the following: 第5章介绍了基本的DataFrame概念和抽象。本章涵盖了构建表达式，它们是Spark结构化操作的基础。我们还将回顾使用各种不同类型的数据的工作，包括以下内容： Booleans Numbers Strings Dates and timestamps Handling null 处理空 Complex types 复杂类型 User-defined functions 用户定义的函数 Where to Look for APIs 在何处查找API Before we begin, it’s worth explaining where you as a user should look for transformations. Spark is a growing project, and any book (including this one) is a snapshot in time. One of our priorities in this book is to teach where, as of this writing, you should look to find functions to transform your data. Following are the key places to look : 在开始之前，值得解释一下您作为用户应该在哪里寻求转换。 Spark是一个正在发展的项目，任何书籍（包括本书）都是及时的快照。在本书中，我们的优先重点之一是自本书开始教您应该在哪里寻找用于转换数据的函数。 以下是查找的主要地方： DataFrame (Dataset) Methods This is actually a bit of a trick because a DataFrame is just a Dataset of Row types, so you’ll actually end up looking at the Dataset methods, which are available at this link. 这实际上是一个技巧，因为DataFrame只是行类型的数据集，因此您实际上最终将查看Dataset方法，该方法在此链接可以得到。 Dataset submodules like DataFrameStatFunctions and DataFrameNaFunctions have more methods that solve specific sets of problems. DataFrameStatFunctions, for example, holds a variety of statistically related functions, whereas DataFrameNaFunctions refers to functions that are relevant when working with null data. Dataset 子模块，例如 DataFrameStatFunctions 和 DataFrameNaFunctions 具有更多解决特定问题集的方法。例如，DataFrameStatFunctions 拥有各种与统计相关的功能，而 DataFrameNaFunctions 指的是在处理空数据时相关的函数。 Column Methods 列方法 These were introduced for the most part in Chapter 5. They hold a variety of general column related methods like alias or contains. You can find the API Reference for Column methods here. 这些在第5章中进行了大部分介绍。它们具有与列相关的各种常规方法，如“ alias”或“ contains”。您可以在这里找到Column方法的API参考链接。 org.apache.spark.sql.functions contains a variety of functions for a range of different data types. Often, you’ll see the entire package imported because they are used so frequently. You can find SQL and DataFrame functions here. org.apache.spark.sql.functions 包含用于各种不同数据类型的各种功能。通常，您会看到导入的整个程序包，因为它们是如此频繁地使用。您可以在此处找到SQL和DataFrame函数。 Now this may feel a bit overwhelming but have no fear, the majority of these functions are ones that you will find in SQL and analytics systems. All of these tools exist to achieve one purpose, to transform rows of data in one format or structure to another. This might create more rows or reduce the number of rows available. To begin, let’s read in the DataFrame that we’ll be using for this analysis: 现在，这可能会让人感到有些不知所措，但请放心，这些功能大多数都是您可以在SQL和分析系统中找到的。存在所有这些工具以实现一个目的，即将一种格式或结构的数据行转换为另一种格式或结构。这可能会创建更多的行或减少可用的行数。首先，让我们阅读将用于此分析的 DataFrame ：1234567// in Scalaval df = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/data/retail-data/by-day/2010-12-01.csv")df.printSchema()df.createOrReplaceTempView("dfTable") 1234567# in Pythondf = spark.read.format("csv")\.option("header", "true")\.option("inferSchema", "true")\.load("/data/retail-data/by-day/2010-12-01.csv")df.printSchema()df.createOrReplaceTempView("dfTable") Here’s the result of the schema and a small sample of the data: 这是模式的结果和一小部分数据示例： 1234567891011121314151617181920root|-- InvoiceNo: string (nullable = true)|-- StockCode: string (nullable = true)|-- Description: string (nullable = true)|-- Quantity: integer (nullable = true)|-- InvoiceDate: timestamp (nullable = true)|-- UnitPrice: double (nullable = true)|-- CustomerID: double (nullable = true)|-- Country: string (nullable = true)+---------+---------+--------------------+--------+-------------------+---- |InvoiceNo|StockCode| Description |Quantity| InvoiceDate |Unit +---------+---------+--------------------+--------+-------------------+---- | 536365 | 85123A |WHITE HANGING HEA...| 6 |2010-12-01 08:26:00| ...| 536365 | 71053 | WHITE METAL LANTERN| 6 |2010-12-01 08:26:00| ......| 536367 | 21755 |LOVE BUILDING BLO...| 3 |2010-12-01 08:34:00| ...| 536367 | 21777 |RECIPE BOX WITH M...| 4 |2010-12-01 08:34:00| ...+---------+---------+--------------------+--------+-------------------+---- Converting to Spark Types 转换为Spark类型One thing you’ll see us do throughout this chapter is convert native types to Spark types. We do this by using the first function that we introduce here, the lit function. This function converts a type in another language to its corresponding Spark representation. Here’s how we can convert a couple of different kinds of Scala and Python values to their respective Spark types: 在本章中，您将看到我们要做的一件事是将本地类型转换为Spark类型。我们通过使用此处介绍的第一个函数 “lit” 函数来实现此目的。此函数将另一种语言的类型转换为其对应的Spark表示形式。我们将如何将几种不同的Scala和Python值转换为各自的Spark类型： 123// in Scalaimport org.apache.spark.sql.functions.litdf.select(lit(5), lit("five"), lit(5.0)) 123# in Pythonfrom pyspark.sql.functions import litdf.select(lit(5), lit("five"), lit(5.0)) There’s no equivalent function necessary in SQL, so we can use the values directly: SQL中没有等效的功能，因此我们可以直接使用这些值： 12-- in SQLSELECT 5, "five", 5.0 Working with Booleans 使用布尔值Booleans are essential when it comes to data analysis because they are the foundation for all filtering. Boolean statements consist of four elements: and, or, true, and false. We use these simple structures to build logical statements that evaluate to either true or false. These statements are often used as conditional requirements for when a row of data must either pass the test (evaluate to true) or else it will be filtered out. 在数据分析中，布尔是必不可少的，因为它们是所有过滤的基础。布尔语句由四个元素组成：and, or, true, 和 false。我们使用这些简单的结构来构建评估为true或false的逻辑语句。当一行数据必须通过测试（评估为true）或将其过滤掉时，这些语句通常用作条件要求。 Let’s use our retail dataset to explore working with Booleans. We can specify equality as well as less-than or greater-than: 让我们使用零售数据集探索使用布尔值的方法。我们可以指定相等以及小于或大于：12345// in Scalaimport org.apache.spark.sql.functions.coldf.where(col("InvoiceNo").equalTo(536365)).select("InvoiceNo", "Description").show(5, false) WARNING 警告Scala has some particular semantics regarding the use of == and ===. In Spark, if you want to filter by equality you should use === (equal) or =!= (not equal). You can also use the not function and the equal To method. Scala对于==和===的使用具有一些特殊的语义。 在Spark中，如果要按相等过滤，则应使用 ===（等于）或 =!= （不等于）。 您还可以使用not函数和equal To方法。 12345// in Scalaimport org.apache.spark.sql.functions.coldf.where(col("InvoiceNo") === 536365).select("InvoiceNo", "Description").show(5, false) Python keeps a more conventional notation : Python保留了一个更常规的符号： 12345# in Pythonfrom pyspark.sql.functions import coldf.where(col("InvoiceNo") != 536365)\.select("InvoiceNo", "Description")\.show(5, False) 1234567+---------+-----------------------------+|InvoiceNo| Description |+---------+-----------------------------+| 536366 | HAND WARMER UNION JACK |...| 536367 | POPPY'S PLAYHOUSE KITCHEN |+---------+-----------------------------+ Another option—and probably the cleanest—is to specify the predicate as an expression in a string. This is valid for Python or Scala. Note that this also gives you access to another way of expressing “does not equal”: 另一个选择（可能是最简洁的选择）是将谓词指定为字符串中的表达式。 这对Python或Scala有效。 请注意，这还使您可以使用另一种表示“不相等”的方式： 1df.where("InvoiceNo = 536365").show(5, false) 1df.where("InvoiceNo &lt;&gt; 536365").show(5, false) We mentioned that you can specify Boolean expressions with multiple parts when you use and or or. In Spark, you should always chain together and filters as a sequential filter. 我们提到过，当您使用and或or时，可以指定包含多个部分的布尔表达式。在Spark中，您应始终链接在一起并将过滤器作为顺序过滤器。 The reason for this is that even if Boolean statements are expressed serially (one after the other), Spark will flatten all of these filters into one statement and perform the filter at the same time, creating the and statement for us. Although you can specify your statements explicitly by using and if you like, they’re often easier to understand and to read if you specify them serially. or statements need to be specified in the same statement: 原因是，即使布尔语句以串行方式（一个接一个地表达），Spark也会将所有这些过滤器展平为一个语句并同时执行过滤器，从而为我们创建了and语句。尽管您可以使用和根据需要明确指定语句，但是如果您依次指定它们，通常更易于理解和阅读。或需要在同一条语句中指定的语句：12345// in Scalaval priceFilter = col("UnitPrice") &gt; 600val descripFilter = col("Description").contains("POSTAGE")df.where(col("StockCode").isin("DOT")).where(priceFilter.or(descripFilter)).show() 12345# in Pythonfrom pyspark.sql.functions import instrpriceFilter = col("UnitPrice") &gt; 600descripFilter = instr(df.Description, "POSTAGE") &gt;= 1df.where(df.StockCode.isin("DOT")).where(priceFilter | descripFilter).show() 123-- in SQLSELECT * FROM dfTable WHERE StockCode in ("DOT") AND(UnitPrice &gt; 600 ORinstr(Description, "POSTAGE") &gt;= 1) 123456+---------+---------+--------------+--------+-------------------+---------+...|InvoiceNo|StockCode| Description |Quantity| InvoiceDate |UnitPrice|...+---------+---------+--------------+--------+-------------------+---------+...| 536544 | DOT |DOTCOM POSTAGE| 1 |2010-12-01 14:32:00| 569.77 |...| 536592 | DOT |DOTCOM POSTAGE| 1 |2010-12-01 17:06:00| 607.49 |...+---------+---------+--------------+--------+-------------------+---------+... Boolean expressions are not just reserved to filters. To filter a DataFrame, you can also just specify a Boolean column: 布尔表达式不仅保留给过滤器。 要过滤DataFrame，您还可以仅指定一个布尔列： 1234567// in Scalaval DOTCodeFilter = col("StockCode") === "DOT"val priceFilter = col("UnitPrice") &gt; 600val descripFilter = col("Description").contains("POSTAGE")df.withColumn("isExpensive", DOTCodeFilter.and(priceFilter.or(descripFilter))).where("isExpensive").select("unitPrice", "isExpensive").show(5) 12345678# in Pythonfrom pyspark.sql.functions import instrDOTCodeFilter = col("StockCode") == "DOT"priceFilter = col("UnitPrice") &gt; 600descripFilter = instr(col("Description"), "POSTAGE") &gt;= 1df.withColumn("isExpensive", DOTCodeFilter &amp; (priceFilter | descripFilter))\.where("isExpensive")\.select("unitPrice", "isExpensive").show(5) 123456-- in SQLSELECT UnitPrice, (StockCode = 'DOT' AND(UnitPrice &gt; 600 OR instr(Description, "POSTAGE") &gt;= 1)) as isExpensiveFROM dfTableWHERE (StockCode = 'DOT' AND(UnitPrice &gt; 600 OR instr(Description, "POSTAGE") &gt;= 1)) Notice how we did not need to specify our filter as an expression and how we could use a column name without any extra work. 请注意，我们如何不需要将过滤器指定为表达式，以及如何无需任何额外工作就可以使用列名。 If you’re coming from a SQL background, all of these statements should seem quite familiar. Indeed, all of them can be expressed as a where clause. In fact, it’s often easier to just express filters as SQL statements than using the programmatic DataFrame interface and Spark SQL allows us to do this without paying any performance penalty. For example, the following two statements are equivalent: 如果您来自SQL背景，那么所有这些语句似乎都应该很熟悉。 实际上，所有这些都可以表示为where子句。 实际上，仅将过滤器表示为SQL语句通常比使用程序化DataFrame接口更容易，并且Spark SQL允许我们执行此操作而无需付出任何性能损失。 例如，以下两个语句是等效的： 12345678910// in Scalaimport org.apache.spark.sql.functions.&#123;expr, not, col&#125;df.withColumn("isExpensive", not(col("UnitPrice").leq(250))).filter("isExpensive").select("Description", "UnitPrice").show(5)df.withColumn("isExpensive", expr("NOT UnitPrice &lt;= 250")).filter("isExpensive").select("Description", "UnitPrice").show(5) Here’s our state definition: 这是我们的语句定义： 12345# in Pythonfrom pyspark.sql.functions import exprdf.withColumn("isExpensive", expr("NOT UnitPrice &lt;= 250"))\.where("isExpensive")\.select("Description", "UnitPrice").show(5) WARNING 警告One “gotcha” that can come up is if you’re working with null data when creating Boolean expressions. If there is a null in your data, you’ll need to treat things a bit differently. Here’s how you can ensure that you perform a null-safe equivalence test: 一个可能出现的“陷阱”是创建布尔表达式时是否使用空数据。 如果您的数据为空，则需要对数据进行一些不同的处理。 您可以通过以下方式确保执行空值安全等效测试： 1df.where(col("Description").eqNullSafe("hello")).show() Although not currently available (Spark 2.2), IS [NOT] DISTINCT FROM will be coming in Spark 2.3 to do the same thing in SQL. 尽管目前尚不可用（Spark 2.2），但 IS [NOT] DISTINCT FROM 将在Spark 2.3中使用SQL进行相同的操作。 Working with Numbers 使用数字When working with big data, the second most common task you will do after filtering things is counting things. For the most part, we simply need to express our computation, and that should be valid assuming that we’re working with numerical data types. 在处理大数据时，过滤事物后要做的第二个最常见的任务是计数。 在大多数情况下，我们只需要表达我们的计算量，并且假设我们使用的是数值数据类型，那么这应该是有效的。 To fabricate a contrived example, let’s imagine that we found out that we mis-recorded the quantity in our retail dataset and the true quantity is equal to (the current quantity * the unit price) + 5. This will introduce our first numerical function as well as the pow function that raises a column to the expressed power: 为了构造一个人为的示例，让我们想象一下，我们发现我们在零售数据集中错误地记录了数量，真实数量等于（当前数量*单价）+5。这将引入我们的第一个数值函数为以及 pow 函数，该函数将列提高到表示的功效： 1234// in Scalaimport org.apache.spark.sql.functions.&#123;expr, pow&#125;val fabricatedQuantity = pow(col("Quantity") * col("UnitPrice"), 2) + 5df.select(expr("CustomerId"), fabricatedQuantity.alias("realQuantity")).show(2) 1234# in Pythonfrom pyspark.sql.functions import expr, powfabricatedQuantity = pow(col("Quantity") * col("UnitPrice"), 2) + 5df.select(expr("CustomerId"), fabricatedQuantity.alias("realQuantity")).show(2) 123456+----------+------------------+|CustomerId| realQuantity |+----------+------------------+| 17850.0 |239.08999999999997|| 17850.0 | 418.7156 |+----------+------------------+ Notice that we were able to multiply our columns together because they were both numerical. Naturally we can add and subtract as necessary, as well. In fact, we can do all of this as a SQL expression, as well: 注意，我们能够将列相乘，因为它们都是数值。 当然，我们也可以根据需要添加和减去。 实际上，我们也可以将所有这些操作都作为SQL表达式来完成： 1234// in Scaladf.selectExpr("CustomerId","(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity").show(2) 1234# in Pythondf.selectExpr("CustomerId","(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity").show(2) 123-- in SQLSELECT customerId, (POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantityFROM dfTable Another common numerical task is rounding. If you’d like to just round to a whole number, oftentimes you can cast the value to an integer and that will work just fine. However, Spark also has more detailed functions for performing this explicitly and to a certain level of precision. In the following example, we round to one decimal place: 另一个常见的数字任务是四舍五入。 如果您想四舍五入为整数，通常可以将值转换为整数，这样就可以正常工作。 但是，Spark还具有更详细的功能，可以显式执行此操作并达到一定的精度。 在下面的示例中，我们四舍五入到小数点后一位： 123// in Scalaimport org.apache.spark.sql.functions.&#123;round, bround&#125;df.select(round(col("UnitPrice"), 1).alias("rounded"), col("UnitPrice")).show(5) By default, the round function rounds up if you’re exactly in between two numbers. You can round down by using the bround: 默认情况下，如果您恰好在两个数字之间，用 round 函数会四舍五入。 您可以使用 bround 向下取整： 123// in Scalaimport org.apache.spark.sql.functions.litdf.select(round(lit("2.5")), bround(lit("2.5"))).show(2) 123# in Pythonfrom pyspark.sql.functions import lit, round, brounddf.select(round(lit("2.5")), bround(lit("2.5"))).show(2) 12-- in SQLSELECT round(2.5), bround(2.5) 123456+-------------+--------------+|round(2.5, 0)|bround(2.5, 0)|+-------------+--------------+| 3.0 | 2.0 || 3.0 | 2.0 |+-------------+--------------+ Another numerical task is to compute the correlation of two columns. For example, we can see the Pearson correlation coefficient for two columns to see if cheaper things are typically bought in greater quantities. We can do this through a function as well as through the DataFrame statistic methods: 另一个数字任务是计算两列的相关性。 例如，我们可以看到两列的Pearson相关系数，以查看是否通常会更大量地购买便宜的东西。 我们可以通过一个函数以及通过DataFrame统计方法来做到这一点： 1234// in Scalaimport org.apache.spark.sql.functions.&#123;corr&#125;df.stat.corr("Quantity", "UnitPrice")df.select(corr("Quantity", "UnitPrice")).show() 1234# in Pythonfrom pyspark.sql.functions import corrdf.stat.corr("Quantity", "UnitPrice")df.select(corr("Quantity", "UnitPrice")).show() 12-- in SQLSELECT corr(Quantity, UnitPrice) FROM dfTable 12345+-------------------------+|corr(Quantity, UnitPrice)|+-------------------------+| -0.04112314436835551 |+-------------------------+ Another common task is to compute summary statistics for a column or set of columns. We can use the describe method to achieve exactly this. This will take all numeric columns and calculate the count, mean, standard deviation, min, and max. You should use this primarily for viewing in the console because the schema might change in the future: 另一个常见任务是为一列或一组列计算摘要统计信息。我们可以使用describe方法来实现这一目标。这将占用所有数字列，并计算计数，平均值，标准偏差，最小值和最大值。您应该主要在控制台中使用它，因为模式（schema）将来可能会更改： 12// in Scaladf.describe().show() 12# in Pythondf.describe().show() 123456789+-------+------------------+------------------+------------------+|summary| Quantity | UnitPrice | CustomerID |+-------+------------------+------------------+------------------+| count | 3108 | 3108 | 1968 || mean | 8.627413127413128|4.151946589446603 |15661.388719512195|| stddev|26.371821677029203|15.638659854603892|1854.4496996893627|| min | -24 | 0.0 | 12431.0 || max | 600 | 607.49 | 18229.0 |+-------+------------------+------------------+------------------+ If you need these exact numbers, you can also perform this as an aggregation yourself by importing the functions and applying them to the columns that you need: 如果您需要这些确切的数字，也可以通过导入函数并将其应用于所需的列来执行作为聚合： 12// in Scalaimport org.apache.spark.sql.functions.&#123;count, mean, stddev_pop, min, max&#125; 12# in Pythonfrom pyspark.sql.functions import count, mean, stddev_pop, min, max There are a number of statistical functions available in the StatFunctions Package (accessible using stat as we see in the code block below). These are DataFrame methods that you can use to calculate a variety of different things. For instance, you can calculate either exact or approximate quantiles of your data using the approxQuantile method: StatFunctions 程序包中提供了许多统计功能（如下面的代码块所示，可以使用stat访问）。这些是DataFrame方法，可用于计算各种不同的事物。例如，您可以使用 approxQuantile 方法计算数据的精确或近似分位数： 12345// in Scalaval colName = "UnitPrice"val quantileProbs = Array(0.5)val relError = 0.05df.stat.approxQuantile("UnitPrice", quantileProbs, relError) // 2.51 12345# in PythoncolName = "UnitPrice"quantileProbs = [0.5]relError = 0.05df.stat.approxQuantile("UnitPrice", quantileProbs, relError) # 2.51 You also can use this to see a cross-tabulation or frequent item pairs (be careful, this output will be large and is omitted for this reason): 您还可以使用它来查看交叉列表或出现频率很高的的项目对（请注意，此输出将很大，因此被省略）： 12// in Scaladf.stat.crosstab("StockCode", "Quantity").show() 12# in Pythondf.stat.crosstab("StockCode", "Quantity").show() 12// in Scaladf.stat.freqItems(Seq("StockCode", "Quantity")).show() 12# in Pythondf.stat.freqItems(["StockCode", "Quantity"]).show() As a last note, we can also add a unique ID to each row by using the function monotonically_increasing_id. This function generates a unique value for each row, starting with 0: 最后，我们还可以通过使用 monotonically_increasing_id 函数向每行添加唯一的ID。此函数为每一行生成一个唯一值，从0开始： 123// in Scalaimport org.apache.spark.sql.functions.monotonically_increasing_iddf.select(monotonically_increasing_id()).show(2) 123# in Pythonfrom pyspark.sql.functions import monotonically_increasing_iddf.select(monotonically_increasing_id()).show(2) There are functions added with every release, so check the documentation for more methods. For instance, there are some random data generation tools (e.g., rand(), randn()) with which you can randomly generate data; however, there are potential determinism issues when doing so. (You can find discussions about these challenges on the Spark mailing list.) There are also a number of more advanced tasks like bloom filtering and sketching algorithms available in the stat package that we mentioned (and linked to) at the beginning of this chapter. Be sure to search the API documentation for more information and functions. 每个发行版中都添加了功能，因此请查看文档以了解更多方法。例如，有些随机数据生成工具（例如rand(), randn()）可用来随机生成数据；但是，这样做时存在潜在的确定性问题。 （您可以在Spark邮件列表中找到有关这些挑战的讨论）在本章的开头，我们还提到了（并链接到）stat包中还有许多更高级的任务，例如布隆过滤和草图绘制算法(sketching algorithm)。确保搜索API文档以获取更多信息和功能。 Working with Strings 使用字符串String manipulation shows up in nearly every data flow, and it’s worth explaining what you can do with strings. You might be manipulating log files performing regular expression extraction or substitution, or checking for simple string existence, or making all strings uppercase or lowercase. 字符串操作几乎出现在每个数据流中，值得解释如何使用字符串。您可能正在操纵执行正则表达式提取或替换的日志文件，或者检查是否存在简单的字符串，或者将所有字符串都设置为大写或小写。 Let’s begin with the last task because it’s the most straightforward. The initcap function will capitalize every word in a given string when that word is separated from another by a space. 让我们从最后一个任务开始，因为它是最简单的。当一个给定的字符串中每个单词之间用空格隔开时，initcap函数会将每个单词的首字母大写。 123// in Scalaimport org.apache.spark.sql.functions.&#123;initcap&#125;df.select(initcap(col("Description"))).show(2, false) 123# in Pythonfrom pyspark.sql.functions import initcapdf.select(initcap(col("Description"))).show() 12-- in SQLSELECT initcap(Description) FROM dfTable 123456+----------------------------------+| initcap(Description) |+----------------------------------+|White Hanging Heart T-light Holder|| White Metal Lantern |+----------------------------------+ As just mentioned, you can cast strings in uppercase and lowercase, as well: 如前所述，您还可以将字符串转换为大写和小写形式： 12345// in Scalaimport org.apache.spark.sql.functions.&#123;lower, upper&#125;df.select(col("Description"),lower(col("Description")),upper(lower(col("Description")))).show(2) 12345# in Pythonfrom pyspark.sql.functions import lower, upperdf.select(col("Description"),lower(col("Description")),upper(lower(col("Description")))).show(2) 12-- in SQLSELECT Description, lower(Description), Upper(lower(Description)) FROM dfTable 123456+--------------------+--------------------+-------------------------+| Description | lower(Description) |upper(lower(Description))|+--------------------+--------------------+-------------------------+|WHITE HANGING HEA...|white hanging hea...| WHITE HANGING HEA... || WHITE METAL LANTERN| white metal lantern| WHITE METAL LANTERN |+--------------------+--------------------+-------------------------+ Another trivial task is adding or removing spaces around a string. You can do this by using lpad, ltrim, rpadand rtrim, trim: 另一个琐碎的任务是在字符串周围添加或删除空格。 您可以使用lpad，ltrim，rpad 和 rtrim，trim来做到这一点： 12345678// in Scalaimport org.apache.spark.sql.functions.&#123;lit, ltrim, rtrim, rpad, lpad, trim&#125;df.select(ltrim(lit(" HELLO ")).as("ltrim"),rtrim(lit(" HELLO ")).as("rtrim"),trim(lit(" HELLO ")).as("trim"),lpad(lit("HELLO"), 3, " ").as("lp"),rpad(lit("HELLO"), 10, " ").as("rp")).show(2) 12345678# in Pythonfrom pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trimdf.select(ltrim(lit(" HELLO ")).alias("ltrim"),rtrim(lit(" HELLO ")).alias("rtrim"),trim(lit(" HELLO ")).alias("trim"),lpad(lit("HELLO"), 3, " ").alias("lp"),rpad(lit("HELLO"), 10, " ").alias("rp")).show(2) 12345678-- in SQLSELECTltrim(' HELLLOOOO '),rtrim(' HELLLOOOO '),trim(' HELLLOOOO '),lpad('HELLOOOO ', 3, ' '),rpad('HELLOOOO ', 10, ' ')FROM dfTable 123456+---------+---------+-----+---+----------+| ltrim | rtrim | trim| lp| rp |+---------+---------+-----+---+----------+| HELLO | HELLO |HELLO| HE|HELLO || HELLO | HELLO |HELLO| HE|HELLO |+---------+---------+-----+---+----------+ Note that if lpad or rpad takes a number less than the length of the string, it will always remove values from the right side of the string. 请注意，如果lpad或rpad的数字小于字符串的长度，它将始终从字符串的右侧删除值。 Regular Expressions 正则表达式Probably one of the most frequently performed tasks is searching for the existence of one string in another or replacing all mentions of a string with another value. This is often done with a tool called regular expressions that exists in many programming languages. Regular expressions give the user an ability to specify a set of rules to use to either extract values from a string or replace them with some other values. 可能是执行最频繁的任务之一是在另一个字符串中查找一个字符串的存在，或用另一个值替换所有提及的字符串。通常使用许多编程语言中存在的称为正则表达式的工具来完成此操作。正则表达式使用户能够指定一组规则，以用于从字符串中提取值或将其替换为其他值。 Spark takes advantage of the complete power of Java regular expressions. The Java regular expression syntax departs slightly from other programming languages, so it is worth reviewing before putting anything into production. There are two key functions in Spark that you’ll need in order to perform regular expression tasks: regexp_extract and regexp_replace. These functions extract values and replace values, respectively. Spark利用了Java正则表达式的全部功能。 Java正则表达式语法与其他编程语言略有不同，因此在将任何产品投入生产之前，值得回顾一下。为了执行正则表达式任务，Spark中需要两个关键功能：regexp_extract和regexp_replace。这些函数分别提取值和替换值。 Let’s explore how to use the regexp_replace function to replace substitute color names in our description column: 让我们探索一下如何使用 regexp_replace 函数替换描述列中的替代颜色名称：12345678// in Scalaimport org.apache.spark.sql.functions.regexp_replaceval simpleColors = Seq("black", "white", "red", "green", "blue")val regexString = simpleColors.map(_.toUpperCase).mkString("|")// the | signifies `OR` in regular expression syntaxdf.select(regexp_replace(col("Description"), regexString, "COLOR").alias("color_clean"),col("Description")).show(2) 123456# in Pythonfrom pyspark.sql.functions import regexp_replaceregex_string = "BLACK|WHITE|RED|GREEN|BLUE"df.select(regexp_replace(col("Description"), regex_string, "COLOR").alias("color_clean"),col("Description")).show(2) 12345-- in SQLSELECTregexp_replace(Description, 'BLACK|WHITE|RED|GREEN|BLUE', 'COLOR') ascolor_clean, DescriptionFROM dfTable 123456+--------------------+--------------------+| color_clean | Description |+--------------------+--------------------+|COLOR HANGING HEA...|WHITE HANGING HEA...|| COLOR METAL LANTERN| WHITE METAL LANTERN|+--------------------+--------------------+ Another task might be to replace given characters with other characters. Building this as a regular expression could be tedious, so Spark also provides the translate function to replace these values. This is done at the character level and will replace all instances of a character with the indexed character in the replacement string: 另一个任务可能是用其他字符替换给定的字符。 将其构建为正则表达式可能很繁琐，因此Spark还提供了 translation 函数来替换这些值。 这是在字符级别完成的，它将用替换字符串中的索引字符替换字符的所有实例： 123// in Scalaimport org.apache.spark.sql.functions.translatedf.select(translate(col("Description"), "LEET", "1337"), col("Description")).show(2) 1234# in Pythonfrom pyspark.sql.functions import translatedf.select(translate(col("Description"), "LEET", "1337"),col("Description"))\.show(2) 12-- in SQLSELECT translate(Description, 'LEET', '1337'), Description FROM dfTable 123456+----------------------------------+--------------------+|translate(Description, LEET, 1337)| Description |+----------------------------------+--------------------+| WHI73 HANGING H3A... |WHITE HANGING HEA...|| WHI73 M37A1 1AN73RN | WHITE METAL LANTERN|+----------------------------------+--------------------+ We can also perform something similar, like pulling out the first mentioned color: 我们还可以执行类似的操作，例如拉出第一个提到的颜色： 1234567// in Scalaimport org.apache.spark.sql.functions.regexp_extractval regexString = simpleColors.map(_.toUpperCase).mkString("(", "|", ")")// the | signifies OR in regular expression syntaxdf.select(regexp_extract(col("Description"), regexString, 1).alias("color_clean"),col("Description")).show(2) 123456# in Pythonfrom pyspark.sql.functions import regexp_extractextract_str = "(BLACK|WHITE|RED|GREEN|BLUE)"df.select(regexp_extract(col("Description"), extract_str, 1).alias("color_clean"),col("Description")).show(2) 123-- in SQLSELECT regexp_extract(Description, '(BLACK|WHITE|RED|GREEN|BLUE)', 1),Description FROM dfTable 123456+-------------+--------------------+| color_clean | Description |+-------------+--------------------+| WHITE |WHITE HANGING HEA...|| WHITE | WHITE METAL LANTERN|+-------------+--------------------+ Sometimes, rather than extracting values, we simply want to check for their existence. We can do this with the contains method on each column. This will return a Boolean declaring whether the value you specify is in the column’s string: 有时，我们只是想检查它们的存在，而不是提取值。 我们可以在每一列上使用contains方法来做到这一点。 这将返回一个布尔值，该布尔值声明您指定的值是否在该列的字符串中： 123456// in Scalaval containsBlack = col("Description").contains("BLACK")val containsWhite = col("DESCRIPTION").contains("WHITE")df.withColumn("hasSimpleColor", containsBlack.or(containsWhite)).where("hasSimpleColor").select("Description").show(3, false) In Python and SQL, we can use the instr function: 在Python和SQL中，我们可以使用 instr 函数： 1234567# in Pythonfrom pyspark.sql.functions import instrcontainsBlack = instr(col("Description"), "BLACK") &gt;= 1containsWhite = instr(col("Description"), "WHITE") &gt;= 1df.withColumn("hasSimpleColor", containsBlack | containsWhite)\.where("hasSimpleColor")\.select("Description").show(3, False) 123-- in SQLSELECT Description FROM dfTableWHERE instr(Description, 'BLACK') &gt;= 1 OR instr(Description, 'WHITE') &gt;= 1 1234567+----------------------------------+| Description |+----------------------------------+|WHITE HANGING HEART T-LIGHT HOLDER||WHITE METAL LANTERN ||RED WOOLLY HOTTIE WHITE HEART. |+----------------------------------+ This is trivial with just two values, but it becomes more complicated when there are values. Let’s work through this in a more rigorous way and take advantage of Spark’s ability to accept a dynamic number of arguments. When we convert a list of values into a set of arguments and pass them into a function, we use a language feature called varargs. Using this feature, we can effectively unravel an array of arbitrary length and pass it as arguments to a function. This, coupled with select makes it possible for us to create arbitrary numbers of columns dynamically: 仅使用两个值，这是微不足道的，但是当存在值时，它变得更加复杂。 让我们以更严格的方式进行研究，并利用Spark接受动态数量的参数的能力。 当我们将值列表转换为一组参数并将其传递给函数时，我们使用一种称为varargs 的语言功能。 使用此功能，我们可以有效地解开任意长度的数组并将其作为参数传递给函数。 结合select，我们可以动态创建任意数量的列： 12345678// in Scalaval simpleColors = Seq("black", "white", "red", "green", "blue")val selectedColumns = simpleColors.map(color =&gt; &#123;col("Description").contains(color.toUpperCase).alias(s"is_$color")&#125;):+expr("*") // could also append this valuedf.select(selectedColumns:_*).where(col("is_white").or(col("is_red"))).select("Description").show(3, false) 1234567+----------------------------------+| Description |+----------------------------------+|WHITE HANGING HEART T-LIGHT HOLDER||WHITE METAL LANTERN ||RED WOOLLY HOTTIE WHITE HEART. |+----------------------------------+ We can also do this quite easily in Python. In this case, we’re going to use a different function, locate , that returns the integer location (1 based location). We then convert that to a Boolean before using it as the same basic feature: 我们也可以在Python中很容易地做到这一点。在这种情况下，我们将使用另一个函数 locate，该函数返回整数位置（从1开始的位置）。然后，在将其用作相同的基本功能之前，将其转换为布尔值： 1234567891011# in Pythonfrom pyspark.sql.functions import expr, locatesimpleColors = ["black", "white", "red", "green", "blue"]def color_locator(column, color_string):return locate(color_string.upper(), column).cast("boolean")\.alias("is_" + c)selectedColumns = [color_locator(df.Description, c) for c in simpleColors]selectedColumns.append(expr("*")) # has to a be Column typedf.select(*selectedColumns).where(expr("is_white OR is_red"))\.select("Description").show(3, False) This simple feature can often help you programmatically generate columns or Boolean filters in a way that is simple to understand and extend. We could extend this to calculating the smallest common denominator for a given input value, or whether a number is a prime. 这个简单的功能通常可以帮助您以易于理解和扩展的方式以编程方式生成列或布尔过滤器。我们可以将其扩展为计算给定输入值或数字是否为质数的最小公分母。 Working with Dates and Timestamps 使用日期和时间戳记Dates and times are a constant challenge in programming languages and databases. It’s always necessary to keep track of timezones and ensure that formats are correct and valid. Spark does its best to keep things simple by focusing explicitly on two kinds of time-related information. There are dates, which focus exclusively on calendar dates, and timestamps, which include both date and time information. Spark, as we saw with our current dataset, will make a best effort to correctly identify column types, including dates and timestamps when we enable inferSchema. We can see that this worked quite well with our current dataset because it was able to identify and read our date format without us having to provide some specification for it. 在编程语言和数据库中，日期和时间一直是一个挑战。始终需要跟踪时区并确保格式正确和有效。 Spark通过明确关注两种与时间相关的信息来尽力使事情变得简单。有一些日期（仅专注于日历日期）和时间戳（包括日期和时间信息）。正如我们在当前数据集中看到的那样，当启用“ inferSchema”时，Spark将尽最大努力正确识别列类型，包括日期和时间戳。我们可以看到，这对于我们当前的数据集非常有效，因为它能够识别和读取我们的日期格式，而无需我们提供一些规范。 As we hinted earlier, working with dates and timestamps closely relates to working with strings because we often store our timestamps or dates as strings and convert them into date types at runtime. This is less common when working with databases and structured data but much more common when we are working with text and CSV files. We will experiment with that shortly. 正如我们之前所暗示的，使用日期和时间戳与使用字符串紧密相关，因为我们经常将时间戳或日期存储为字符串并将其在运行时转换为日期类型。在使用数据库和结构化数据时，这种情况不太常见，但是在处理文本和CSV文件时，这种情况更为常见。我们将很快对此进行试验。 WARNING 警告There are a lot of caveats, unfortunately, when working with dates and timestamps, especially when it comes to timezone handling. In version 2.1 and before, Spark parsed according to the machine’s timezone if timezones are not explicitly specified in the value that you are parsing. You can set a session local timezone if necessary by setting spark.conf.sessionLocalTimeZone in the SQL configurations. This should be set according to the Java TimeZone format. 不幸的是，在处理日期和时间戳时，尤其是在时区处理方面，有很多警告。 在2.1版及更高版本中，如果未在要解析的值中明确指定时区，则Spark将根据计算机的时区进行解析。 您可以根据需要通过在SQL配置中设置spark.conf.sessionLocalTimeZone 来设置会话本地时区。 应该根据Java TimeZone格式进行设置。 1df.printSchema() 123456789root|-- InvoiceNo: string (nullable = true)|-- StockCode: string (nullable = true)|-- Description: string (nullable = true)|-- Quantity: integer (nullable = true)|-- InvoiceDate: timestamp (nullable = true)|-- UnitPrice: double (nullable = true)|-- CustomerID: double (nullable = true)|-- Country: string (nullable = true) Although Spark will do read dates or times on a best-effort basis. However, sometimes there will be no getting around working with strangely formatted dates and times. The key to understanding the transformations that you are going to need to apply is to ensure that you know exactly what type and format you have at each given step of the way. Another common “gotcha” is that Spark’s TimestampType class supports only second-level precision, which means that if you’re going to be working with milliseconds or microseconds, you’ll need to work around this problem by potentially operating on them as longs. Any more precision when coercing to a TimestampType will be removed. 尽管Spark会尽最大努力读取日期或时间。但是，有时无法解决格式和日期格式异常的问题。理解将要应用的转换的关键是确保您确切地知道在此过程中的每个给定步骤中所具有的类型和格式。另一个常见的“陷阱”是Spark的TimestampType 类仅支持二级精度，这意味着如果您要使用毫秒或微秒，则可能需要长时间对其进行操作来解决此问题。强制转换为 TimestampType 时，将删除任何更高的精度。 Spark can be a bit particular about what format you have at any given point in time. It’s important to be explicit when parsing or converting to ensure that there are no issues in doing so. At the end of the day, Spark is working with Java dates and timestamps and therefore conforms to those standards. Spark可能会在任何给定时间点上对您使用哪种格式有些特殊。 解析或转换时必须明确，以确保这样做没有问题。 归根结底，Spark正在使用Java日期和时间戳，因此符合这些标准。 Let’s begin with the basics and get the current date and the current timestamps: 让我们从基础开始，获取当前日期和当前时间戳： 123456// in Scalaimport org.apache.spark.sql.functions.&#123;current_date, current_timestamp&#125;val dateDF = spark.range(10).withColumn("today", current_date()).withColumn("now", current_timestamp())dateDF.createOrReplaceTempView("dateTable") 123456# in Pythonfrom pyspark.sql.functions import current_date, current_timestampdateDF = spark.range(10)\.withColumn("today", current_date())\.withColumn("now", current_timestamp())dateDF.createOrReplaceTempView("dateTable")dateDF.printSchema() 1234root|-- id: long (nullable = false)|-- today: date (nullable = false)|-- now: timestamp (nullable = false) Now that we have a simple DataFrame to work with, let’s add and subtract five days from today. These functions take a column and then the number of days to either add or subtract as the arguments: 现在我们有了一个简单的DataFrame，让我们从今天开始增加和减少5天。 这些函数使用一列，然后加上要加或减的天数作为参数： 123// in Scalaimport org.apache.spark.sql.functions.&#123;date_add, date_sub&#125;dateDF.select(date_sub(col("today"), 5), date_add(col("today"), 5)).show(1) 123# in Pythonfrom pyspark.sql.functions import date_add, date_subdateDF.select(date_sub(col("today"), 5), date_add(col("today"), 5)).show(1) 12-- in SQLSELECT date_sub(today, 5), date_add(today, 5) FROM dateTable 12345+------------------+------------------+|date_sub(today, 5)|date_add(today, 5)|+------------------+------------------+| 2017-06-12 | 2017-06-22 |+------------------+------------------+ Another common task is to take a look at the difference between two dates. We can do this with the datediff function that will return the number of days in between two dates. Most often we just care about the days, and because the number of days varies from month to month, there also exists a function, months_between, that gives you the number of months between two dates: 另一个常见的任务是查看两个日期之间的差异。 我们可以使用 datediff 函数来执行此操作，该函数将返回两个日期之间的天数。 大多数情况下，我们只关心日期，并且由于天数每个月都不同，因此还存在一个 months_between 函数，该函数可为您提供两个日期之间的月数： 12345678// in Scalaimport org.apache.spark.sql.functions.&#123;datediff, months_between, to_date&#125;dateDF.withColumn("week_ago", date_sub(col("today"), 7)).select(datediff(col("week_ago"), col("today"))).show(1)dateDF.select(to_date(lit("2016-01-01")).alias("start"),to_date(lit("2017-05-22")).alias("end")).select(months_between(col("start"), col("end"))).show(1) 1234567891011# in Pythonfrom pyspark.sql.functions import datediff, months_between, to_datedateDF.withColumn("week_ago", date_sub(col("today"), 7))\.select(datediff(col("week_ago"), col("today"))).show(1)dateDF.select(to_date(lit("2016-01-01")).alias("start"),to_date(lit("2017-05-22")).alias("end"))\.select(months_between(col("start"), col("end"))).show(1)-- in SQLSELECT to_date('2016-01-01'), months_between('2016-01-01', '2017-01-01'),datediff('2016-01-01', '2017-01-01')FROM dateTable 12345678910+-------------------------+|datediff(week_ago, today)|+-------------------------+| -7 |+-------------------------++--------------------------+|months_between(start, end)|+--------------------------+| -16.67741935 |+--------------------------+ Notice that we introduced a new function: the to_date function. The to_date function allows you to convert a string to a date, optionally with a specified format. We specify our format in the Java SimpleDateFormat which will be important to reference if you use this function: 注意，我们引入了一个新函数：to_date 函数。 to_date 函数允许您将字符串转换为日期，可以选择使用指定格式。 我们在 Java SimpleDateFormat 中指定我们的格式，如果您使用此函数，这对于引用这个函数很重要： 1234// in Scalaimport org.apache.spark.sql.functions.&#123;to_date, lit&#125;spark.range(5).withColumn("date", lit("2017-01-01")).select(to_date(col("date"))).show(1) 1234# in Pythonfrom pyspark.sql.functions import to_date, litspark.range(5).withColumn("date", lit("2017-01-01"))\.select(to_date(col("date"))).show(1) Spark will not throw an error if it cannot parse the date; rather, it will just return null. This can be a bit tricky in larger pipelines because you might be expecting your data in one format and getting it in another. To illustrate, let’s take a look at the date format that has switched from year-month-day to year-day-month. Spark will fail to parse this date and silently return null instead: 如果无法解析日期，Spark不会抛出错误。相反，它将仅返回null。在较大的管道中，这可能会有些棘手，因为您可能期望数据是一种格式并以另一种格式获取。为了说明这一点，让我们看一下从 year-month-day 转换为year-day-month的日期格式。 Spark将无法解析此日期，而是静默返回null： 123456dateDF.select(to_date(lit("2016-20-12")),to_date(lit("2017-12-11"))).show(1)+-------------------+-------------------+|to_date(2016-20-12)|to_date(2017-12-11)|+-------------------+-------------------+| null | 2017-12-11 |+-------------------+-------------------+ We find this to be an especially tricky situation for bugs because some dates might match the correct format, whereas others do not. In the previous example, notice how the second date appears as Decembers 11th instead of the correct day, November 12th. Spark doesn’t throw an error because it cannot know whether the days are mixed up or that specific row is incorrect. 我们发现这种情况对于bug来说尤其棘手，因为某些日期可能与正确的格式匹配，而另一些则不匹配。 在上一个示例中，请注意第二个日期如何显示为12月11日，而不是正确的日期11月12日。 Spark不会引发错误，因为它不知道这些日期是混合的还是特定的行不正确。 Let’s fix this pipeline, step by step, and come up with a robust way to avoid these issues entirely. The first step is to remember that we need to specify our date format according to the Java SimpleDateFormat standard. 让我们逐步解决此问题，并提出一种健壮的方法来完全避免这些问题。第一步是要记住，我们需要根据Java SimpleDateFormat标准 指定日期格式。 We will use two functions to fix this: to_date and to_timestamp. The former optionally expects a format, whereas the latter requires one: 我们将使用两个函数来解决此问题：to_date 和 to_timestamp。前者可以选择一种格式，而后者则需要一种格式： 1234567// in Scalaimport org.apache.spark.sql.functions.to_dateval dateFormat = "yyyy-dd-MM"val cleanDateDF = spark.range(1).select(to_date(lit("2017-12-11"), dateFormat).alias("date"),to_date(lit("2017-20-12"), dateFormat).alias("date2"))cleanDateDF.createOrReplaceTempView("dateTable2") 1234567# in Pythonfrom pyspark.sql.functions import to_datedateFormat = "yyyy-dd-MM"cleanDateDF = spark.range(1).select(to_date(lit("2017-12-11"), dateFormat).alias("date"),to_date(lit("2017-20-12"), dateFormat).alias("date2"))cleanDateDF.createOrReplaceTempView("dateTable2") 123-- in SQLSELECT to_date(date, 'yyyy-dd-MM'), to_date(date2, 'yyyy-dd-MM'), to_date(date)FROM dateTable2 12345+----------+----------+| date | date2 |+----------+----------+|2017-11-12|2017-12-20|+----------+----------+ Now let’s use an example of to_timestamp, which always requires a format to be specified: 现在，我们使用 to_timestamp 的示例，该示例始终需要指定一种格式： 123// in Scalaimport org.apache.spark.sql.functions.to_timestampcleanDateDF.select(to_timestamp(col("date"), dateFormat)).show() 123# in Pythonfrom pyspark.sql.functions import to_timestampcleanDateDF.select(to_timestamp(col("date"), dateFormat)).show() 123-- in SQLSELECT to_timestamp(date, 'yyyy-dd-MM'), to_timestamp(date2, 'yyyy-dd-MM')FROM dateTable2 12345+----------------------------------+|to_timestamp(`date`, &apos;yyyy-dd-MM&apos;)|+----------------------------------+| 2017-11-12 00:00:00 |+----------------------------------+ Casting between dates and timestamps is simple in all languages—in SQL, we would do it in the following way: 在所有语言中，日期和时间戳之间的转换都很简单——在SQL中，我们可以通过以下方式进行： 12-- in SQLSELECT cast(to_date("2017-01-01", "yyyy-dd-MM") as timestamp) After we have our date or timestamp in the correct format and type, comparing between them is actually quite easy. We just need to be sure to either use a date/timestamp type or specify our string according to the right format of yyyy-MM-dd if we’re comparing a date: 在以正确的格式和类型获得日期或时间戳后，实际上比较起来很容易。 如果要比较日期，我们只需要确保使用日期/时间戳类型或根据yyyy-MM-dd的正确格式指定我们的字符串即可： 1cleanDateDF.filter(col("date2") &gt; lit("2017-12-12")).show() One minor point is that we can also set this as a string, which Spark parses to a literal: 一小点是，我们还可以将其设置为字符串，Spark将其解析为文字： 1cleanDateDF.filter(col("date2") &gt; "'2017-12-12'").show() WARNING 警告Implicit type casting is an easy way to shoot yourself in the foot, especially when dealing with null values or dates in different timezones or formats. We recommend that you parse them explicitly instead of relying on implicit conversions. 隐式类型转换是一种使自己步履蹒跚的简便方法，尤其是在处理具有不同时区或格式的空值或日期时。 我们建议您显式解析它们，而不要依赖隐式转换。 Working with Nulls in Data 在数据中的空值As a best practice, you should always use nulls to represent missing or empty data in your DataFrames. Spark can optimize working with null values more than it can if you use empty strings or other values. The primary way of interacting with null values, at DataFrame scale, is to use the .na subpackage on a DataFrame. There are also several functions for performing operations and explicitly specifying how Spark should handle null values. For more information, see Chapter 5 (where we discuss ordering), and also refer back to “Working with Booleans”. 最佳做法是，应始终使用null来表示DataFrame中丢失或为空的数据。与使用空字符串或其他值相比，Spark可以优化使用null的工作。在DataFrame规模上，与null进行交互的主要方式是在DataFrame上使用 .na 子包。还有一些函数可以执行操作并明确指定Spark应该如何处理null。有关更多信息，请参见第5章（我们将在其中讨论排序），另请参考“使用布尔值”。 WARNING 警告Nulls are a challenging part of all programming, and Spark is no exception. In our opinion, being explicit is always better than being implicit when handling null values. For instance, in this part of the book, we saw how we can define columns as having null types. However, this comes with a catch. When we declare a column as not having a null time, that is not actually enforced. To reiterate, when you define a schema in which all columns are declared to not have null values, Spark will not enforce that and will happily let null values into that column. The nullable signal is simply to help Spark SQL optimize for handling that column. If you have null values in columns that should not have null values, you can get an incorrect result or see strange exceptions that can be difficult to debug. null 是所有编程中具有挑战性的一部分，Spark也不例外。我们认为，在处理null时，显式总是比隐式好。例如，在本书的这一部分中，我们看到了如何将列定义为具有null类型。但是，这有一个陷阱。当我们声明一列不具有 null 时，实际上并没有强制执行。重申一下，当您定义一个模式，在该模式中声明所有列都不具有null时，Spark将不强制执行该操作，并且会很乐意让null进入该列。可为空的信号仅是为了帮助Spark SQL优化处理该列。如果不含null的列中包含null，则可能会得到错误的结果，或者会看到难以调试的奇怪异常。 There are two things you can do with null values: you can explicitly drop nulls or you can fill them with a value (globally or on a per-column basis). Let’s experiment with each of these now. 使用空值可以做两件事：您可以显式删除空值，也可以用一个值（全局或基于每个列）填充空值。让我们现在尝试其中的每一个。 Coalesce 合并Spark includes a function to allow you to select the first non-null value from a set of columns by using the coalesce function. In this case, there are no null values, so it simply returns the first column: Spark包含一个函数，该函数允许您使用合并（coalesce）函数从一组列中选择第一个非空值。 在这种情况下，没有空值，因此它仅返回第一列： 123// in Scalaimport org.apache.spark.sql.functions.coalescedf.select(coalesce(col("Description"), col("CustomerId"))).show() 123# in Pythonfrom pyspark.sql.functions import coalescedf.select(coalesce(col("Description"), col("CustomerId"))).show() ifnull, nullIf, nvl, and nvl2There are several other SQL functions that you can use to achieve similar things. ifnull allows you to select the second value if the first is null, and defaults to the first. Alternatively, you could use nullif, which returns null if the two values are equal or else returns the second if they are not. nvl returns the second value if the first is null, but defaults to the first. Finally, nvl2 returns the second value if the first is not null; otherwise, it will return the last specified value (else_value in the following example): 您还可以使用其他几个SQL函数来实现类似的功能。 如果第一个为 null，则 ifnull 允许您选择第二个值，默认为第一个。 或者，您可以使用 nullif，如果两个值相等，则返回 null；否则，返回第二个值。 如果第一个为空，则 nvl 返回第二个值，但默认为第一个。 最后，如果第一个不为 null，则nvl2返回第二个值。 否则，它将返回最后指定的值（在以下示例中为 else_value）： 1234567-- in SQLSELECTifnull(null, 'return_value'),nullif('value', 'value'),nvl(null, 'return_value'),nvl2('not_null', 'return_value', "else_value")FROM dfTable LIMIT 1 12345+------------+----+------------+------------+| a | b | c | d |+------------+----+------------+------------+|return_value|null|return_value|return_value|+------------+----+------------+------------+ Naturally, we can use these in select expressions on DataFrames, as well. 自然，我们也可以在DataFrames的select表达式中使用它们。 drop 删除The simplest function is drop, which removes rows that contain nulls. The default is to drop any row in which any value is null: 最简单的函数是drop，它删除包含空值的行。 缺省值为删除任何值为null的行： 12df.na.drop()df.na.drop("any") In SQL, we have to do this column by column: 在SQL中，我们必须逐列进行此操作： 12-- in SQLSELECT * FROM dfTable WHERE Description IS NOT NULL Specifying “any” as an argument drops a row if any of the values are null. Using “all” drops the row only if all values are null or NaN for that row: 如果任何值均为空，则将“ any”指定为参数将删除一行。 仅当该行的所有值均为null或NaN时，才使用“ all”删除该行： 1df.na.drop("all") We can also apply this to certain sets of columns by passing in an array of columns: 我们还可以通过传递一个列数组来将其应用于某些列集： 12// in Scaladf.na.drop("all", Seq("StockCode", "InvoiceNo")) 12# in Pythondf.na.drop("all", subset=["StockCode", "InvoiceNo"]) fillUsing the fill function, you can fill one or more columns with a set of values. This can be done by specifying a map—that is a particular value and a set of columns. For example, to fill all null values in columns of type String, you might specify the following: 使用填充功能，可以用一组值填充一个或多个列。 这可以通过指定一个映射来完成，该映射是一个特定的值和一组列。 例如，要填充字符串类型的列中的所有空值，可以指定以下内容： 1df.na.fill("All Null values become this string") We could do the same for columns of type Integer by using df.na.fill(5:Integer), or for Doubles df.na.fill(5:Double). To specify columns, we just pass in an array of column names like we did in the previous example: 我们可以使用 df.na.fill(5:Integer) 对Integer类型的列执行相同的操作，也可以对 df.na.fill(5:Double) 进行Double操作。 要指定列，我们只需要像上一个示例一样传递一个列名数组： 12// in Scaladf.na.fill(5, Seq("StockCode", "InvoiceNo")) 12# in Pythondf.na.fill("all", subset=["StockCode", "InvoiceNo"]) We can also do this with with a Scala Map, where the key is the column name and the value is the value we would like to use to fill null values: 我们也可以使用Scala Map来做到这一点，其中的键是列名，值是我们想要用来填充空值的值： 123// in Scalaval fillColValues = Map("StockCode" -&gt; 5, "Description" -&gt; "No Value")df.na.fill(fillColValues) 123# in Pythonfill_cols_vals = &#123;"StockCode": 5, "Description" : "No Value"&#125;df.na.fill(fill_cols_vals) replace 替换In addition to replacing null values like we did with drop and fill, there are more flexible options that you can use with more than just null values. Probably the most common use case is to replace all values in a certain column according to their current value. The only requirement is that this value be the same type as the original value: 除了像用drop and fill替换空值那样，您还可以使用更多灵活的选项，而不仅仅是空值。可能最常见的用例是根据其当前值替换特定列中的所有值。唯一的要求是该值必须与原始值具有相同的类型： 12// in Scaladf.na.replace("Description", Map("" -&gt; "UNKNOWN")) 12# in Pythondf.na.replace([""], ["UNKNOWN"], "Description") Ordering 排序As we discussed in Chapter 5, you can use asc_nulls_first, desc_nulls_first, asc_nulls_last, or desc_nulls_last to specify where you would like your null values to appear in an ordered DataFrame. 正如我们在第5章中讨论的那样，您可以使用 asc_nulls_first，desc_nulls_first，asc_nulls_last 或desc_nulls_last 来指定希望空值出现在有序DataFrame中的位置。 Working with Complex Types 使用复杂类型Complex types can help you organize and structure your data in ways that make more sense for the problem that you are hoping to solve. There are three kinds of complex types: structs, arrays, and maps. 复杂类型可以帮助您以对希望解决的问题更有意义的方式组织和构造数据。 复杂类型共有三种：结构（struct），数组（array）和映射（map）。 Structs 结构You can think of structs as DataFrames within DataFrames. A worked example will illustrate this more clearly. We can create a struct by wrapping a set of columns in parenthesis in a query: 您可以将结构视为DataFrame中的DataFrame。一个可行的示例将更清楚地说明这一点。我们可以通过在查询中用括号括起一组列来创建结构： 12345678df.selectExpr("(Description, InvoiceNo) as complex", "")df.selectExpr("struct(Description, InvoiceNo) as complex", "")// in Scalaimport org.apache.spark.sql.functions.structval complexDF = df.select(struct("Description", "InvoiceNo").alias("complex"))complexDF.createOrReplaceTempView("complexDF") 12345# in Pythonfrom pyspark.sql.functions import structcomplexDF = df.select(struct("Description", "InvoiceNo").alias("complex"))complexDF.createOrReplaceTempView("complexDF") We now have a DataFrame with a column complex. We can query it just as we might another DataFrame, the only difference is that we use a dot syntax to do so, or the column method getField: 现在，我们有了一个带有列复合体的DataFrame。 我们可以像查询另一个DataFrame一样查询它，唯一的区别是我们使用点语法或列方法getField进行查询： 12complexDF.select("complex.Description")complexDF.select(col("complex").getField("Description")) We can also query all values in the struct by using *. This brings up all the columns to the top-level DataFrame: 我们还可以使用*查询结构中的所有值。 这将所有列调到顶级DataFrame： 1complexDF.select("complex.*") 12-- in SQLSELECT complex.* FROM complexDF Arrays 数组To define arrays, let’s work through a use case. With our current data, our objective is to take every single word in our Description column and convert that into a row in our DataFrame. The first task is to turn our Description column into a complex type, an array. 要定义数组，让我们研究一下用例。 使用我们当前的数据，我们的目标是获取Description列中的每个单词，并将其转换为DataFrame中的一行。 第一个任务是将我们的Description列转换为复杂类型，即数组。 split 拆分We do this by using the split function and specify the delimiter: 我们通过使用split函数并指定定界符来做到这一点： 123// in Scalaimport org.apache.spark.sql.functions.splitdf.select(split(col("Description"), " ")).show(2) 123# in Pythonfrom pyspark.sql.functions import splitdf.select(split(col("Description"), " ")).show(2) 12-- in SQLSELECT split(Description, ' ') FROM dfTable 123456+---------------------+|split(Description, ) |+---------------------+| [WHITE, HANGING, ...|| [WHITE, METAL, LA...|+---------------------+ This is quite powerful because Spark allows us to manipulate this complex type as another column. We can also query the values of the array using Python-like syntax: 这非常强大，因为Spark允许我们将这种复杂类型作为另一列进行操作。 我们还可以使用类似Python的语法查询数组的值： 123// in Scaladf.select(split(col("Description"), " ").alias("array_col")).selectExpr("array_col[0]").show(2) 123# in Pythondf.select(split(col("Description"), " ").alias("array_col"))\.selectExpr("array_col[0]").show(2) 12-- in SQLSELECT split(Description, ' ')[0] FROM dfTable This gives us the following result: 这给我们以下结果： 123456+------------+|array_col[0]|+------------+| WHITE || WHITE |+------------+ Array Length 数组长度We can determine the array’s length by querying for its size: 我们可以通过查询数组的大小来确定数组的长度： 123// in Scalaimport org.apache.spark.sql.functions.sizedf.select(size(split(col("Description"), " "))).show(2) // shows 5 and 3 123# in Pythonfrom pyspark.sql.functions import sizedf.select(size(split(col("Description"), " "))).show(2) # shows 5 and 3 array_containsWe can also see whether this array contains a value: 我们还可以查看此数组是否包含值： 123// in Scalaimport org.apache.spark.sql.functions.array_containsdf.select(array_contains(split(col("Description"), " "), "WHITE")).show(2) 123# in Pythonfrom pyspark.sql.functions import array_contains.select(array_contains(split(col("Description"), " "), "WHITE")).show(2) 12-- in SQLSELECT array_contains(split(Description, ' '), 'WHITE') FROM dfTable This gives us the following result: 这给我们以下结果： 123456+--------------------------------------------+|array_contains(split(Description, ), WHITE) |+--------------------------------------------+| true || true |+--------------------------------------------+ However, this does not solve our current problem. To convert a complex type into a set of rows (one per value in our array), we need to use the explode function. 但是，这不能解决我们当前的问题。 要将复杂类型转换为一组行（数组中的每个值一个），我们需要使用explode函数。 explode 展开The explode function takes a column that consists of arrays and creates one row (with the rest of the values duplicated) per value in the array. Figure 6-1 illustrates the process. explode函数采用由数组组成的列，并为数组中的每个值创建一行（其余值重复）。 图6-1说明了该过程。 12345// in Scalaimport org.apache.spark.sql.functions.&#123;split, explode&#125;df.withColumn("splitted", split(col("Description"), " ")).withColumn("exploded", explode(col("splitted"))).select("Description", "InvoiceNo", "exploded").show(2) 12345# in Pythonfrom pyspark.sql.functions import split, explodedf.withColumn("splitted", split(col("Description"), " "))\.withColumn("exploded", explode(col("splitted")))\.select("Description", "InvoiceNo", "exploded").show(2) 1234-- in SQLSELECT Description, InvoiceNo, explodedFROM (SELECT *, split(Description, " ") as splitted FROM dfTable)LATERAL VIEW explode(splitted) as exploded This gives us the following result: 这给我们以下结果： 123456+--------------------+---------+--------+| Description |InvoiceNo|exploded|+--------------------+---------+--------+|WHITE HANGING HEA...| 536365 | WHITE ||WHITE HANGING HEA...| 536365 | HANGING|+--------------------+---------+--------+ Maps 映射Maps are created by using the map function and key-value pairs of columns. You then can select them just like you might select from an array: 使用映射函数和列的键值对创建映射。 然后，您可以像从数组中选择一样选择它们： 123// in Scalaimport org.apache.spark.sql.functions.mapdf.select(map(col("Description"), col("InvoiceNo")).alias("complex_map")).show(2) 1234# in Pythonfrom pyspark.sql.functions import create_mapdf.select(create_map(col("Description"), col("InvoiceNo")).alias("complex_map"))\.show(2) 123-- in SQLSELECT map(Description, InvoiceNo) as complex_map FROM dfTableWHERE Description IS NOT NULL This produces the following result: 这将产生以下结果： 123456+--------------------+| complex_map |+--------------------+|Map(WHITE HANGING...||Map(WHITE METAL L...|+--------------------+ You can query them by using the proper key. A missing key returns null: 您可以使用适当的键查询它们。 缺少键将返回null： 123// in Scaladf.select(map(col("Description"), col("InvoiceNo")).alias("complex_map")).selectExpr("complex_map['WHITE METAL LANTERN']").show(2) 123# in Pythondf.select(map(col("Description"), col("InvoiceNo")).alias("complex_map"))\.selectExpr("complex_map['WHITE METAL LANTERN']").show(2) This gives us the following result: 这给我们以下结果： 123456+--------------------------------+|complex_map[WHITE METAL LANTERN]|+--------------------------------+| null || 536365 |+--------------------------------+ You can also explode map types, which will turn them into columns: 您还可以展开映射类型，这会将它们转换为列： 123// in Scaladf.select(map(col("Description"), col("InvoiceNo")).alias("complex_map")).selectExpr("explode(complex_map)").show(2) 123# in Pythondf.select(map(col("Description"), col("InvoiceNo")).alias("complex_map"))\.selectExpr("explode(complex_map)").show(2) This gives us the following result: 这给我们以下结果： 123456+--------------------+------+| key | value|+--------------------+------+|WHITE HANGING HEA...|536365|| WHITE METAL LANTERN|536365|+--------------------+------+ Working with JSON 使用JSONSpark has some unique support for working with JSON data. You can operate directly on strings of JSON in Spark and parse from JSON or extract JSON objects. Let’s begin by creating a JSON column: Spark对使用JSON数据提供了一些独特的支持。 您可以直接在Spark中对JSON字符串进行操作，并从JSON进行解析或提取JSON对象。 首先创建一个JSON列： 123// in Scalaval jsonDF = spark.range(1).selectExpr("""'&#123;"myJSONKey" : &#123;"myJSONValue" : [1, 2, 3]&#125;&#125;' as jsonString""") 123# in PythonjsonDF = spark.range(1).selectExpr("""'&#123;"myJSONKey" : &#123;"myJSONValue" : [1, 2, 3]&#125;&#125;' as jsonString""") You can use the get_json_object to inline query a JSON object, be it a dictionary or array. You can use json_tuple if this object has only one level of nesting: 您可以使用 get_json_object 内联查询 JSON 对象（无论是字典还是数组）。 如果此对象只有一层嵌套，则可以使用 json_tuple： 12345// in Scalaimport org.apache.spark.sql.functions.&#123;get_json_object, json_tuple&#125;jsonDF.select(get_json_object(col("jsonString"), "$.myJSONKey.myJSONValue[1]") as "column",json_tuple(col("jsonString"), "myJSONKey")).show(2) 12345# in Pythonfrom pyspark.sql.functions import get_json_object, json_tuplejsonDF.select(get_json_object(col("jsonString"), ".myJSONKey.myJSONValue[1]") as "column",json_tuple(col("jsonString"), "myJSONKey")).show(2) Here’s the equivalent in SQL : 12jsonDF.selectExpr("json_tuple(jsonString, '.myJSONKey.myJSONValue[1]') as column").show(2) This results in the following table: 结果如下表所示： 12345+------+--------------------+|column| c0 |+------+--------------------+| 2 |&#123;&quot;myJSONValue&quot;:[1...|+------+--------------------+ You can also turn a StructType into a JSON string by using the to_json function: 您还可以使用 to_json 函数将 StructType 转换为 JSON 字符串： 1234// in Scalaimport org.apache.spark.sql.functions.to_jsondf.selectExpr("(InvoiceNo, Description) as myStruct").select(to_json(col("myStruct"))) 1234# in Pythonfrom pyspark.sql.functions import to_jsondf.selectExpr("(InvoiceNo, Description) as myStruct")\.select(to_json(col("myStruct"))) This function also accepts a dictionary (map) of parameters that are the same as the JSON data source. You can use the from_json function to parse this (or other JSON data) back in. This naturally requires you to specify a schema, and optionally you can specify a map of options, as well: 此函数还接受与JSON数据源相同的参数字典（映射）。 您可以使用from_json函数将其（或其他JSON数据）解析回去。这自然要求您指定一个模式，并且还可以指定一个的选项映射： 123456789// in Scalaimport org.apache.spark.sql.functions.from_jsonimport org.apache.spark.sql.types._val parseSchema = new StructType(Array(new StructField("InvoiceNo",StringType,true),new StructField("Description",StringType,true)))df.selectExpr("(InvoiceNo, Description) as myStruct").select(to_json(col("myStruct")).alias("newJSON")).select(from_json(col("newJSON"), parseSchema), col("newJSON")).show(2) 123456789# in Pythonfrom pyspark.sql.functions import from_jsonfrom pyspark.sql.types import *parseSchema = StructType((StructField("InvoiceNo",StringType(),True),StructField("Description",StringType(),True)))df.selectExpr("(InvoiceNo, Description) as myStruct")\.select(to_json(col("myStruct")).alias("newJSON"))\.select(from_json(col("newJSON"), parseSchema), col("newJSON")).show(2) This gives us the following result: 这给我们以下结果： 123456+----------------------+--------------------+|jsontostructs(newJSON)| newJSON |+----------------------+--------------------+| [536365,WHITE HAN... |&#123;"InvoiceNo":"536...|| [536365,WHITE MET... |&#123;"InvoiceNo":"536...|+----------------------+--------------------+ User-Defined Functions 用户定义的函数One of the most powerful things that you can do in Spark is define your own functions. These user-defined functions (UDFs) make it possible for you to write your own custom transformations using Python or Scala and even use external libraries. UDFs can take and return one or more columns as input. Spark UDFs are incredibly powerful because you can write them in several different programming languages; you do not need to create them in an esoteric format or domain-specific language. They’re just functions that operate on the data, record by record. By default, these functions are registered as temporary functions to be used in that specific SparkSession or Context. 您可以在Spark中执行的最强大的功能之一就是定义自己的函数。这些用户定义函数（UDF）使您可以使用Python或Scala甚至使用外部库来编写自己的自定义转换。 UDF可以接受并返回一列或多列作为输入。 Spark UDF非常强大，因为您可以用几种不同的编程语言编写它们。您无需以深奥的格式或特定于域的语言创建它们。它们只是对数据进行操作的功能，逐条记录。默认情况下，这些功能被注册为在该特定SparkSession或Context中使用的临时功能。 Although you can write UDFs in Scala, Python, or Java, there are performance considerations that you should be aware of. To illustrate this, we’re going to walk through exactly what happens when you create UDF, pass that into Spark, and then execute code using that UDF. 尽管您可以使用Scala，Python或Java编写UDF，但仍应注意一些性能注意事项。为了说明这一点，我们将详细介绍创建UDF，将其传递给Spark并使用该UDF执行代码时发生的情况。 The first step is the actual function. We’ll create a simple one for this example. Let’s write a power3 function that takes a number and raises it to a power of three: 第一步是实际功能。我们将为此示例创建一个简单的示例。让我们编写一个power3函数，该函数接受一个数字并将其提高为三的幂： 1234// in Scalaval udfExampleDF = spark.range(5).toDF("num")def power3(number:Double):Double = number * number * numberpower3(2.0) 12345# in PythonudfExampleDF = spark.range(5).toDF("num")def power3(double_value):return double_value ** 3power3(2.0) In this trivial example, we can see that our functions work as expected. We are able to provide an individual input and produce the expected result (with this simple test case). Thus far, our expectations for the input are high: it must be a specific type and cannot be a null value (see “Working with Nulls in Data”). 在这个简单的示例中，我们可以看到我们的功能按预期工作。我们能够提供单独的输入并产生预期的结果（使用这个简单的测试用例）。到目前为止，我们对输入的期望很高：它必须是特定类型，不能为空值（请参阅“在数据中使用空值”）。 Now that we’ve created these functions and tested them, we need to register them with Spark so that we can use them on all of our worker machines. Spark will serialize the function on the driver and transfer it over the network to all executor processes. This happens regardless of language. 现在我们已经创建了这些功能并对其进行了测试，我们需要在Spark上注册它们，以便可以在所有工作计算机上使用它们。 Spark将序列化驱动程序上的函数，并将其通过网络传输到所有执行程序进程。无论使用哪种语言，都会发生这种情况。 When you use the function, there are essentially two different things that occur. If the function is written in Scala or Java, you can use it within the Java Virtual Machine (JVM). This means that there will be little performance penalty aside from the fact that you can’t take advantage of code generation capabilities that Spark has for built-in functions. There can be performance issues if you create or use a lot of objects; we cover that in the section on optimization in Chapter 19. 使用该函数时，实际上会发生两种不同的情况。如果该函数是用Scala或Java编写的，则可以在Java虚拟机（JVM）中使用它。这意味着除了您无法利用Spark内置函数的代码生成功能之外，几乎没有性能损失。如果创建或使用很多对象，可能会出现性能问题；我们将在第19章中的“优化”部分中进行介绍。 If the function is written in Python, something quite different happens. Spark starts a Python process on the worker, serializes all of the data to a format that Python can understand (remember, it was in the JVM earlier), executes the function row by row on that data in the Python process, and then finally returns the results of the row operations to the JVM and Spark. Figure 6-2 provides an overview of the process. 如果该函数是用Python编写的，则会发生完全不同的事情。 Spark在工作程序上启动一个Python进程，将所有数据序列化为Python可以理解的格式（请记住，它早先在JVM中），在Python进程中逐行对该数据执行函数，然后最终返回将行操作的结果传递给JVM和Spark。图6-2概述了该过程。 WARNING 警告Starting this Python process is expensive, but the real cost is in serializing the data to Python. This is costly for two reasons: it is an expensive computation, but also, after the data enters Python, Spark cannot manage the memory of the worker. This means that you could potentially cause a worker to fail if it becomes resource constrained (because both the JVM and Python are competing for memory on the same machine). We recommend that you write your UDFs in Scala or Java—the small amount of time it should take you to write the function in Scala will always yield significant speed ups, and on top of that, you can still use the function from Python! 启动此Python进程非常昂贵，但实际成本是将数据序列化为Python。 这是昂贵的，原因有两个：这是昂贵的计算，而且，在数据输入Python之后，Spark无法管理工作程序的内存。 这意味着，如果工作程序受到资源限制，则有可能导致它失败（因为JVM和Python都在同一台机器上争夺内存）。 我们建议您使用Scala或Java编写UDF——用少量时间在Scala中编写函数将始终能够显着提高速度，最重要的是，您仍然可以使用Python中的函数！ Now that you have an understanding of the process, let’s work through an example. First, we need to register the function to make it available as a DataFrame function: 现在您已经了解了该过程，下面以一个示例为例。首先，我们需要注册该函数以使其可用作DataFrame函数： 123// in Scalaimport org.apache.spark.sql.functions.udfval power3udf = udf(power3(_:Double):Double) We can use that just like any other DataFrame function: 12// in ScalaudfExampleDF.select(power3udf(col("num"))).show() The same applies to Python—first, we register it: 同样适用于Python——首先，我们注册它： 123# in Pythonfrom pyspark.sql.functions import udfpower3udf = udf(power3) Then, we can use it in our DataFrame code: 然后，我们可以在DataFrame代码中使用它： 123# in Pythonfrom pyspark.sql.functions import coludfExampleDF.select(power3udf(col("num"))).show(2) 123456+-----------+|power3(num)|+-----------+| 0 || 1 |+-----------+ At this juncture, we can use this only as a DataFrame function. That is to say, we can’t use it within a string expression, only on an expression. However, we can also register this UDF as a Spark SQL function. This is valuable because it makes it simple to use this function within SQL as well as across languages. Let’s register the function in Scala: 目前，我们只能将其用作DataFrame函数。 也就是说，我们不能在字符串表达式中使用它，而只能在表达式中使用它。 但是，我们也可以将此UDF注册为Spark SQL函数。 这很有价值，因为它使在SQL以及跨语言中使用此功能变得简单。 让我们在Scala中注册该功能： 123// in Scalaspark.udf.register("power3", power3(_:Double):Double)udfExampleDF.selectExpr("power3(num)").show(2) Because this function is registered with Spark SQL—and we’ve learned that any Spark SQL function or expression is valid to use as an expression when working with DataFrames—we can turn around and use the UDF that we wrote in Scala, in Python. However, rather than using it as a DataFrame function, we use it as a SQL expression: 由于此函数已在Spark SQL中注册——并且我们了解到，在使用DataFrames时，任何Spark SQL函数或表达式都可有效地用作表达式——我们可以转而使用Scala或用Python编写的UDF。 但是，不是将其用作DataFrame函数，而是将其用作SQL表达式： 123# in PythonudfExampleDF.selectExpr("power3(num)").show(2)# registered in Scala We can also register our Python function to be available as a SQL function and use that in any language, as well. 我们还可以注册Python函数以将其作为SQL函数使用，也可以在任何语言中使用它。 One thing we can also do to ensure that our functions are working correctly is specify a return type. As we saw in the beginning of this section, Spark manages its own type information, which does not align exactly with Python’s types. Therefore, it’s a best practice to define the return type for your function when you define it. It is important to note that specifying the return type is not necessary, but it is a best practice. 为了确保我们的功能正常运行，我们还可以做的一件事就是指定返回类型。 正如我们在本节开头所看到的，Spark管理自己的类型信息，该信息与Python的类型不完全一致。 因此，最佳做法是在定义函数时定义返回类型。 重要的是要注意，没有必要指定返回类型，但这是最佳实践。 If you specify the type that doesn’t align with the actual type returned by the function, Spark will not throw an error but will just return null to designate a failure. You can see this if you were to switch the return type in the following function to be a DoubleType: 如果您指定的类型与该函数返回的实际类型不符，Spark将不会抛出错误，而只会返回null来表示失败。如果要在以下函数中将返回类型切换为DoubleType，则可以看到此信息： 123# in Pythonfrom pyspark.sql.types import IntegerType, DoubleTypespark.udf.register("power3py", power3, DoubleType()) 123# in PythonudfExampleDF.selectExpr("power3py(num)").show(2)# registered via Python This is because the range creates integers. When integers are operated on in Python, Python won’t convert them into floats (the corresponding type to Spark’s double type), therefore we see null. We can remedy this by ensuring that our Python function returns a float instead of an integer and the function will behave correctly. 这是因为 range 创建整数。 当在Python中对整数进行运算时，Python不会将其转换为浮点数（与Spark的double类型相对应的类型），因此我们会看到null。 我们可以通过确保Python函数返回浮点数而不是整数来补救此问题，并且该函数将正常运行。 Naturally, we can use either of these from SQL, too, after we register them: 自然地，在注册它们之后，我们也可以在SQL中使用它们之一： 12-- in SQLSELECT power3(12), power3py(12) --doesn't work because of return type When you want to optionally return a value from a UDF, you should return None in Python and an Option type in Scala: 当您希望从UDF返回值时，应在Python中返回None，在Scala中返回Option类型： Hive UDFsAs a last note, you can also use UDF/UDAF creation via a Hive syntax. To allow for this, first you must enable Hive support when they create their SparkSession (via SparkSession.builder().enableHiveSupport() ). Then you can register UDFs in SQL. This is only supported with precompiled Scala and Java packages, so you’ll need to specify them as a dependency: 最后，您还可以通过Hive语法使用 UDF / UDAF 创建。 为此，首先在创建 SparkSession 时必须启用Hive的支持（通过 SparkSession.builder().enableHiveSupport() ）。 然后，您可以在SQL中注册UDF。 仅预编译的Scala和Java软件包支持此功能，因此您需要将它们指定为依赖项： 12-- in SQLCREATE TEMPORARY FUNCTION myFunc AS 'com.organization.hive.udf.FunctionName' Additionally, you can register this as a permanent function in the Hive Metastore by removing TEMPORARY. 此外，您可以通过删除TEMPORARY将其注册为Hive Metastore中的永久函数。 Conclusion 结论This chapter demonstrated how easy it is to extend Spark SQL to your own purposes and do so in a way that is not some esoteric, domain-specific language but rather simple functions that are easy to test and maintain without even using Spark! This is an amazingly powerful tool that you can use to specify sophisticated business logic that can run on five rows on your local machines or on terabytes of data on a 100-node cluster! 本章展示了将Spark SQL扩展到自己的目的有多么容易，并且这样做不是某种深奥的，特定于领域的语言，而是一种简单的函数，即使不使用Spark也不容易测试和维护！ 这是一个非常强大的工具，可用于指定复杂的业务逻辑，这些逻辑可以在本地计算机上的五行上运行，也可以在100节点群集上的TB级数据上运行！]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 22 Event-Time and Stateful Processing]]></title>
    <url>%2F2019%2F07%2F28%2FChapter22_EventTimeAndStatefulProcessing(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 22 Event-Time and Stateful ProcessingChapter 21 covered the core concepts and basic APIs; this chapter dives into event-time and stateful processing. Event-time processing is a hot topic because we analyze information with respect to the time that it was created, not processed. The key idea between this style of processing is that over the lifetime of the job, Spark will maintain relevant state that it can update over the course of the job before outputting it to the sink. 第21章介绍了核心概念和基本API；本章深入讨论事件时间（event-time）和状态处理（stateful processing）。事件时间处理是一个热门话题，因为我们分析与创建时间相关的信息，而不是与处理时间相关的信息。这种处理方式之间的关键思想是，在作业的整个生命周期中，Spark 将保持相关状态，以便在将作业输出到接收器之前在作业的整个过程中进行更新。 Let’s cover these concepts in greater detail before we begin working with code to show they work. 在我们开始使用代码来展示这些概念的工作之前，让我们更详细地介绍它们。 Event Time 事件时间Event time is an important topic to cover discretely because Spark’s DStream API does not support processing information with respect to event-time. At a higher level, in stream-processing systems there are effectively two relevant times for each event: the time at which it actually occurred (event time), and the time that it was processed or reached the stream-processing system (processing time). 事件时间是一个重要的主题，需要离散地涵盖，因为 Spark 的 DStream API 不支持与事件时间相关的处理信息。在更高的层次上，在流处理系统中，每个事件实际上有两个相关的时间：它实际发生的时间（事件时间），以及它被处理或到达流处理系统的时间（处理时间）。 Event time 事件时间 Event time is the time that is embedded in the data itself. It is most often, though not required to be, the time that an event actually occurs. This is important to use because it provides a more robust way of comparing events against one another. The challenge here is that event data can be late or out of order. This means that the stream processing system must be able to handle out-oforder or late data. 事件时间是嵌入到数据本身中的时间。它通常是事件实际发生的时间，尽管不要求如此。这一点很重要，因为它提供了一种更强大的方法来比较事件。这里的挑战是事件数据可能会延迟或无序。这意味着流处理系统必须能够处理无序或延迟的数据。 Processing time 处理时间 Processing time is the time at which the stream-processing system actually receives data. This is usually less important than event time because when it’s processed is largely an implementation detail. This can’t ever be out of order because it’s a property of the streaming system at a certain time (not an external system like event time). Those explanations are nice and abstract, so let’s use a more tangible example. Suppose that we have a datacenter located in San Francisco. An event occurs in two places at the same time: one in Ecuador, the other in Virginia (see Figure 22-1)。 处理时间是流处理系统实际接收数据的时间。这通常不如事件时间重要，因为处理时间在很大程度上是一个实现细节。这永远不会有问题，因为它是流系统在特定时间（而不是外部系统，如事件时间）的属性。这些解释既好又抽象，所以让我们用一个更具体的例子。假设我们有一个位于旧金山的数据中心。一个事件同时发生在两个地方：一个在厄瓜多尔，另一个在弗吉尼亚（见图22-1：遍布世界的事件时间）。 Due to the location of the datacenter, the event in Virginia is likely to show up in our datacenter before the event in Ecuador. If we were to analyze this data based on processing time, it would appear that the event in Virginia occurred before the event in Ecuador: something that we know to be wrong. However, if we were to analyze the data based on event time (largely ignoring the time at which it’s processed), we would see that these events occurred at the same time. 由于数据中心的位置，弗吉尼亚州的事件可能会在厄瓜多尔的事件之前出现在我们的数据中心。如果我们根据处理时间来分析这些数据，弗吉尼亚州的事件似乎发生在厄瓜多尔事件之前：我们知道这是错误的。但是，如果我们基于事件时间分析数据（主要忽略处理数据的时间），我们将看到这些事件同时发生。 As we mentioned, the fundamental idea is that the order of the series of events in the processing system does not guarantee an ordering in event time. This can be somewhat unintuitive, but is worth reinforcing. Computer networks are unreliable. That means that events can be dropped, slowed down, repeated, or be sent without issue. Because individual events are not guaranteed to suffer one fate orthe other, we must acknowledge that any number of things can happen to these events on the way from the source of the information to our stream processing system. For this reason, we need to operate on event time and look at the overall stream with reference to this information contained in the data rather than on when it arrives in the system. This means that we hope to compare events based on the time at which those events occurred. 正如我们所提到的，基本思想是处理系统中一系列事件的顺序并不保证事件时间的顺序。这可能有点不切实际，但值得加强。计算机网络不可靠。这意味着事件可以被删除、减速、重复或毫无问题地发送。由于个别事件不一定会遭受一种或另一种命运，我们必须承认，在从信息源到我们的流处理系统的过程中，这些事件可能会发生任何数量的事情。因此，我们需要在事件时间上操作，并参考数据中包含的信息查看整个流，而不是在数据到达系统时查看。这意味着我们希望根据这些事件发生的时间来比较事件。 Stateful ProcessingThe other topic we need to cover in this chapter is stateful processing. Actually, we already demonstrated this many times in Chapter 21. Stateful processing is only necessary when you need to use or update intermediate information (state) over longer periods of time (in either a microbatch or a record-at-a-time approach). This can happen when you are using event time or when you are performing an aggregation on a key, whether that involves event time or not. 在本章中，我们需要讨论的另一个主题是状态处理。实际上，我们已经在第21章中演示过很多次了。只有在需要使用或更新较长时间的中间信息（状态）时才需要状态处理（通过微批次或记录一次处理方法）。无论是否涉及事件时间，当使用事件时间或对键执行聚合时，都可能发生这种情况。 For the most part, when you’re performing stateful operations. Spark handles all of this complexity for you. For example, when you specify a grouping, Structured Streaming maintains and updates the information for you. You simply specify the logic. When performing a stateful operation, Spark stores the intermediate information in a state store. Spark’s current state store implementation is an inmemory state store that is made fault tolerant by storing intermediate state to the checkpoint directory. 在大多数情况下，当您执行有状态操作时。Spark 为您处理所有这些复杂性。 例如，当您指定分组时，Structured Streaming 将为您维护和更新信息。您只需指定逻辑。在执行状态操作时，Spark 将中间信息存储在状态存储中。Spark当前的状态存储实现是一个 InMemory 状态存储，通过将中间状态存储到检查点（checkpoint ）目录中，使其具有容错性。 Arbitrary Stateful Processing 任意（或自定义）状态处理The stateful processing capabilities described above are sufficient to solve many streaming problems. However, there are times when you need fine-grained control over what state should be stored, how it is updated, and when it should be removed, either explicitly or via a time-out. This is called arbitrary (or custom) stateful processing and Spark allows you to essentially store whatever information you like over the course of the processing of a stream. This provides immense flexibility and power and allows for some complex business logic to be handled quite easily. Just as we did before, let’s ground this with some examples : 上面描述的状态处理功能足以解决许多流问题。但是，有时需要对什么状态应该存储、如何更新状态以及何时删除状态进行细粒度控制，无论是显式还是通过超时。这被称为任意（或自定义）状态处理，Spark 允许您在处理流的过程中基本上存储您喜欢的任何信息。这提供了巨大的灵活性和能力，并允许非常容易地处理一些复杂的业务逻辑。正如我们之前所做的，让我们用一些例子来说明这一点： You’d like to record information about user sessions on an ecommerce site. For instance, you might want to track what pages users visit over the course of this session in order to provide recommendations in real time during their next session. Naturally, these sessions have completely arbitrary start and stop times that are unique to that user. 您希望在电子商务网站上记录有关用户会话的信息。例如，您可能希望跟踪用户在本会话过程中访问的页面，以便在下次会话期间实时提供建议。当然，这些会话的开始和停止时间完全是该用户独有的。 Your company would like to report on errors in the web application but only if five events occur during a user’s session. You could do this with count-based windows that only emit a result if five events of some type occur. 您的公司希望报告Web应用程序中的错误，但前提是在用户会话期间发生五个事件。您可以对基于计数的窗口执行此操作，该窗口仅在发生某种类型的五个事件时发出结果。 You’d like to deduplicate records over time. To do so, you’re going to need to keep track of every record that you see before deduplicating it. 您希望随着时间的推移消除重复记录。要做到这一点，您需要在消除重复数据之前跟踪您看到的每个记录。 Now that we’ve explained the core concepts that we’re going to need in this chapter, let’s cover all of this with some examples that you can follow along with and explain some of the important caveats that you need to consider when processing in this manner. 既然我们已经在本章中解释了我们将需要的核心概念，那么让我们用一些例子来涵盖所有这些，您可以跟随这些例子，并解释在以这种方式处理时需要考虑的一些重要注意事项。 Event-Time Basics 事件时间基础知识Let’s begin with the same dataset from the previous chapter. When working with event time, it’s just another column in our dataset, and that’s really all we need to concern ourselves with; we simply use that column, as demonstrated here : 让我们从上一章的相同数据集开始。在处理事件时间时，它只是数据集中的另一个列，而这正是我们需要关注的全部内容；我们只需使用该列，如下所示： 12345678// in Scalaspark.conf.set("spark.sql.shuffle.partitions", 5)val static = spark.read.json("/data/activity-data")val streaming = spark.readStream.schema(static.schema).option("maxFilesPerTrigger", 10).json("/data/activity-data") 12345678# in Pythonspark.conf.set("spark.sql.shuffle.partitions", 5)static = spark.read.json("/data/activity-data")streaming = spark\.readStream\.schema(static.schema)\.option("maxFilesPerTrigger", 10)\.json("/data/activity-data") 1streaming.printSchema() 1234567891011root|-- Arrival_Time: long (nullable = true)|-- Creation_Time: long (nullable = true)|-- Device: string (nullable = true)|-- Index: long (nullable = true)|-- Model: string (nullable = true)|-- User: string (nullable = true)|-- gt: string (nullable = true)|-- x: double (nullable = true)|-- y: double (nullable = true)|-- z: double (nullable = true) In this dataset, there are two time-based columns. The Creation_Time column defines when an event was created, whereas the Arrival_Time defines when an event hit our servers somewhere upstream. We will use Creation_Time in this chapter. This example reads from a file but, as we saw in the previous chapter, it would be simple to change it to Kafka if you already have a cluster up and running. 在这个数据集中，有两个基于时间的列。Creation_Time 列定义事件的创建时间，而 Arrival_Time 则定义事件在上游某个位置在我们的服务器检索到的时间。我们将在本章中使用 Creation_Time 。这个例子是从一个文件中读取的，但是，正如我们在上一章中看到的，如果已经启动并运行了集群，那么将其更改为 Kafka 将非常简单。 Windows on Event Time 基于事件时间的窗口The first step in event-time analysis is to convert the timestamp column into the proper Spark SQL timestamp type. Our current column is unixtime nanoseconds (represented as a long), therefore we’re going to have to do a little manipulation to get it into the proper format : 事件时间分析的第一步是将 timestamp 列转换为正确的 Spark SQL timestamp 类型。我们当前的列是 unixtime纳秒（表示为长列），因此我们需要做一些操作才能将其转换为正确的格式： 12345// in Scalaval withEventTime = streaming.selectExpr( "*", "cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time") 12345# in PythonwithEventTime = streaming\.selectExpr( "*", "cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time") Tumbling Windows 滚动的窗口The simplest operation is simply to count the number of occurrences of an event in a given window. Figure 22-2 depicts the process when performing a simple summation based on the input data and a key. 最简单的操作就是计算给定窗口中事件的发生次数。图22-2，描述了基于输入数据和键，执行简单求和时的过程。 We’re performing an aggregation of keys over a window of time. We update the result table(depending on the output mode) when every trigger runs, which will operate on the data received since the last trigger. In the case of our actual dataset (and Figure 22-2), we’ll do so in 10-minute windows without any overlap between them (each, and only one event can fall into one window). This will update in real time, as well, meaning that if new events were being added upstream to our system, Structured Streaming would update those counts accordingly. This is the complete output mode, Spark will output the entire result table regardless of whether we’ve seen the entire dataset : 我们在一段时间内执行 keys 聚合。当每个触发器运行时，我们更新结果表（取决于输出模式），它将对自上一个触发器以来接收的数据进行操作。在实际数据集（和图22-2）的情况下，我们将在10分钟的窗口中完成这项工作，它们之间没有任何重叠（每个窗口中只能有一个事件落在一个窗口中）。这也将实时更新，这意味着如果在系统上游添加新事件，Structured Streaming 将相应地更新这些计数。这是完整的输出模式，Spark 将输出整个结果表，而不管我们是否看到了整个数据集 ： 12345678// in Scalaimport org.apache.spark.sql.functions.&#123;window, col&#125;withEventTime.groupBy(window(col("event_time"), "10 minutes")).count().writeStream.queryName("events_per_window").format("memory").outputMode("complete").start() 12345678# in Pythonfrom pyspark.sql.functions import window, colwithEventTime.groupBy(window(col("event_time"), "10 minutes")).count()\.writeStream\.queryName("events_per_window")\.format("memory")\.outputMode("complete")\.start() Now we’re writing out to the in-memory sink for debugging, so we can query it with SQL after we have the stream running : 现在，我们正在写入内存中的接收器进行调试，以便在流运行后使用 SQL 查询它： 1spark.sql("SELECT * FROM events_per_window").show() 1SELECT * FROM events_per_window This shows us something like the following result, depending on the amount of data processed when you had run the query : 根据运行查询时处理的数据量，这将向我们显示如下结果 ： 123456789+---------------------------------------------+-----+| window |count|+---------------------------------------------+-----+|[2015-02-23 10:40:00.0,2015-02-23 10:50:00.0]|11035||[2015-02-24 11:50:00.0,2015-02-24 12:00:00.0]|18854|...|[2015-02-23 13:40:00.0,2015-02-23 13:50:00.0]|20870||[2015-02-23 11:20:00.0,2015-02-23 11:30:00.0]|9392 |+---------------------------------------------+-----+ For reference, here’s the schema we get from the previous query: 为了参考，这里我们从前一个查询获得的模式（schema）： 1234root|-- window: struct (nullable = false)| |-- start: timestamp (nullable = true)| |-- end: timestamp (nullable = true)|-- count: long (nullable = false) Notice how window is actually a struct (a complex type). Using this we can query this struct for the start and end times of a particular window. Of importance is the fact that we can also perform an aggregation on multiple columns, including the event time column. Just like we saw in the previous chapter, we can even perform these aggregations using methods like cube. While we won’t repeat the fact that we can perform the multi-key aggregation below, this does apply to any window-style aggregation (or stateful computation) we would like : 注意窗口实际上是一个结构（复杂类型）。使用这个，我们可以查询这个结构来获取特定窗口的开始和结束时间。重要的是，我们还可以对多个列（包括事件时间列）执行聚合。正如我们在上一章中看到的，我们甚至可以使用类似于 Cube 的方法来执行这些聚合。虽然我们不会重复这样一个事实，即我们可以执行下面的多键聚合，但这确实适用于我们想要的任何窗口式聚合（或状态计算）： 12345678// in Scalaimport org.apache.spark.sql.functions.&#123;window, col&#125;withEventTime.groupBy(window(col("event_time"), "10 minutes"), "User").count().writeStream.queryName("events_per_window").format("memory").outputMode("complete").start() 12345678# in Pythonfrom pyspark.sql.functions import window, colwithEventTime.groupBy(window(col("event_time"), "10 minutes"), "User").count()\.writeStream\.queryName("pyevents_per_window")\.format("memory")\.outputMode("complete")\.start() Sliding windows 滑动窗口The previous example was simple counts in a given window. Another approach is that we can decouple the window from the starting time of the window. Figure 22-3 illustrates what we mean. 前一个例子是给定窗口中的简单计数。另一种方法是我们可以将窗口与窗口的开始时间分离。图22-3说明了我们的意思。 In the figure, we are running a sliding window through which we look at an hour increment, but we’d like to get the state every 10 minutes. This means that we will update the values over time and will include the last hours of data. In this example, we have 10-minute windows, starting every five minutes. Therefore each event will fall into two different windows. You can tweak this further according to your needs: 在图中，我们运行一个滑动窗口，通过它我们可以看到一个小时增量，但是我们希望每10分钟获得一个状态。这意味着我们将随着时间的推移更新这些值，并包括最后几个小时的数据。在这个例子中，我们有10分钟的窗口，每5分钟启动一次。因此，每个事件将分为两个不同的窗口。您可以根据需要进一步调整： 123456789// in Scalaimport org.apache.spark.sql.functions.&#123;window, col&#125;withEventTime.groupBy(window(col("event_time"), "10 minutes", "5 minutes")).count().writeStream().queryName("events_per_window").format("memory").outputMode("complete").start() 123456789# in Pythonfrom pyspark.sql.functions import window, colwithEventTime.groupBy(window(col("event_time"), "10 minutes", "5 minutes"))\.count()\.writeStream\.queryName("pyevents_per_window")\.format("memory")\.outputMode("complete")\.start() Naturally, we can query the in-memory table: 当然，我们可以查询内存中的表： 1SELECT * FROM events_per_window This query gives us the following result. Note that the starting times for each window are now in 5-minute intervals instead of 10, like we saw in the previous query: 此查询提供以下结果。注意，现在每个窗口的开始时间间隔为5分钟，而不是10分钟，就像我们在前面的查询中看到的那样： 123456789+---------------------------------------------+-----+| window |count|+---------------------------------------------+-----+|[2015-02-23 14:15:00.0,2015-02-23 14:25:00.0]|40375||[2015-02-24 11:50:00.0,2015-02-24 12:00:00.0]|56549|...|[2015-02-24 11:45:00.0,2015-02-24 11:55:00.0]|51898||[2015-02-23 10:40:00.0,2015-02-23 10:50:00.0]|33200|+---------------------------------------------+-----+ Handling Late Data with Watermarks 使用水印处理延迟数据The preceding examples are great, but they have a flaw. We never specified how late we expect to see data. This means that Spark is going to need to store that intermediate data forever because we never specified a watermark, or a time at which we don’t expect to see any more data. This applies to all stateful processing that operates on event time. We must specify this watermark in order to age-out data in the stream (and, therefore, state) so that we don’t overwhelm the system over a long period of time. 前面的例子很好，但是它们有一个缺陷。我们从未具体说明我们期望看到数据的时间有多晚。这意味着 Spark 需要永远存储中间数据，因为我们从未指定过水印，或者我们不希望看到更多数据的时间。这适用于对事件时间进行操作的所有状态处理。我们必须指定这个水印，以便消除流中的数据（因此，状态），以便在很长一段时间内不会压垮系统。 Concretely, a watermark is an amount of time following a given event or set of events after which we do not expect to see any more data from that time. We know this can happen due to delays on the network, devices that lose a connection, or any number of other issues. In the DStreams API, there was no robust way to handle late data in this way—if an event occurred at a certain time but did not make it to the processing system by the time the batch for a given window started, it would show up in other processing batches. Structured Streaming remedies this. In event time and stateful processing, a given window’s state or set of data is decoupled from a processing window. That means that as more events come in, Structured Streaming will continue to update a window with more information. 具体来说，水印是给定事件或一组事件之后的一段时间，在此时间之后，我们不希望看到更多的数据。我们知道这可能是由于网络延迟、设备断开连接或任何其他问题造成的。在 DStreams API 中，如果某个事件在某个时间发生，但在某个给定窗口的批处理开始时未到达处理系统，则没有可靠的方法以这种方式处理延迟数据，它将显示在其他批处理中。Structured Streaming 补救了这点。在事件时间和状态处理中，给定窗口的状态或数据集与处理窗口分离。这意味着，随着更多事件的到来，Structured Streaming 将继续更新包含更多信息的窗口。 Let’s return back to our event time example from the beginning of the chapter, shown now in Figure 22-4 让我们回到本章开头的事件时间示例，如图22-4所示。 In this example, let’s imagine that we frequently see some amount of delay from our customers in Latin America. Therefore, we specify a watermark of 10 minutes. When doing this, we instruct Spark that any event that occurs more than 10 “event-time” minutes past a previous event should be ignored. Conversely, this also states that we expect to see every event within 10 minutes. After that, Spark should remove intermediate state and, depending on the output mode, do something with the result. As mentioned at the beginning of the chapter, we need to specify watermarks because if we did not, we’d need to keep all of our windows around forever, expecting them to be updated forever. This brings us to the core question when working with event-time: “how late do I expect to see data?” The answer to this question will be the watermark that you’ll configure for your data. 在这个例子中，假设我们经常看到拉丁美洲客户的延迟。因此，我们指定10分钟的水印。在执行此操作时，我们指示 Spark，应忽略发生在上一个事件之后超过10“事件时间”分钟的任何事件。相反，这也说明我们期望在10分钟内看到每个事件。之后，Spark 应消除中间状态，并根据输出模式对结果进行处理。正如本章开头所提到的，我们需要指定水印，因为如果没有，我们将需要永远保留所有窗口，期待它们永远更新。在处理事件时间时，这就给我们带来了一个核心问题：“我希望看到数据有多晚？“此问题的答案将是为数据配置的水印。 Returning to our dataset, if we know that we typically see data as produced downstream in minutes but we have seen delays in events up to five hours after they occur (perhaps the user lost cell phone connectivity), we’d specify the watermark in the following way : 返回到我们的数据集，如果我们知道我们通常在数分钟内看到下游生成的数据，但我们已经看到事件发生后长达5小时的延迟（可能是用户丢失了手机连接），我们将按以下方式指定水印 ： 123456789101112// in Scalaimport org.apache.spark.sql.functions.&#123;window, col&#125;withEventTime.withWatermark("event_time", "5 hours").groupBy(window(col("event_time"), "10 minutes", "5 minutes")).count().writeStream.queryName("events_per_window").format("memory").outputMode("complete").start() 123456789101112# in Pythonfrom pyspark.sql.functions import window, colwithEventTime\.withWatermark("event_time", "30 minutes")\.groupBy(window(col("event_time"), "10 minutes", "5 minutes"))\.count()\.writeStream\.queryName("pyevents_per_window")\.format("memory")\.outputMode("complete")\.start() It’s pretty amazing, but almost nothing changed about our query. We essentially just added another configuration. Now, Structured Streaming will wait until 30 minutes after the final timestamp of this 10-minute rolling window before it finalizes the result of that window. We can query our table and see the intermediate results because we’re using complete mode—they’ll be updated over time. In append mode, this information won’t be output until the window closes. 这很神奇，但我们的查询几乎没有任何变化。我们实际上只是添加了另一个配置。现在，Structured Streaming 将在这个10分钟滚动窗口的最后时间戳之后等待30分钟，然后最终确定该窗口的结果。我们可以查询表并查看中间结果，因为我们使用的是完整模式，它们将随着时间的推移而更新。在附加模式下，在窗口关闭之前不会输出此信息。 1SELECT * FROM events_per_window 123456789+---------------------------------------------+-----+| window |count|+---------------------------------------------+-----+|[2015-02-23 14:15:00.0,2015-02-23 14:25:00.0]|9505 ||[2015-02-24 11:50:00.0,2015-02-24 12:00:00.0]|13159|...|[2015-02-24 11:45:00.0,2015-02-24 11:55:00.0]|12021||[2015-02-23 10:40:00.0,2015-02-23 10:50:00.0]|7685 |+---------------------------------------------+-----+ At this point, you really know all that you need to know about handling late data. Spark does all of the heavy lifting for you. Just to reinforce the point, if you do not specify how late you think you will see data, then Spark will maintain that data in memory forever. Specifying a watermark allows it to free those objects from memory, allowing your stream to continue running for a long time. 此时，您真正了解处理延迟数据所需的所有信息。Spark 为你做了所有的重担。为了强调这一点，如果您不指定您认为看到数据的时间有多晚，那么 Spark 将永远在内存中维护该数据。指定水印允许它从内存中释放这些对象，从而允许流长时间继续运行。 Dropping Duplicates in a Stream 在流中删除重复项One of the more difficult operations in record-at-a-time systems is removing duplicates from the stream. Almost by definition, you must operate on a batch of records at a time in order to find duplicates—there’s a high coordination overhead in the processing system. Deduplication is an important tool in many applications, especially when messages might be delivered multiple times by upstream systems. A perfect example of this are Internet of Things (IoT) applications that have upstream producers generating messages in nonstable network environments, and the same message might end up being sent multiple times. Your downstream applications and aggregations should be able to assume that there is only one of each message. 一次记录系统中比较困难的操作之一是从流中删除重复项。几乎按照定义，您必须一次对一批记录进行操作，以便找到重复的记录。处理系统中的协调开销很高。在许多应用程序中，重复数据消除是一个重要的工具，特别是当消息可能被上游系统多次传递时。一个很好的例子是物联网（IOT）应用程序，它让上游生产者在不稳定的网络环境中生成消息，而相同的消息最终可能会被多次发送。下游应用程序和聚合应该能够假定每条消息只有一条。 Essentially, Structured Streaming makes it easy to take message systems that provide at-least-once semantics, and convert them into exactly-once by dropping duplicate messages as they come in, based on arbitrary keys. To de-duplicate data, Spark will maintain a number of user specified keys and ensure that duplicates are ignored. 从本质上讲，Structured Streaming 使获取至少提供一次语义的消息系统变得容易，并根据任意键删除重复的消息从而将它们转换为恰好一次的语义。为了消除重复数据，Spark 将维护许多用户指定的 keys，并确保忽略重复的keys。 WARNINGJust like other stateful processing applications, you need to specify a watermark to ensure that the maintained state does not grow infinitely over the course of your stream. 与其他状态处理应用程序一样，您需要指定一个水印，以确保维护状态不会在流过程中无限增长。 Let’s begin the de-duplication process. The goal here will be to de-duplicate the number of events per user by removing duplicate events. Notice how you need to specify the event time column as a duplicate column along with the column you should de-duplicate. The core assumption is that duplicate events will have the same timestamp as well as identifier. In this model, rows with two different timestamps are two different records: 让我们开始重复数据消除过程。这里的目标是通过删除重复事件来消除每个用户的事件数。请注意，您需要如何将事件时间列指定为重复的列以及应消除重复的列。核心假设是重复事件将具有相同的时间戳和标识符。在此模型中，具有两个不同时间戳的行是两个不同的记录： 123456789101112// in Scalaimport org.apache.spark.sql.functions.exprwithEventTime.withWatermark("event_time", "5 seconds").dropDuplicates("User", "event_time").groupBy("User").count().writeStream.queryName("deduplicated").format("memory").outputMode("complete").start() 1234567891011# in Pythonfrom pyspark.sql.functions import exprwithEventTime\.withWatermark("event_time", "5 seconds")\.dropDuplicates(["User", "event_time"])\.groupBy("User")\.count()\.writeStream\.queryName("pydeduplicated")\.format("memory")\.outputMode("complete")\.start() The result will be similar to the following and will continue to update over time as more data is read by your stream: 结果将类似于以下内容，随着流读取更多数据，结果将继续更新： 12345678910111213+----+-----+|User|count|+----+-----+| a | 8085|| b | 9123|| c | 7715|| g | 9167|| h | 7733|| e | 9891|| f | 9206|| d | 8124|| i | 9255|+----+-----+ Arbitrary Stateful Processing 任意状态处理The first section if this chapter demonstrates how Spark maintains information and updates windows based on our specifications. But things differ when you have more complex concepts of windows; this is, where arbitrary stateful processing comes in. This section includes several examples of different use cases along with examples that show you how you might go about setting up your business logic. Stateful processing is available only in Scala in Spark 2.2. This will likely change in the future. 第一节如果这一章演示 Spark 如何维护信息和更新基于我们的规范的窗口。但是，当您对窗口有更复杂的概念时，情况就不同了；这就是任意状态处理的出现之处。本节包括几个不同用户案例以及一些示例，这些示例向您展示了如何设置业务逻辑。状态处理仅在 Spark 2.3 中的 Scala 中可用。这在将来可能会改变。 When performing stateful processing, you might want to do the following: 在执行状态处理时，您可能需要执行以下操作： Create window based on counts of a given key基于给定键的计数创建窗口。 Emit an alert if there is a number of events within a certain time frame如果在某个时间范围内有多个事件，则发出警报。 Maintain user sessions of an undetermined amount of time and save those sessions to perform some analysis on later.保持不确定多长时间的用户会话，并保存这些会话以便稍后执行一些分析。 At the end of the day, there are two things you will want to do when performing this style of processing: 一天结束时，在执行这种类型的处理时，有两件事要做： Map over groups in your data, operate on each group of data, and generate at most a single row for each group. The relevant API for this use case is mapGroupsWithState. 映射数据中的组，对每组数据进行操作，并为每组最多生成一行。此用户案例的相关API是MapGroupsWithState 。 Map over groups in your data, operate on each group of data, and generate one or more rows for each group. The relevant API for this use case is flatMapGroupsWithState. 映射数据中的组，对每组数据进行操作，并为每组生成一行或多行。此用户案例的相关API是FlatmapGroupsWithState 。 When we say “operate” on each group of data, that means that you can arbitrarily update each group independent of any other group of data. This means that you can define arbitrary window types that don’t conform to tumbling or sliding windows like we saw previously in the chapter. One important benefit that we get when we perform this style of processing is control over configuring time-outs on state. With windows and watermarks, it’s very simple: you simply time-out a window when the watermark passes the window start. This doesn’t apply to arbitrary stateful processing, because you manage the state based on user-defined concepts. Therefore, you need to properly time-out your state. 当我们对每一组数据说“操作”时，这意味着您可以独立于任何其他数据组任意更新每一组数据。这意味着您可以定义不符合滚动或滑动窗口的任意窗口类型，正如我们在本章前面看到的那样。当我们执行这种类型的处理时，我们得到的一个重要好处是对状态的超时配置进行控制。有窗口和水印，这非常简单：当水印（设置的时间）超过窗口时，只需超时即可开始。这不适用于任意状态处理，因为您是基于用户定义的概念来管理状态的。因此，您需要适当地超时您的状态。 Let’s discuss this a bit more. 我们再讨论一下。 Time-OutsAs mentioned in Chapter 21, a time-out specifies how long you should wait before timing-out some intermediate state. A time-out is a global parameter across all groups that is configured on a pergroup basis. Time-outs can be either based on processing time (GroupStateTimeout.ProcessingTimeTimeout) or event time (GroupStateTimeout.EventTimeTimeout). When using time-outs, check for time-out first before processing the values. You can get this information by checking the state.hasTimedOut flag or checking whether the values iterator is empty. You need to set some state (i.e., state must be defined, not removed) for time-outs to be set. With a time-out based on processing time, you can set the time-out duration by calling GroupState.setTimeoutDuration (we’ll see code examples of this later in this section of the chapter). The time-out will occur when the clock has advanced by the set duration. Guarantees provided by this time-out with a duration of D ms are as follows : 如第21章所述，超时指定在超时某个中间状态之前应等待多长时间。超时是在每个组基础上配置的所有组的全局参数。超时可以基于处理时间（GroupStateTimeout.ProcessingTimeTimeTimeout）或事件时间（GroupStateTimeout.EventTimeTimeTimeTimeout）。使用超时时，请先检查超时，然后再处理值。您可以通过检查 state.hasTimedOut 标志或检查值迭代器是否为空来获取此信息。您需要设置一些状态（即必须定义状态，而不是删除状态），以便设置超时。使用基于处理时间的超时，可以通过调用GroupState.SetTimeOutDuration 来设置超时的持续时间（我们将在本章后面的部分中看到这方面的代码示例）。当时钟提前到设定的持续时间时，将发生超时。持续时间为 d ms 的超时提供的保证如下： Time-out will never occur before the clock time has advanced by D ms。 在时钟时间提前 d ms 之前，不会发生超时。 Time-out will occur eventually when there is a trigger in the query (i.e., after D ms). So there is a no strict upper bound on when the time-out would occur. For example, the trigger interval of the query will affect when the time-out actually occurs. If there is no data in the stream (for any group) for a while, there won’t be any trigger and the time-out function call will not occur until there is data. 当查询中存在触发器时（即，在 d ms 之后），最终会发生超时。所以在什么时候会发生超时没有严格的上限。例如，查询的触发器间隔将影响什么时候实际发生超时。如果流中（对于任何组）有一段时间没有数据，则不会有任何触发器，并且在有数据之前不会发生超时函数调用。 Because the processing time time-out is based on the clock time, it is affected by the variations in the system clock. This means that time zone changes and clock skew are important variables to consider. 由于处理超时基于时钟时间，因此它受系统时钟变化的影响。这意味着时区变化和时钟偏移是要考虑的重要变量。 With a time-out based on event time, the user also must specify the event-time watermark in the query using watermarks. When set, data older than the watermark is filtered out. As the developer, you can set the timestamp that the watermark should reference by setting a time-out timestamp using the GroupState.setTimeoutTimestamp(…) API. The time-out would occur when the watermark advances beyond the set timestamp. Naturally, you can control the time-out delay by either specifying longer watermarks or simply updating the time-out as you process your stream. Because you can do this in arbitrary code, you can do it on a per-group basis. The guarantee provided by this time-out is that it will never occur before the watermark has exceeded the set time-out. 对于基于事件时间的超时，用户还必须使用水印在查询中指定事件时间水印。设置后，将过滤掉比水印早的数据。作为开发人员，可以通过使用 Groupstate.setTimeoutTimestamp(…) API 设置超时时间戳来设置水印应引用的时间戳。当水印超过设置的时间戳时，将发生超时。当然，您可以通过指定更长的水印或在处理流时简单地更新超时来控制超时延迟。因为您可以在任意代码中执行此操作，所以可以在每个组的基础上执行此操作。此超时提供的保证是，在水印超过设置的超时之前，它将永远不会发生。 Similar to processing-time time-outs, there is a no strict upper bound on the delay when the time-out actually occurs. The watermark can advance only when there is data in the stream, and the event time of the data has actually advanced. 与处理时间（processing-time）超时类似，实际发生超时时，延迟没有严格的上限。只有当流中有数据，并且数据的事件时间实际提前时，水印才能前进。 NOTE 注意We mentioned this a few moments ago, but it’s worth reinforcing. Although time-outs are important, they might not always function as you expect. For instance, as of this writing, Structured Streaming does not have asynchronous job execution, which means that Spark will not output data (or time-out data) between the time that a epoch finishes and the next one starts, because it is not processing any data at that time. Also, if a processing batch of data has no records (keep in mind this is a batch, not a group), there are no updates and there cannot be an event-time time-out. This might change in future versions. 我们几分钟前提到过，但值得加强。尽管超时很重要，但它们可能并不总是如您所期望的那样工作。例如，在撰写本文时，Structured Streaming 没有异步作业执行，这意味着 Spark 不会在一个 epoch 完成和下一个 epoch 开始之间输出数据（或超时数据），因为它当时不处理任何数据。此外，如果一批处理数据没有记录（请记住，这是一批数据，而不是一个组），则不会有更新，也不会有事件超时。这在未来的版本中可能会改变。 Output ModesOne last “gotcha” when working with this sort of arbitrary stateful processing is the fact that not all output modes discussed in Chapter 21 are supported. This is sure to change as Spark continues to change, but, as of this writing, mapGroupsWithState supports only the update output mode, whereas flatMapGroupsWithState supports append and update. append mode means that only after the time-out (meaning the watermark has passed) will data show up in the result set. This does not happen automatically, it is your responsibility to output the proper row or rows. Please see Table 21-1 to see which output modes can be used when. 在处理这种任意状态处理时，最后一个“发现”是，并非第21章中讨论的所有输出模式都受支持。这肯定会随着 Spark 的不断变化而改变，但是，在本文中，mapGroupsWithState 只支持更新输出模式，而flatMapGroupsWithState 支持 append 和 update。附加（append）模式意味着只有在超时（即水印已通过）之后，数据才会显示在结果集中。这不会自动发生，您有责任输出正确的行。请参阅表21-1，了解在什么情况下可以使用哪些输出模式。 mapGroupsWithStateOur first example of stateful processing uses a feature called mapGroupsWithState. This is similar to a user-defined aggregation function that takes as input an update set of data and then resolves it down to a specific key with a set of values. There are several things you’re going to need to define along the way : 我们的第一个状态处理示例使用了一个名为 mapGroupsWithState 的特性。这类似于一个用户定义的聚合函数，它将一组更新数据作为输入，然后用一组值将其解析为一个特定的键。 在这一过程中，您需要定义以下几个方面： Three class definitions: an input definition, a state definition, and optionally an output definition. 三个类定义：输入定义、状态定义和输出定义（可选）。 A function to update the state based on a key, an iterator of events, and a previous state. 基于键、事件迭代器和前一状态更新状态的函数 。 A time-out parameter (as described in the time-outs section). 超时参数（如超时部分所述）。 With these objects and definitions, you can control arbitrary state by creating it, updating it over time, and removing it. Let’s begin with a example of simply updating the key based on a certain amount of state, and then move onto more complex things like sessionization. 通过这些对象和定义，您可以通过创建、随时间更新和删除任意状态来控制它。让我们从一个简单的基于一定数量状态更新键的示例开始，然后转到更复杂的事情，比如会话化。 Because we’re working with sensor data, let’s find the first and last timestamp that a given user performed one of the activities in the dataset. This means that the key we will be grouping on (and mapping on) is a user and activity combination. 因为我们正在处理传感器数据，所以让我们找到给定用户执行数据集中某个活动的第一个和最后一个时间戳。这意味着我们将分组（和映射）的key 是用户和活动的组合。 NOTE 注意When you use mapGroupsWithState, the output of the dream will contain only one row per key (or group) at all times. If you would like each group to have multiple outputs, you should use flatMapGroupsWithState (covered shortly). 当您使用 mapGroupsWithState 时，梦想的输出将始终只包含每个键（或组）一行。如果希望每个组有多个输出，则应使用 flatmapGroupsWithState（稍后介绍）。 Let’s establish the input, state, and output definitions: 让我们建立输入、状态和输出定义 ： 12345case class InputRow(user:String, timestamp:java.sql.Timestamp, activity:String)case class UserState(user:String,var activity:String,var start:java.sql.Timestamp,var end:java.sql.Timestamp) For readability, set up the function that defines how you will update your state based on a given row: 为了便于阅读，请设置一个函数，该函数定义如何根据给定的行更新状态： 1234567891011121314151617181920def updateUserStateWithEvent(state:UserState, input:InputRow):UserState = &#123; if (Option(input.timestamp).isEmpty) &#123; return state &#125; if (state.activity == input.activity) &#123; if (input.timestamp.after(state.end)) &#123; state.end = input.timestamp &#125; if (input.timestamp.before(state.start)) &#123; state.start = input.timestamp &#125; &#125; else &#123; if (input.timestamp.after(state.end)) &#123; state.start = input.timestamp state.end = input.timestamp state.activity = input.activity &#125; &#125; state&#125; Now, write the function that defines the way state is updated based on an epoch of rows: 现在，编写一个函数，该函数定义了基于行的 epoch 更新状态的方式： 123456789101112131415161718import org.apache.spark.sql.streaming.&#123;GroupStateTimeout, OutputMode, GroupState&#125;def updateAcrossEvents(user:String, inputs: Iterator[InputRow], oldState: GroupState[UserState]):UserState = &#123; var state:UserState = if (oldState.exists) oldState.get else UserState(user, "", new java.sql.Timestamp(6284160000000L), new java.sql.Timestamp(6284160L) ) // we simply specify an old date that we can compare against and // immediately update based on the values in our data for (input &lt;- inputs) &#123; state = updateUserStateWithEvent(state, input) oldState.update(state) &#125; state&#125; When we have that, it’s time to start your query by passing in the relevant information. The one thing that you’re going to have to add when you specify mapGroupsWithState is whether you need to timeout a given group’s state. This just gives you a mechanism to control what should be done with state that receives no update after a certain amount of time. In this case, you want to maintain state indefinitely, so specify that Spark should not time-out. Use the update output mode so that you get updates on the user activity: 当我们得到这些信息时，是时候通过传递相关信息来开始您的查询了。当您指定 mapGroupsWithState 时，您需要添加的一件事是您是否需要使给定组的状态超时。这只是为您提供了一种机制，用于控制在一定时间后不接收任何更新的状态应执行的操作。在这种情况下，您希望无限期地保持状态，因此请指定 Spark 不应超时。使用更新输出模式，以便获得用户活动的更新： 123456789101112131415import org.apache.spark.sql.streaming.GroupStateTimeoutwithEventTime.selectExpr("User as user", "cast(Creation_Time/1000000000 as timestamp) as timestamp", "gt as activity").as[InputRow].groupByKey(_.user).mapGroupsWithState(GroupStateTimeout.NoTimeout)(updateAcrossEvents).writeStream.queryName("events_per_window").format("memory").outputMode("update").start()SELECT * FROM events_per_window order by user, start Here’s a sample of our result set: 以下是我们的结果集示例： 12345678+----+--------+--------------------+--------------------+|user|activity| start | end |+----+--------+--------------------+--------------------+| a | bike |2015-02-23 13:30:...|2015-02-23 14:06:...|| a | bike |2015-02-23 13:30:...|2015-02-23 14:06:...| ...| d | bike |2015-02-24 13:07:...|2015-02-24 13:42:...|+----+--------+--------------------+--------------------+ An interesting aspect of our data is that the last activity performed at any given time is “bike.” This is related to how the experiment was likely run, in which they had each participant perform the same activities in order。 我们数据的一个有趣的方面是，在任何给定时间最后一次执行的活动是“自行车”。这与实验可能的运行方式有关，其中每个参与者依次执行相同的活动。 EXAMPLE: COUNT-BASED WINDOWSTypical window operations are built from start and end times for which all events that fall in between those two points contribute to the counting or summation that you’re performing. However, there are times when instead of creating windows based on time, you’d rather create them based on a number of events regardless of state and event times, and perform some aggregation on that window of data. For example, we may want to compute a value for every 500 events received, regardless of when they are received. The next example analyzes the activity dataset from this chapter and outputs the average reading of each device periodically, creating a window based on the count of events and outputting it each time it has accumulated 500 events for that device. You define two case classes for this task: the input row format (which is simply a device and a timestamp); and the state and output rows (which contain the current count of records collected, device ID, and an array of readings for the events in the window). 典型的窗口操作是从开始和结束时间开始构建的，在这两个时间点之间的所有事件都有助于您正在执行的计数或求和。但是，有时您不希望基于时间创建窗口，而是基于许多事件创建窗口，而不管状态和事件时间如何，并对该数据窗口执行一些聚合。例如，我们可能希望为每接收500个事件计算一个值，而不管它们何时被接收。下一个示例分析本章中的活动数据集，定期输出每个设备的平均读数，根据事件计数创建一个窗口，并在该设备累计500个事件时输出该窗口。为此任务定义两个案例类（case class）：输入行格式（简单地说是一个设备和一个时间戳）；状态行和输出行（包含收集的记录的当前计数、设备ID和窗口中事件的读取数组）。 Here are our various, self-describing case class definitions : 下面是我们的各种自我描述的案例类定义： 123case class InputRow(device: String, timestamp: java.sql.Timestamp, x : Double)case class DeviceState(device: String, var values: Array[Double], var count: Int)case class OutputRow(device: String, previousAverage: Double) Now, you can define the function to update the individual state based on a single input row. You could write this inline or in a number of other ways, but this example makes it easy to see exactly how you update based on a given row : 现在，您可以定义函数来更新基于单个输入行的单个状态。您可以以内联或其他多种方式编写此代码，但此示例使您很容易确切地了解如何根据给定行进行更新： 123456def updateWithEvent(state:DeviceState, input:InputRow):DeviceState = &#123; state.count += 1 // maintain an array of the x-axis values state.values = state.values ++ Array(input.x) state&#125; Now it’s time to define the function that updates across a series of input rows. Notice in the example that follows that we have a specific key, the iterator of inputs, and the old state, and we update that old state over time as we receive new events. This, in turn, will return our output rows with the updates on a per-device level based on the number of counts it sees. This case is quite straightforward, after a given number of events, you update the state and reset it. You then create an output row. You can see this row in the output table : 现在是定义跨一系列输入行更新的函数的时候了。注意在下面的示例中，我们有一个特定的键、输入的迭代器和旧状态，在接收新事件时，我们会随着时间的推移更新旧状态。反过来，这将返回我们的输出行，并根据它看到的计数数在每个设备级别上进行更新。这种情况非常简单，在给定数量的事件之后，您将更新状态并重置它。然后创建一个输出行。您可以在输出表中看到此行： 123456789101112131415161718192021import org.apache.spark.sql.streaming.&#123;GroupStateTimeout, OutputMode,GroupState&#125;def updateAcrossEvents(device:String, inputs: Iterator[InputRow], oldState : GroupState[DeviceState]) : Iterator[OutputRow] = &#123; inputs.toSeq.sortBy(_.timestamp.getTime).toIterator.flatMap &#123; input =&gt; val state = if (oldState.exists) oldState.get else DeviceState(device, Array(), 0) val newState = updateWithEvent(state, input) if (newState.count &gt;= 500) &#123; // One of our windows is complete; replace our state with an empty // DeviceState and output the average for the past 500 items from // the old state oldState.update(DeviceState(device, Array(), 0)) Iterator(OutputRow(device, newState.values.sum / newState.values.length.toDouble)) &#125; else &#123; // Update the current DeviceState object in place and output no // records oldState.update(newState) Iterator() &#125; &#125;&#125; Now you can run your stream. You will notice that you need to explicitly state the output mode, which is append. You also need to set a GroupStateTimeout. This time-out specifies the amount of time you want to wait before a window should be output as complete (even if it did not reach the required count). In that case, set an infinite time-out, meaning if a device never gets to that required 500 count threshold, it will maintain that state forever as “incomplete” and not output it to the result table. By specifying both of those parameters you can pass in the updateAcrossEvents function and start the stream: 现在你可以运行你的流。您将注意到您需要显式地声明输出模式，即 append。还需要设置 GroupStateTimeout。此超时指定在窗口输出完成之前要等待的时间量（即使它未达到所需的计数）。在这种情况下，设置一个无限的超时，这意味着如果一个设备永远无法达到所需的500计数阈值，它将永远保持该状态为“不完整”，而不会将其输出到结果表。通过指定这两个参数，可以在 updateAcrossEvents 函数中传递并启动流： 123456789101112import org.apache.spark.sql.streaming.GroupStateTimeoutwithEventTime.selectExpr("Device as device","cast(Creation_Time/1000000000 as timestamp) as timestamp", "x").as[InputRow].groupByKey(_.device).flatMapGroupsWithState(OutputMode.Append, GroupStateTimeout.NoTimeout)(updateAcrossEvents).writeStream.queryName("count_based_device").format("memory").outputMode("append").start() After you start the stream, it’s time to query it. Here are the results: 启动流之后，是时候查询它了。结果如下： 1SELECT * FROM count_based_device 12345678+--------+--------------------+| device | previousAverage |+--------+--------------------+|nexus4_1| 4.660034012E-4 ||nexus4_1|0.001436279298199...||nexus4_1|1.049804683999999...||nexus4_1|-0.01837188737960...|+--------+--------------------+ You can see the values change over each of those windows as you append new data to the result set. 当您向结果集附加新数据时，可以看到这些窗口中的每个窗口的值都发生了变化。 flatMapGroupsWithStateOur second example of stateful processing will use a feature called flatMapGroupsWithState. This is quite similar to mapGroupsWithState except that rather than just having a single key with at most one output, a single key can have many outputs. This can provide us a bit more flexibility and the same fundamental structure as mapGroupsWithState applies. Here’s what we’ll need to define. 我们的第二个状态处理示例将使用名为 flatmapGroupsWithState 的功能。这与 mapGroupsWithState 非常相似，只是一个键最多只能有一个输出，而不是只有一个键可以有多个输出。这可以为我们提供更多的灵活性和与mapGroupsWithState 应用相同的基本结构。以下是我们需要定义的内容。 Three class definitions: an input definition, a state definition, and optionally an output definition.三个类定义：输入定义、状态定义和输出定义（可选）。 A function to update the state based on a key, an iterator of events, and a previous state.基于键、事件迭代器和前一状态更新状态的函数。 A time-out parameter (as described in the time-outs section).超时参数（如超时部分所述）。 With these objects and definitions, we can control arbitrary state by creating it, updating it over time, and removing it. Let’s start with an example of sessionization. 通过这些对象和定义，我们可以通过创建、随时间更新和删除任意状态来控制它。让我们从会话化的例子开始。 EXAMPLE: SESSIONIZATIONSessions are simply unspecified time windows with a series of events that occur. Typically, you want to record these different events in an array in order to compare these sessions to other sessions in the future. In a session, you will likely have arbitrary logic to maintain and update your state over time as well as certain actions to define when state ends (like a count) or a simple time-out. Let’s build on the previous example and define it a bit more strictly as a session. At times, you might have an explicit session ID that you can use in your function. This obviously makes it much easier because you can just perform a simple aggregation and might not even need your own stateful logic. In this case, you’re creating sessions on the fly from a user ID and some time information and if you see no new event from that user in five seconds, the session terminates. You’ll also notice that this code uses time-outs differently than we have in other examples. 会话仅是具有一系列事件的不明确时间的窗口。通常，您希望在数组中记录这些不同的事件，以便将来将这些会话与其他会话进行比较。在会话中，您可能拥有随时间去维护和更新状态的任意逻辑，以及定义状态何时结束（如计数）或简单超时的某些操作。让我们在前面的示例基础上进行构建，并将其更严格地定义为一个会话。有时，您可能有一个显式会话ID，可以在函数中使用。这显然使它变得更容易，因为您可以执行简单的聚合，甚至可能不需要自己的状态逻辑。在这种情况下，您将根据用户ID和一些时间信息动态创建会话，如果在五秒钟内没有看到该用户的新事件，会话将终止。您还将注意到，此代码使用超时的方式与其他示例中的不同。 You can follow the same process of creating your classes, defining our single event update function and then the multievent update function: 您可以遵循创建类，定义单个事件更新函数，然后定义多事件更新函数的相同过程 123456789101112131415161718192021222324252627282930313233343536373839404142434445case class InputRow(uid:String, timestamp:java.sql.Timestamp, x:Double,activity:String)case class UserSession(val uid:String, var timestamp:java.sql.Timestamp,var activities: Array[String], var values: Array[Double])case class UserSessionOutput(val uid:String, var activities: Array[String],var xAvg:Double)def updateWithEvent(state:UserSession, input:InputRow):UserSession = &#123; // handle malformed dates if (Option(input.timestamp).isEmpty) &#123; return state &#125; state.timestamp = input.timestamp state.values = state.values ++ Array(input.x) if (!state.activities.contains(input.activity)) &#123; state.activities = state.activities ++ Array(input.activity) &#125; state&#125; import org.apache.spark.sql.streaming.&#123;GroupStateTimeout, OutputMode,GroupState&#125;def updateAcrossEvents(uid:String, inputs: Iterator[InputRow],oldState: GroupState[UserSession]):Iterator[UserSessionOutput] = &#123; inputs.toSeq.sortBy(_.timestamp.getTime).toIterator.flatMap &#123; input =&gt; val state = if (oldState.exists) oldState.get else UserSession(uid, new java.sql.Timestamp(6284160000000L),Array(), Array()) val newState = updateWithEvent(state, input) if (oldState.hasTimedOut) &#123; val state = oldState.get oldState.remove() Iterator(UserSessionOutput(uid, state.activities, newState.values.sum / newState.values.length.toDouble)) &#125; else if (state.values.length &gt; 1000) &#123; val state = oldState.get oldState.remove() Iterator(UserSessionOutput(uid, state.activities, newState.values.sum / newState.values.length.toDouble)) &#125; else &#123; oldState.update(newState) oldState.setTimeoutTimestamp(newState.timestamp.getTime(), "5 seconds") Iterator() &#125; &#125;&#125; You’ll see in this one that we only expect to see an event at most five seconds late. Anything other than that and we will ignore it. We will use an EventTimeTimeout to set that we want to time-out based on the event time in this stateful operation : 你会看到在这一个，我们只希望看到一个事件最迟迟五秒。除此之外，我们将忽略它。我们将使用EventTimeTimeout 来设置希望基于此状态操作中的事件时间超时： 1234567891011121314import org.apache.spark.sql.streaming.GroupStateTimeoutwithEventTime.where("x is not null").selectExpr("user as uid", "cast(Creation_Time/1000000000 as timestamp) as timestamp", "x", "gt as activity").as[InputRow].withWatermark("timestamp", "5 seconds").groupByKey(_.uid).flatMapGroupsWithState(OutputMode.Append,GroupStateTimeout.EventTimeTimeout)(updateAcrossEvents).writeStream.queryName("count_based_device").format("memory").start() Querying this table will show you the output rows for each user over this time period: 查询此表将显示此时间段内每个用户的输出行： As you might expect, sessions that have a number of activities in them have a higher x-axis gyroscope value than ones that have fewer activities. It should be trivial to extend this example to problem sets more relevant to your own domain, as well. 正如您可能期望的那样，具有许多活动的会话比具有较少活动的会话具有更高的X轴陀螺仪值。将这个示例扩展到与您自己的域更相关的问题集应该是很简单的。 Conclusion 结论This chapter covered some of the more advanced topics in Structured Streaming, including event time and stateful processing. This is effectively the user guide to help you actually build out your application logic and turn it into something that provides value. Next, we will discuss what we’ll need to do in order to take this application to production and maintain and update it over time. 本章介绍结构化流中的一些更高级的主题，包括事件时间和状态处理。这实际上是一个用户指南，可以帮助您实际构建应用程序逻辑，并将其转化为提供价值的东西。接下来，我们将讨论需要做什么，以便将此应用程序投入生产，并随着时间的推移对其进行维护和更新。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 21 Structured Streaming Basics]]></title>
    <url>%2F2019%2F06%2F28%2FChapter21_StructuredStreamingBasics(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 21 Structured Streaming Basics 结构化流基础Now that we have covered a brief overview of stream processing, let’s dive right into Structured Streaming. In this chapter, we will, again, state some of the key concepts behind Structured Streaming and then apply them with some code examples that show how easy the system is to use. 既然我们已经介绍了流处理的简要概述，那么让我们直接深入到 Structured Streaming 中。在本章中，我们将再次说明 Structured Streaming 背后的一些关键概念，然后将它们与一些代码示例一起应用，这些代码示例说明系统的易用性。 Structured Streaming Basics 结构化流基础Structured Streaming, as we discussed at the end of Chapter 20, is a stream processing framework built on the Spark SQL engine. Rather than introducing a separate API, Structured Streaming uses the existing structured APIs in Spark (DataFrames, Datasets, and SQL), meaning that all the operations you are familiar with there are supported. Users express a streaming computation in the same way they’d write a batch computation on static data. Upon specifying this, and specifying a streaming destination, the Structured Streaming engine will take care of running your query incrementally and continuously as new data arrives into the system. These logical instructions for the computation are then executed using the same Catalyst engine discussed in Part II of this book, including query optimization, code generation, etc. Beyond the core structured processing engine, Structured Streaming includes a number of features specifically for streaming. For instance, Structured Streaming ensures end-to-end, exactly-once processing as well as fault-tolerance through checkpointing and write-ahead logs. Structured Streaming，正如我们在第20章末尾所讨论的，是基于Spark SQL引擎构建的流处理框架。Structured Streaming 处理不引入单独的API，而是使用Spark中现有的结构化API（DataFrames、Datasets 和 SQL），这意味着支持您熟悉的所有操作。用户表达流计算的方式与对静态数据编写批处理计算的方式相同。在指定了这一点并指定了流目的地之后，Structured Streaming 引擎将负责在新数据到达系统时以增量和连续的方式运行查询。然后使用本书第二部分中讨论的相同Catalyst引擎执行这些计算逻辑指令，包括查询优化、代码生成等。除了核心结构化处理引擎之外，Structured Streaming 还包括一些专门与流相关的特性。例如，Structured Streaming 通过检查点和提前写入日志确保端到端、一次性处理以及容错性。 The main idea behind Structured Streaming is to treat a stream of data as a table to which data is continuously appended. The job then periodically checks for new input data, process it, updates some internal state located in a state store if needed, and updates its result. A cornerstone of the API is that you should not have to change your query’s code when doing batch or stream processing—you should have to specify only whether to run that query in a batch or streaming fashion. Internally, Structured Streaming will automatically figure out how to “incrementalize” your query, i.e., update its result efficiently whenever new data arrives, and will run it in a fault-tolerant fashion. Structured Streaming 背后的主要思想是将数据流视为一个连续追加数据的表。然后，该作业会定期检查新的输入数据、处理它、根据需要更新位于状态存储中的某些内部状态，并更新其结果。API的一个基石是，在进行批处理或流处理时，不必更改查询的代码，只需指定是以批处理还是流方式运行该查询。在内部，Structured Streaming 将自动计算出如何“增量化”查询，即在新数据到达时高效地更新其结果，并以容错的方式运行它。 In simplest terms, Structured Streaming is “your DataFrame, but streaming.” This makes it very easy to get started using streaming applications. You probably already have the code for them! There are some limits to the types of queries Structured Streaming will be able to run, however, as well as some new concepts you have to think about that are specific to streaming, such as event-time and out of-order data. We will discuss these in this and the following chapters. 简单来说，Structured Streaming 是“你的DataFrame，却是流”，这使得开始使用流应用程序变得非常容易。你可能已经有了他们的代码了！但是，结构化流能够运行的查询类型有一些限制，以及一些您必须考虑的特定于流媒体的新概念，例如事件时间和无序数据。我们将在本章和以下章节中讨论这些问题。 Finally, by integrating with the rest of Spark, Structured Streaming enables users to build what we call continuous applications. A continuous application is an end-to-end application that reacts to data in real time by combining a variety of tools: streaming jobs, batch jobs, joins between streaming and offline data, and interactive ad-hoc queries. Because most streaming jobs today are deployed within the context of a larger continuous application, the Spark developers sought to make it easy to specify the whole application in one framework and get consistent results across these different portions of it. For example, you can use Structured Streaming to continuously update a table that users query interactively with Spark SQL, serve a machine learning model trained by MLlib, or join streams with offline data in any of Spark’s data sources—applications that would be much more complex to build using a mix of different tools. 最后，通过与Spark的其余部分集成，Structured Streaming 使用户能够构建我们所称的连续应用程序。连续应用程序是一个端到端的应用程序，它通过组合各种工具实时响应数据：流式作业、批处理作业、流式和离线数据之间的连接以及交互式临时查询。由于目前大多数流作业都部署在一个更大的连续应用程序的环境中，Spark开发人员试图让在一个框架中指定整个应用程序这件事变得容易，并在其中的不同部分获得一致的结果。例如，您可以使用 Structured Streaming 来连续更新用户使用 Spark Sql 进行交互查询的表，提供由 MLlib 训练的机器学习模型，或者连接流和在spark的任何数据源应用程序中使用的离线数据，混合使用不同的工具来构建这些应用程序会复杂得多。 Core Concepts 核心概念Now that we introduced the high-level idea, let’s cover some of the important concepts in a Structured Streaming job. One thing you will hopefully find is that there aren’t many. That’s because Structured Streaming is designed to be simple. Read some other big data streaming books and you’ll notice that they begin by introducing terminology like distributed stream processing topologies for skewed data reducers (a caricature, but accurate) and other complex verbiage. Spark’s goal is to handle these concerns automatically and give users a simple way to run any Spark computation on a stream. Transformations and Actions Structured Streaming maintains the same concept of transformations and actions that we have seen throughout this book. The transformations available in Structured Streaming are, with a few restrictions, the exact same transformations that we saw in Part II. The restrictions usually involve some types of queries that the engine cannot incrementalize yet, although some of the limitations are being lifted in new versions of Spark. There is generally only one action available in Structured Streaming: that of starting a stream, which will then run continuously and output results. 既然我们介绍了高级概念，那么让我们来介绍 Structured Streaming 作业中的一些重要概念。希望你能发现的一件事是没有太多。这是因为 Structured Streaming 的设计很简单。阅读其他一些大数据流书籍，您会注意到，它们首先介绍了一些术语，如用于倾斜的数据的分布式数据流处理拓扑（一个漫画，但准确）和其他复杂的措辞。Spark的目标是自动处理这些问题，并为用户提供一种在流上运行任何Spark计算的简单方法。转换（Transformations ）和动作（Actions ）Structured Streaming 保持了与我们在本书中看到的转换和操作的相同概念。Structured Streaming 中可用的转换，有一些限制，与第二部分中看到的转换完全相同。这些限制通常涉及到一些类型的查询，但是引擎还不能增加这些查询，尽管在新版本的spark中正在解除一些限制。在Structured Streaming 中通常只有一个操作可用：启动流，然后该流将连续运行并输出结果。 Input Sources 输入源Structured Streaming supports several input sources for reading in a streaming fashion. As of Spark 2.2, the supported input sources are as follows: Structured Streaming 支持以流方式读取的多个输入源。从Spark 2.2 开始，支持的输入源如下： Apache Kafka 0.10 Files on a distributed file system like HDFS or S3 (Spark will continuously read new files in a directory) 分布式文件系统上的文件，如 hdfs 或 s3（Spark将连续读取目录中的新文件）。 A socket source for testing 用于测试的套接字源。 We discuss these in depth later in this chapter, but it’s worth mentioning that the authors of Spark are working on a stable source API so that you can build your own streaming connectors. 在本章后面我们将深入讨论这些内容，但值得一提的是，Spark的作者正在开发一个稳定的源API，以便您可以构建自己的流式连接器。 Sinks 接收器Just as sources allow you to get data into Structured Streaming, sinks specify the destination for the result set of that stream. Sinks and the execution engine are also responsible for reliably tracking the exact progress of data processing. Here are the supported output sinks as of Spark 2.2 : 正如源允许您将数据获取到 Structured Streaming 中一样，接收器为该流的结果集指定目标。接收器和执行引擎还负责可靠地跟踪数据处理的准确进度。以下是从 spark 2.2 开始支持的输出接收器： Apache Kafka 0.10 Almost any file format 几乎所有文件格式 A foreach sink for running arbitary computation on the output records 用于对输出记录进行任意计算的 foreach 接收器 A console sink for testing 用于测试的控制台接收器 A memory sink for debugging 用于调试的内存接收器 We discuss these in more detail later in the chapter when we discuss sources. 在本章后面讨论来源时，我们将更详细地讨论这些内容。 Output Modes 输出模式Defining a sink for our Structured Streaming job is only half of the story. We also need to define how we want Spark to write data to that sink. For instance, do we only want to append new information? Do we want to update rows as we receive more information about them over time (e.g., updating the click count for a given web page)? Do we want to completely overwrite the result set every single time (i.e. always write a file with the complete click counts for all pages)? To do this, we define an output mode, similar to how we define output modes in the static Structured APIs. 为我们的 Structured Streaming 作业定义一个接收器只是故事的一半。我们还需要定义如何希望 Spark 将数据写入该接收器。例如，我们只想附加新信息吗？我们是否希望随着时间的推移更新有关行的更多信息（例如，更新给定网页的单击计数）？是否每次都要完全覆盖结果集（即始终为所有页面编写具有完整单击计数的文件）？为此，我们定义了一个输出模式，类似于我们如何在静态结构化API中定义输出模式。 The supported output modes are as follows: 支持的输出模式如下： Append (only add new records to the output sink) 追加（仅向输出接收器添加新记录） Update (update changed records in place) 更新（更新已更改的记录） Complete (rewrite the full output)完整（重写完整输出） One important detail is that certain queries, and certain sinks, only support certain output modes, as we will discuss later in the book. For example, suppose that your job is just performing a map on a stream. The output data will grow indefinitely as new records arrive, so it would not make sense to use Complete mode, which requires writing all the data to a new file at once. In contrast, if you are doing an aggregation into a limited number of keys, Complete and Update modes would make sense, but Append would not, because the values of some keys’ need to be updated over time. 一个重要的细节是，某些查询和某些接收器只支持某些输出模式，正如我们将在本书后面讨论的那样。例如，假设您的工作只是在流上执行映射。当新记录到达时，输出数据将无限增长，因此使用完整模式是没有意义的，这需要立即将所有数据写入新文件。相反，如果您将聚合到有限数量的键中，则完整和更新模式是有意义的，但追加模式是没有意义的，因为某些键的值需要随着时间的推移而更新。 Triggers 触发器Whereas output modes define how data is output, triggers define when data is output—that is, when Structured Streaming should check for new input data and update its result. By default, Structured Streaming will look for new input records as soon as it has finished processing the last group of input data, giving the lowest latency possible for new results. However, this behavior can lead to writing many small output files when the sink is a set of files. Thus, Spark also supports triggers based on processing time (only look for new data at a fixed interval). In the future, other types of triggers may also be supported. 虽然输出模式定义了数据的输出方式，但触发器定义了何时输出数据，也就是说，Structured Streaming 应该检查新的输入数据并更新其结果。默认情况下，Structured Streaming 将在处理完最后一组输入数据后立即查找新的输入记录，从而为新结果提供尽可能低的延迟。但是，当接收器是一组文件时，这种行为会导致写入许多小的输出文件。因此，Spark 还支持基于处理时间的触发器（只在固定的时间间隔内查看新数据）。将来，还可能支持其他类型的触发器。 Event-Time Processing 事件时间处理Structured Streaming also has support for event-time processing (i.e., processing data based on timestamps included in the record that may arrive out of order). There are two key ideas that you will need to understand here for the moment; we will talk about both of these in much more depth in the next chapter, so don’t worry if you’re not perfectly clear on them at this point. Structured Streaming 还支持事件时间处理（即基于记录中可能出现无序的时间戳处理数据）。现在有两个关键的想法需要你理解；我们将在下一章更深入地讨论这两个问题，因此，如果你在这一点上不完全清楚，不要担心。 Event-time data 事件-时间数据Event-time means time fields that are embedded in your data. This means that rather than processing data according to the time it reaches your system, you process it according to the time that it was generated, even if records arrive out of order at the streaming application due to slow uploads or network delays. Expressing event-time processing is simple in Structured Streaming. Because the system views the input data as a table, the event time is just another field in that table, and your application can do grouping, aggregation, and windowing using standard SQL operators. However, under the hood, Structured Streaming can take some special actions when it knows that one of your columns is an event time field, including optimizing query execution or determining when it is safe to forget state about a time window. Many of these actions can be controlled using watermarks. 事件-时间表示嵌入在数据中的时间字段。这意味着，您不必根据数据到达系统的时间来处理数据，而是根据数据生成的时间来处理数据，即使由于上传速度慢或网络延迟，记录在流式应用程序中出现无序。在 Structured Streaming 中，表示事件时间处理很简单。因为系统将输入数据视为一个表，所以事件时间只是该表中的另一个字段，您的应用程序可以使用标准的 SQL 运算符进行分组、聚合和窗口化。然而，在底层，当结构化流知道某个列是事件时间字段时，它可以采取一些特殊的操作，包括优化查询执行或确定何时可以安全地忘记时间窗口的状态。其中许多操作可以使用水印进行控制。 Watermarks 水印Watermarks are a feature of streaming systems that allow you to specify how late they expect to see data in event time. For example, in an application that processes logs from mobile devices, one might expect logs to be up to 30 minutes late due to upload delays. Systems that support event time, including Structured Streaming, usually allow setting watermarks to limit how long they need to remember old data. Watermarks can also be used to control when to output a result for a particular event time window (e.g., waiting until the watermark for it has passed). 水印是流系统的一个特性，它允许您指定它们期望在事件时间内看到数据的时间有多晚。例如，在处理来自移动设备的日志的应用程序中，由于上载延迟，日志可能会延迟30分钟。支持事件时间（包括 Structured Streaming ）的系统通常允许设置水印以限制它们需要记住旧数据的时间。水印还可用于控制何时为特定事件时间窗口输出结果（例如，等待水印通过）。 Structured Streaming in Action 结构化流动作Let’s get to an applied example of how you might use Structured Streaming. For our examples, we’re going to be working with the Heterogeneity Human Activity Recognition Dataset. The data consists of smartphone and smartwatch sensor readings from a variety of devices—specifically, the accelerometer and gyroscope, sampled at the highest possible frequency supported by the devices. Readings from these sensors were recorded while users performed activities like biking, sitting, standing, walking, and so on. There are several different smartphones and smartwatches used, and nine total users. You can download the data here, in the activity data folder. 让我们来看一个如何使用结构化流的应用示例。对于我们的例子，我们将使用 Heterogeneity Human Activity Recognition（多种不同人类活动识别）数据集。数据由智能手机和智能手表传感器组成，这些传感器的读数来自各种设备，特别是加速度计和陀螺仪，它们以设备支持的最高频率进行采样。当用户进行诸如骑自行车、坐、站、走等活动时，这些传感器的读数被记录下来。使用了几种不同的智能手机和智能手表，总共有九个用户。您可以在“activity data”文件夹中的此处下载数据。 TIP 提示This Dataset is fairly large. If it’s too large for your machine, you can remove some of the files and it will work just fine. 这个数据集相当大。如果它对您的机器来说太大，您可以删除一些文件，它将正常工作。 Let’s read in the static version of the dataset as a DataFrame : 让我们将数据集的静态版本作为 DataFrame 读取 123// in Scalaval static = spark.read.json("/data/activity-data/")val dataSchema = static.schema 1234//in Pythonstatic = spark.read.json("/data/activity-data/")dataSchema = static.schemaHere’s the schema: Here’s the schema: 这是数据模式： 123456789101112root|-- Arrival_Time: long (nullable = true)|-- Creation_Time: long (nullable = true)|-- Device: string (nullable = true)|-- Index: long (nullable = true)|-- Model: string (nullable = true)|-- User: string (nullable = true)|-- corrupt_record: string (nullable = true)|-- gt: string (nullable = true)|-- x: double (nullable = true)|-- y: double (nullable = true)|-- z: double (nullable = true) Here’s a sample of the DataFrame: 这是 DataFrame 的一个样例 123456+-------------+------------------+--------+-----+------+----+--------+-----+-----| Arrival_Time| Creation_Time | Device |Index| Model|User|_c...ord|. gt | x|1424696634224|142469663222623685|nexus4_1| 62 |nexus4| a | null |stand|-0......|1424696660715|142469665872381726|nexus4_1| 2342|nexus4| a | null |stand|-0...+-------------+------------------+--------+-----+------+----+--------+-----+----- You can see in the preceding example, which includes a number of timestamp columns, models, user, and device information. The gt field specifies what activity the user was doing at that time. 您可以在前面的示例中看到，其中包括许多时间戳列、模型、用户和设备信息。gt 字段指定用户当时正在执行的活动。 Next, let’s create a streaming version of the same Dataset, which will read each input file in the dataset one by one as if it was a stream. 接下来，让我们创建同一个数据集的流式版本，它将逐个读取数据集中的每个输入文件，就像它是一个流一样。 Streaming DataFrames are largely the same as static DataFrames. We create them within Spark applications and then perform transformations on them to get our data into the correct format. Basically, all of the transformations that are available in the static Structured APIs apply to Streaming DataFrames. However, one small difference is that Structured Streaming does not let you perform schema inference without explicitly enabling it. You can enable schema inference for this by setting the configuration spark.sql.streaming.schemaInference to true. Given that fact, we will read the schema from one file (that we know has a valid schema) and pass the data Schema object from our static DataFrame to our streaming DataFrame. As mentioned, you should avoid doing this in a production scenario where your data may (accidentally) change out from under you : Streaming DataFrames 在很大程度上与静态 DataFrames 相同。我们在 Spark 应用程序中创建它们，然后对它们进行转换，以将数据转换为正确的格式。基本上，静态结构化 API 中可用的所有转换都应用于 Streaming DataFrames。然而，一个小的区别是结构化流不允许您在没有显式启用模式推断的情况下执行模式推断。您可以通过设置配置为此启用模式推断 spark.sql.streaming.schemaInterrusion 为true。鉴于这个事实，我们将从一个文件（我们知道有一个有效的模式）中读取模式，并将数据模式对象从静态 DataFrame 传递到 streaming DataFrame。如前所述，在生产场景中，您应该避免这样做，因为您的数据可能（意外）从您的下面更改： 123// in Scalaval streaming = spark.readStream.schema(dataSchema).option("maxFilesPerTrigger", 1).json("/data/activity-data") 123# in Pythonstreaming = spark.readStream.schema(dataSchema).option("maxFilesPerTrigger", 1)\.json("/data/activity-data") NOTE 注释We discuss maxFilesPerTrigger a little later on in this chapter but essentially it allows you to control how quickly Spark will read all of the files in the folder. By specifying this value lower, we’re artificially limiting the flow of the stream to one file per trigger. This helps us demonstrate how Structured Streaming runs incrementally in our example, but probably isn’t something you’d use in production. 我们稍后在本章中讨论 MaxFilePertrigger，但实际上它允许您控制Spark读取文件夹中所有文件的速度。通过将该值指定得更低，我们人为地将流限制为每个触发器读取一个文件。这有助于我们演示 Structured Streaming 在我们的示例中是如何增量运行的，但可能不是您在生产中使用的。 Just like with other Spark APIs, streaming DataFrame creation and execution is lazy. In particular, wecan now specify transformations on our streaming DataFrame before finally calling an action to start the stream. In this case, we’ll show one simple transformation—we will group and count data by the gt column, which is the activity being performed by the user at that point in time: 与其他Spark API一样，streaming DataFrame 的创建和执行也是惰性的。特别是，我们现在可以在最后调用一个动作来启动流之前，在 streaming DataFrame 上指定转换。在这种情况下，我们将展示一个简单的转换，我们将按gt列对数据进行分组和计数，这是用户在该时间点执行的活动： 12// in Scalaval activityCounts = streaming.groupBy("gt").count() 12# in PythonactivityCounts = streaming.groupBy("gt").count() Because this code is being written in local mode on a small machine, we are going to set the shuffle partitions to a small value to avoid creating too many shuffle partitions: 因为这段代码是在一台小型机器上以本地模式写入的，所以我们要将洗牌（shuffle）分区设置为一个较小的值，以避免创建过多的洗牌（shuffle）分区： 1spark.conf.set("spark.sql.shuffle.partitions", 5) Now that we set up our transformation, we need only to specify our action to start the query. As mentioned previously in the chapter, we will specify an output destination, or output sink for our result of this query. For this basic example, we are going to write to a memory sink which keeps an in-memory table of the results. 既然我们已经设置了转换，我们只需要指定我们的操作来启动查询。如前一章所述，我们将为这个查询的结果指定一个输出目的地或输出接收器（sink）。对于这个基本的例子，我们将数据写到一个内存接收器，它保存一个结果的内存表。 In the process of specifying this sink, we’re going to need to define how Spark will output that data. In this example, we use the complete output mode. This mode rewrites all of the keys along with their counts after every trigger: 在指定这个接收器（sink）的过程中，我们需要定义Spark将如何输出这些数据。在这个例子中，我们使用完整的输出模式。此模式在每次触发后重写所有键及其计数： 1234// in Scalaval activityQuery = activityCounts.writeStream.queryName("activity_counts").format("memory").outputMode("complete").start() 1234# in PythonactivityQuery = activityCounts.writeStream.queryName("activity_counts")\.format("memory").outputMode("complete")\.start() We are now writing out our stream! You’ll notice that we set a unique query name to represent this stream, in this case activity_counts. We specified our format as an in-memory table and we set the output mode. 我们现在正在写我们的流！您会注意到，我们设置了一个唯一的查询名称来表示这个流，在本例中是 activity_counts 。我们将格式指定为内存中的表，并设置输出模式。 When we run the preceding code, we also want to include the following line: 当我们运行上述代码时，我们还希望包括以下行： 1activityQuery.awaitTermination() After this code is executed, the streaming computation will have started in the background. The query object is a handle to that active streaming query, and we must specify that we would like to wait for the termination of the query using activityQuery.awaitTermination() to prevent the driver process from exiting while the query is active. We will omit this from our future parts of the book for readability, but it must be included in your production applications; otherwise, your stream won’t be able to run. 执行此代码后，流计算将在后台启动。查询对象是该活动流查询的句柄（handle），我们必须指定要使用 activityquery.wait termination()等待查询的终止，以防止查询（query）还在激活时驱动程序进程退出。为了可读性，我们将在书的未来部分中省略这一点，但它必须包含在您的生产应用程序中；否则，流将无法运行。 Spark lists this stream, and other active ones, under the active streams in our SparkSession. We can see a list of those streams by running the following: Spark 在 SparkSession 中的活动流下列出此流和其他活动流。我们可以通过运行以下命令来查看这些流的列表： 1spark.streams.active Spark also assigns each stream a UUID, so if need be you could iterate through the list of running streams and select the above one. In this case, we assigned it to a variable, so that’s not necessary. Spark 还为每个流分配一个 UUID，因此如果需要，您可以遍历正在运行的流列表并选择上面的流。在这种情况下，我们把它赋给一个变量，这样就不需要了。 Now that this stream is running, we can experiment with the results by querying the in-memory table it is maintaining of the current output of our streaming aggregation. This table will be called activity_counts, the same as the stream. To see the current data in this output table, we simply need to query it! We’ll do this in a simple loop that will print the results of the streaming query every second: 现在这个流正在运行，我们可以通过查询内存中的表来试验结果，该表维护流聚合的当前输出。此表将被称为 activity_counts ，与流相同。要查看这个输出表中的当前数据，我们只需要查询它！我们将在一个简单的循环中执行此操作，该循环将每秒打印流式查询的结果： 12345// in Scalafor( i &lt;- 1 to 5 ) &#123;spark.sql("SELECT * FROM activity_counts").show()Thread.sleep(1000)&#125; 12345#in Pythonfrom time import sleepfor x in range(5):spark.sql("SELECT * FROM activity_counts").show()sleep(1) As the preceding queries run, you should see the counts for each activity change over time. For instance, the first show call displays the following result (because we queried it while the stream was reading the first file): 当前面的查询运行时，您应该看到每个活动的计数随着时间的推移而变化。例如，第一个show调用显示以下结果（因为我们在流读取第一个文件时查询了它）： 1234+---+-----+| gt|count|+---+-----++---+-----+ The previous show call shows the following result—note that the result will probably vary when you’re running this code personally because you will likely start it at a different time: 上一个 show 调用显示以下结果注意，当您亲自运行此代码时，结果可能会有所不同，因为您可能会在不同的时间启动它： 12345678+----------+-----+| gt |count|+----------+-----+| sit | 8207|...| null | 6966|| bike | 7199|+----------+-----+ With this simple example, the power of Structured Streaming should become clear. You can take the same operations that you use in batch and run them on a stream of data with very few code changes (essentially just specifying that it’s a stream). The rest of this chapter touches on some of the details about the various manipulations, sources, and sinks that you can use with Structured Streaming 通过这个简单的例子，Structured Streaming 的力量应该变得清晰。您可以采用批处理中使用的相同操作，并在代码更改很少的数据流上运行它们（本质上只是指定它是一个流）。本章的其余部分将详细介绍可用于 Structured Streaming 的各种操作、源和接收器。 Transformations on Streams 流上的转换Streaming transformations, as we mentioned, include almost all static DataFrame transformations that you already saw in Part II. All select, filter, and simple transformations are supported, as are all DataFrame functions and individual column manipulations. The limitations arise on transformations that do not make sense in context of streaming data. For example, as of Apache Spark 2.2, users cannot sort streams that are not aggregated, and cannot perform multiple levels of aggregation without using Stateful Processing (covered in the next chapter). These limitations may be lifted as Structured Streaming continues to develop, so we encourage you to check the documentation of your version of Spark for updates. 正如我们所提到的，流式转换几乎包括了您在第二部分中已经看到的所有静态 DataFrame 转换。支持所有选择、筛选和简单转换，以及所有 DataFrame 函数和单个列操作。在流式数据环境中，转换会产生一些不合理的限制。例如，从 Apache Spark 2.2 开始，用户不能对未聚合的流进行排序，并且不能在不使用状态处理（Stateful Processing）的情况下执行多个级别的聚合（在下一个章中介绍）。随着结构化流的不断发展，这些限制可能会被解除，因此我们鼓励您检查 Spark 版本的文档以获取更新。 Selections and Filtering 选择和筛选All select and filter transformations are supported in Structured Streaming, as are all DataFrame functions and individual column manipulations. We show a simple example using selections and filtering below. In this case, because we are not updating any keys over time, we will use the Append output mode, so that new results are appended to the output table: Structured Streaming 中支持所有选择和筛选转换，以及所有 DataFrame 函数和单个列操作。我们用下面的选择和过滤来展示一个简单的例子。在这种情况下，由于我们不会随时间更新任何键，因此我们将使用附加（Append ）输出模式，以便将新结果附加到输出表： 1234567891011// in Scalaimport org.apache.spark.sql.functions.exprval simpleTransform = streaming.withColumn("stairs", expr("gt like '%stairs%'")).where("stairs").where("gt is not null").select("gt", "model", "arrival_time", "creation_time").writeStream.queryName("simple_transform").format("memory").outputMode("append").start() 1234567891011# in Pythonfrom pyspark.sql.functions import exprsimpleTransform = streaming.withColumn("stairs", expr("gt like '%stairs%'"))\.where("stairs")\.where("gt is not null")\.select("gt", "model", "arrival_time", "creation_time")\.writeStream\.queryName("simple_transform")\.format("memory")\.outputMode("append")\.start() Aggregations 聚合Structured Streaming has excellent support for aggregations. You can specify arbitrary aggregations, as you saw in the Structured APIs. For example, you can use a more exotic aggregation, like a cube, on the phone model and activity and the average x, y, z accelerations of our sensor (jump back to Chapter 7 in order to see potential aggregations that you can run on your stream): Structured Streaming 对聚合有极好的支持。您可以指定任意聚合，正如您在结构化API中看到的那样。例如，您可以在电话模型和活动中使用更奇特的聚合，如 cube，以及传感器的平均 x、y、z 加速度（请跳回到第7章，以查看您可以在流中运行的潜在聚合）： 1234567// in Scalaval deviceModelStats = streaming.cube("gt", "model").avg().drop("avg(Arrival_time)").drop("avg(Creation_Time)").drop("avg(Index)").writeStream.queryName("device_counts").format("memory").outputMode("complete").start() 12345678# in PythondeviceModelStats = streaming.cube("gt", "model").avg()\.drop("avg(Arrival_time)")\.drop("avg(Creation_Time)")\.drop("avg(Index)")\.writeStream.queryName("device_counts").format("memory")\.outputMode("complete")\.start() Querying that table allows us to see the results: 查询该表可以查看结果： 1SELECT * FROM device_counts 12345678910+----------+------+------------------+--------------------+--------------------+| gt | model| avg(x) | avg(y) | avg(z) |+----------+------+------------------+--------------------+--------------------+| sit | null |-3.682775300344...|1.242033094787975...|-4.22021191297611...|| stand | null |-4.415368069618...|-5.30657295890281...|2.264837548081631...|...| walk |nexus4|-0.007342235359...|0.004341030525168...|-6.01620400184307...||stairsdown|nexus4|0.0309175199508...|-0.02869185568293...| 0.11661923308518365|...+----------+------+------------------+--------------------+--------------------+ In addition to these aggregations on raw columns in the dataset, Structured Streaming has special support for columns that represent event time, including watermark support and windowing. We will discuss these in more detail in Chapter 22. 除了对数据集中原始列的聚合之外，Structured Streaming 还特别支持表示事件时间的列，包括水印支持和窗口化。我们将在第22章中更详细地讨论这些问题。 NOTE 注意As of Spark 2.2, the one limitation of aggregations is that multiple “chained” aggregations (aggregations on streaming aggregations) are not supported at this time. However, you can achieve this by writing out to an intermediate sink of data, like Kafka or a file sink. This will change in the future as the Structured Streaming community adds this functionality. 从 Spark 2.2 开始，聚合的一个限制是此时不支持多个“链接（chained）”聚合（流聚合上的聚合）。但是，您可以通过将数据写到中间的接收器（如 Kafka 或 文件接收器）来实现这一点。随着 Structured Streaming 社区添加了这一功能，这一点将来会有所改变。 JoinsAs of Apache Spark 2.2, Structured Streaming supports joining streaming DataFrames to static DataFrames. Spark 2.3 will add the ability to join multiple streams together. You can do multiple column joins and supplement streaming data with that from static data sources: 从 Apache Spark 2.2 开始，Structured Streaming 支持将 streaming DataFrames 连接到 static DataFrames。Spark 2.3 将添加将多个流连接在一起的能力。您可以执行多个列连接，并使用静态数据源中的数据补充流数据： 1234567// in Scalaval historicalAgg = static.groupBy("gt", "model").avg()val deviceModelStats = streaming.drop("Arrival_Time", "Creation_Time", "Index").cube("gt", "model").avg().join(historicalAgg, Seq("gt", "model")).writeStream.queryName("device_counts").format("memory").outputMode("complete").start() 12345678# in PythonhistoricalAgg = static.groupBy("gt", "model").avg()deviceModelStats = streaming.drop("Arrival_Time", "Creation_Time", "Index")\.cube("gt", "model").avg()\.join(historicalAgg, ["gt", "model"])\.writeStream.queryName("device_counts").format("memory")\.outputMode("complete")\.start() In Spark 2.2, full outer joins, left joins with the stream on the right side, and right joins with the stream on the left are not supported. Structured Streaming also does not yet support stream-to-stream joins, but this is also a feature under active development. 在 Spark 2.2 中，不支持完全外部连接、左侧连接到右侧的流、右侧连接到左侧的流。Structured Streaming 还不支持流到流的连接，但这也是一个正在积极开发的特性。 Input and Output 输入和输出This section dives deeper into the details of how sources, sinks, and output modes work in Structured Streaming. Specifically, we discuss how, when, and where data flows into and out of the system. As of this writing, Structured Streaming supports several sources and sinks, including Apache Kafka, files, and several sources and sinks for testing and debugging. More sources may be added over time, so be sure to check the documentation for the most up-to-date information. We discuss the source and sink for a particular storage system together in this chapter, but in reality you can mix and match them (e.g., use a Kafka input source with a file sink). 本节深入介绍了源、汇和输出模式在结构化流中的工作方式。具体来说，我们将讨论数据如何、何时和在何处流入和流出系统。在撰写本文时，Structured Streaming 支持多个源和接收器，包括 Apache Kafka、文件以及用于测试和调试的多个源和接收器。随着时间的推移，可能会添加更多的源，因此一定要检查文档以获取最新的信息。在本章中，我们一起讨论特定存储系统的源和接收器，但实际上，您可以混合和匹配它们（例如，使用 Kafka 输入源和文件接收器）。 Where Data Is Read and Written (Sources and Sinks)Structured Streaming supports several production sources and sinks (files and Apache Kafka), as well as some debugging tools like the memory table sink. We mentioned these at the beginning of the chapter, but now let’s cover the details of each one. Structured Streaming 支持多个生产源和接收器（文件和Apache Kafka），以及一些调试工具，如内存表接收器。我们在本章的开头提到了这些，但现在让我们来介绍每一个的细节。 File source and sink 文件源和寄接收器Probably the simplest source you can think of is the simple file source. It’s easy to reason about and understand. While essentially any file source should work, the ones that we see in practice are Parquet, text, JSON, and CSV. The only difference between using the file source/sink and Spark’s static file source is that with streaming, we can control the number of files that we read in during each trigger via the · option that we saw earlier. Keep in mind that any files you add into an input directory for a streaming job need to appear in it atomically. Otherwise, Spark will process partially written files before you have finished. On file systems that show partial writes, such as local files or HDFS, this is best done by writing the file in an external directory and moving it into the input directory when finished. On Amazon S3, objects normally only appear once fully written. 您能想到的最简单的源可能是简单的文件源。这很容易解释和理解。虽然基本上任何文件源都可以工作，但我们在实践中看到的是 Parquet、文本、JSON 和 CSV。使用文件源/接收器（sink ）和 Spark 的静态文件源之间的唯一区别是，使用流式处理，我们可以通过前面看到的 maxFilesPerTrigger 选项控制在每个触发器期间读取的文件数。请记住，为流式作业添加到输入目录中的任何文件都需要以原子（atomically）的方式出现在文件夹里。否则，Spark将在完成之前处理部分写入的文件。在显示部分写入（如本地文件或HDF）的文件系统上，最好将文件写入外部目录，并在完成后将其移动到输入目录。在Amazon S3 上，对象通常只出现一次完全写入。 Kafka source and sink Kafka源和接收器Apache Kafka is a distributed publish-and-subscribe system for streams of data. Kafka lets you publish and subscribe to streams of records like you might do with a message queue—these are stored as streams of records in a fault-tolerant way. Think of Kafka like a distributed buffer. Kafka lets you store streams of records in categories that are referred to as topics. Each record in Kafka consists of a key, a value, and a timestamp. Topics consist of immutable sequences of records for which the position of a record in a sequence is called an offset. Reading data is called subscribing to a topic and writing data is as simple as publishing to a topic. Apache Kafka 是一个分布式数据流发布和订阅系统。Kafka 允许您发布和订阅记录流，就像处理消息队列一样。这些记录以容错方式存储为记录流。把 Kafka 想象成一个分布式缓冲区。Kafka 允许您将记录流存储在称为主题（topic）的类别中。Kafka 中的每个记录都由一个键（key）、一个值（value）和一个时间戳（timestamp）组成。主题由不可变的记录序列组成，对于这些记录序列中的记录位置称为偏移量（offset）。读数据被称为订阅主题，写数据和发布主题一样简单。 Spark allows you to read from Kafka with both batch and streaming DataFrames. Spark允许您使用批处理和 streaming DataFrames 从Kafka中读取数据。 As of Spark 2.2, Structured Streaming supports Kafka version 0.10. This too is likely to expand in the future, so be sure to check the documentation for more information about the Kafka versions available. There are only a few options that you need to specify when you read from Kafka. 从Spark 2.2开始，Structured Streaming 支持Kafka版本 0.10。这在将来也可能会扩展，因此请务必查看文档以获取有关可用 Kafka 版本的更多信息。当您从 Kafka 阅读时，只有几个选项需要指定。 Reading from the Kafka Source 从Kafka源读取数据To read, you first need to choose one of the following options: assign, subscribe, or subscribePattern. Only one of these can be present as an option when you go to read from Kafka. Assign is a fine-grained way of specifying not just the topic but also the topic partitions from which you would like to read. This is specified as a JSON string {“topicA”:[0,1],”topicB”:[2,4]}. subscribe and subscribePattern are ways of subscribing to one or more topics either by specifying a list of topics (in the former) or via a pattern (via the latter). Second, you will need to specify the kafka.bootstrap.servers that Kafka provides to connect to the service. 要从 Kafka 读取数据，首先需要选择以下选项之一：assign、subscribe 或 subscribePattern 。当你从 Kafka 的读取数据，只有其中一个可以作为选项出现。assign 是一种细粒度的方法，它不仅指定主题，还指定要从中读取的主题分区。这被指定为JSON字符串 {&quot;topicA&quot;:[0,1],&quot;topicB&quot;:[2,4]}。subscribe 和 subscribePattern 是通过指定主题列表（前者）或通过模式（后者）订阅一个或多个主题的方法。其次，您需要指定 Kafka 提供的 Kafka.bootstrap.servers 来连接到服务。 After you have specified your options, you have several other options to specify: 在指定了选项之后，还可以指定其他几个选项： startingOffsets and endingOffsets 起始偏移量和结束偏移量The start point when a query is started, either earliest, which is from the earliest offsets; latest, which is just from the latest offsets; or a JSON string specifying a starting offset for each TopicPartition . In the JSON, -2 as an offset can be used to refer to earliest, -1 to latest. For example, the JSON specification could be {“topicA”:{“0”:23,”1”:-1},”topicB”: {“0”:-2}}. This applies only when a new Streaming query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest. The ending offsets for a given query.开始查询的起始点，可以是最早的，也可以是最早的偏移量；最迟的，只不过是最新的偏移量；或者是指定每个 TopicPartition 的起始偏移量的 JSON 字符串。在 JSON 中，可以使用 -2 作为偏移量来引用最早的，-1表示引用最晚的。例如，JSON 规范可以是 {&quot;topicA&quot;:{&quot;0&quot;:23,&quot;1&quot;:-1},&quot;topicB&quot;: {&quot;0&quot;:-2}} 。这仅在启动新的流式查询时适用，并且恢复将始终从查询停止的位置开始。查询期间新发现的分区将最早开始。给定查询的结束偏移量。 failOnDataLoss 故障数据丢失Whether to fail the query when it’s possible that data is lost (e.g., topics are deleted, or offsets are out of range). This might be a false alarm. You can disable it when it doesn’t work as you expected. The default is true.当数据可能丢失时（例如，删除主题或偏移量超出范围），是否使查询失败。这可能是虚惊一场。当它不能按预期工作时，您可以禁用它。默认值为true。maxOffsetsPerTrigger The total number of offsets to read in a given trigger. 在给定触发器中要读取的偏移量总数。 There are also options for setting Kafka consumer timeouts, fetch retries, and intervals. 还有设置 Kafka 消费者（consumer）超时、获取重试和间隔的选项。 To read from Kafka, do the following in Structured Streaming: 要读取Kafka，请在 Structured Streaming 处理中执行以下操作： 12345678910111213141516// in Scala// Subscribe to 1 topicval ds1 = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "host1:port1,host2:port2").option("subscribe", "topic1").load()// Subscribe to multiple topicsval ds2 = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "host1:port1,host2:port2").option("subscribe", "topic1,topic2").load()// Subscribe to a pattern of topicsval ds3 = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "host1:port1,host2:port2").option("subscribePattern", "topic.*").load() Python is quite similar: Python是十分的类似 123456789101112131415# in Python# Subscribe to 1 topicdf1 = spark.readStream.format("kafka")\.option("kafka.bootstrap.servers", "host1:port1,host2:port2")\.option("subscribe", "topic1")\.load()# Subscribe to multiple topicsdf2 = spark.readStream.format("kafka")\.option("kafka.bootstrap.servers", "host1:port1,host2:port2")\.option("subscribe", "topic1,topic2")\.load()# Subscribe to a patterndf3 = spark.readStream.format("kafka")\.option("kafka.bootstrap.servers", "host1:port1,host2:port2")\.option("subscribePattern", "topic.*")\.load() Each row in the source will have the following schema:源中的每一行将具有以下模式 key: binary value: binary topic: string partition: int offset: long timestamp: long Each message in Kafka is likely to be serialized in some way. Using native Spark functions in the Structured APIs, or a User-Defined Function (UDF), you can parse the message into a more structured format analysis. A common pattern is to use JSON or Avro to read and write to Kafka. Kafka 的每个消息都可能以某种方式序列化。使用 Structured API 中的本地 Spark 函数或用户定义函数（UDF），您可以将消息解析为更结构化的格式便于分析。一种常见的模式是使用 JSON 或 AVRO 读写 Kafka。 Writing to the Kafka Sink 给 Kafka 接收器写消息Writing to Kafka queries is largely the same as reading from them except for fewer parameters. You’ll still need to specify the Kafka bootstrap servers, but the only other option you will need to supply is either a column with the topic specification or supply that as an option. For example, the following writes are equivalent: 写入Kafka查询与从中读取基本相同，只是参数较少。您仍然需要指定 kafka.bootstrap.servers，但是您将需要提供的唯一其他选项要么是具有主题规范的列，要么是将其作为选项提供。例如，以下写操作是等效的： 12345678910111213// in Scalads1.selectExpr("topic", "CAST(key AS STRING)", "CAST(value AS STRING)").writeStream.format("kafka").option("checkpointLocation", "/to/HDFS-compatible/dir").option("kafka.bootstrap.servers", "host1:port1,host2:port2").start()ds1.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)").writeStream.format("kafka").option("kafka.bootstrap.servers", "host1:port1,host2:port2").option("checkpointLocation", "/to/HDFS-compatible/dir")\.option("topic", "topic1").start() 1234567891011121314# in Pythondf1.selectExpr("topic", "CAST(key AS STRING)", "CAST(value AS STRING)")\.writeStream\.format("kafka")\.option("kafka.bootstrap.servers", "host1:port1,host2:port2")\.option("checkpointLocation", "/to/HDFS-compatible/dir")\.start()df1.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")\.writeStream\.format("kafka")\.option("kafka.bootstrap.servers", "host1:port1,host2:port2")\.option("checkpointLocation", "/to/HDFS-compatible/dir")\.option("topic", "topic1")\.start() Foreach sinkThe foreach sink is akin to foreachPartitions in the Dataset API. This operation allows arbitrary operations to be computed on a per-partition basis, in parallel. This is available in Scala and Java initially, but it will likely be ported to other languages in the future. To use the foreach sink, you must implement the ForeachWriter interface, which is available in the Scala/Java documents, which contains three methods: open, process, and close. The relevant methods will be called whenever there is a sequence of rows generated as output after a trigger. Foreach 接收器类似于数据集 API 中的 Foreach 分区。此操作允许以每个分区为基础并行计算任意操作。这在Scala 和 Java 最初是可用的，但它将来可能会移植到其他语言。要使用 Foreach ，必须实现 ForeachWriter 接口，该接口在 Scala/Java 文档中可用，它包含三种方法：打开（open）、进程（process）和关闭（close）。每当有一系列行在触发器之后作为输出生成时，都将调用相关方法。 Here are some important details: 以下是一些重要的细节： The writer must be Serializable, as it were a UDF or a Dataset map function. 编写器（writer ）必须是可序列化的，因为它是一个 UDF 或 数据集映射函数。 The three methods (open, process, close) will be called on each executor. 将对每个执行器调用三个方法（open、process、close）。 The writer must do all its initialization, like opening connections or starting transactions only in the open method. A common source of errors is that if initialization occurs outside of the open method (say in the class that you’re using), that happens on the driver instead of the executor. 编写器（writer ）必须执行其所有初始化，例如只在 open方法中打开连接或启动事务。一个常见的错误源是：如果初始化发生在 open 方法之外（比如在您正在使用的类中），则会发生在驱动程序上，而不是执行器上。 Because the Foreach sink runs arbitrary user code, one key issue you must consider when using it is fault tolerance. If Structured Streaming asked your sink to write some data, but then crashed, it cannot know whether your original write succeeded. Therefore, the API provides some additional parameters to help you achieve exactly-once processing. 因为 Foreach 接收器运行任意用户代码，所以在使用它时必须考虑的一个关键问题是容错。如果 Structured Streaming 请求接收器写入一些数据，但随后崩溃，则无法知道原始写入是否成功。因此，API提供了一些额外的参数来帮助您实现精确的一次性处理。 First, the open call on your · receives two parameters that uniquely identify the set of rows that need to be acted on. The version parameter is a monotonically increasing ID that increases on a per-trigger basis, and partitionId is the ID of the partition of the output in your task. Your open method should return whether to process this set of rows. If you track your sink’s output externally and see that this set of rows was already output (e.g., you wrote the last version and partitionId written in your storage system), you can return false from open to skip processing this set of rows. Otherwise, return true. Your ForeachWriter will be opened again for each trigger’s worth of data to write. 首先，Foreachwriter 上的 open 调用接收两个参数，它们唯一标识了需要对其执行操作的行集。version参数是一个单调递增的ID，它在每个触发器的基础上递增，而 partitionId 是任务中输出分区的ID。您的 open方法应该返回是否处理这行集。如果从外部跟踪接收器的输出，并看到这行集已被输出（例如，您编写了存储系统中写入的最后一个版本和 partitionId ），则可以从 open返回 false 以跳过处理这组行。否则，返回true。您的ForEachWriter将再次打开，以便为每个触发器的数据量进行写入。 Next, the process method will be called for each record in the data, assuming your open method returned true. This is fairly straightforward—just process or write your data. 接下来，将为数据中的每个记录调用 process 方法，假定open方法返回true。这相当简单，只需处理或编写数据即可。 Finally, whenever open is called, the close method is also called (unless the node crashed before that), regardless of whether open returned true. If Spark witnessed an error during processing, the close method receives that error. It is your responsibility to clean up any open resources duringclose. 最后，无论何时调用 open，也会调用 close 方法（除非节点在此之前崩溃），无论 open 是否返回 true。如果Spark 在处理过程中发现错误，close方法将接收该错误。您有责任在关闭期间清理任何打开的资源。 Together, the ForeachWriter interface effectively lets you implement your own sink, including your own logic for tracking which triggers’ data has been written or safely overwriting it on failures. We show an example of passing a ForeachWriter below: 总之，Foreachwriter 接口有效地让您实现自己的接收器，包括跟踪哪些触发器的数据已被写入或在失败时安全覆盖的逻辑。下面是传递 ForEachWriter 的示例： 1234567891011//in ScaladatasetOfString.write.foreach(new ForeachWriter[String] &#123; def open(partitionId: Long, version: Long): Boolean = &#123; // open a database connection &#125; def process(record: String) = &#123; // write string to connection &#125; def close(errorOrNull: Throwable): Unit = &#123; // close the connection &#125; &#125;) Sources and sinks for testing 用于测试的源和接收器Spark also includes several test sources and sinks that you can use for prototyping or debugging your streaming queries (these should be used only during development and not in production scenarios, because they do not provide end-to-end fault tolerance for your application): Spark 还包括几个用于测试的源和接收器，您可以使用它们来进行原型设计或调试流式查询（这些测试源和接收器只应在开发期间使用，而不应在生产场景中使用，因为它们不为您的应用程序提供端到端的容错性）： Socket source 套接字源 The socket source allows you to send data to your Streams via TCP sockets. To start one, specify a host and port to read data from. Spark will open a new TCP connection to read from that address. The socket source should not be used in production because the socket sits on the driver and does not provide end-to-end fault-tolerance guarantees. 套接字源允许您通过TCP套接字向流发送数据。要启动一个，请指定要从中读取数据的主机和端口。Spark 将打开一个新的TCP连接来读取该地址。在生产中不应使用套接字源，因为套接字位于驱动程序上，并且不提供端到端的容错保证。 Here is a short example of setting up this source to read from localhost:9999: 以下是设置此源以从 localhost:9999 读取的简短示例： 123// in Scalaval socketDF = spark.readStream.format("socket").option("host", "localhost").option("port", 9999).load() 123# in PythonsocketDF = spark.readStream.format("socket")\.option("host", "localhost").option("port", 9999).load() If you’d like to actually write data to this application, you will need to run a server that listens on port 9999. On Unix-like systems, you can do this using the NetCat utility, which will let you type text into the first connection that is opened to port 9999. Run the command below before starting your Spark application, then write into it: 1nc -lk 9999 如果您真的想向这个应用程序写入数据，您需要运行一个在端口 9999 上监听的服务器。在类似Unix的系统上，您可以使用 NetCat 工具来执行此操作，该工具允许您在打开到端口 9999 的第一个连接中键入文本。在启动Spark应用程序之前运行下面的命令，然后写入它： 1nc -lk 9999 The socket source will return a table of text strings, one per line in the input data. 套接字源将返回一个文本字符串表，在输入数据中每行一个。 Console sink 控制台接收器 The console sink allows you to write out some of your streaming query to the console. This is useful for debugging but is not fault-tolerant. Writing out to the console is simple and only prints some rows of your streaming query to the console. This supports both append and complete output modes: 控制台接收器允许您向控制台写出一些流式查询。这对调试很有用，但不是容错的。写入控制台很简单，只将流式查询的一些行打印到控制台。这支持附加和完整输出模式： 1activityCounts.format("console").write() Memory sink 内存接收器 The memory sink is a simple source for testing your streaming system. It’s similar to the console sink except that rather than printing to the console, it collects the data to the driver and then makes the data available as an in-memory table that is available for interactive querying. This sink is not fault tolerant, and you shouldn’t use it in production, but is great for testing and querying your stream during development. This supports both append and complete output modes: 内存接收器是测试流系统的简单源。它类似于控制台接收器，只是它不是打印到控制台，而是将数据收集到驱动程序，然后将数据作为内存中的表提供，以用于交互式查询。这个接收器不是容错的，您不应该在生产中使用它，但是对于开发期间测试和查询流非常有用。这支持附加和完整输出模式： If you do want to output data to a table for interactive SQL queries in production, the authors recommend using the Parquet file sink on a distributed file system (e.g., S3). You can then query the data from any Spark application. 如果您确实希望将数据输出到生产中用于交互式 SQL 查询的表中，那么作者建议在分布式文件系统（如S3）上使用 Parquet 文件接收器。然后您可以从任何 Spark 应用程序查询数据。 How Data Is Output (Output Modes) 如何输出数据（输出模式）Now that you know where your data can go, let’s discuss how the result Dataset will look when it gets there. This is what we call the output mode. As we mentioned, they’re the same concept as save modes on static DataFrames. There are three modes supported by Structured Streaming. Let’s look at each of them. 既然您已经知道了数据的去向，那么让我们讨论一下当结果数据集到达那里时它将如何显示。这就是我们所说的输出模式。正如我们提到的，它们与静态数据帧上的保存模式是相同的概念。Structured Streaming 支持三种模式。让我们看看每一个。 Append mode 附加模式Append mode is the default behavior and the simplest to understand. When new rows are added to the result table, they will be output to the sink based on the trigger (explained next) that you specify. This mode ensures that each row is output once (and only once), assuming that you have a fault-tolerant sink. When you use append mode with event-time and watermarks (covered in Chapter 22), only the final result will output to the sink. 附加模式是默认行为，也是最容易理解的。当新的行添加到结果表中时，它们将根据您指定的触发器（解释如下）输出到接收器。此模式确保每行输出一次（并且仅输出一次），前提是您有一个容错接收器。在事件时间和水印（见第22章）中使用附加模式时，只有最终结果才会输出到接收器。 Complete mode 完整模式Complete mode will output the entire state of the result table to your output sink. This is useful when you’re working with some stateful data for which all rows are expected to change over time or thesink you are writing does not support row-level updates. Think of it like the state of a stream at the time the previous batch had run. 完成模式将结果表的整个状态输出到输出接收器。当您处理的某些状态数据的所有行都会随着时间的推移而改变，或者您正在写入的链接不支持行级更新时，这非常有用。把它想象成前一批处理运行时的流状态。 Update mode 更新模式Update mode is similar to complete mode except that only the rows that are different from the previous write are written out to the sink. Naturally, your sink must support row-level updates to support this mode. If the query doesn’t contain aggregations, this is equivalent to append mode. 更新模式与完成模式类似，除了不同于上一次写入的行会被写入接收器。当然，您的接收器必须支持行级更新才能支持此模式。如果查询不包含聚合，则这相当于追加模式。 When can you use each mode? 你什么时候可以使用每种模式？Structured Streaming limits your use of each mode to queries where it makes sense. For example, if your query just does a map operation, Structured Streaming will not allow complete mode, because this would require it to remember all input records since the start of the job and rewrite the whole output table. This requirement is bound to get prohibitively expensive as the job runs. We will discuss when each mode is supported in more detail in the next chapter, once we also cover event-time processing and watermarks. If your chosen mode is not available, Spark Streaming will throw an exception when you start your stream. Structured Streaming 将每种模式的使用限制为有意义的查询。例如，如果查询只是执行映射操作，Structured Streaming 将不允许使用完整模式，因为这将要求它记住自作业开始以来的所有输入记录并重写整个输出表。随着作业的运行，这一要求必然会变得非常昂贵。在下一章中，我们将更详细地讨论支持每种模式的时间，我们也将讨论事件时间处理和水印。如果您选择的模式不可用，当您启动流时，Spark流将抛出一个异常。 Here’s a handy table from the documentation that lays all of this out. Keep in mind that this will change in the future, so you’ll want to check the documentation for the most up-to-date version. 这是文档中列出所有这些内容的一个方便查询的表。请记住，这将在将来发生变化，因此您需要检查文档中的最新版本。 Table 21-1 shows when you can use each output mode. 表21-1显示了您何时可以使用每个输出模式。 Query Type Query type(continued) SupportedOutputModes Notes Queries withaggregation Aggregation on event-time with watermark按照有水印的事件时间来聚合 Append,Update,Complete Append mode uses watermark to drop old aggregation state. This means that as new rows are brought into the table, Spark will only keep around rows that are below the “watermark”. Update mode also uses the watermark to remove old aggregation state. By definition, complete mode does not drop old aggregation state since this mode preserves all data in the Result Table.附加模式使用水印删除旧的聚合状态。这意味着当新的行被放入表中时，spark将只保留在“水印”下的行。更新模式还使用水印删除旧的聚合状态。根据定义，完整模式不会删除旧的聚合状态，因为此模式将保留结果表中的所有数据。 Otheraggregations Complete,Update Since no watermark is defined (only defined in other category), old aggregation state is not dropped. Append mode is not supported as aggregates can update thus violating the semantics of this mode.由于未定义水印（仅在其他类别中定义），因此不会删除旧的聚合状态。不支持追加模式，因为聚合可能更新，从而违反了此模式的语义。 Queries withmapGroupsWithState Update Queries withflatMapGroupsWithState Appendoperationmode Append Aggregations are allowed after flatMapGroupsWithState. Updateoperationmode Update Aggregations not allowed after flatMapGroupsWithState. Other queries Append,Update Complete mode not supported as it is infeasible to keep all unaggregated data in the Result Table.不支持完整模式，因为无法将所有未聚合的数据保留在结果表中。 When Data Is Output (Triggers) 数据输出时（触发器）To control when data is output to our sink, we set a trigger. By default, Structured Streaming will start data as soon as the previous trigger completes processing. You can use triggers to ensure that you do not overwhelm your output sink with too many updates or to try and control file sizes in the output. Currently, there is one periodic trigger type, based on processing time, as well as a “once” trigger to manually run a processing step once. More triggers will likely be added in the future. 为了控制数据何时输出到接收器，我们设置了一个触发器。默认情况下，Structured Streaming 将在上一个触发器完成处理后立即启动数据。您可以使用触发器来确保不会用太多的更新覆盖了输出接收器，或者尝试控制输出中的文件大小。目前，有一种基于处理时间的定期触发器类型，以及一种“一次”触发器，用于手动运行一次处理步骤。未来可能会增加更多的触发器。 Processing time trigger 处理时间触发器For the processing time trigger, we simply specify a duration as a string (you may also use a Duration in Scala or TimeUnit in Java). We’ll show the string format below. 对于处理时间触发器，我们只需将一个持续时间指定为一个字符串（您也可以在 Java 中使用 TimeUnit 或 在 Scala 中使用 Duration）。我们将在下面显示字符串格式。 1234// in Scalaimport org.apache.spark.sql.streaming.TriggeractivityCounts.writeStream.trigger(Trigger.ProcessingTime("100 seconds")).format("console").outputMode("complete").start() 12# in PythonactivityCounts.writeStream.trigger(processingTime='5 seconds')\.format("console").outputMode("complete").start() The ProcessingTime trigger will wait for multiples of the given duration in order to output data. For example, with a trigger duration of one minute, the trigger will fire at 12:00, 12:01, 12:02, and so on. If a trigger time is missed because the previous processing has not yet completed, then Spark will wait until the next trigger point (i.e., the next minute), rather than firing immediately after the previous processing completes. ProcessingTime 触发器将等待数倍的给定的持续时间以输出数据。例如，触发器持续时间为一分钟，触发器将在12:00、12:01、12:02等时间触发。如果由于前一个处理尚未完成而错过触发时间，则 Spark 将等待下一个触发点（即下一分钟），而不是在前一个处理完成后立即触发。 Once trigger 一次触发You can also just run a streaming job once by setting that as the trigger. This might seem like a weird case, but it’s actually extremely useful in both development and production. During development, you can test your application on just one trigger’s worth of data at a time. During production, the Once trigger can be used to run your job manually at a low rate (e.g., import new data into a summary table just occasionally). Because Structured Streaming still fully tracks all the input files processed and the state of the computation, this is easier than writing your own custom logic to track this in a batch job, and saves a lot of resources over running a continuous job 24/7 : 您也可以通过将其设置为触发器来运行一次流作业。这看起来是一个奇怪的使用案例，但实际上在开发和生产中都非常有用。在开发过程中，一次只能在一个触发器的数据上测试应用程序。在生产过程中，可以使用ONCE触发器以较低的速度手动运行作业（例如，有时将新数据导入摘要表）。由于 Structured Streaming 处理仍然完全跟踪所有处理的输入文件和计算状态，因此这比在批处理作业中编写自己的自定义逻辑来跟踪这一点要容易得多，并且在每天24小时每周七天的连续作业的运行过程中节省了大量资源： 1234// in Scalaimport org.apache.spark.sql.streaming.TriggeractivityCounts.writeStream.trigger(Trigger.Once()).format("console").outputMode("complete").start() 123# in PythonactivityCounts.writeStream.trigger(once=True)\.format("console").outputMode("complete").start() Streaming Dataset API 流数据集APIOne final thing to note about Structured Streaming is that you are not limited to just the DataFrame API for streaming. You can also use Datasets to perform the same computation but in type-safe manner. You can turn a streaming DataFrame into a Dataset the same way you did with a static one. As before, the Dataset’s elements need to be Scala case classes or Java bean classes. Other than that, the DataFrame and Dataset operators work as they did in a static setting, and will also turn into a streaming execution plan when run on a stream. 关于 Structured Streaming 的最后一点需要注意的是，您不仅限于流的 DataFrame API。您也可以使用 Datasets 以类型安全的方式执行相同的计算。您可以像处理静态 Datasets 一样，将流式 DataFrame 转换为 Dataset 。如前所述，Dataset 的元素需要是 Scala case 类或 Java bean 类。除此之外，DataFrame 和 Dataset 算子（operators）的工作方式与静态设置中的相同，并且在流上运行时也将转变为流执行计划。 Here’s an example using the same dataset that we used in Chapter 11: 下面是使用第11章中使用的相同数据集的示例： 1234567891011121314151617181920// in Scalacase class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String,count: BigInt)val dataSchema = spark.read.parquet("/data/flight-data/parquet/2010-summary.parquet/").schemaval flightsDF = spark.readStream.schema(dataSchema).parquet("/data/flight-data/parquet/2010-summary.parquet/")val flights = flightsDF.as[Flight]def originIsDestination(flight_row: Flight): Boolean = &#123; return flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME&#125; flights.filter(flight_row =&gt; originIsDestination(flight_row)).groupByKey(x =&gt; x.DEST_COUNTRY_NAME).count().writeStream.queryName("device_counts").format("memory").outputMode("complete").start() Conclusion 结论It should be clear that Structured Streaming presents a powerful way to write streaming applications. Taking a batch job you already run and turning it into a streaming job with almost no code changes is both simple and extremely helpful from an engineering standpoint if you need to have this job interact closely with the rest of your data processing application. Chapter 22 dives into two advanced streaming-related concepts: event-time processing and stateful processing. Then, after that, Chapter 23 addresses what you need to do to run Structured Streaming in production. 很明显，Structured Streaming 为编写流应用程序提供了一种强大的方式。如果您需要在与数据处理应用程序的其余部分紧密联系，从工程的角度来看，将已经运行的批处理作业转换为几乎没有代码更改的流作业既简单又非常有用。第22章深入介绍了两个与流相关的高级概念：事件时间处理和状态处理。然后，在这之后，第23章介绍了在生产环境中运行 Structured Streaming 处理需要做什么。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
</search>
