<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Transformer论文简记]]></title>
    <url>%2F2019%2F03%2F10%2FTransformer_learning-resources%2F</url>
    <content type="text"><![CDATA[资源Transformer来自论文: All Attention Is You Need 别人的总结资源： Attention机制详解（二）——Self-Attention与Transformer 谷歌软件工程师 一个是Jay Alammar可视化地介绍Transformer的博客文章 The Illustrated Transformer，非常容易理解整个机制，建议先从这篇看起，这是中文翻译版本； 放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较 中科院软件所 · 自然语言处理 /搜索 10年工作经验的博士（阿里，微博）； Calvo的博客：Dissecting BERT Part 1: The Encoder，尽管说是解析Bert，但是因为Bert的Encoder就是Transformer，所以其实它是在解析Transformer，里面举的例子很好； 再然后可以进阶一下，参考哈佛大学NLP研究组写的“The Annotated Transformer. ”，代码原理双管齐下，讲得也很清楚。 《Attention is All You Need》浅读（简介+代码） 这个总结的角度也很棒。 总结这里总结的思路：自顶向下方法 model architecture一图胜千言，6层编码器和解码器，论文中没有说为什么是6这个特定的数字 Encoder Decoder如果我们想做堆叠了2个Encoder和2个Decoder的Transformer，那么它可视化就会如下图所示： 翻译输出的时候，前一个时间步的输出，要作为下一个时间步的解码器端的输入，下图展示第2~6步： 下面是一个单层：Nx 表示 N1, … , N6 层 partsMulti-head Attention其实就是多个Self-Attention结构的结合，每个head学习到在不同表示空间中的特征，所谓“多头”（Multi-Head），就是做h次同样的事情（参数不共享），然后把结果拼接。 Self-Attention实际上是scaled dot-product attention 缩放的点积注意力： Addresidual connection: skip connection 跳跃了解 Normlayer norm 归一化层 Positional encodinggoogle的这个位置编码很魔幻，是两个周期函数：sine cosine数学系出生的博主的解释：《Attention is All You Need》浅读（简介+代码），相比之下Bert的位置编码直观的多。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle首战Titanic 0.82275-Top3% & 0.83732-Top2%]]></title>
    <url>%2F2019%2F01%2F06%2FTitanic_with_name_sex_age_and_ticket_features-0.82275-0.83732%2F</url>
    <content type="text"><![CDATA[本文用数据分析探索规律，效果好于一堆的随机森林和xgboost，超过参加这个比赛的很多ensemble模型，至少排在前156/10021（Top 2%），最终只选择 name，sex，age，Ticket 4个特征，构建出新的特征，然后进行规则判断，即多个嵌套的if-else，再一次感受到了特征工程的强大。省了数据缺失弥补，其他繁琐的数据预处理，数据清洗，后续的调参和集成模型。需要注意的是：需要自己定制交叉验证函数。 具体方案细节，查看我的jupyter notebook： Titanic_with_name_sex_age_and_ticket_features-0.82275.ipynb Titanic_with_name_sex_age_and_ticket_features-0.83732.ipynb]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[极大化似然估计与贝叶斯估计区别]]></title>
    <url>%2F2019%2F01%2F01%2FMaximum-Likelihood-Estimation_VS_Bayes-Estimation%2F</url>
    <content type="text"><![CDATA[即使学了很久，很多人都没弄清楚极大化似然估计与贝叶斯估计区别，本文将简要概述一下区别，如果要详细搞清楚，建议食用MIT概率论教材《Introduction to probability》的第8, 9两章，本文只是简单总结和《统计学习方法》第4章——朴素贝叶斯方法中的例子。 正文下图摘录自教材《Introduction to probability》的第8章 参考 http://www.cnblogs.com/little-YTMM/p/5399532.html]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>probability</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark编程基础笔记]]></title>
    <url>%2F2018%2F12%2F28%2Ffoundations_of_spark_programming%2F</url>
    <content type="text"><![CDATA[花了4天时间快速过了一遍这个课程，这个课程好处都是有代码实例，毕竟再复杂的工程都是一个个模块堆积起来的。笔记 pdf百度链接链接：提取码：d72o，自己做的书签。 第2章 scala语言基础 第3章 spark的设计与运行原理 第4章 Spark环境搭建和使用方法 第5章 RDD编程 第6章 Spark SQL 第7章 Spark Streaming 第8章 Spark MLlib]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从特征工程到XGBoost参数调优]]></title>
    <url>%2F2018%2F12%2F18%2Fget_started_feature-engineering%2F</url>
    <content type="text"><![CDATA[前言本文陈述脉络：理论结合kaggle上一个具体的比赛。 正文数据科学的一般流程 指南 特征工程 评价指标 XGBoost参数调优 XGBoost并行处理 特征工程结合以下案例分析： Two Sigma Connect: Rental Listing Inquiries 任务：根据公寓的listing 内容，预测纽约市某公寓租赁listing的受欢迎程度标签： interest_level，该listing被咨询的次数 选择这个案例是因为小而精，虽然只有14维特征，但是基本上都涉及各种类型特征。 有三个取值：: ‘high’, ‘medium’, ‘low’，是一个多类分类任务 Listing内容有： 浴室和卧室的数目bathrooms， bedrooms 地理位置（ longitude 、 latitude ） 地址： display_address、 street_address building_id、 listing_id、 manager_id Created：创建日期 Description：更多描述信息 features: 公寓的一些特征描述 photos: a list of photo links 价格：price 数据分析方法对数据进行探索性的分析的工具包：pandas、 matplotlib／seaborn 读取训练数据，取少量样本进行观测，并查看数据规模和数据类型 标签、特征意义、特征类型等 分析每列特征的分布 直方图 包括标签列（对分类问题，可看出类别样本是否均衡） 检测奇异点（outliers） 分析每两列特征之间的相关性 – 特征与特征之间信息是否冗余 – 特征与标签是否线性相关 histogram 直方图 直方图：每个取值在数据集中出现的次数，可视为概率函 数（PDF）的估计（seaborn可视化工具比较简单） 123import seaborn as sns%matplotlib inline（ seaborn 是基于matplotlib 的）sns.distplot(train.price.values, bins=50, kde=True) 核密度估计 Kernel Density Estimation, KDE 对直方图的加窗平滑 在分类任务中，我们关心不同类别的特征分布 violinplot 提供不同类别条件下特征更多的分部信息 核密度估计（KDE） 三个4分位数（quartile）：1/4, 1/2, 3/4 1.5倍四分数间距（nterquartile range, IQR） IQR ：第三四分位数和第一分位数的区别（即Q1~Q3的差距），表示变量的分散情况，播放差更稳健的统计量 12order = ['low', 'medium', 'high']sns.violinplot(x='interest_level', y='price', data=train, order = order) outliers 奇异点奇异点：或称离群点，指远离大多数样本的样本点。通常认为这些点是噪声，对模型有坏影响 可以通过直方图或散点图发现奇异点 直方图的尾巴 散点图上孤立的点 12345plt.figure(figsize=(8,6))plt.scatter(range(train_df.shape[0]), train_df.price.values, color = color[6])plt.xlabel('the number of train data', fontsize=12)plt.ylabel('price', fontsize=12)plt.show() 可以通过只保留某些分位数内的点去掉奇异点 如0.5%-99.5%，或&gt;99% 12ulimit = np.percentile(train.price.values, 99)train['price'].loc[train['price']&gt;ulimit] = ulimit correlation 相关性 相关性可以通过计算相关系数或打印散点图来发现 相关系数：两个随机变量x,y之间的线性相关程度，不线性相关并不代表不相关，可能高阶相关，如 $y=x^2$ $r = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=0}^{n}}(x_i-\bar{x})^2\sum_{i=0}^{n}(y_i-\bar{y})^2}, -1\le r \le 1$ 通常 $|r| &gt; 0.5$ ，认为两者相关性比较强 $r\cases{=0, &amp;\text{完全线性不相关}\\ &gt;0, &amp;\text{正相关}\\ &lt;0, &amp;\text{负相关}}$ 相关性只能是数值型特征之间相关性 我们希望特征与标签强相关，分类直方图可以从某种程度上看出特征与标签的相关性：不同类别的直方图差异大 1234order = ['low', 'medium', 'high']sns.stripplot(train_df.interest_level, train_df.price.values, jitter=True, order=order)plt.title("Price VS Interest Level")plt.show() 特征与特征之间强相关的话意味着信息冗余 可以两个特征可以只保留一个特征 或采用主成分分析（PCA）等降维 123456789101112contFeaturelist = []contFeaturelist.append('bathrooms')contFeaturelist.append('bedrooms')contFeaturelist.append('price')correlationMatrix = train_df[contFeaturelist].corr().abs()plt.subplots()sns.heatmap(correlationMatrix, annot=True)#Mask unimportant featuressns.heatmap(correlationMatrix, mask=correlationMatrix &lt; 1, cbar = False)plt.show() 数据类型XGBoost 模型内部将所有的问题都建模成一个回归预测问题，输入特征只能是数值型。如果给定的数据是不同的类型，必须先将数据变成数值型。 类别型特征（ categorical features） LabelEncoder： 对不连续的数字或者文本进行编号 可用在对字符串型的标签编码（测试结果需进行反变换） 编号默认有序数关系 存储量小 如不希望有序数关系： OneHotEncoder：将类别型整数输入从 1 维 K 维的稀疏编码（K 种类别） 对XGBoost，OneHotEncoder不是必须，因为XGBoost对特征进行排序从而进行分裂建树；如果用OneHotEncoder得到稀疏编码，XGBoost建树过程中对稀疏特征处理速度块 输入必须是数值型数据（对字符串输入，先调用LabelEncoder变成数字，再用OneHotEncoder ） 存储要求高 低基数（low-cardinality ）类别型特征： OneHotEncoder 1维到K维， K为该特征不同的取值数目 通常在K &lt;10的情况下采用 高基数（high-cardinality）类别型特征：通常有成百上千个不同的取值，可先降维，如：邮政编码、街道名称… 聚类（Clustering）： 1 维 到 K维，K为聚类的类别数 主成分分析（principle component analysis, PCA）：但对大矩阵操作费资源 均值编码：在贝叶斯的架构下，利用标签变量，有监督地确定最适合特定特征的编码方式。均值编码详细参考： Mean Encoding: A Preprocessing Scheme for High-Cardinality Categorical Features 平均数编码：针对高基数定性特征（类别特征）的数据预处理/特征工程 日期型特征 日期特征：年月日 时间特征：小时分秒 时间段：早中晚 星期，工作日／周末 123456789train_test['Date'] = pd.to_datetime(train_test['created'])train_test['Year'] = train_test['Date'].dt.yeartrain_test['Month'] = train_test['Date'].dt.monthtrain_test['Day'] = train_test['Date'].dt.daytrain_test['Wday'] = train_test['Date'].dt.dayofweektrain_test['Yday'] = train_test['Date'].dt.dayofyeartrain_test['hour'] = train_test['Date'].dt.hour train_test = train_test.drop(['Date', 'created'], axis=1) 文本型特征 可用词云（wordcloud）可视化 文本词频统计函数，自动统计词的个数，以字典形式内部存储，在显示的时候词频大的词的字体更大 123456789# wordcloud for street addressplt.figure()wordcloud = WordCloud(background_color='white', width=600, height=300, max_font_size=50, max_words=40)wordcloud.generate(text_street)wordcloud.recolor(random_state=0)plt.imshow(wordcloud)plt.title("Wordcloud for street address", fontsize=30)plt.axis("off")plt.show() TF-IDF 通俗易懂原理参考：廖雪峰老师的TF-IDF，概率解释参考：CoolShell 陈皓的 TF-IDF 实战参考官网和使用sklearn提取文本的tfidf特征 下面是个例子123456789101112from sklearn.feature_extraction.text import TfidfVectorizerX_train = ['This is the first document.', 'This is the second document.']X_test = ['This is the third document.']vectorizer = TfidfVectorizer()# 用X_train数据来fitvectorizer.fit(X_train)# 得到tfidf的矩阵tfidf_train = vectorizer.transform(X_train)tfidf_test = vectorizer.transform(X_test)tfidf_train.toarray() 数据预处理from sklearn.preprocessing import … 数据标准化 数据归一化 数据二值化 数据缺失 XGBoost对数据预处理要求少，以上操作都不是必须 特征工程小结 如果知道数据的物理意义（领域专家），可能可以设计更多特征 如Higgs Boson任务中有几维特征是物理学家设计的，还有些有高能物理 研究经验的竞赛者设计了其他一些特征 如房屋租赁任务中，利用常识可设计出一些特征，例子：租金/卧室数目=单价 如果不是领域专家，一些通用的规则： 字符串型特征：Label编码 时间特征：年月日、时间段（早中晚）… 数值型特征：加减乘除，多项式，log, exp 低基数类别特征：one-hot编码 高基数类别特征：先降维，再one-hot编码；均值编码 非结构化特征 文本 语音 图像／视频 fMRI … 利用领域知识设计特征 如曾经流行的图像目标检测特征HOG… 利用深度学习从数据中学习特征表示 采用end-to-end方式一起学习特征和分类／回归／排序 学习好特征可以送入XGBoost学习器 信息泄漏训练数据特征不应该包含标签的信息– 如Rent Listing Inquries任务中图片压缩文件里文件夹的创造时间：加入这个特征后，模型普遍能提高0.01的public LB分数 特征工程案例实践这是我的 jupyter notebook: Rent Listing Inquries 评价指标回归问题的评价指标损失函数可以作为评价指标，以下约定俗成： $\hat{y_i}$ 是预测值，$y$ 是标签值 L1: mean absolute error (MAE) $MAE = \frac{1}{N}\sum_{i=0}^{N}|\hat{y_i}-y_i|$ L2: Root Mean Squared Error(RMSE) $RMSE = \sqrt{\frac{1}{N}\sum_{i=0}^{N}|\hat{y_i}-y_i|}$ Root Mean Sqared Logarithmic Error (RMSLE) $RMLSE = \sqrt{\frac{1}{N}\sum_{i=0}^{N} \big( \log(\hat{y_i}+1) - \log(y_i+1) \big) ^2}$ https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError 当不想在给预测值与真值差距施加很大惩罚时，采用RMSLE 分类任务的评价指标同样地，损失函数可以作为评价指标 logistic/负log似然损失 $\text{logloss}= -{1\over N}\sum_{i=0}^{N}\sum_{j=0}^{M}y_{ij}\log{P_{ij}}$，M是类别数，$y_{ij}$ 为二值函数，当 i 个样本是第 j 类时 $y_{ij}=1$ ，否则取 $0$ ；$p_{ij}$ 为模型预测的第 i 个样本为第 j 类的概率。当 $M=2$ 时， $\text{logloss} = -{1\over N}\sum_{i=0}^{N}\big(y_i\log p_i + (1-y_i)\log(1-p_i)\big)$ ，$y_i$ 为第 i 个样本类别，$p_i$ 为模型预测的第 i 个样本为第 1 类的概率。 0-1损失对应的Mean Consequential Error (MCME) $\text{MCE}=-\frac{1}{N}\sum\limits_{\hat{y_i}\ne y_i}1$ 两类分类任务中更多评价指标 ROC／AUC PR曲线 MAP@n 0-1损失：假设两种错误的代价相等 ​ False Positive （FP） &amp; False Negative（FN） 有些任务中可能某一类错误的代价更大 如蘑菇分类中将毒蘑菇误分为可食用代价更大 因此单独列出每种错误的比例：混淆矩阵 混淆矩阵（confusion matrix） 真正的正值（true positives） 假的正值（false positives） 真正的负值（true negatives） 假的负值（false negatives ） SciKit-Learn实现了多类分类任务的混淆矩阵 sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None) y_true： N个样本的标签真值 y_pred： N个样本的预测标签值 labels：C个类别在矩阵的索引顺序，缺省为y_true或y_pred类别出现的顺序 sample_weight： N个样本的权重 Receiver Operating Characteristic (ROC)上面我们讨论给定阈值 $τ$ 的TPR和FPR 如果不是只考虑一个阈值，而是在一些列阈值上运行检测器，并画出TPR和FPR为阈值 $τ$ 的隐式函数，得到ROC曲线在此处键入公式。 PR曲线Precision and Recall (PR曲线)：用于稀有事件检测，如目标检测、信息检索 负样本非常多，因此 FPR = FP /N_ 很小，比较 TPR 和 FPR 不是很有用的信息 （ROC曲线中只有左边很小一部分有意义） $\rightarrow$ 只讨论正的样本 Precision（精度，查准率，正确 率）：以信息检索为例，对于一个查询，返回了一系列的文档，正确率指的是返回结果中相关文档占的比例 $\text{precision}= TP /\hat{N}_+$ 预测结果真正为正的比例 Recall（召回率，查全率）：返回结果中相关文档占所有相关文档的比例 $\text{Recall}=TP/N_+$ 被正确预测到正样本的比例 Precision and Recall (PR曲线) 阈值变化时的P 和R ，只考虑了返回结果中相关文档的个数，没有考虑文档之间的序。 对一个搜索引擎或推荐系统而言，返回的结果必然是有序的，而且越相关的文档排的越靠前越好，于是有了 AP 的概念。 AP: Average Precision，对不同召回率点上的正确率进行平均。 Average PrecisionPrecision只考虑了返回结果中相关文档的个数，没有考虑文档之间的序。 对一个搜索引擎或推荐系统而言，返回的结果必然是有序的，而且越相关的文档排的越靠前越好，于是有了AP的概念。 AP: Average Precision，对不同召回率点上的正确率进行平均 $AP = \int_{0}^{1}p(k)dr = \sum_{k=0}^{n}p(k)\Delta r(k)$ 即 PR 曲线下的面积（Recall: AUC 为 ROC 下的面积） 其中 k 为返回文档中序位，n 为返回文档的数目，$p(k)$ 为列表中k截至点的precision，$\Delta r(k)$ 表示从 $k-1$ 到 $k$ 召回率的变化 上述离散求和表示等价于 $AP=\sum_{k=0}^{n}p(k)rel(k)/\text{相关文档的数目}$ ，其中 $rel(k)$ 为示性函数，即第 $k$ 个位置为相关文档则取1，否则取0。 计算每个位置上的 precision，如果该位置的文档是不相关的则该位置 precision=0，然后对所有的位置的precision 再做 average 。 MAP: Mean Average Precision $MAP = (\sum_{q=0}^{Q}AP(q)/(Q))$ ，其中 $Q$ 为查询的数目，$n$ 为文档数目。 MAP@K （MAPK） 在现代web信息检索中，recall其实已经没有意义，因为相关文档有成千上万个，很少有人会关心所有文档 Precision@K：在第K个位置上的Precision 对于搜索引擎，考虑到大部分作者只关注前一、两页的结果，所以Precision @10， Precision @20对大规模搜索引擎非常有效 MAP@K：多个查询Precision@K的平均 F1 分数/调和平均 亦被称为F1 score, balanced F-score or F-measure Precision 和 Recall 加权平均： $F1=\frac{2(\text{Precision Recall)}}{(\text{Precision + Recall)}}$ 最好为1，最差为0 多类：每类的F1平均值 Scikit-Learn: Scoring 用交叉验证（cross_val_score和GridSearchCV）评价模型性能时，用scoring参数定义评价指标。 评价指标是越高越好，因此用一些损失函数当评价指标时，需要再加负号，如neg_log_loss，neg_mean_squared_error 详见sklearn文档： http://scikit-learn.org/stable/modules/model_evaluation.html | Scoring | Function | Comment | | —————————— | ———————————————————— | ——————————– | | Classification | | | | ‘accuracy’ | metrics.accuracy_score | | | ‘balanced_accuracy’ | metrics.balanced_accuracy_score | for binary targets | | ‘average_precision’ | metrics.average_precision_score | | | ‘brier_score_loss’ | metrics.brier_score_loss | | | ‘f1’ | metrics.f1_score | for binary targets | | ‘f1_micro’ | metrics.f1_score | micro-averaged | | ‘f1_macro’ | metrics.f1_score | macro-averaged | | ‘f1_weighted’ | metrics.f1_score | weighted average | | ‘f1_samples’ | metrics.f1_score | by multilabel sample | | ‘neg_log_loss’ | metrics.log_loss | requires predict_proba support | | ‘precision’ etc. | metrics.precision_score | suffixes apply as with ‘f1’ | | ‘recall’ etc. | metrics.recall_score | suffixes apply as with ‘f1’ | | ‘roc_auc’ | metrics.roc_auc_score | | | Clustering | | | | ‘adjusted_mutual_info_score’ | metrics.adjusted_mutual_info_score | | | ‘adjusted_rand_score’ | metrics.adjusted_rand_score | | | ‘completeness_score’ | metrics.completeness_score | | | ‘fowlkes_mallows_score’ | metrics.fowlkes_mallows_score | | | ‘homogeneity_score’ | metrics.homogeneity_score | | | ‘mutual_info_score’ | metrics.mutual_info_score | | | ‘normalized_mutual_info_score’ | metrics.normalized_mutual_info_score | | | ‘v_measure_score’ | metrics.v_measure_score | | | Regression | | | | ‘explained_variance’ | metrics.explained_variance_score | | | ‘neg_mean_absolute_error’ | metrics.mean_absolute_error | | | ‘neg_mean_squared_error’ | metrics.mean_squared_error | | | ‘neg_mean_squared_log_error’ | metrics.mean_squared_log_error | | | ‘neg_median_absolute_error’ | metrics.median_absolute_error | | | ‘r2’ | metrics.r2_score | | SciKit-Learn：sklearn.metrics metrics模块还提供为其他目的而实现的预测误差评估函数 分类任务的评估函数如表所示，其他任务评估函数请见：http://scikitlearn.org/stable/modules/classes.html#module-sklearn.metrics Classification metrics See the Classification metrics section of the user guide for further details. | metrics.accuracy_score(y_true, y_pred[, …]) | Accuracy classification score. | | ———————————————————— | ———————————————————— | | metrics.auc(x, y[, reorder]) | Compute Area Under the Curve (AUC) using the trapezoidal rule | | metrics.average_precision_score(y_true, y_score) | Compute average precision (AP) from prediction scores | | metrics.balanced_accuracy_score(y_true, y_pred) | Compute the balanced accuracy | | metrics.brier_score_loss(y_true, y_prob[, …]) | Compute the Brier score. | | metrics.classification_report(y_true, y_pred) | Build a text report showing the main classification metrics | | metrics.cohen_kappa_score(y1, y2[, labels, …]) | Cohen’s kappa: a statistic that measures inter-annotator agreement. | | metrics.confusion_matrix(y_true, y_pred[, …]) | Compute confusion matrix to evaluate the accuracy of a classification | | metrics.f1_score(y_true, y_pred[, labels, …]) | Compute the F1 score, also known as balanced F-score or F-measure | | metrics.fbeta_score(y_true, y_pred, beta[, …]) | Compute the F-beta score | | metrics.hamming_loss(y_true, y_pred[, …]) | Compute the average Hamming loss. | | metrics.hinge_loss(y_true, pred_decision[, …]) | Average hinge loss (non-regularized) | | metrics.jaccard_similarity_score(y_true, y_pred) | Jaccard similarity coefficient score | | metrics.log_loss(y_true, y_pred[, eps, …]) | Log loss, aka logistic loss or cross-entropy loss. | | metrics.matthews_corrcoef(y_true, y_pred[, …]) | Compute the Matthews correlation coefficient (MCC) | | metrics.precision_recall_curve(y_true, …) | Compute precision-recall pairs for different probability thresholds | | metrics.precision_recall_fscore_support(…) | Compute precision, recall, F-measure and support for each class | | metrics.precision_score(y_true, y_pred[, …]) | Compute the precision | | metrics.recall_score(y_true, y_pred[, …]) | Compute the recall | | metrics.roc_auc_score(y_true, y_score[, …]) | Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores. | | metrics.roc_curve(y_true, y_score[, …]) | Compute Receiver operating characteristic (ROC) | | metrics.zero_one_loss(y_true, y_pred[, …]) | Zero-one classification loss. | Regression metrics See the Regression metrics section of the user guide for further details. | metrics.explained_variance_score(y_true, y_pred) | Explained variance regression score function | | ———————————————————— | ———————————————————— | | metrics.mean_absolute_error(y_true, y_pred) | Mean absolute error regression loss | | metrics.mean_squared_error(y_true, y_pred[, …]) | Mean squared error regression loss | | metrics.mean_squared_log_error(y_true, y_pred) | Mean squared logarithmic error regression loss | | metrics.median_absolute_error(y_true, y_pred) | Median absolute error regression loss | | metrics.r2_score(y_true, y_pred[, …]) | R^2 (coefficient of determination) regression score function. | Multilabel ranking metrics See the Multilabel ranking metrics section of the user guide for further details. | metrics.coverage_error(y_true, y_score[, …]) | Coverage error measure | | ———————————————————— | ————————————— | | metrics.label_ranking_average_precision_score(…) | Compute ranking-based average precision | | metrics.label_ranking_loss(y_true, y_score) | Compute Ranking loss measure | XGBoost 原理部分参考： XGBoost第一课 XGBoost支持的目标函数objective参数，这个参数在 XGBoost 里面属于任务参数（Learning Task Parameters） [default=reg:linear] reg:linear: linear regression reg:logistic: logistic regression binary:logistic: logistic regression for binary classification, output probability binary:logitraw: logistic regression for binary classification, output score before logistic transformation binary:hinge: hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities. count:poisson –poisson regression for count data, output mean of poisson distribution 计数问题的poisson回归，输出结果为poisson分布。 max_delta_step is set to 0.7 by default in poisson regression (used to safeguard optimization) survival:cox: Cox regression for right censored survival time data (negative values are considered right censored). Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function h(t) = h0(t) * HR). multi:softmax: set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes) 让XGBoost采用softmax目标函数处理多分类问题 multi:softprob: same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata * nclass matrix. The result contains predicted probability of each data point belonging to each class. 和softmax一样，但是输出的是ndata * nclass的向量，可以将该向量 reshape成ndata行nclass列的矩阵。没行数据表示样本所属于每个类别的概率。 rank:pairwise: Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized rank:ndcg: Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized rank:map: Use LambdaMART to perform list-wise ranking where Mean Average Precision (MAP) is maximized reg:gamma: gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed. reg:tweedie: Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed. XGBoost自定义目标函数 在GBDT训练过程，当每步训练得到一棵树，要调用目标函数得到其梯度作为下一棵树拟合的目标 XGBoost在调用obj函数时会传入两个参数：preds和dtrain preds为当前模型完成训练时，所有训练数据的预测值 dtrain为训练集，可以通过dtrain.get_label()获取训练样本的label 同时XGBoost规定目标函数需返回当前preds基于训练label的一阶和二阶梯度 例子 参考官网：https://github.com/dmlc/xgboost/blob/master/demo/guide-python/custom_objective.py 12345678#user define objective function, given prediction, return gradient and second order gradient#this is log likelihood lossdef logregobj(preds, dtrain): #自定义损失函数 labels = dtrain.get_label() preds = 1.0 / (1.0 + np.exp(-preds)) grad = preds - labels #梯度 hess = preds * (1.0-preds) #2阶导数 return grad, hess 调用的时候：123# training with customized objective, we can also do step by step training# simply look at xgboost.py's implementation of trainbst = xgb.train(param, dtrain, num_round, watchlist, obj=logregobj, feval=evalerror) XGBoost支持的评价函数eval_metric参数，这个参数在 XGBoost 里面属于任务参数（Learning Task Parameters） [default according to objective] Evaluation metrics for validation data, a default metric will be assigned according to objective (rmse for regression, and error for classification, mean average precision for ranking) User can add multiple evaluation metrics. Python users: remember to pass the metrics in as list of parameters pairs instead of map, so that latter eval_metric won’t override previous one The choices are listed below: rmse: root mean square error mae: mean absolute error logloss: negative log-likelihood error: Binary classification error rate. It is calculated as #(wrong cases)/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances. error@t: a different than 0.5 binary classification threshold value could be specified by providing a numerical value through ‘t’. merror: Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases). mlogloss: Multiclass logloss. auc: Area under the curve aucpr: Area under the PR curve ndcg: Normalized Discounted Cumulative Gain map: Mean Average Precision ndcg@n, map@n: ‘n’ can be assigned as an integer to cut off the top positions in the lists for evaluation. ndcg-, map-, ndcg@n-, map@n-: In XGBoost, NDCG and MAP will evaluate the score of a list without any positive samples as 1. By adding “-” in the evaluation metric XGBoost will evaluate these score as 0 to be consistent under some conditions. poisson-nloglik: negative log-likelihood for Poisson regression gamma-nloglik: negative log-likelihood for gamma regression cox-nloglik: negative partial log-likelihood for Cox proportional hazards regression gamma-deviance: residual deviance for gamma regression tweedie-nloglik: negative log-likelihood for Tweedie regression (at a specified value of the tweedie_variance_power parameter) XGBoost自定义评价函数例子参考官网：https://github.com/dmlc/xgboost/blob/master/demo/guide-python/custom_objective.py 123456789101112131415# user defined evaluation function, return a pair metric_name, result# NOTE: when you do customized loss function, the default prediction value is margin# this may make builtin evaluation metric not function properly# for example, we are doing logistic loss, the prediction is score before logistic transformation# the builtin evaluation error assumes input is after logistic transformation# Take this in mind when you use the customization, and maybe you need write customized evaluation functiondef evalerror(preds, dtrain): labels = dtrain.get_label() # return a pair metric_name, result. The metric name must not contain a colon (:) or a space # since preds are margin(before logistic transformation, cutoff at 0) return 'my-error', float(sum(labels != (preds &gt; 0.0))) / len(labels)# training with customized objective, we can also do step by step training# simply look at xgboost.py's implementation of trainbst = xgb.train(param, dtrain, num_round, watchlist, obj=logregobj, feval=evalerror) 调用的时候： 123# training with customized objective, we can also do step by step training# simply look at xgboost.py's implementation of trainbst = xgb.train(param, dtrain, num_round, watchlist, obj=logregobj, feval=evalerror) XGBoost参数调优XGBoost参数列表 参数 说明 max_depth 树的最大深度。树越深通常模型越复杂，更容易过拟合。 learning_rate 学习率或收缩因子。学习率和迭代次数／弱分类器数目n_estimators相关。 缺省：0.1 n_estimators 弱分类器数目. 缺省:100 slient 参数值为1时，静默模式开启，不输出任何信息 objective 待优化的目标函数，常用值有： binary:logistic 二分类的逻辑回归，返回预测的概率 multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。 multi:softprob 和 multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。支持用户自定义目标函数 nthread 用来进行多线程控制。 如果你希望使用CPU全部的核，那就不用缺省值-1，算法会自动检测它。 booster 选择每次迭代的模型，有两种选择： gbtree：基于树的模型，为缺省值。 gbliner：线性模型 gamma 节点分裂所需的最小损失函数下降值 min_child_weight 叶子结点需要的最小样本权重（hessian）和 max_delta_step 允许的树的最大权重 subsample 构造每棵树的所用样本比例（样本采样比例），同GBM colsample_bytree 构造每棵树的所用特征比例 colsample_bylevel 树在每层每个分裂的所用特征比例 reg_alpha L1/L0正则的惩罚系数 reg_lambda L2正则的惩罚系数 scale_pos_weight 正负样本的平衡 base_score 每个样本的初始估计，全局偏差 random_state 随机种子 seed 随机种子 missing 当数据缺失时的填补值。缺省为np.nan kwargs XGBoost Booster的Keyword参数 参数类别 通用参数：这部分参数通常我们不需要调整，默认值就好 学习目标参数：与任务有关，定下来后通常也不需要调整 booster参数：弱学习器相关参数，需要仔细调整，会影响模型性能 通用参数 booster：弱学习器类型 可选gbtree（树模型）或gbliner（线性模型） 或 dart （参考我的另一篇博文： XGBoost第一课） 默认为gbtree（树模型为非线性模型，能处理更复杂的任务） silent：是否开启静默模式 1：静默模式开启，不输出任何信息 默认值为0：输出一些中间信息，以助于我们了解模型的状态 nthread：线程数 默认值为-1，表示使用系统所有CPU核 学习目标参数 objective: 损失函数 支持分类／回归／排 eval_metric：评价函数 seed：随机数的种子 默认为0 设置seed可复现随机数据的结果，也可以用于调整参数 booster参数弱学习器的参数，尽管有两种booster可供选择，这里只介绍gbtree learning_rate : 收缩步长 vs. n_estimators：树的数目 较小的学习率通常意味着更多弱分学习器 通常建议学习率较小（ $\eta &lt; 0.1$ ）弱学习器数目n_estimators大 $f_m(x_i)=f_{m-1}(x_i)+\eta\beta_m WeakLearner_m(x_i) $ 可以设置较小的学习率，然后用交叉验证确定n_estimators 行（subsample）列（colsample_bytree、colsample_bylevel）下采样比例 默认值均为1，即不进行下采样，使用所有数据 随机下采样通常比用全部数据的确定性过程效果更好，速度更快 建议值：0.3 - 0.8 树的最大深度： max_depth max_depth越大，模型越复杂，会学到更具体更局部的样本 需要使用交叉验证进行调优，默认值为6，建议3-10 min_child_weight ：孩子节点中最小的样本权重和 如果一个叶子节点的样本权重和小于min_child_weight则分裂过程 结束 Kaggle竞赛优胜者的建议 Tong He（XGBoost R语言版本开发者）： 三个最重要的参数为：树的数目、树的深度和学习率。建议参数调整策略为： 采用默认参数配置试试 如果系统过拟合了，降低学习率 如果系统欠拟合，加大学习率 油管上作者视频：Kaggle Winning Solution Xgboost Algorithm - Learn from Its Author, Tong He Owen Zhang （常使用XGBoost）建议： n_estimators和learning_rate：固定n_estimators为100（数目不大，因为树的深度较大，每棵树比较复杂），然后调整learning_rate 树的深度max_depth：从6开始，然后逐步加大 $\text{min_child_weight}={1\over\sqrt{\text{rare_events}}}$ ，其中 rare_events 为稀有事件的数目 列采样 ${\text{colsample_bytree}\over \text{colsample_bylevel}}$ ：在 $[0.3,0.5]$ 之间进行网格搜索 行采样subsample：固定为1 gamma: 固定为0.0 油管上大神的视频：Learn Kaggle techniques from Kaggle #1, Owen Zhang 参数调优的一般方法 选择较高的学习率(learning rate)，并选择对应于此学习率的理想的树数量 学习率以工具包默认值为0.1。 XGBoost直接引用函数“cv”可以在每一次迭代中使用交叉验证，并返回理想的树数量（因为交叉验证很慢，所以可以import两种XGBoost：直接引用xgboost（用“cv”函数调整树的数目）和XGBClassifier —xgboost的sklearn包（用GridSearchCV调整其他参数 ）。 对于给定的学习率和树数量，进行树参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree, colsample_bylevel) xgboost的正则化参数(lambda, alpha)的调优 降低学习率，确定理想参数 XGBoost参数调优案例分析 竞赛官网：Otto Group Product Classification Challenge 是关于电商商品分类的案例，其中 Target：共9个商品类别 93个特征：整数型特征 详细请看我的jupyter notebook: kaggle Titanic 案例 详细请看我的jupyter notebook: XGBoost并行处理XGBoost工程实现 XGBoost用C++实现，显示地采用OpenMP API做并行处理 建单棵树时并行（Random Forest在建不同树时并行，但Boosting增加树是一个串行操作） XGBoost的scikit-learn接口中的参数 nthread 可指定线程数 -1 表示使用系统所有的核资源 model = XGBClassifier(nthread=-1) 在准备建树数据时高效（近似建树、稀疏、 Cache、数据分块） 交叉验证也支持并行（由scikit-learn 提供支持） scikit-learn 支持的k折交叉验证也支持多线程 cross_val_score() 函数中的参数：n_ jobs = -1 表示使用系统所有的CPU核 results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring= ’neg_log_loss’ , n_jobs=-1, verbose=1) 并行处理的三种配置 交叉验证并行，XGBoost建树不并行 交叉验证不并行，XGBoost建树并行 交叉验证并行，XGBoost建树并行 Otto数据集上的10折交叉验证实验结果： Single Thread XGBoost, Parallel Thread CV: 359.854589 Parallel Thread XGBoost, Single Thread CV: 330.498101 Parallel Thread XGBoost and CV: 313.382301，并行 XGBoost 比并行交叉验证好，两者都并行更好 例子查看 12345678910# evaluate the effect of the number of threads results = [] num_threads = [1, 2, 3, 4] for n in num_threads: start = time.time() model = XGBClassifier(nthread=n) model.fit(X_train, y_train) elapsed = time.time() - start print(n, elapsed) results.append(elapsed) XGBoost总结 XGBoost是一个用于监督学习的非参数模型 目标函数（损失函数、正则项） 参数（树的每个分支分裂特征及阈值） 优化：梯度下降 参数优化 决定模型复杂度的重要参数：learning_rate, n_estimators, max_depth, min_child_weight, gamma, reg_alpha, reg_lamba 随机采样参数也影响模型的推广性： subsample, colsample_bytree, colsample_bylevel 其他未涉及的部分 分布式XGBoost AWS YARN Cluster … GPU加速 并行计算与内存优化的细节 主要关注XGBoost的对外接口 XGBoost资源 XGBoost官方文档：https://xgboost.readthedocs.io/en/latest/ Python API：http://xgboost.readthedocs.io/en/latest/python/python_api.html Github： https://github.com/dmlc/xgboost 很多有用的资源：https://github.com/dmlc/xgboost/blob/master/demo/README.md GPU加速：https://github.com/dmlc/xgboost/blob/master/plugin/updater_gpu/README.md XGBoost原理：XGBoost: A Scalable Tree Boosting System https://arxiv.org/abs/1603.02754 其他资源 XGBoost参数调优： https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuningxgboost-with-codes-python/ 中文版：http://blog.csdn.net/u010657489/article/details/51952785 Owen Zhang, Winning Data Science Competitions https://www.slideshare.net/OwenZhang2/tips-for-data-sciencecompetitions?from_action=save XGBoost User Group： https://groups.google.com/forum/#!forum/xgboost-user/]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning， feature engineering</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost原理和底层实现剖析]]></title>
    <url>%2F2018%2F10%2F02%2Fget-started-XGBoost%2F</url>
    <content type="text"><![CDATA[前言在深度学习火起来之前，集成学习 （ensemble learning 包括 boosting: GBDT, XGBoost）是 kaggle 等比赛中的利器，所以集成学习是机器学习必备的知识点，如果提升树或者GBDT不熟悉，最好先看一下我的另一文： 《统计学习方法》第8章 提升方法之AdaBoost\BoostingTree\GBDT ，陈天奇 的 XGBoost (eXtreme Gradient Boosting) 和 微软的 lightGBM 是 GBDT 算法模型的实现，非常巧妙，是比赛的屠龙之器，算法不仅仅是数学，还涉及系统设计和工程优化。以下引用陈天奇 XGBoost论文 的一段话： Among the 29 challenge winning solutions 3 published at Kaggle’s blog during 2015, 17 solutions used XGBoost. Among these solutions, eight solely used XGBoost to train the model, while most others combined XGBoost with neural nets in ensembles. For comparison, the second most popular method, deep neural nets, was used in 11 solutions. The success of the system was also witnessed in KDDCup 2015, where XGBoost was used by every winning team in the top-10. Moreover, the winning teams reported that ensemble methods outperform a well-configured XGBoost by only a small amount [1]. 正文分成以下几个部分 快速了解：来自陈天奇的ppt XGBoost的设计精髓：来自陈天奇的关于XGBoost的论文 参数详解：结合原理+XGBoost官网API的翻译 正文XGBoost快速了解这部分内容基本上是对陈天奇幻灯片：官网幻灯片 outlook 幻灯片大纲• 监督学习的主要概念的回顾• 回归树和集成模型 (What are we Learning)• 梯度提升 (How do we Learn)• 总结 Review of key concepts of supervised learning 监督学习的关键概念的回顾概念 符号 含义 $R^d$ 特征维度为d的数据集 $x_i∈R^d$ 第i个样本 $w_j$ 第j个特征的权重 $\hat{y}_i$ $x_i$ 的预测值 $y_i$ 第i个训练集的对应的标签 $\Theta$ 特征权重的集合 模型 基本上相关的所有模型都是在下面这个线性式子上发展起来的$$\hat y_i = \sum_{j = 0}^{d} w_j x_{ij}$$上式中 $x_0=1$，就是引入了一个偏差量，或者说加入了一个常数项。由该式子可以得到一些模型： 线性模型，最后的得分就是 $\hat{y}_i$ 。 logistic模型，最后的得分是sigmoid函数 $\frac{1}{1+e^{−\hat{y}_i}}$ 。然后设置阀值，转为正负实例。 其余的大部分也是基于 $\hat{y}_i$ 做了一些运算得到最后的分数 参数 参数就是 $\Theta=\{w_j|j=1,…,d\}$ ，这也正是我们所需要通过训练得出的。 训练时的目标函数 训练时通用的目标函数如下：$$Obj(\Theta)=L(\Theta)+Ω(\Theta)$$在上式中 $L(\Theta)$ 代表的是训练误差，表示该模型对于训练集的匹配程度。$Ω(\Theta)$ 代表的是正则项，表明的是模型的复杂度。 训练误差可以用 $L = \sum_{i = 1}^n l(y_i, \hat y_i)$ 来表示，一般有方差和logistic误差。 方差: $l(y_i,\hat y_i) = (y_i - \hat y_i)^2$ logstic误差: $l(y_i, \hat y_i) = y_i ln(1 + e^{- \hat y_i}) + (1 - y_i)ln(1 + e^{\hat y_i})$ 正则项按照Andrew NG的话来说，就是避免过拟合的。为什么能起到这个作用呢？正是因为它反应的是模型复杂度。模型复杂度，也就是我们的假设的复杂度，按照奥卡姆剃刀的原则，假设越简单越好。所以我们需要这一项来控制。 L2 范数: $Ω(w)=λ||w||_2$ L1 范数(lasso): $Ω(w)=λ||w||_1$ 常见的优化函数有有岭回归，logstic回归和Lasso，具体的式子如下​ 岭回归，这是最常见的一种，由线性模型，方差和L2范数构成。具体式子为 $\sum\limits^n_{i=1}(y_i−w^Tx_i)2+λ||w||_2$ logstic回归，这也是常见的一种，主要是用于二分类问题，比如爱还是不爱之类的。由线性模型，logistic 误差和L2范数构成。具体式子为 $\sum\limits^n_{i=1} [y_iln(1+e^{−w^Tx_i})+(1−y_i)ln(1+e^{w^Tx_i})]+λ||w||_2$ lasso比较少见，它是由线性模型，方差和L1范数构成的。具体式子为 $\sum\limits_{i = 1}^n (y_i - w^T x_i)^2 + \lambda \vert \vert w \vert \vert _1$ 我们的目标的就是让 $Obj(\Theta)$ 最小。那么由上述分析可见，这时必须让 $L(\Theta$ ) 和 $Ω(\Theta)$ 都比较小。而我们训练模型的时候，要在 bias 和 variance 中间找平衡点。bias 由 $L(\Theta)$ 控制，variance 由 $Ω(\Theta)$ 控制。欠拟合，那么 $L(\Theta)$ 和 $Ω(\Theta)$ 都会比较大，过拟合的话 $Ω(\Theta)$ 会比较大，因为模型的扩展性不强，或者说稳定性不好。 回归树和集成模型 (What are we Learning)Regression Tree (CART) 回归树，也叫做分类与回归树，我认为就是一个叶子节点具有权重的二叉决策树。它具有以下两点特征 决策规则与决策树的一样。 每个叶子节点上都包含了一个权重，也有人叫做分数。 下图就是一个回归树的示例： 回归树的集成模型 回归 小男孩落在第一棵树的最左叶子和第二棵树的最左叶子，所以它的得分就是这两片叶子的权重之和，其余也同理。 树有以下四个优点： 使用范围广，像GBM，随机森林等。(PS: 据陈天奇大神的统计，至少有超过半数的竞赛优胜者的解决方案都是用回归树的变种) 对于输入范围不敏感，所以并不需要对输入归一化 能学习特征之间更高级别的相互关系 很容易对其扩展 模型和参数 假设我们有 $K$ 棵树，那么$$\hat y_i = \sum_{k = 1}^K f_k(x_i),\ \ f_k \in \cal F$$上式中 $\cal F$ 表示的是回归森林中的所有函数空间。$f_k(x_i)$ 表示的就是第 $i$ 个样本在第 $k$ 棵树中落在的叶子的权重。那么现在我们需要求的参数就是每棵树的结构和每片叶子的权重，或者简单的来说就是求 $f_k$ 。那么为了和上一节所说的通用结构统一，可以设$$\Theta = \lbrace f_1,f_2,f_3, \cdots ,f_k \rbrace$$ 在单一变量上学习一棵树 定义一个目标对象，优化它。 例如： 考虑这样一个问题：在输入只有时间（t）的回归树 我想预测在时间是t的时候，我是否喜欢浪漫风格的音乐？ 可见分段函数的分割点就是回归树的非叶子节点，分段函数每一段的高度就是回归树叶子的权重。那么就可以直观地看到欠拟合和过拟合曲线所对应的回归树的结构。根据我们上一节的讨论，$Ω(f)$ 表示模型复杂度，那么在这里就对应着分段函数的琐碎程度。$L(f)$ 表示的就是函数曲线和训练集的匹配程度。 学习阶跃函数 第二幅图：太多的分割点，$\Omega(f)$ 即模型复杂度很高；第三幅图：错误的分割点，$L(f)$ 即损失函数很高。第四幅图：在模型复杂度和损失函数之间取得很好的平衡。 综上所述 模型：假设我们有k棵树，那么模型的表达式 $\hat{y}_i = \sum\limits_{k=1}^{K}f_k(x_i), f_k\in \cal{F}$ 目标函数：$Obj =\underbrace{\sum_{i=1}^{n}l(y_i, \hat{y_i})}_{训练误差} +\underbrace{\sum_{k=1}^{K}\Omega(f_k)}_{树的复杂度}$ 定义树的复杂度几种方式 树的节点数或深度 树叶子节点的L2范式 …（后面会介绍有更多的细节） 目标函数 vs 启发式当你讨论决策树，它通常是启发式的 按信息增益 对树剪枝 最大深度 对叶子节点进行平滑 大多数启发式可以很好地映射到目标函数 信息增益 -&gt; 训练误差 剪枝 -&gt; 按照树节点的数目定义的正则化项 最大深度 -&gt; 限制函数空间 对叶子值进行平滑操作 -&gt; 叶子权重的L2正则化项 回归树不仅仅用于回归 回归树的集成模型定义了你如何创建预测的分数，它能够用于 分类，回归，排序 … … 回归树的功能取决于你怎么定义目标函数 目前为止我们已经学习过 使用方差损失（Square Loss） $l(y_i, \hat{y_i})=(y_i-\hat{y}_i)$ ，这样就产生了普通的梯度提升机（common gradient boosted machine） 使用逻辑损失（Logistic loss）$l(y, \hat{y}_i)=y_i\ln(1+e^{-\hat{y}_i}) + (1-y_i)\ln(1+e^{\hat{y}_i})$ ，这样就产生了逻辑梯度提升（LogitBoost）。 梯度提升Gradient Boosting (How do we Learn) 那怎么学习？ 目标对象：$\sum_{i=1}^{n}l(y_i,\hat{y_i}) + \sum_k\Omega(f_k), f_k \in \cal{F}$ 我们不能用像SGD（随机梯度下降）这样的方法去找到 f，因为他们是树而不是仅仅是数值向量。 解决方案：加法训练 Additive Training（提升方法boosting） 从常量方法开始，每一次（轮）添加一个新的方法 这个算法的思想很简单，一棵树一棵树地往上加，一直到 $K$ 棵树停止。过程可以用下式表达：$$\begin{align}\hat y_i^{(0)} &amp;= 0 \\\hat y_i^{(1)} &amp;= f_1(x_i) = \hat y_i^{(0)} + f_1(x_i) \\\hat y_i^{(2)} &amp;= f_1(x_i) + f_2(x_i) = \hat y_i^{(1)} + f_2(x_i) \\&amp; \cdots \\\hat y_i^{(t)} &amp;= \sum_{k = 1}^t f_k(x_i) = \hat y_i^{(t - 1)} + f_t(x_i)\end{align}$$ 加法训练 我们如何决定什么样的 $f$ 加到模型中？ 优化目标 在 $t$ 轮的预测是：$\hat y_i^{(t)} = \hat y_i^{(t - 1)} + f_t(x_i) $ 加号右边这一项就是我们在 t 轮需要决定的东西 $$ \begin{align} Obj^{(t)} &amp;= \sum_{i = 1}^n l(y_i, \hat y_i^{(t)}) + \sum_{i = 1}^t \Omega (f_i) \\ &amp;= \sum_{i = 1}^n l(y_i, \hat y_i^{(t - 1)} + f_t(x_i)) + \Omega (f_t) + constant \end{align} $$ 考虑平方误差 $$ \begin{align} Obj^{(t)} &amp;= \sum_{i=1}^{n} \left \{y_i-(\hat{y}^{(t-1)}_i)+f_t(x_i)\right \}^2 +\Omega(f_t)+const \\ &amp;= \sum_{i=1}^{n} \left \{2(\hat{y}^{(t-1)}_i-y_i)+f_t(x_i)^2\right \} +\Omega(f_t)+const \\ \end{align} $$ $(\hat{y}^{(t-1)}_i-y_i)$ 称为残差。 损失函数的泰勒展开可由泰勒公式得到下式$$f(x + \Delta x) \approx f(x) +f^{\prime}(x) \Delta x + \frac 1 2 f^{\prime \prime}(x) \Delta x^2$$那么现在可以把 $y^{(t)}_i$看成上式中的 $f(x+Δx)$ ，$y^{(t−1)}_i$ 就是 $f(x)$ ，$f_t(x_i)$ 为 $Δx$ 。然后设 $g_i$ 代表 $f′(x)$ ，也就是 $g_i = {\partial}_{\hat y^{(t - 1)}} \ l(y_i, \hat y^{(t - 1)})$ 用 $h_i$ 代表 $f′′(x)$， 于是 $h_i = {\partial}_{\hat y^{(t - 1)}}^2 \ l(y_i, \hat y^{(t - 1)})$ 于是现在目标函数就为下式:$$\begin{align}Obj^{(t)} &amp;\approx \sum_{i = 1}^n [l(y_i, \hat y_i^{(t - 1)}) + g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t) + constant \\&amp;= \sum_{i = 1}^n [g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t) + [\sum_{i = 1}^n l(y_i, \hat y_i^{(t - 1)}) + constant]\end{align}$$可以用平方误差的例子进行泰勒展开看看结果是否一致，很明显，上式中后面那项 $[\sum_{i = 1}^n l(y_i, \hat y_i^{(t - 1)}) + constant]$ 对于该目标函数我们求最优值点的时候并无影响，所以，现在有了新的优化目标$$Obj^{(t)} \approx \sum_{i = 1}^n [g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t)$$ 这么苦逼图啥？ 改进树的定义 Refine the definition of tree上一节讨论了 $f_t(x)$ 的物理意义，现在我们对其进行数学公式化。设 $w∈R^T$ ， $w$ 为树叶的权重序列，$q:R^d \rightarrow \lbrace 1,2, \cdots ,T \rbrace$ ，$q$ 为树的结构。那么 $q(x)$ 表示的就是样本 $x$ 所落在树叶的位置。可以用下图形象地表示 现在对训练误差部分的定义已经完成。那么对模型的复杂度应该怎么定义呢？ 定义树的复杂度 Define Complexity of a Tree树的深度？最小叶子权重？叶子个数？叶子权重的平滑程度？等等有许多选项都可以描述该模型的复杂度。为了方便，现在用叶子的个数和叶子权重的平滑程度来描述模型的复杂度。可以得到下式：$$\Omega(f_t) = \gamma T + \frac 1 2 \lambda \sum_{j = 1}^T w_j^2$$说明：上式中前一项用叶子的个数乘以一个收缩系数，后一项用L2范数来表示叶子权重的平滑程度。 下图就是计算复杂度的一个示例： 修改目标函数 Revisit the Objectives最后再增加一个定义，用 $I_j$ 来表示第 $j$ 个叶子里的样本集合。也就是上图中，第 $j$ 个圈，就用 $I_j$ 来表示。$$I_j = \lbrace i|q(x_i) = j \rbrace$$好了，最后把优化函数重新按照每个叶子组合,并舍弃常数项：$$\begin{align}Obj^{(t)} &amp;\approx \sum_{i = 1}^n [g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t) \\ &amp;= \sum_{i = 1}^n [ g_i w_{q(x_i)} + \frac 1 2 h_i w_{q(x)}^2] + \gamma T + \frac 1 2 \lambda \sum_{j = 1}^T w_j^2 \\ &amp;= \sum_{j = 1}^T [(\sum_{i \in I_j } g_i)w_j + \frac 1 2 (\sum_{i \in I_j}h_i + \lambda)w_j^2] + \gamma T\end{align}$$ 这是 $T$ 个独立的二次函数的和。 结构分 The Structure Score初中时所学的二次函数的最小值可以推广到矩阵函数里$$\mathop{\min_x}\{Gx+ \frac 1 2 Hx^2\} = - \frac 1 2 \frac {G^2} H, \quad H \gt 0 \\\mathop{\arg\min_x}\{Gx+\frac{1}{2}Hx^2\} = -\frac{G}{H}，H \ge 0$$设 $G_j = \sum_{i \in I_j } g_i,\ H_j = \sum_{i \in I_j}h_i$ ，那么$$\begin{align}Obj^{(t)} &amp;= \sum_{j = 1}^T [(\sum_{i \in I_j } g_i)w_j + \frac 1 2 (\sum_{i \in I_j}h_i + \lambda)w_j^2] + \gamma T \\ &amp;= \sum_{j = 1}^T [G_j w_j + \frac 1 2 (H_j + \lambda)w_j^2] + \gamma T\end{align}$$因此，若假设我们的树的结构已经固定，就是 $q(x)$ 已经固定，那么$$\begin{align}W_j^* &amp;= - \frac {G_j}{H_j + \lambda} \\Obj &amp;= - \frac 1 2 \sum_{j = 1}^T \frac {G_j^2}{H_j + \lambda} + \gamma T\end{align}$$例子 用于单棵树的搜索算法 Searching Algorithm for Single Tree现在只要知道树的结构，就能得到一个该结构下的最好分数。可是树的结构应该怎么确定呢？ 枚举可能的树结构 q 使用分数公式来计算 q 的结构分： $Obj = -\frac{1}{2} \sum\limits_{j=1}^{T}\frac{G_j^2}{H_j+\lambda} + \gamma T$ 找到最好的树结构，然后使用优化的叶子权重： $w^*_j=-\frac{G_j}{H_j+\lambda}$ 但是这可能有无限多个可能的树结构 树的贪婪学习 Greedy Learning of the Tree 从深度为 0 的树开始 对树的每个叶子节点，试着添加一个分裂点。添加这个分裂点后目标函数（即损失函数）的值变化 $$ \begin{align} Obj_{split} &amp;= - \frac{1}{2}[\underbrace{\frac{G_L^2}{H_L+\lambda}}_{左孩子节点分数} + \underbrace {\frac{G^2_R}{H_R+\lambda}}_{右孩子节点分数}] + \gamma T_{split} \\ Obj_{unsplit} &amp;= - \frac{1}{2}\underbrace{\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}}_{分裂前的分数} + \gamma T_{unsplit} \\ Gain &amp;= Obj_{unsplit} - Obj_{split} \\ &amp;= \frac 1 2 [\frac {G_L^2}{H_L + \lambda} + \frac {G_R^2}{H_R + \lambda} - \frac {(G_L + G_R)^2}{H_L + H_R + \lambda}] - \gamma(T_{split} - T_{unsplit}) \end{align} $$ 剩下的问题：我们如何找到最好的分裂点？ 最好分裂点的查找 Efficient Finding of the Best Split 当分裂规则是 $x_j&lt;a$ 时，树的增益是 ? 假设 $x_j$ 是年龄 我们所需要就是上图的两边 $g$ 和 $h$ 的和，然后计算 $$ Gain = \frac{G_L^2}{H_L+\lambda} + \frac{G_L^2}{H_L+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda} - \gamma $$ 在一个特征上，从左至右对已经排序的实例进行线性扫描能够决定哪个是最好的分裂点。 分裂点查找算法 An Algorithm for Split Finding 对于每个节点，枚举所有的特征 对于每个特征，根据特征值对实例（样本）进行排序 在这个特征上，使用线性扫描决定哪个是最好的分裂点 在所有特征上采用最好分裂点的方案 深度为 $K$ 的生长树的时间复杂度 $O(K\ d\ n\log n)$ ：每一层需要 $O(n\ \log n)$ 时间去排序，且需要在 $d$ 个特征上排序，我们需要在 $K$ 层进行这些排序。（补充：$O(n)$ 时间计算当前特征的最佳分裂点，即最后实际上 $O(d\ K\ (n\log n +n)$） 这些可以进一步优化（例如：使用近似算法和缓存已经排序的特征） 能够拓展到非常大的数据集 类变量（categorical variables） 有一些树处理分开处理类变量和连续值的变量 xgboost可以简单地使用之前推导的分数公式去计算基于类变量的分裂分数 实际上，没有必要分开处理类变量 我们可以使用独热编码（one-hot encoding）将类变量编码成数值向量。分配一个维度为类数量的向量。 $$ z_j=\cases{1,\quad &amp;\text{if $x$ is in category $j$}\\ 0,\quad &amp;otherwise} $$ 如果有很多类变量，这个数值向量将是稀疏的，xgboost学习算法被设计成偏爱处理稀疏数据。 补充：对某个节点的分割时，是需要按某特征的值排序，那么对于无序的类别变量，就需要进行one-hot化。否则，举个例子：假设某特征有1，2，3三种变量，进行比较时，就会只比较左子树为1, 2或者右子树为2, 3，或者不分割，哪个更好，但是左子树为 1,3 的分割的这种情况就会忘记考虑。因为 $Gain$ 于特征的值范围是无关的，它采用的是已经生成的树的结构与权重来计算的。所以不需要对特征进行归一化处理。 剪枝和正则化 Pruning and Regularization 回忆一下增益公式： $Gain=\underbrace{\frac{G^2_L}{H_L+\lambda} + \frac{G^2_R}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda}}_{训练损失的减少量} - \underbrace{\gamma}_{正则项}$ 当训练损失减少量小于正则项的时候，分裂后的增益就变成负的。 在树的简化度（simplicity）和预测性能（predictiveness）的权衡（trade-off） 提早终止（Pre-stopping） 如果最好的分裂产生的增益计算出来是负的，那么停止分裂。 但是（当前的）一个分裂可能对未来的分裂有益。 后剪枝 （Post-Prunning） 生长一棵树到最大深度，再递归地剪枝所有具有负增益的叶子分裂节点。 回顾提升树算法 Recap: Boosted Tree Algorithm 每一轮添加一棵树 每一轮开始的时候，计算 $g_i=\partial_{\hat{y}_i^{(t-1)}}l(y_i,\hat{y}^{(t-1)}), h_i=\partial_{\hat{y}^{(t-1)}}l(y_i, \hat{y}^{(t-1)})$ 使用统计学知识（统计所有分裂点信息：一节梯度和二阶梯度），用贪婪的方式生长一棵树 $f_t(x)$ ： $$ Obj = -\frac{1}{2}\sum\limits_{j=1}^{T}\frac{G_j^2}{H_j+\lambda} + \gamma T $$ 添加 $f_t(x)$ 到模型 $\hat{y}_i^{(t)}=\hat{y}_i^{(t-1)} + f_t(x_i)$ 通常，我们这么做令 $\hat{y}_i^{(t)}=\hat{y}_i^{(t-1)} + \epsilon f_t(x_i)$ $\epsilon$ 称为步伐大小（step-size）或者收缩（shrinkage），通常设置为大约 0.1 这意味着在每一步我们做完全优化，是为了给未来的轮次保留机会（去进一步优化），这样做有助于防止过拟合。 —————————————————————幻灯片内容结束———————————————————————- XGBoost 系统设计的精髓这部分内容主要来自陈天奇的论文 XGBoost: A Scalable Tree Boosting System 缩小和列抽样 shrinkage and column subsampling随机森林中的用法和目的一样，用来防止过拟合，主要参考论文2.3节 这个xgboost与现代的gbdt一样，都有shrinkage参数 （最原始的gbdt没有这个参数）类似于梯度下降算法中的学习速率，在每一步tree boosting之后增加了一个参数 $\eta$（被加入树的权重），通过这种方式来减小每棵树的影响力，给后面的树提供空间去优化模型。 column subsampling 列（特征）抽样，这个经常用在随机森林，不过据XGBoost的使用者反馈，列抽样防止过拟合的效果比传统的行抽样还好（xgboost也提供行抽样的参数供用户使用），并且有利于后面提到的并行化处理算法。 查找分裂点的近似算法 Approximate Algorithm主要参考论文3.2节 当数据量十分庞大，以致于不能全部放入内存时，精确的贪婪算法就不可能很有效率，通样的问题也出现在分布式的数据集中，为了高效的梯度提升算法，在这两种背景下，近似的算法被提出使用，算法的伪代码如下图所示 概括一下：枚举所有特征，根据特征，比如是第 $k$ 个特征的分布的分位数来决定出 $l$ 个候选切分点 $S_k = \{s_{k1},s_{k2},\cdots s_{kl}\}$ ，然后根据这些候选切分点把相应的样本映射到对应的桶中，对每个桶的 $G,H$ 进行累加。最后在候选切分点集合上贪心查找，和Exact Greedy Algorithm类似。 特征分布的分位数的理解 此图来自知乎weapon大神的《 GBDT算法原理与系统设计简介》 论文给出近似算法的2种变体，主要是根据候选点的来源不同区分： 在建树之前预先将数据进行全局（global）分桶，需要设置更小的分位数间隔，这里用 ϵ 表示，3分位的分位数间隔就是 $1/3$，产生更多的桶，特征分裂查找基于候选点多，计算较慢，但只需在全局执行一次，全局分桶多次使用。 每次分裂重新局部（local）分桶，可以设置较大的 $ϵ$ ，产生更少的桶，每次特征分裂查找基于较少的候选点，计算速度快，但是需要每次节点分裂后重新执行，论文中说该方案更适合树深的场景。 论文给出Higgs案例下，方案1全局分桶设置 $ϵ=0.05$ 与精确算法效果差不多，方案2局部分桶设置 $ϵ=0.3$ 与精确算法仅稍差点，方案1全局分桶设置 $ϵ=0.3$ 则效果极差，如下图： 由此可见，局部选择的近似算法的确比全局选择的近似算法优秀的多，所得出的结果和贪婪算法几乎不相上下。 最后很重的是：使用哪种方案，xgboost用户可以自由选择。 Notably, it is also possible to directly construct approximate histograms of gradient statistics. Our system efficiently supports exact greedy for the single machine setting, as well as approximate algorithm with both local and global proposal methods for all settings. Users can freely choose between the methods according to their needs. 这里直方图算法，常用于GPU的内存优化算法，leetcode上也有人总结出来：LeetCode Largest Rectangle in Histogram O(n) 解法详析， Maximal Rectangle 带权的分位方案 Weighted Quantile Sketch主要参考论文3.3节 在近似的分裂点查找算法中，一个步骤就是提出候选分裂点，通常情况下，一个特征的分位数使候选分裂点均匀地分布在数据集上，就像前文举的关于特征分位数的例子。 考虑 $\cal{D}_k = \lbrace (x_{1k},h_1), (x_{2k},h_2), (x_{3k},h_3), \cdot \cdot \cdot , (x_{nk},h_n)\rbrace$ 代表每个样本的第 $k$ 个特征和其对应的二阶梯度所组成的集合。那么我们现在就能用分位数来定义下面的这个排序函数 $r_k:\Bbb R \rightarrow [0,1]$$$r_k(z) = \frac 1 {\sum_{(x,h) \in \cal{D}_k}h} \sum_{(x,h)\in \cal{D}_k,x \lt z} h$$上式表示的就是该特征的值小于 $z$ 的样本所占总样本的比例。于是我们就能用下面这个不等式来寻找分裂候选点$\lbrace s_{k1},s_{k2},s_{k3}, \cdots, s_{kl} \rbrace$$$|r_k(s_{k,j}) - r_k(s_{k, j+1})| \lt \epsilon,\ s_{k1}=\underset{i}{min}\ x_{ik},s_{kl}=\underset{i}{max}\ x_{ik}$$上式中 $\epsilon$ 的作用：控制让相邻两个候选分裂点相差不超过某个值 $\epsilon$ ，那么 $1/\epsilon$ 的整数值就代表几分位，举例 $\epsilon=1/3$ ，那么就是三分位，即有 $3-1$ 个候选分裂点。数学上，从最小值开始，每次增加 $ϵ∗(\underset{i}\max x_{ik}−\underset{i}\min x_{ik})$ 作为分裂候选点。然后在这些分裂候选点中选择一个最大分数作为最后的分裂点，而且每个数据点的权重是 $h_i$ ，原因如下：$$\begin{align}Obj^{(t)} &amp;\approx \sum_{i = 1}^n [g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t) \\&amp;=\sum_{i=1}^N\frac{1}{2}h_i\left(2\frac{g_i}{h_i}f_t({\bf x_i}) + f_t^2({\bf x_i})\right) + \Omega(f_t) \\&amp;=\sum_{i=1}^N \frac{1}{2}h_i\left(\frac{g_i^2}{h_i^2} +2\frac{g_i}{h_i}f_t({\bf x_i}) + f_t^2({\bf x_i})\right) + \Omega(f_t) - \frac{g_i^2}{2h_i} \\&amp;=\sum_{i=1}^N \frac{1}{2}{\color{green}h_i}\left( f_t({\bf x_i}) – ({\color{green}- \frac{g_i}{h_i}})\right)^2 + \Omega(f_t) - \frac{g_i^2}{2h_i} \\&amp;=\sum_{i=1}^N \frac{1}{2}{\color{green}h_i}\left( f_t({\bf x_i}) – ({\color{green}- \frac{g_i}{h_i}})\right)^2 + \Omega(f_t) - constant\end{align}$$说明：这部分论文原文推导有些错误，国外问答网站 stack exchange 给出很明确的答复， 上式可以视为标签为 $-\frac{g_i}{h_i}$ 且权重为 $h_i$ 的平方误差，此时视 $\frac{g_i^2}{2h_i}$ 常数 （因为是来自上一轮的梯度和二阶梯度）。 现在应该明白 Weighted Quantile Sketch 带权的分位方案的由来，下面举个例子： 即要切分为3个，总和为1.8，因此第1个在0.6处，第2个在1.2处。此图来自知乎weapon大神的《 GBDT算法原理与系统设计简介》 注意稀疏问题的分裂点查找 Sparsity-aware Split Finding主要参考论文3.4节 对于数据缺失数据、one-hot编码等造成的特征稀疏现象，作者在论文中提出可以处理稀疏特征的分裂算法，主要是对稀疏特征值缺失的样本学习出默认节点分裂方向： 默认miss value进右子树，对non-missing value的样本在左子树的统计值 $G_L$ 与 $H_L$，右子树为 $G-G_L$ 与$H−H_L$，其中包含miss的样本，统计这种方案（默认miss value进右子树）的分数。 默认miss value进左子树，对non-missing value的样本在右子树的统计值 $G_R$ 与 $H_R$，左子树为 $G-G_R$ 与$H−H_R$ ，其中包含miss的样本，统计这种方案（默认miss value进左子树）的分数。 选择分数（即增益）比较大的方案。 这样最后求出增益最大的特征值以及 miss value 的分裂方向，作者在论文中提出基于稀疏分裂算法： （修正：下文 “Input: d feature dimension” 这里 “d” 应该改为 “m”） 使用了该方法，相当于比传统方法多遍历了一次，但是它只在非缺失值的样本上进行迭代，因此其复杂度与非缺失值的样本成线性关系。在 Allstate-10k 数据集上，比传统方法快了50倍： 旨在并行学习的列块结构 Column Block for Parallel Learning主要参考论文4.1节 CSR vs CSC 稀疏矩阵的压缩存储形式，比较常见的其中两种：压缩的稀疏行（Compressed Sparse Row）和 压缩的稀疏列（Compressed Sparse Row） CSR结构包含非0数据块values，行偏移offsets，列下标indices。offsets数组大小为（总行数目+1），CSR 是对稠密矩阵的压缩，实际上直接访问稠密矩阵元素 $(i,j)$ 并不高效，毕竟损失部分信息，访问过程如下： 根据行 $i$ 得到偏移区间开始位置 offsets[i]与区间结束位置 offsets[i+1]-1，得到 $i$ 行数据块 values[offsets[i]..(offsets[i+1]-1)]， 与非0的列下表indices[offsets[i]..(offsets[i+1]-1)] 在列下标数据块中二分查找 $j$，找不到则返回0，否则找到下标值 $k$，返回 values[offsets[i]+k] 从访问单个元素来说，相比坐标系的存储结构，那么从 $O(1)$ 时间复杂度升到 $O(\log N)$, N 为该行非稀疏数据项个数。但是如果要遍历访问整行非0数据，则无需访问indices数组，时间复杂度反而更低，因为少了大量的稀疏为0的数据访问。 CSC 与 CSR 变量结构上并无差别，只是变量意义不同 values仍然为矩阵的非0数据块 offsets为列偏移，即特征id对应数组 indices为行下标，对应样本id数组 XBGoost使用CSC 主要用于对特征的全局预排序。预先将 CSR 数据转化为无序的 CSC 数据，遍历每个特征，并对每个特征 $i$ 进行排序：sort(&amp;values[offsets[i]], &amp;values[offsets[i+1]-1])。全局特征排序后，后期节点分裂可以复用全局排序信息，而不需要重新排序。 矩阵的存储形式，参考此文：稀疏矩阵存储格式总结+存储效率对比:COO,CSR,DIA,ELL,HYB 采取这种存储结构的好处 未完待续。。。。。 关注缓存的存取 Cache-aware Access使用Block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的。这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。 因此，对于exact greedy算法中, 使用缓存预取。具体来说，对每个线程分配一个连续的buffer，读取梯度信息并存入Buffer中（这样就实现了非连续到连续的转化），然后再统计梯度信息。该方式在训练样本数大的时候特别有用，见下图： 在近似算法中，对块的大小进行了合理的设置。定义Block的大小为Block中最多的样本数。设置合适的大小是很重要的，设置过大则容易导致命中率低，过小则容易导致并行化效率不高。经过实验，发现 $2^{16}$ 比较好，那么上文提到CSC存储结构的 indices 数组（存储的行下表）的元素占用的字节数就是 16/8 = 2 。 核外块的计算 Blocks for Out-of-core ComputationXGBoost 中提出 Out-of-core Computation优化，解决了在硬盘上读取数据耗时过长，吞吐量不足 多线程对数据分块压缩 Block Compression 存储在硬盘上，再将数据传输到内存，最后再用独立的线程解压缩，核心思想：将磁盘的读取消耗转换为解压缩所消耗的计算资源。 分布式数据库系统的常见设计：Block Sharding 将数据分片到多块硬盘上，每块硬盘分配一个预取线程，将数据fetche到in-memory buffer中。训练线程交替读取多块缓存的同时，计算任务也在运转，提升了硬盘总体的吞吐量。 注：这部分内容属于外存算法External_memory_algorithm XGBoost 对 GBDT 实现的不同之处这部分内容主要参考了知乎上的一个问答 机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？ - 知乎 根据他们的总结和我自己对论文的理解和补充。 传统GBDT以CART作为基分类器，xgboost支持多种基础分类器。比如，线性分类器，这个时候xgboost相当于带 L1 和 L2正则化项 的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 可以通过booster [default=gbtree] 设置参数，详细参照官网 gbtree: tree-based models gblinear: linear models DART: Dropouts meet Multiple Additive Regression Trees dropout 在深度学习里面也经常使用，需要注意的是无论深度学习还是机器学习：使用droput训练出来的模型，预测的时候要使dropout失效。 传统GBDT在优化时只用到一阶导数信息，xgboost则对损失函数函数进行了二阶泰勒展开，同时用到了一阶和二阶导数，这样相对会精确地代表损失函数的值。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导，详细参照官网API。 并行处理，相比GBM有了速度的飞跃 借助 OpenMP ，自动利用单机CPU的多核进行并行计算 支持GPU加速 支持分布式 剪枝 当新增分裂带来负增益时，GBM会停止分裂（贪心策略，非全局的剪枝） XGBoost一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝（事后，进行全局剪枝） xgboost在代价函数里加入了显示的正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和，防止过拟合，这也是xgboost优于传统GBDT的一个特性。正则化的两个部分，都是为了防止过拟合，剪枝是都有的，叶子结点输出L2平滑是新增的。 Built-in Cross-Validation 内置交叉验证 XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.This is unlike GBM where we have to run a grid-search and only a limited values can be tested. XGBoost允许在每一轮boosting迭代中使用交叉验证，这样可以方便地获得最优boosting迭代次数 GBM使用网格搜索，只能检测有限个值 continue on Existing Model 可以保存模型下次接着训练，方便在线学习 User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications.GBM implementation of sklearn also has this feature so they are even on this point. High Flexibility 可定制损失函数，只要这个损失函数2阶可导 XGBoost allow users to define custom optimization objectives and evaluation criteria.This adds a whole new dimension to the model and there is no limit to what we can do. 提供多语言接口 命令行（Command Line Interface， CLI） C++/Python（可以和scikit-learn结合）/R（可以和caret包结合）/Julia/JAVA和JVM语言（如Scala、 Hadoop平台等） xgboost工具支持并行，执行速度确实比其他Gradient Boosting实现快 模型性能：在结构化数据集上，在分类／回归/排序预测建模上表现突出，相比之下，神经网络尤其擅长非结构化的数据集（比如：图片，语音） 注意xgboost不同于随机森林中的并行粒度是：tree，xgboost与其他提升方法（比如GBDT）一样，也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。 我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 总体来说，这部分内容需要学习很多，特别是涉及到分布式地并发优化和资源调度算法，这就不仅仅是数学模型的问题了，还涉及到系统设计，程序运行性能的优化，本人实在是才疏学浅，这部分内容理解尚浅，进一步学习还需要其他论文和看XGBoost源码，有些优化的地方也不是作者首创，表示从附录的论文中得以学习集成到XGBoost中，真的是集万千之大作，作者不愧是上海交大ACM班出身。大神的访谈：https://cosx.org/2015/06/interview-of-tianqi/ 优化的角度马琳同学的回答 非常棒，真是让我感受到了：横看成岭侧成峰 高可用的xgboost由于xgboost发展平稳成熟，现在已经非常易用，下图来自官网 hello world来自官网，其他复杂的demo，参看github的demo目录 Python 12345678910import xgboost as xgb# read in datadtrain = xgb.DMatrix('demo/data/agaricus.txt.train')dtest = xgb.DMatrix('demo/data/agaricus.txt.test')# specify parameters via mapparam = &#123;'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' &#125;num_round = 2bst = xgb.train(param, dtrain, num_round)# make predictionpreds = bst.predict(dtest) 在jupter notebook中运行结果 树形提升器1234import xgboost as xgb# read in datadtrain = xgb.DMatrix('demo/data/agaricus.txt.train')dtest = xgb.DMatrix('demo/data/agaricus.txt.test') [18:22:42] 6513x127 matrix with 143286 entries loaded from demo/data/agaricus.txt.train [18:22:42] 1611x127 matrix with 35442 entries loaded from demo/data/agaricus.txt.test 1234# specify parameters via mapparam = &#123;'max_depth':3, 'eta':1, 'silent': 0, 'objective':'binary:logistic' &#125;num_round = 2bst = xgb.train(param, dtrain, num_round) [18:22:42] C:\Users\Administrator\Desktop\xgboost\src\tree\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3 [18:22:42] C:\Users\Administrator\Desktop\xgboost\src\tree\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3 1234# make predictionpreds = bst.predict(dtest)print(preds)print(bst.eval(dtest)) [0.10828121 0.85500014 0.10828121 ... 0.95467216 0.04156424 0.95467216] [0] eval-error:0.000000 DART提升器 Dropouts meet Multiple Additive Regression Trees123456789101112param = &#123;'booster': 'dart', 'max_depth': 4, 'eta': 0.001, 'objective': 'binary:logistic', 'silent': 0, 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.5, 'skip_drop': 0.0&#125;#Command Line Parameters: 提升的轮次数num_round = 2bst = xgb.train(param, dtrain, num_round) 1234[18:22:42] C:\Users\Administrator\Desktop\xgboost\src\tree\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=4[18:22:42] C:\Users\Administrator\Desktop\xgboost\src\gbm\gbtree.cc:494: drop 0 trees, weight = 1[18:22:42] C:\Users\Administrator\Desktop\xgboost\src\tree\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=4[18:22:42] C:\Users\Administrator\Desktop\xgboost\src\gbm\gbtree.cc:494: drop 1 trees, weight = 0.999001 1234# make predictionpreds = bst.predict(dtest, ntree_limit=num_round)print(preds)print(bst.eval(dtest)) [0.4990105 0.5009742 0.4990105 ... 0.5009742 0.4990054 0.5009742] [0] eval-error:0.007449 参数详解官网，看懂参数的前提是把前文数学公式和理论看懂，这部分内容主要是对官网的翻译。 运行XGBoost之前，我们必须设置3种类型的参数：通用参数（general parameters），提升器参数（booster paramter），任务参数（task parameter）。 通用参数：与我们所使用的提升器（通常是树型提升器或者线性提升器）的提升算法相关。 提升器参数：取决于你所选择的哪种提升器 学习任务的参数：这些参数决定了学习的方案（learning scenario）。例如：在排名任务场景下，回归任务可能使用不同的参数。 命令行参数：与 XGBoost 的命令行接口（CLI）版本的行为相关。 Note Parameters in R package In R-package, you can use . (dot) to replace underscore(与underline同义) in the parameters, for example, you can use max.depth to indicate max_depth. The underscore parameters are also valid in R. General Parameters Parameters for Tree Booster Additional parameters for Dart Booster (booster=dart) Parameters for Linear Booster (booster=gblinear) Parameters for Tweedie Regression (objective=reg:tweedie) Learning Task Parameters Command Line Parameters 通用参数 general parameters booster [default=gbtree] 设定基础提升器的参数 Which booster to use. Can be gbtree, gblinear or dart; gbtree and dart use tree based models while gblinear uses linear functions. silent [default=0]: 设置成1则没有运行信息的输出，最好是设置为0. nthread [default to maximum number of threads available if not set]：线程数 disable_default_eval_metric [default=0] Flag to disable default metric. Set to &gt;0 to disable. ，使默认的模型评估器失效的标识 num_pbuffer [set automatically by XGBoost, no need to be set by user] Size of prediction buffer, normally set to number of training instances. The buffers are used to save the prediction results of last boosting step. num_feature [set automatically by XGBoost, no need to be set by user] Feature dimension used in boosting, set to maximum dimension of the feature 提升器参数 Booster parameters树提升器参数 Parameters for Tree Booster eta [default=0.3], range $[0, 1]$ shrinkage参数，用于更新叶子节点权重时，乘以该系数，避免步长过大。参数值越大，越可能无法收敛。把学习率 eta 设置的小一些，小学习率可以使得后面的学习更加仔细。 gamma [default=0 alias: min_split_loss] , range $[0, \infty]​$ 功能与min_split_loss 一样，（alias是“别名，又名”的意思，联想linux命令：alias就非常容易理解，即给相应的命令起了新的名字，引用同一个程序，功能是一样的），损失函数减少的最小量。 max_depth [default=6], range $[0, \infty]$ 每颗树的最大深度，树高越深，越容易过拟合。 min_child_weight [default=1], range: $[0, \infty]$ 这个参数默认是 1，是每个叶子里面loss函数二阶导（ hessian）的和至少是多少，对正负样本不均衡时的 0-1 分类而言，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 max_delta_step [default=0] , range: $[0, \infty]$ Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update. 这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。这个参数一般用不到，但是你可以挖掘出来它更多的用处。 subsample [default=1], range: $[0, 1]$ 训练实例的抽样率，较低的值使得算法更加保守，防止过拟合，但是太小的值也会造成欠拟合。如果设置0.5，那就意味着随机树的生长之前，随机抽取训练数据的50%做样本。 colsample_bytree [default=1], range: $[0, 1]$ 在构建每棵树的时候，特征（这里说是列，因为样本是按行存储的，那么列就是相应的特征）的采样率，用的特征进行列采样. colsample_bytree 表示的是每次分割节点时，抽取特征的比例。 lambda [default=1, alias: reg_lambda] 作用于权重值的 L2 正则化项参数，参数越大，模型越不容易过拟合。 alpha [default=0, alias: reg_alpha] 作用于权重值的 L1 正则项参数，参数值越大，模型越不容易过拟合。 tree_method string [default=auto] 用来设定树的构建算法，欲知详情请看陈天奇论文中的引用资料： reference paper. The tree construction algorithm used in XGBoost. See description in the reference paper. 分布式和外存版本仅仅支持 tree_method=approx Distributed and external memory version only support tree_method=approx. 选项：auto, exact, approx, hist, gpu_exact, gpu_hist, auto Choices: auto,exact,approx,hist,gpu_exact,gpu_hist,auto auto: Use heuristic to choose the fastest method. 启发式地选择快速算法 ​ - For small to medium dataset, exact greedy (exact) will be used. 中小数据量采用精确的贪婪搜索算法（指代前文说的树的生长过程中，节点分裂算法，所以很好理解） ​ - For very large dataset, approximate algorithm (approx) will be chosen. 非常大的数据集，近似算法将被选用。 ​ - Because old behavior is always use exact greedy in single machine, user will get a message when approximate algorithm is chosen to notify this choice. 因为旧的行为总是使用精确的贪婪算法，所以在近似算法被选用的时候，用户会收到一个通知消息，告诉用户近似算法被选用。 exact: Exact greedy algorithm. 精确地贪婪算法 approx: Approximate greedy algorithm using quantile sketch and gradient histogram. 近似算法采用分位方案和梯度直方图方案。 hist: Fast histogram optimized approximate greedy algorithm. It uses some performance improvements such as bins caching. 优化过的近似贪婪算法的快速算法，这个快速算法采用一些性能改善（的策略），例如桶的缓存（这里桶指的是直方图算法中所用的特征数据划分成不同的桶，欲知详情，查看陈天奇论文以及论文的引用资料） gpu_exact: GPU implementation of exact algorithm. gpu_hist: GPU implementation of hist algorithm. sketch_eps [default=0.03], range: (0, 1) 全称：sketch epsilon 即 分位算法中的 $\epsilon$ 参数 Only used for tree_method=approx. 仅仅用于近似算法 This roughly translates into O(1 / sketch_eps) number of bins. Compared to directly select number of bins, this comes with theoretical guarantee with sketch accuracy. 大致理解为桶数的倒数值。与直接给出桶数相比，这个与带权分位草案（Weighted Quantitle Sketch）能够保证理论上一致 Usually user does not have to tune this. But consider setting to a lower number for more accurate enumeration of split candidates. 通常情况下，不需要用户调试这个参数，但是考虑到设置一个更低的值能够枚举更精确的分割候选点。 scale_pos_weight [default=1] 正标签的权重缩放值 Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances). 控制样本正负标签的平衡，对于标签不平衡的样本有用，一个经典的值是：训练样本中具有负标签的实例数量/训练样本中正标签的实例数量。（举例：-1:2000个 +1:8000个，那么训练过程中每个正标签实例权重只有负标签实例的25%） See Parameters Tuning for more discussion. Also, see Higgs Kaggle competition demo for examples: R, py1, py2, py3. updater [default=grow_colmaker,prune] 逗号分割的字符串定义树的生成器和剪枝，注意这些生成器已经模块化，只要指定名字即可。 A comma separated string defining the sequence of tree updaters to run, providing a modular way to construct and to modify the trees. This is an advanced parameter that is usually set automatically, depending on some other parameters. However, it could be also set explicitly by a user. The following updater plugins exist: grow_colmaker: non-distributed column-based construction of trees. 单机版本下的基于列数据生长树，这里distributed tree 是xgboost有两种策略：单机版non-distributed和distributed分布式版本，比如单机版用的是精确贪婪的方式寻找分割数据点，分布式版本在采用的是近似直方图算法） distcol: distributed tree construction with column-based data splitting mode. 用基于列数据的分割模式来构建一个树（即：生长一棵树），且树是按照分布式版本的算法构建的。 grow_histmaker: distributed tree construction with row-based data splitting based on global proposal of histogram counting. 基于全局数据的直方图统计信息，并按照行分割的方式地进行树的生长。 grow_local_histmaker: based on local histogram counting. 基于局部数据（当前节点，非整棵树）的直方图统计 grow_skmaker: uses the approximate sketching algorithm. 使用近似草案算法。 sync: synchronizes trees in all distributed nodes. 在分布式地所有节点中同步树（的信息） refresh: refreshes tree’s statistics and/or leaf values based on the current data. Note that no random subsampling of data rows is performed. 刷新树的统计信息或者基于当前数据的叶子节点的值，注意：没有进行数据行的随机子抽样。 prune: prunes the splits where loss &lt; min_split_loss (or $\gamma$). 在当前节点小于被定义的最小分割损失时，那么进行剪枝。 In a distributed setting, the implicit updater sequence value would be adjusted to grow_histmaker,prune.在分布式环境下，这个参数值被显示地调整为grow_histmaker,prune refresh_leaf [default=1] This is a parameter of the refresh updater plugin. When this flag is 1, tree leafs as well as tree nodes’ stats are updated. When it is 0, only node stats are updated. 用来标记是否刷新叶子节点信息的标识。当这个标志位为0时，只有节点的统计信息被更新。 process_type [default=default] A type of boosting process to run. Choices:default,update default: The normal boosting process which creates new trees. update: Starts from an existing model and only updates its trees. In each boosting iteration, a tree from the initial model is taken, a specified sequence of updater plugins is run for that tree, and a modified tree is added to the new model. The new model would have either the same or smaller number of trees, depending on the number of boosting iteratons performed. Currently, the following built-in updater plugins could be meaningfully used with this process type: refresh, prune. With process_type=update, one cannot use updater plugins that create new trees. grow_policy [default=depthwise] 树的生长策略，基于深度或者基于最高损失变化 Controls a way new nodes are added to the tree. Currently supported only if tree_method is set to hist. Choices:depthwise, lossguide depthwise: split at nodes closest to the root. 按照离根节点最近的节点进行分裂 lossguide: split at nodes with highest loss change. max_leaves [default=0] 叶子节点的最大数目，只有当参数grow_policy=lossguide`才相关（起作用） Maximum number of nodes to be added. Only relevant when grow_policy=lossguide is set. max_bin, [default=256] 桶的最大数目 Only used if tree_method is set to hist.只有参数 tree_method=hist 时，这个参数才被使用。 Maximum number of discrete bins to bucket continuous features. 用来控制将连续特征离散化为多个直方图的直方图数目。 Increasing this number improves the optimality of splits at the cost of higher computation time. 增加此值提高了拆分的最优性, 但是是以更多的计算时间为代价的。 predictor , [default=cpu_predictor] 设定预测器算法的参数 The type of predictor algorithm to use. Provides the same results but allows the use of GPU or CPU. cpu_predictor: Multicore CPU prediction algorithm. 多核cpu预测器算法 gpu_predictor: Prediction using GPU. Default when tree_method is gpu_exact or gpu_hist. GPU预测器算法，当参数 tree_method = gpu_exact or gpu_hist 时，预测器算法默认采用 gpu_predictor 。 Additional parameters for Dart Booster (booster=dart)此部分可参考：原始论文 和 DART介绍 Note 在测试集上预测的时候，必须通过参数 ntree_limits 要关闭掉dropout功能 Using predict() with DART booster If the booster object is DART type, predict() will perform dropouts, i.e. only some of the trees will be evaluated. This will produce incorrect results if data is not the training data. To obtain correct results on test sets, set ntree_limit to a nonzero value, e.g. 12&gt;preds = bst.predict(dtest, ntree_limit=num_round)&gt; sample_type [default=uniform] 设定抽样算法的类型 Type of sampling algorithm. uniform: dropped trees are selected uniformly. 所有的树被统一处理，指的是权重一样，同样的几率被选为辍学树（被选为辍学的树，即不参与训练的学习过程） weighted: dropped trees are selected in proportion to weight. 选择辍学树的时候是正比于权重。 normalize_type [default=tree] 归一化（又名：标准化）算法的的类型，这个地方是与深度学习中的dropout不太一样。 Type of normalization algorithm. tree: new trees have the same weight of each of dropped trees. 新树拥有跟每一颗辍学树一样的权重 Weight of new trees are 1 / (k + learning_rate). Dropped trees are scaled by a factor of k / (k + learning_rate). forest: new trees have the same weight of sum of dropped trees (forest).新树的权重等于所有辍学树的权重总和 Weight of new trees are 1 / (1 + learning_rate). Dropped trees are scaled by a factor of 1 / (1 + learning_rate). rate_drop [default=0.0], range: [0.0, 1.0] 辍学率，与深度学习中的一样意思 Dropout rate (a fraction of previous trees to drop during the dropout). one_drop [default=0] 设置是否在选择辍学的过程中，至少一棵树被选为辍学树。 When this flag is enabled, at least one tree is always dropped during the dropout (allows Binomial-plus-one or epsilon-dropout from the original DART paper). skip_drop [default=0.0], range: [0.0, 1.0] 在提升迭代的过程中，跳过辍学过程的概率，即不执行dropout功能的概率 Probability of skipping the dropout procedure during a boosting iteration. If a dropout is skipped, new trees are added in the same manner as gbtree. Note that non-zero skip_drop has higher priority than rate_drop or one_drop. 注意到非0值得skip_drop参数比rate_drop和one_drop参数拥有更高的优先级。 学习任务的参数 Learning Task ParametersSpecify the learning task and the corresponding learning objective. The objective options are below: objective[default=reg:linear] 这个参数定义需要被最小化的损失函数 reg:linear: linear regression reg:logistic: logistic regression binary:logistic: logistic regression for binary classification, output probability binary:logitraw: logistic regression for binary classification, output score before logistic transformation binary:hinge: hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities. 2分类的链式损失 gpu:reg:linear, gpu:reg:logistic, gpu:binary:logistic, gpu:binary:logitraw: versions of the corresponding objective functions evaluated on the GPU; note that like the GPU histogram algorithm, they can only be used when the entire training session uses the same dataset count:poisson –poisson regression for count data, output mean of poisson distribution max_delta_step is set to 0.7 by default in poisson regression (used to safeguard optimization) survival:cox: Cox regression for right censored survival time data (negative values are considered right censored). Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function h(t) = h0(t) * HR). 比例风险回归模型(proportional hazards model，简称Cox模型)” 这块不太懂 multi:softmax: set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes) 多分类输出one-hot向量 multi:softprob: same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata * nclass matrix. The result contains predicted probability of each data point belonging to each class. 多分类输出各个类的概率向量 rank:pairwise: Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized rank:ndcg: Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized rank:map: Use LambdaMART to perform list-wise ranking where Mean Average Precision (MAP) is maximized reg:gamma: gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed. reg:tweedie: Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed. base_score [default=0.5] The initial prediction score of all instances, global bias For sufficient number of iterations, changing this value will not have too much effect. eval_metric [default according to objective] 对于有效数据的度量方法 Evaluation metrics for validation data, a default metric will be assigned according to objective (rmse for regression, and error for classification, mean average precision for ranking) User can add multiple evaluation metrics. Python users: remember to pass the metrics in as list of parameters pairs instead of map, so that latter eval_metric won’t override previous one The choices are listed below: rmse: root mean square error 均方根误差 mae: mean absolute error 平均绝对误差 logloss: negative log-likelihood 负对数似然函数值 error: Binary classification error rate. It is calculated as #(wrong cases)/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances. 二分类错误率(阈值为0.5) error@t: a different than 0.5 binary classification threshold value could be specified by providing a numerical value through ‘t’指定2分类误差率的阈值t merror: Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases). 多分类错误率 mlogloss: Multiclass logloss. 多分类的负对数似然函数值 auc: Area under the curve 曲线下面积 aucpr: Area under the PR curve 准确率和召回率曲线下的面积 ndcg: Normalized Discounted Cumulative Gain map: Mean Average Precision 主集合的平均准确率(MAP)是每个主题的平均准确率的平均值 ndcg@n, map@n: ‘n’ can be assigned as an integer to cut off the top positions in the lists for evaluation. ndcg-, map-, ndcg@n-, map@n-: In XGBoost, NDCG and MAP will evaluate the score of a list without any positive samples as 1. By adding “-” in the evaluation metric XGBoost will evaluate these score as 0 to be consistent under some conditions. poisson-nloglik: negative log-likelihood for Poisson regression gamma-nloglik: negative log-likelihood for gamma regression cox-nloglik: negative partial log-likelihood for Cox proportional hazards regression gamma-deviance: residual deviance for gamma regression tweedie-nloglik: negative log-likelihood for Tweedie regression (at a specified value of the tweedie_variance_power parameter) seed [default=0] 随机数的种子 Random number seed. 设置它可以复现随机数据的结果，也可以用于调整参数 命令行参数 Command Line ParametersThe following parameters are only used in the console version of XGBoost num_round The number of rounds for boosting data The path of training data test:data The path of test data to do prediction save_period [default=0] The period to save the model. Setting save_period=10 means that for every 10 rounds XGBoost will save the model. Setting it to 0 means not saving any model during the training. task [default=train] options:train,pred,eval,dump train: training using data pred: making prediction for test:data eval: for evaluating statistics specified by eval[name]=filename dump: for dump the learned model into text format model_in [default=NULL] Path to input model, needed for test, eval, dump tasks. If it is specified in training, XGBoost will continue training from the input model. model_out [default=NULL] Path to output model after training finishes. If not specified, XGBoost will output files with such names as 0003.model where 0003 is number of boosting rounds. model_dir [default=models/] The output directory of the saved models during training fmap Feature map, used for dumping model dump_format [default=text] options:text, json Format of model dump file name_dump [default=dump.txt] Name of model dump file name_pred [default=pred.txt] Name of prediction file, used in pred mode pred_margin [default=0] Predict margin instead of transformed probabilityXGBoost GPU SupportXGBoost Python Package 调参调参主要参考 Complete Guide to Parameter Tuning in XGBoost (with codes in Python) ，有空再详细说明。 https://www.cnblogs.com/infaraway/p/7890558.html 引用 陈天奇的论文 XGBoost: A Scalable Tree Boosting System 陈天奇的演讲视频 XGBoost A Scalable Tree Boosting System June 02, 2016 演讲幻灯片 和 官网幻灯片 XGBoost 官网 XGBoost的贡献者之一的 演讲 机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？ - 知乎]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《统计学习方法总结》]]></title>
    <url>%2F2018%2F10%2F01%2Fsummary_of_LiHang_Statistical-learning-methods%2F</url>
    <content type="text"><![CDATA[这是个人学习完李航 《统计学习方法》 的总结笔记，之前是在纸质版书籍上做的笔记，接下来会陆续更新每一章总结，由于时间有限，将以PDF注解的方式梳理10个主要统计学习方法。那么注意勘误(errata)，首先将引入作者自己的总结，而后，我再对每个具体算法和关联性进行总结，逐渐形成系统性的理解，算法之间的分类，联系，优缺点对比，以及适用场景才是我们学习的重点。 总结 1 适用问题分类问题是从实例的特征向量到类标记的预测问题；标注问题是从观测序列到标记序列(或状态序列)的预测问题。可以认为分类问题是标注问题的特殊情况。 分类问题中可能的预测结果是二类或多类；而标注问题中可能的预测结果是所有的标记序列，其数目是指数级的。 感知机、k近邻法、朴素贝叶斯法、决策树是简单的分类方法，具有模型直观、方法简单、实现容易等特点； 逻辑斯谛回归与最大熵模型、支持向量机、提升方法是更复杂但更有效的分类方法，往往分类准确率更高； 隐马尔可夫模型、条件随机场是主要的标注方法。通常条件随机场的标注准确率更高。 2 模型分类问题与标注问题的预测模型都可以认为是表示从输入空间到输出空间的映射.它们可以写成条件概率分布 $P(Y|X)$ 或决策函数 $Y=f(X)$ 的形式。前者表示给定输入条件下输出的概率模型，后者表示输入到输出的非概率模型。有的模型只是其中一种，有的模型可以看成2者兼有。 朴素贝叶斯法、隐马尔可夫模型是概率模型；感知机、k近邻法、支持向量机、提升方法是非概率模型；而决策树、逻辑斯谛回归与最大熵模型、条件随机场既可以看作是概率模型，又可以看作是非概率模型。 直接学习条件概率分布 $P(Y | X)$ 或决策函数 $Y=f(X)$ 的方法为判别方法，对应的模型是判别模型：感知机、k近邻法、决策树、逻辑斯谛回归与最大熵模型、支持向量机、提升方法、条件随机场是判别方法。 首先学习联合概率分布 $P(X,Y)$，从而求得条件概率分布 $P(Y|X)$ 的方法是生成方法，对应的模型是生成模型：朴素贝叶斯法、隐马尔可夫模型是生成方法。 决策树是定义在一般的特征空间上的，可以含有连续变量或离散变量。感知机、支持向量机、k近邻法的特征空间是欧氏空间(更一般地，是希尔伯特空间)。提升方法的模型是弱分类器的线性组合，弱分类器的特征空间就是提升方法模型的特征空间。 感知机模型是线性模型；而逻辑斯谛回归与最大熵模型、条件随机场是对数线性模型；k近邻法、决策树、支持向量机(包含核函数)、提升方法使用的是非线性模型。 3 学习策略在二类分类的监督学习中，支持向量机、逻辑斯谛回归与最大熵模型、提升方法各自使用合页损失函数、逻辑斯谛损失函数、指数损失函数，分别写为 这3种损失函数都是0-1损失函数的上界，具有相似的形状。 从上图可以认为支持向量机、逻辑斯谛回归与最大熵模型、提升方法使用不同的代理损失函数(surrogateloas Punotion)表示分类的损失，定义经验风险或结构风险函数，实现二类分类学习任务。学习的策略是优化以下结构风险函数 第1项为经验风险(经验损失)，第2项为正则化项，L为损失函数，J(f)为模型的复杂度。 支持向量机用L2范数表示模型的复杂度。原始的逻辑斯谛回归与最大熵模型没有正则化项，可以给它们加上L2范数正则化项。提升方法没有显式的正则化项，通常通过早停止(early stopping)的方法达到正则化的效果。 概率模型的学习可以形式化为极大似然估计或贝叶斯估计的极大后验概率估计。学习的策略是极小化对数似然损失或极小化正则化的对数似然损失。极大后验概率估计时，正则化项是先验概率的负对数。 决策树学习的策略是正则化的极大似然估计，损失函数是对数似然损失，正则化项是决策树的复杂度。 逻辑斯谛回归与最大熵模型、条件随机场的学习策略既可以看成是极大似然估计(或正则化的极大似然估计)，又可以看成是极小化逻辑斯谛损失(或正则化的逻辑斯谛损失)。 朴素贝叶斯模型、隐马尔可夫模型的非监督学习也是极大似然估计或极大后验概率估计，但这时模型含有隐变量。 4 学习算法 朴素贝叶斯法与隐马尔可夫模型的监督学习，最优解即极大似然估计值，可以由概率计算公式直接计算。 感知机、逻辑斯谛回归与最大熵模型、条件随机场的学习利用梯度下降法、拟牛顿法等一般的无约束最优化问题的解法。 支持向量机学习，可以解凸二次规划的对偶问题。有序列最小最优化算法等方法。 决策树学习是基于启发式算法的典型例子。可以认为特征选择、生成、剪枝是启发式地进行正则化的极大似然估计。 提升方法利用学习的模型是加法模型、损失函数是指数损失函数的特点，启发式地从前向后逐步学习模型，以达到逼近优化目标函数的目的。 EM算法是一种迭代的求解含隐变量概率模型参数的方法，它的收敛性可以保证，但是不能保证收敛到全局最优。 支持向量机学习、逻辑斯谛回归与最大熵模型学习、条件随机场学习是凸优化问题，全局最优解保证存在。而其他学习问题则不是凸优化问题。 分章节总结数学基础不再重复，请参考我的数学笔记线性代数总结 和 MIT的概率论教材 《introduction to probability》 和 凸优化 第8章 提升方法之AdaBoost\BoostingTree\GBDT MaxEnt HMM CRF 第9章EM/GMM/F-MM/GEM SVM 第4章 朴素贝叶斯]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《统计学习方法》第8章 提升方法之AdaBoost\BoostingTree\GBDT]]></title>
    <url>%2F2018%2F10%2F01%2F8.Booting-Methods_LiHang-Statistical-Learning-Methods%2F</url>
    <content type="text"><![CDATA[前言在深度学习火起来之前，提升方法（包括AdaBoost, GBDT, XGBoost）是 kaggle 等比赛中的利器，所以提升方法 （boosting） 是必备的知识点。李航《统计学习方法》第8章——提升方法主要内容：AdaBoost, Boosting Tree, GBDT(这一块原文不够详细，将补充一些)。写本文主要目的是复习（毕竟之前看纸质版做的笔记）， 对于证明比较跳跃和勘误的地方我都做了注解，以便初学者快速阅读理解不会卡住，另外本文拓展部分补充了：集成学习。另外李航这本书著作于2012年，陈天奇 的 XGBoost (eXtreme Gradient Boosting) 在2015年惊艳四方，本文暂时不叙述这个，将会另开一篇：XGBoost入门，包括微软的LightGBM，但是本文是 XGBoost 和 LightGBM 的基础，所以要先总结本文。初学者看到前言这么多名词不必畏惧，直接看正文，本人也是从本章正文学习，再拓展出这些的，初学直接看正文，一步一步往下顺。 正文提升（boosting） 方法是一种常用的统计学习方法， 应用广泛且 有效。 在分类问题中， 它通过改变训练样本的权重， 学习多个分类 器， 并将这些分类器进行线性组合， 提高分类的性能。 本章主要内容 提升方法的思路和代表性的提升算法AdaBoost； 通过训练误差分析探讨AdaBoost为什么能够提高学习精度； 并且从 前向分步加法模型的角度解释AdaBoost； 然后叙述提升方法更具体的 实例——提升树（boosting tree）和GBDT 。 8.1 提升方法AdaBoost算法8.1.1 提升方法的基本思路提升方法的思想对于一个复杂任务来说， 将多个专 家的判断进行适当的综合所得出的判断， 要比其中任何一个专家单独 的判断好。 实际上， 就是“三个臭皮匠顶个诸葛亮”的道理 历史背景历史上， Kearns和Valiant首先提出了“强可学习（strongly learnable） ”和“弱可学习（weakly learnable） ”的概念。 指出： 在概率近似正确（probably approximately correct， PAC） 学习的框架中， 一 个概念（一个类） ， 如果存在一个多项式的学习算法能够学习它， 并 且正确率很高， 那么就称这个概念是强可学习的； 一个概念， 如果存 在一个多项式的学习算法能够学习它， 学习的正确率仅比随机猜测略 好， 那么就称这个概念是弱可学习的。 非常有趣的是Schapire后来证 明强可学习与弱可学习是等价的， 也就是说， 在PAC学习的框架下， 一个概念是强可学习的充分必要条件是这个概念是弱可学习的。 这样一来， 问题便成为， 在学习中， 如果已经发现了“弱学习算 法”， 那么能否将它提升（boost） 为“强学习算法”。 大家知道， 发现 弱学习算法通常要比发现强学习算法容易得多。 那么如何具体实施提 升， 便成为开发提升方法时所要解决的问题。 关于提升方法的研究很 多， 有很多算法被提出。 最具代表性的是AdaBoost算法（AdaBoost algorithm） 。 对于分类问题而言， 给定一个训练样本集， 求比较粗糙的分类规 则（弱分类器） 要比求精确的分类规则（强分类器） 容易得多。提升方法就是从弱学习算法出发， 反复学习， 得到一系列弱分类器（又称 为基本分类器） ， 然后组合这些弱分类器， 构成一个强分类器。 大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分 布） ， 针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。 提升方法的核心问题和思想对提升方法来说， 有两个问题需要回答： 一是在每一轮如 何改变训练数据的权值或概率分布； 二是如何将弱分类器组合成一个强分类器。 关于第1个问题， AdaBoost的做法是提高那些被前一轮弱分类器错误分类样本的权值， 而降低那些被正确分类样本的权值。 这样一来， 那些没有得到正确分类的数据， 由于其权值的加大而受到后一轮的弱分类器的更大关注。 于是， 分类问题被一系列的弱分类器“分而治之”。 至于第2个问题， 即弱分类器的组合， AdaBoost采取加权多数表决的方法。 具体地， 加大分类误差率小的弱分类器的权值， 使其在表决中起较大的作用， 减小分类误差率大的弱分类器的权值， 使其在表决中起较小的作用。 AdaBoost的巧妙之处就在于它将这些想法自然且有效地实现在一 种算法里。 8.1.2 AdaBoost算法 8.1.3 AdaBoost的例子 8.2 AdaBoost算法的训练误差分析 8.3 AdaBoost算法的解释 8.3.1 前向分步算法 8.3.2 前向分步算法与AdaBoost 8.4 提升树 8.4.1 提升树模型 8.4.2 提升树算法 8.4.3 梯度提升Gradient Boosting有前面的例子非常容易理解这部分内容，不再赘述， GBDT如果看懂前文，那么理解GBDT就简单很多。有一篇博客，清晰易懂，总结很不错，推荐看一下： 梯度提升树(GBDT)原理小结 ，里面的这有一个提示：关于多分类问题：每轮都在拟合概率向量 [类别1概率，类别2概率…,类别k的概率] 的伪残差。 以下摘录自：大名鼎鼎的 Stanford 统计教材 《The Elements of Statistical Learning》。根据《李航统计学习方法》的提升方法这一章的参考附录正是此书，对比之下，异曲同工，这里不再赘述，快速总结一下。 仔细体会这句话：它和梯度下降十分相似，不同在于GBDT 就是在函数空间的“梯度下降”，以前学习ML和DL的梯度下降是在参数空间的梯度下降。 在梯度下降中，不断减去 $\frac{\partial{f(x)}}{\partial{\theta}}$，希望求得 $min_{\theta}f(x)$ ; 同理梯度提升中不断减去$\frac{\partial{L(y,f(x))}}{f(x)}$，希望求得 $min_{f(x)}L(y, f(x))$ 。这里引用 https://wepon.me 的介绍： 注意：《统计学习方法》这一章介绍的都是最原始的提升树算法，实际上有进一步改进的调整： 步进参数 $\nu$ ，专业名词为shrinkage，又称收缩，类似梯度下降算法的学习率，这里为什么称为步进参数，因为前文提到：提升方法是加法模型的前向分布算法，提升树也属于提升方法中一种（基学习器是决策树）。 例如：运用在GBDT中 $f_m(x)=f_{m-1}(x)+\nu\cdot \sum\limits_{j=1}^{J}\gamma_{jm}I(x\in R_{jm})$ 正则化手段：subsampling 子抽样（包括：样本子抽样和特征的子抽样），提高泛化能力。 例如：在随机森林中，每棵树的训练样本都是对原始训练集有放回的子抽样，好处如下： 如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，依然没有解决决策树过拟合问题。随机抽样是为了保证不同决策树之间的多样性，从而提高模型的泛化能力。使得随机森林不容易陷入过拟合，并且具有较好的抗噪能力（比如：对缺省值不敏感）。 而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是”求同”。如果是无放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是”有偏的”，从而影响最终的投票结果。为了保证最终结果的可靠性，同时又要保证模型的泛化能力，需要每一颗树既要“求同“ 又要 ”存异”。 深入理解请看GDBT的论文：Greedy Function Approximation：A Gradient Boosting Machine 拓展—集成学习实际上，提升方法（boosting） 属于集成学习（ensemble learning） 的一种，集成模型还有2类比较出名：bagging 方法（Bagging 由来：Bootstrap aggregating）和 Stacking方法，随机森林属于第二类。由于随机森林不像提升树不用等待上一轮（上一棵树）结果，各棵树都可以独立求解（包括独立进行子抽样），即可以在树这个粒度下并发进行运算求解，在大规模数据下并发性能良好，随机森林比较简单。后来 XGBoost 和 lightGBM 的良好地实现了算法模型GBDT，虽然依然不是在树粒度层面的并发，但是运行时间和效果在kaggle、KDD Cup等比赛中惊艳四方，将会在日后另一篇中总结xgboost。 进一步学习的经典资料 wiki关于集成模型的介绍 https://en.wikipedia.org/wiki/Ensemble_learning 周志华老师：《Ensemble Methods Foundations and Algorithms》 用 Stanford 的统计教材 The Elements of Statistical Learning 进一步学习与补充 第9章 Additive Models, Trees, and Related Methods 第10章 Boosting and Additive Trees 第14 章 Random Forests 第15 章 Ensemble learning 知乎探讨：为什么说bagging是减少variance，而boosting是减少bias? - 知乎 https://www.zhihu.com/question/26760839 为什么在实际的kaggle比赛中，GBDT和Random Forest效果非常好以下来自马超博士的回答 - 知乎 这是一个非常好，也非常值得思考的问题。换一个方式来问这个问题：为什么基于 tree-ensemble 的机器学习方法，在实际的 kaggle 比赛中效果非常好？ 通常，解释一个机器学习模型的表现是一件很复杂事情，而这篇文章尽可能用最直观的方式来解释这一问题。 我主要从三个方面来回答楼主这个问题。 理论模型 （站在 vc-dimension 的角度） 实际数据 系统的实现 （主要基于 xgboost） 通常决定一个机器学习模型能不能取得好的效果，以上三个方面的因素缺一不可。 站在理论模型的角度统计机器学习里经典的 vc-dimension 理论告诉我们：一个机器学习模型想要取得好的效果，这个模型需要满足以下两个条件： 模型在我们的训练数据上的表现要不错，也就是 trainning error 要足够小。 模型的 vc-dimension 要低。换句话说，就是模型的自由度不能太大，以防overfit. 当然，这是我用大白话描述出来的，真正的 vc-dimension 理论需要经过复杂的数学推导，推出 vc-bound. vc-dimension 理论其实是从另一个角度刻画了一个我们所熟知的概念，那就是 bias variance trade-off. 好，现在开始让我们想象一个机器学习任务。对于这个任务，一定会有一个 “上帝函数” 可以完美的拟合所有数据（包括训练数据，以及未知的测试数据）。很可惜，这个函数我们肯定是不知道的 （不然就不需要机器学习了）。我们只可能选择一个 “假想函数” 来 逼近 这个 “上帝函数”，我们通常把这个 “假想函数” 叫做 hypothesis. 在这些 hypothesis 里，我们可以选择 svm, 也可以选择 logistic regression. 可以选择单棵决策树，也可以选择 tree-ensemble (gbdt, random forest). 现在的问题就是，为什么 tree-ensemble 在实际中的效果很好呢？ 区别就在于 “模型的可控性”。 先说结论，tree-ensemble 这样的模型的可控性是好的，而像 LR 这样的模型的可控性是不够好的（或者说，可控性是没有 tree-ensemble 好的）。为什么会这样？别急，听我慢慢道来。 我们之前说，当我们选择一个 hypothsis 后，就需要在训练数据上进行训练，从而逼近我们的 “上帝函数”。我们都知道，对于 LR 这样的模型。如果 underfit，我们可以通过加 feature，或者通过高次的特征转换来使得我们的模型在训练数据上取得足够高的正确率。而对于 tree-enseble 来说，我们解决这一问题的方法是通过训练更多的 “弱弱” 的 tree. 所以，这两类模型都可以把 training error 做的足够低，也就是说模型的表达能力都是足够的。但是这样就完事了吗？没有，我们还需要让我们的模型的 vc-dimension 低一些。而这里，重点来了。在 tree-ensemble 模型中，通过加 tree 的方式，对于模型的 vc-dimension 的改变是比较小的。而在 LR 中，初始的维数设定，或者说特征的高次转换对于 vc-dimension 的影响都是更大的。换句话说，tree-ensemble 总是用一些 “弱弱” 的树联合起来去逼近 “上帝函数”，一次一小步，总能拟合的比较好。而对于 LR 这样的模型，我们很难去猜到这个“上帝函数”到底长什么样子（到底是2次函数还是3次函数？上帝函数如果是介于2次和3次之间怎么办呢？）。所以，一不小心我们设定的多项式维数高了，模型就 “刹不住车了”。俗话说的好，步子大了，总会扯着蛋。这也就是我们之前说的，tree-ensemble 模型的可控性更好，也即更不容易 overfit. 站在数据的角度除了理论模型之外, 实际的数据也对我们的算法最终能取得好的效果息息相关。kaggle 比赛选择的都是真实世界中的问题。所以数据多多少少都是有噪音的。而基于树的算法通常抗噪能力更强。比如在树模型中，我们很容易对缺失值进行处理。除此之外，基于树的模型对于 categorical feature 也更加友好。 除了数据噪音之外，feature 的多样性也是 tree-ensemble 模型能够取得更好效果的原因之一。通常在一个kaggle任务中，我们可能有年龄特征，收入特征，性别特征等等从不同 channel 获得的特征。而特征的多样性也正是为什么工业界很少去使用 svm 的一个重要原因之一，因为 svm 本质上是属于一个几何模型，这个模型需要去定义 instance 之间的 kernel 或者 similarity （对于linear svm 来说，这个similarity 就是内积）。这其实和我们在之前说过的问题是相似的，我们无法预先设定一个很好的similarity。这样的数学模型使得 svm 更适合去处理 “同性质”的特征，例如图像特征提取中的 lbp 。而从不同 channel 中来的 feature 则更适合 tree-based model, 这些模型对数据的 distributation 通常并不敏感。 站在系统实现的角度除了有合适的模型和数据，一个良好的机器学习系统实现往往也是算法最终能否取得好的效果的关键。一个好的机器学习系统实现应该具备以下特征： 正确高效的实现某种模型。我真的见过有些机器学习的库实现某种算法是错误的。而高效的实现意味着可以快速验证不同的模型和参数。 系统具有灵活、深度的定制功能。 系统简单易用。 系统具有可扩展性, 可以从容处理更大的数据。 到目前为止，xgboost 是我发现的唯一一个能够很好的满足上述所有要求的 machine learning package. 在此感谢青年才俊 陈天奇。 在效率方面，xgboost 高效的 c++ 实现能够通常能够比其它机器学习库更快的完成训练任务。在灵活性方面，xgboost 可以深度定制每一个子分类器，并且可以灵活的选择 loss function（logistic，linear，softmax 等等）。除此之外，xgboost还提供了一系列在机器学习比赛中十分有用的功能，例如 early-stop， cv 等等在易用性方面，xgboost 提供了各种语言的封装，使得不同语言的用户都可以使用这个优秀的系统。最后，在可扩展性方面，xgboost 提供了分布式训练（底层采用 rabit 接口），并且其分布式版本可以跑在各种平台之上，例如 mpi, yarn, spark 等等。 有了这么多优秀的特性，自然这个系统会吸引更多的人去使用它来参加 kaggle 比赛。 综上所述，理论模型，实际的数据，良好的系统实现，都是使得 tree-ensemble 在实际的 kaggle 比赛中“屡战屡胜”的原因。 用一句话与大家共勉：算法学习要学习算法之间的联系与区别，优缺点和适用场合，这样能做到融会贯通。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《统计学习方法》第9章 EM/GMM/F-MM/GEM]]></title>
    <url>%2F2018%2F10%2F01%2F9.EM_and_GEM_LiHang-Statistical-Learning-Methods%2F</url>
    <content type="text"><![CDATA[前言EM（期望最大）算法有很多的应用，最广泛的就是混合高斯模型、聚类、HMM等等，本质上就是一种优化算法，不断迭代，获得优值，与梯度下降、牛顿法、共轭梯度法都起到同一类的作用。 本文是对李航《统计学习方法》的第9章复习总结，主要内容如下 EM（期望最大）算法证明有跳跃性的地方全部事无巨细地写出来， 在 三硬币例子解析 这一节将会把这个例子跟公式一一对应起来 GMM（高斯混合模型）迭代公式证明 F函数的极大-极大算法（Maximization-Maximization-algorithm）和GEM 详细证明 当然大家也可以参考 Stanford 吴恩达主讲的 CS299 Machine Learning 的 EM课件 ，相比之下《统计学习方法》这本书在 Jensen‘s inequality（琴声不等式）讲的不够详细，其他都差不多，只是Q函数定义不同，这两种定义都很流行所以后文也会介绍区别。 正文9.1 EM算法的引入概率模型有时既含有观测变量（observable variable） ， 又含有隐变量（hidden variable）或潜在变量（latent variable） 。 如果概率模型的变量都是观测变量， 那么给定数据， 可以直接用极大似然估计法或贝叶斯估计法估计模型参数。 但是， 当模型含有隐变量时， 就不能简单地使用这些估计方法。 EM算法就是含有隐变量的概率模型参数的极大似然估计法， 或极大后验概率估计法。 我们仅讨论极大似然估计， 极大后验概率估计与其类似。 9.1.1 EM算法 这里， 随机变量 $y$ 是观测变量， 表示一次试验观测的结果是1或0； 随机变量 $z$ 是隐变量， 表示未观测到的掷硬币 $A$ 的结果； $\theta＝( \pi ,p， q)$ 是模型参数。 这一模型是以上数据的生成模型。 注意， 随机变量 $y$ 的数据可以观测， 随机变量 $z$ 的数据不可观测。$$\begin{align}P(y|\theta) &amp;= \sum\limits_{z}P(y,z|\theta)=\sum\limits_{z}\frac{P(z,\theta)}{P(\theta)}\cdot\frac{P(y,z,\theta)}{P(z, \theta)}=\sum\limits_{z}P(z|\theta)P(y|z,\theta) \\&amp;= P(z=1|\theta)P(y|z=1, \theta) + P(z=0|\theta)P(y|z=0, \theta)\\&amp;= \pi p^y(1-p)^{(1-y)} + (1 - \pi) q^y(1-q)^{(1-y)} \tag{9.1}\\&amp;= \begin{cases} \pi p + (1 - \pi) q, &amp;y=1\\ \pi (1-p) + (1-\pi)(1-q), &amp;y=0\end{cases}\end{align}$$将观测数据表示为 $Y＝(Y_1， Y_2,…,Y_n)^T$， 未观测数据表示为 $Z＝(Z_1,Z_2,…,Z_n)^T$， 则观测数据的似然函数为$$P(Y|\theta) = \sum\limits_{Z}P(Y,Z|\theta)=\sum\limits_{Z}P(Z|\theta)P(Y|Z,\theta) \tag{9.2}$$即：$$P(Y|\theta)= \prod_{j=1}^{n}\left\{\pi p^{y_j}(1-p)^{(1-y_j)} + (1 - \pi) q^{y_j}(1-q)^{(1-y_j)}\right\} \tag{9.3}$$考虑求模型参数 $\theta =(\pi, p, q) $ 的极大似然估计，即：$$\begin{align}\hat{\theta}&amp;=\mathop{\arg\max}_{\theta} \mathrm{log}P(Y|\theta) \\&amp;= \mathop{\arg\max}_{\theta}\log\prod_{j=1}^{n}P(Y|\theta) \Leftarrow\text{n次抛硬币试验都是独立} \\&amp;= \mathop{\arg\max}_{\theta}\sum\limits_{j=1}^{n}\log P(Y|\theta) \\&amp;= \mathop{\arg\max}_{\theta}\sum\limits_{j=1}^{n}\log\left\{\sum\limits_{Z}{P(Z|\theta)P(Y|Z,\theta)}\right\} \tag{9-3}\end{align}$$问题：这里为什么要取对数？ 取对数之后累积变为累和，求导更加方便（后面三硬币例子解析将会看到） 概率累积会出现数值非常小的情况，比如1e-30，由于计算机的精度是有限的，无法识别这一类数据，取对数之后，更易于计算机的识别(1e-30以10为底取对数后便得到-30)。 这个问题没有解析解，因为隐变量数据无法获得，只有通过迭代的方法求解。 EM算法就是可以用于求解这个问题的一种迭代算法。 一般地， 用 $Y$ 表示观测随机变量的数据， $Z$ 表示隐随机变量的数据。 $Y$ 和 $Z$ 连在一起称为完全数据（complete-data） ， 观测数据 $Y$ 又称为不完全数据（incomplete-data） 。 假设给定观测数据 $Y$， 其概率分布是 $P(Y|\theta)$， 其中是需要估计的模型参数， 那么不完全数据 $Y$ 的似然函数是 $P(Y|\theta)$， 对数似然函数 $L(\theta)＝\mathrm{log}P(Y|\theta)$ ； 假设 $Y$ 和 $Z$ 的联合概率分布是 $P(Y, Z|\theta)$， 那么完全数据的对数似然函数是 $\mathrm{log}P(Y, Z|\theta)$。 9.1.2 EM算法的导出 注：书上给出琴声不等式（$\ln\sum_j\lambda_jy_j\geq \sum_j\lambda_j\log y_j,\quad \lambda_j\ge 0,\sum_j\lambda_j=1$），自行维基百科一下了解详情。最后一步源自于 $Z$ 所有可能取值的概率和为1$$\mathrm{log}P(Y|\theta^{(i)})=\mathrm{log}P(Y|\theta^{(i)}) \cdot \sum\limits_{Z}P(Z|Y, \theta^{(i)})$$$$\begin{align}\theta^{(i+1)} &amp;= \mathop{\arg\max}_{\theta} \left\{ L(\theta^{(i)}) + \sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}\right\} \\&amp;= \mathop{\arg\max}_{\theta}\left\{ \mathrm{log}P(Y|\theta^{(i)})\sum\limits_{Z}P(Z|Y, \theta^{(i)}) + \sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})} \right\} \\\end{align}$$加号右边，利用对数函数的性质得到：$$\begin{align}&amp;\sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})} \\&amp;=\sum\limits_{Z}P(Z|Y,\theta^{(i)})\left\{\mathrm{log}[P(Y|Z,\theta)P(Z|\theta)] - \mathrm{log}[P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})]\right\} \\&amp;=\sum\limits_{Z}P(Z|Y,\theta^{(i)})\left\{\mathrm{log}[P(Y|Z,\theta)P(Z|\theta)] - \mathrm{log}P(Z|Y,\theta^{(i)})-\mathrm{log}P(Y|\theta^{(i)})\right\} \\&amp;= \sum\limits_{Z}P(Z|Y,\theta^{(i)})\mathrm{log}[P(Y|Z,\theta)P(Z|\theta)] - \sum\limits_{Z}P(Z|Y,\theta^{(i)})\mathrm{log}P(Z|Y,\theta^{(i)})-\sum\limits_{Z}P(Z|Y,\theta^{(i)})\mathrm{log}P(Y|\theta^{(i)}) \\\end{align}$$代入上式可得：$$\begin{align}\theta^{(i+1)} &amp;= \mathop{\arg\max}_{\theta} \left\{ \sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}[P(Y|Z,\theta)P(Z|\theta)]-\sum\limits_{Z}P(Z|Y,\theta^{(i)})\mathrm{log}P(Z|Y,\theta^{(i)}) \right\} \\\end{align}$$ 由于在迭代求第 $i+1$ 步时，$\theta^{(i)}$ 是已知的，那么由训练数据中可以求得 $P(Z|Y,\theta^{(i)})$ ，所以在 $\theta^{(i)}$ 值确定的情况下，$P(Z|Y,\theta^{(i)})$ 的值也是确定的而不是变量，那么对上式极大化等价求解对下面式子的极大化$$\begin{align}\theta^{(i+1)} &amp;= \mathop{\arg\max}_{\theta} \left\{ \sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}[P(Y|Z,\theta)P(Z|\theta)]\right\} \\&amp;= \mathop{\arg\max}_{\theta} \left\{ \sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}P(Y,Z|\theta)\right\} \\&amp;= \mathop{\arg\max}_{\theta}Q(\theta, \theta^{(i)}) \tag{9.17}\end{align}$$ Q函数 EM算法 EM算法解释 9.1.3 EM算法在非监督学习中的应用 9.2 EM算法的收敛性这一部分原书讲的比较详细，不画蛇添足，贴上来。 三硬币例子解析前文讲到抛硬币的例子，现在重新详细推导一下三硬币这个例子。 $j$ 是训练集中的数据编号，实际上书上这里求得是$$\begin{align}P(Z|y_j,\theta^{(i)}) = \cases{P(Z=1|y_j,\theta^{(i)})=\mu_{j}^{(i+1)} \\ P(Z=0|y_j,\theta^{(i)})=1- \mu_{j}^{(i+1)}}\end{align}$$前文已知Q函数：$$Q(\theta, \theta^{(i)})=\sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}P(Y,Z|\theta)$$ 第一步求期望即求Q函数，由本文开头的 9.1.1 EM算法 这一节的公式 (9-3) 和 Q函数得到，在多个样本情况下 Q 函数为：$$\begin{align}Q(\theta, \theta^{(i)}) &amp;= \sum\limits_{j=1}^{n}\sum\limits_{Z}P(Z|y_j, \theta^{(i)})\log P(y_j,Z|\theta)\\&amp;= \sum\limits_{j=1}^{n}\left\{ P(Z=1|y_j, \theta^{(i)})\mathrm{log}P(y_j,Z=1|\theta) + P(Z=0|y_j, \theta^{(i)})\mathrm{log}P(y_j,Z=0|\theta) \right\}\\&amp;= \sum\limits_{j=1}^{n}\left\{\mu_{j}^{(i+1)}log P(y_j,Z=1|\theta) + (1-\mu_{j}^{(i+1)})\mathrm{log}P(y_j,Z=0|\theta) \right\}\\&amp;= \sum\limits_{j=1}^{n}\left\{\mu_{j}^{(i+1)}\log [\pi p^{y_j}(1-p)^{1-y_j}]+(1-\mu_{j}^{(i+1)})\log [(1-\pi )q^{y_j}(1-q)^{1-y_j}] \right\}\\\end{align}$$ 第二步极大化Q函数$\begin{align}\theta^{(i+1)} = \mathop{\arg\max}_{\theta}Q(\theta, \theta^{(i)}) = \mathop{\arg\max}_{\theta} \left\{\sum\limits_{j=1}^{n} \sum\limits_{Z}P(Z|y_j, \theta^{(i)})\log P(y_j,Z|\theta)\right\}\end{align}$ 用微积分求解最大值，先求导数为0点（为了求导方便令对数的底数为e，即认为此处对数函数为自然对数）：$$\begin{aligned} \frac{\partial Q(\theta,\theta^{(i)})}{\partial \pi}&amp;=\sum_{j=1}^N\{\frac{\mu_{j}^{(i+1)}\ln [\pi p^{y_j}(1-p)^{1-y_j}]+(1-\mu_{j}^{(i+1)})\ln [(1-\pi )q^{y_j}(1-q)^{1-y_j}] }{\partial \pi}\}\\&amp;=\sum_{j=1}^N\{ \mu_{j}^{(i+1)}\frac{p^{y_j}(1-p)^{1-y_j}}{\pi p^{y_j}(1-p)^{1-y_j}}+(1-\mu_{j}^{(i+1)})\frac{-q^{y_j}(1-q)^{1-y_j}}{(1-\pi )q^{y_j}(1-q)^{1-y_j}} \}\\&amp;=\sum_{j=1}^N\{ \frac{\mu_{j}^{(i+1)}-\pi }{\pi (1-\pi)}\}\\&amp;=\frac{(\sum_{j=1}^N\mu_{j}^{(i+1)})-n\pi }{\pi (1-\pi)} \end{aligned}$$ $$\begin{aligned}\because \quad\frac{\partial Q(\theta,\theta^{(i)})}{\partial \pi}=0 &amp;\implies \pi =\frac 1n\sum_{j=1}^N\mu_{j}^{(i+1)}\\\therefore \quad \pi^{(i+1)}&amp;=\frac 1n\sum_{j=1}^N\mu_{j}^{(i+1)} \end{aligned}$$ $$\begin{aligned} \frac{\partial Q(\theta,\theta^{(i)})}{\partial p}&amp;=\sum_{j=1}^N\{\frac{\mu_{j}^{(i+1)}\ln [\pi p^{y_j}(1-p)^{1-y_j}]+(1-\mu_{j}^{(i+1)})\ln [(1-\pi )q^{y_j}(1-q)^{1-y_j}] }{\partial p}\}\\&amp;=\sum_{j=1}^N\{\mu_{j}^{(i+1)}\frac{\pi (y_jp^{y_j-1}(1-p)^{1-y_j}+p^{y_j}(-1)(1-y_j)(1-p)^{1-y_j-1})}{\pi p^{y_j}(1-p)^{1-y_j}}+0 \}\\&amp;=\sum_{j=1}^N\{ \frac{\mu_{j}^{(i+1)}(y_j-p) }{p(1-p)}\}\\&amp;=\frac{(\sum_{j=1}^N\mu_{j}^{(i+1)}y_j)-(p\sum_{j=1}^N\mu_{j}^{(i+1)}) }{p(1-p)} \end{aligned}$$ $$\begin{aligned}\because \quad \frac{\partial Q(\theta,\theta^{(i)})}{\partial p}=0 &amp;\implies p =\frac{\sum_{j=1}^N \mu^{(i+1)}_j y_j}{\sum_{j=1}^N\mu^{(i+1)}_j} \\\therefore \quad p^{(i+1)}&amp;=\frac{\sum_{j=1}^N\mu^{(i+1)}_j y_j}{\sum_{j=1}^N\mu^{(i+1)}_j} \\q^{(i+1)}&amp;=\frac{\sum_{j=1}^N(1-\mu^{(i+1)}_j)y_j}{\sum_{j=1}^N(1-\mu^{(i+1)}_j)}\end{aligned}$$可以参照书上的结果，一模一样： CS299 EM算法与《统计学习方法》的表述不同点 《统计学习方法》这部分术语源自于鼎鼎大名的ESL 全称：The Elements of Statistical Learning，这也是Stanford统计经典巨作。 Stanford 吴恩达主讲的 CS299 Machine Learning 的 EM课件 由本文的推导，易得 ESL 中的 $ Q_{ESL} = Q_{CS299}\frac{\log P(X,Z;\theta)}{Q_{CS299}} $ 9.3 EM算法在高斯混合模型学习中的应用EM算法的一个重要应用是高斯混合模型的参数估计。 高斯混合模型应用广泛， 在许多情况下， EM算法是学习高斯混合模型（Gaussian misture model） 的有效方法。 9.3.1 高斯混合模型 9.3.2 高斯混合模型参数估计的EM算法 注意：上面的极大化的求混合模型参数迭代公式的过程参考： 大牛JerryLead 的 （EM算法）The EM Algorithm 与K-means比较相同点：都是可用于聚类的算法；都需要指定K值。 不同点：GMM可以给出一个样本属于某类的概率是多少。 9.4 EM算法的推广EM算法还可以解释为F函数（F function） 的极大-极大算法（maximization maximization algorithm） ， 基于这个解释有若干变形与推广， 如广义期望极大（generalized expectation maximization，GEM） 算法。 注：原文引理(9.1)(9.2)的证明有坑需要注意，先看原文，后面列出详细过程 9.4.1 F函数的极大-极大算法 熵这块，不清楚的可以回顾一下我的另一篇总结：《机器学习中的信息论基础》 。 引理9.1需要更详细说明：$$L=E_{\tilde{p}}\log P(Y,Z|\theta) - E_{\tilde{p}}\log \tilde{P}(Z) + \lambda\left\{1-\sum\limits_{Z}\tilde{P}(Z)\right\}$$证明过程思路：拉格朗日求有约束的极大值。需要注意，由累加号和均值可以看出这里的 $Z$ 是指 $Z_i, i$ 这里是 $Z$ 的离散值的标号 ，因此需要重写公式 (9.35) 比较清楚：$$L=\sum\limits_{Z_i}{\tilde{P}(Z_i)}\log P(Y,Z_i|\theta) - \sum\limits_{Z_i}{\tilde{P}(Z_i)}\log \tilde{P}(Z_i)+\lambda\left\{1-\sum\limits_{Z_i}\tilde{P}(Z_i)\right\}$$所以这里其实是 $L$ 关于 $P(Z_i)$的求导（这里作者求导的时候把对数函数默认当做自然对数）：$$\begin{align}&amp;\frac{\partial{L}}{\partial{\tilde{P}(Z_i)}}=\log P(Y,Z_i|\theta)-\log \tilde{P}(Z_i)-1-\lambda \\&amp;\because\quad\frac{\partial{L}}{\partial{\tilde{P}(Z_i)}}=0\\ &amp;\therefore\quad \lambda=\log P(Y,Z_i|\theta)-\log \tilde{P}(Z_i)-1\end{align}$$上式两端同取对数：$$\begin{align}\lambda+1&amp;=\log P(Y,Z_i|\theta)-\log \tilde{P}(Z_i) \\ &amp;\Rightarrow e^{\lambda+1}=\frac{P(Y,Z_i|\theta)}{\tilde{P}(Z_i)} \\&amp;\Rightarrow\tilde{P}(Z_i)=\frac{P(Y,Z_i|\theta)}{e^{\lambda+1}} \tag{9-1}\end{align}$$由离散变量的概率和为1，得到：$$\begin{align}\sum\limits_{Z_i}e^{\lambda+1} &amp;= \frac{\sum\limits_{Z_i}P(Y,Z_i|\theta)}{\sum\limits_{Z_i}\tilde{P}(Z_i)} \Rightarrow\\e^{\lambda+1} &amp;= P(Y|\theta) \tag{9-2}\end{align}$$将 (9-2) 代入 (9-1)​ 式，得到$$\begin{align}\tilde{P}(Z_i)&amp;=\frac{P(Y,Z_i|\theta)}{P(Y|\theta)} \\&amp;=\frac{P(Y,Z_i,\theta)}{p(\theta)}\frac{P(\theta)}{P(Y,\theta)} \\&amp;= P(Z_i|Y,\theta)\end{align}$$这里前提条件是 $\theta$ 是固定情况下的推导过程，所以原文给上式标记出了 $\theta$ ，又因为每个 $Z_i$ 都符合这个式子，那么可重写上式：$$\tilde{P}_{\theta}(Z) = P(Z|Y,\theta)$$这样引理9.1证明完毕。 引理9.2如下 由公式 $(9.33)$ 和 $(9.34)$ :$$F(\tilde{P}, \theta)=E_{\tilde{p}}[\log P(Y,Z|\theta)] + H(\tilde{P}) \\\tilde{P}_{\theta}(Z) = P(Z|Y,\theta)$$得到：$$\begin{align}F(\tilde{P}, \theta)&amp;=\sum\limits_{Z}{\tilde{P}_{\theta}(Z)}\log P(Y,Z|\theta) - \sum\limits_{Z}{\tilde{P}_{\theta}(Z)}\log \tilde{P}_{\theta}(Z) \\&amp;=\sum\limits_{Z} P(Z|Y,\theta)\log P(Y,Z|\theta) - \sum\limits_{Z}P(Z|Y,\theta)\log P(Z|Y,\theta) \\&amp;=\sum\limits_{Z}P(Z|Y,\theta)[\log P(Y,Z|\theta) - \log P(Z|Y,\theta)] \\&amp;=\sum\limits_{Z}P(Z|Y,\theta)\log\frac{P(Y,Z|\theta)}{P(Z|Y,\theta)}\\&amp;=\sum\limits_{Z}P(Z|Y,\theta)\log\left\{\frac{P(Y,Z,\theta)}{p(\theta)}\frac{P(Y,\theta)}{P(Y,Z,\theta)}\right\}\\&amp;= \sum\limits_{Z}P(Z|Y,\theta)\log P(Y|\theta) \\&amp;= \log P(Y|\theta) \\ \end{align}$$引理9.2证明完毕 9.4.2 GEM算法 本章概要 引用 The Expectation Maximization Algorithm: A short tutorial - Sean Borman 李航《统计学习方法》 大牛JerryLead 的 （EM算法）The EM Algorithm 人人都懂EM算法 EM算法简述及简单示例（三硬币模型）]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《统计学习方法》第4章 NaiveBayes]]></title>
    <url>%2F2018%2F10%2F01%2F4.Naive-Bayes_LiHang-Statistical-Learning-Methods%2F</url>
    <content type="text"><![CDATA[前言写本文章主要目的是复习（毕竟之前看纸质版做的笔记）， 对于证明比较跳跃和勘误的地方我都做了注解，以便初学者和以后复习地时候快速阅读理解不会卡住。 朴素贝叶斯法 4.1 朴素贝叶斯法的学习与分类4.1.1 基本方法 4.1.2 后验概率最大化的含义 4.2 朴素贝叶斯法的参数估计4.2.1 极大似然估计 4.2.2 学习与分类算法 例子 4.2.3 贝叶斯估计 本章概要 习题]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[summary of learning Deep Learning Specialization]]></title>
    <url>%2F2018%2F06%2F28%2Fsummary_of_learning_of_Deep_Learning_Specializatio_on_Coursera%2F</url>
    <content type="text"><![CDATA[EnglishThis is my summary of learning Deep Learning Specialization on Coursera, which consists of 5 courses as following: 1st course: Neural Networks and Deep Learning 2nd course: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization 3rd course: Structuring Machine Learning Projects 4th course: Convolutional Neural Networks 5th course: Sequence Models And, here are my summaries of them: 1st course: summary_of_neural-networks-deep-learning 2nd course: summary_of_Improving-Deep-Neural-Networks 3rd course: summary_of_Structuring-Machine-Learning-Projects 4th course: summary_of_convolutional-neural-networks 5th course: summary_of_nlp-sequence-models I spent about 45 days in finishing this Deep learning Specialization and the personal lecture notes, summaries and assignments, but as the saying goes, “gain new knowledge by reviewing the old”. Therefore, I will stick at learning more about Deep Learning and renew the content of this specilization. if you need more details about this Deep Learning Specilization in English, please refer deeplearning.ai or the specialization on Coursera. Tip: if you are familiar with Chinese, you can read the content as following. 中文本文是我个人对吴恩达的深度学习专项课程的学习总结，此文有5个子课程，总结如下： 1st course: summary_of_neural-networks-deep-learning 2nd course: summary_of_Improving-Deep-Neural-Networks 3rd course: summary_of_Structuring-Machine-Learning-Projects 4th course: summary_of_convolutional-neural-networks 5th course: summary_of_nlp-sequence-models 此专项课程的中文目录如下: 如果需要详细视频内容和课程ppt，请参考网易云课堂：吴恩达给你的人工智能第一课， 但是网易并没有提供完成作业的平台，完成作业还需要到 Coursera。]]></content>
      <categories>
        <category>english, 中文</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[summary of nlp sequence models]]></title>
    <url>%2F2018%2F06%2F06%2Fsummary_of_nlp-sequence-models%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal summary after studying the course, nlp sequence models, which belongs to Deep Learning Specialization. and the copyright belongs to deeplearning.ai. My personal note$1_{st}$ week : Building a Recurrent Neural Network Step by Step 01_why-sequence-models 02_notation 03_recurrent-neural-network-model 04_backpropagation-through-time 05_different-types-of-rnns 06_language-model-and-sequence-generation 07_sampling-novel-sequences 08_vanishing-gradients-with-rnns 09_gated-recurrent-unit-gru 10_long-short-term-memory-lstm 11_bidirectional-rnn 12_deep-rnns $2_{nd}$ week : natural language processing word embeddings 01_introduction-to-word-embeddings 01_word-representation 02_using-word-embeddings 03_properties-of-word-embeddings 04_embedding-matrix 02_learning-word-embeddings-word2vec-glove 01_learning-word-embeddings 02_word2vec 03_negative-sampling 04_glove-word-vectors 03_applications-using-word-embeddings 01_sentiment-classification 02_debiasing-word-embeddings $3_{rd}$ week : sequence models attention mechanism 01_various-sequence-to-sequence-architectures 01_basic-models 02_picking-the-most-likely-sentence 03_beam-search 04_refinements-to-beam-search 05_error-analysis-in-beam-search 06_bleu-score-optional 07_attention-model-intuition 08_attention-model 02_speech-recognition-audio-data 01_speech-recognition 02_trigger-word-detection conclusion of Deep Learning Specialization and thank-you My personal programming assignments$1_{st}$ week: Building a Recurrent Neural Network Step by Step Dinosaurus Island Character level language model final Improvise a Jazz Solo with an LSTM Network $2_{nd}$ week: Word Vector Representation Emojify $3_{rd}$ week: machine translation Trigger word]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Trigger word]]></title>
    <url>%2F2018%2F06%2F06%2FTrigger_word_detection-v1%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 3rd week and the copyright belongs to deeplearning.ai. Trigger Word DetectionWelcome to the final programming assignment of this specialization! In this week’s videos, you learned about applying deep learning to speech recognition. In this assignment, you will construct a speech dataset and implement an algorithm for trigger word detection (sometimes also called keyword detection, or wakeword detection). Trigger word detection is the technology that allows devices like Amazon Alexa, Google Home, Apple Siri, and Baidu DuerOS to wake up upon hearing a certain word. For this exercise, our trigger word will be “Activate.” Every time it hears you say “activate,” it will make a “chiming” sound. By the end of this assignment, you will be able to record a clip of yourself talking, and have the algorithm trigger a chime when it detects you saying “activate.” After completing this assignment, perhaps you can also extend it to run on your laptop so that every time you say “activate” it starts up your favorite app, or turns on a network connected lamp in your house, or triggers some other event? In this assignment you will learn to: Structure a speech recognition project Synthesize and process audio recordings to create train/dev datasets Train a trigger word detection model and make predictions Lets get started! Run the following cell to load the package you are going to use. 12345678910import numpy as npfrom pydub import AudioSegmentimport randomimport sysimport ioimport osimport globimport IPythonfrom td_utils import *%matplotlib inline 1 - Data synthesis: Creating a speech datasetLet’s start by building a dataset for your trigger word detection algorithm. A speech dataset should ideally be as close as possible to the application you will want to run it on. In this case, you’d like to detect the word “activate” in working environments (library, home, offices, open-spaces …). You thus need to create recordings with a mix of positive words (“activate”) and negative words (random words other than activate) on different background sounds. Let’s see how you can create such a dataset. 1.1 - Listening to the dataOne of your friends is helping you out on this project, and they’ve gone to libraries, cafes, restaurants, homes and offices all around the region to record background noises, as well as snippets of audio of people saying positive/negative words. This dataset includes people speaking in a variety of accents. In the raw_data directory, you can find a subset of the raw audio files of the positive words, negative words, and background noise. You will use these audio files to synthesize a dataset to train the model. The “activate” directory contains positive examples of people saying the word “activate”. The “negatives” directory contains negative examples of people saying random words other than “activate”. There is one word per audio recording. The “backgrounds” directory contains 10 second clips of background noise in different environments. Run the cells below to listen to some examples. 1IPython.display.Audio("./raw_data/activates/1.wav") Your browser does not support the audio element. 1IPython.display.Audio("./raw_data/negatives/4.wav") Your browser does not support the audio element. 1IPython.display.Audio("./raw_data/backgrounds/1.wav") Your browser does not support the audio element. You will use these three type of recordings (positives/negatives/backgrounds) to create a labelled dataset. 1.2 - From audio recordings to spectrogramsWhat really is an audio recording? A microphone records little variations in air pressure over time, and it is these little variations in air pressure that your ear also perceives as sound. You can think of an audio recording is a long list of numbers measuring the little air pressure changes detected by the microphone. We will use audio sampled at 44100 Hz (or 44100 Hertz). This means the microphone gives us 44100 numbers per second. Thus, a 10 second audio clip is represented by 441000 numbers (= $10 \times 44100$). It is quite difficult to figure out from this “raw” representation of audio whether the word “activate” was said. In order to help your sequence model more easily learn to detect triggerwords, we will compute a spectrogram of the audio. The spectrogram tells us how much different frequencies are present in an audio clip at a moment in time. (If you’ve ever taken an advanced class on signal processing or on Fourier transforms, a spectrogram is computed by sliding a window over the raw audio signal, and calculates the most active frequencies in each window using a Fourier transform. If you don’t understand the previous sentence, don’t worry about it.) Lets see an example. 1IPython.display.Audio("audio_examples/example_train.wav") Your browser does not support the audio element. 1x = graph_spectrogram("audio_examples/example_train.wav") The graph above represents how active each frequency is (y axis) over a number of time-steps (x axis). Figure 1: Spectrogram of an audio recording, where the color shows the degree to which different frequencies are present (loud) in the audio at different points in time. Green squares means a certain frequency is more active or more present in the audio clip (louder); blue squares denote less active frequencies. The dimension of the output spectrogram depends upon the hyperparameters of the spectrogram software and the length of the input. In this notebook, we will be working with 10 second audio clips as the “standard length” for our training examples. The number of timesteps of the spectrogram will be 5511. You’ll see later that the spectrogram will be the input $x$ into the network, and so $T_x = 5511$. 123_, data = wavfile.read("audio_examples/example_train.wav")print("Time steps in audio recording before spectrogram", data[:,0].shape)print("Time steps in input after spectrogram", x.shape) Time steps in audio recording before spectrogram (441000,) Time steps in input after spectrogram (101, 5511) Now, you can define: 12Tx = 5511 # The number of time steps input to the model from the spectrogramn_freq = 101 # Number of frequencies input to the model at each time step of the spectrogram Note that even with 10 seconds being our default training example length, 10 seconds of time can be discretized to different numbers of value. You’ve seen 441000 (raw audio) and 5511 (spectrogram). In the former case, each step represents $10/441000 \approx 0.000023$ seconds. In the second case, each step represents $10/5511 \approx 0.0018$ seconds. For the 10sec of audio, the key values you will see in this assignment are: $441000$ (raw audio) $5511 = T_x$ (spectrogram output, and dimension of input to the neural network). $10000$ (used by the pydub module to synthesize audio) $1375 = T_y$ (the number of steps in the output of the GRU you’ll build). Note that each of these representations correspond to exactly 10 seconds of time. It’s just that they are discretizing them to different degrees. All of these are hyperparameters and can be changed (except the 441000, which is a function of the microphone). We have chosen values that are within the standard ranges uses for speech systems. Consider the $T_y = 1375$ number above. This means that for the output of the model, we discretize the 10s into 1375 time-intervals (each one of length $10/1375 \approx 0.0072$s) and try to predict for each of these intervals whether someone recently finished saying “activate.” Consider also the 10000 number above. This corresponds to discretizing the 10sec clip into 10/10000 = 0.001 second itervals. 0.001 seconds is also called 1 millisecond, or 1ms. So when we say we are discretizing according to 1ms intervals, it means we are using 10,000 steps. 1Ty = 1375 # The number of time steps in the output of our model 1.3 - Generating a single training exampleBecause speech data is hard to acquire and label, you will synthesize your training data using the audio clips of activates, negatives, and backgrounds. It is quite slow to record lots of 10 second audio clips with random “activates” in it. Instead, it is easier to record lots of positives and negative words, and record background noise separately (or download background noise from free online sources). To synthesize a single training example, you will: Pick a random 10 second background audio clip Randomly insert 0-4 audio clips of “activate” into this 10sec clip Randomly insert 0-2 audio clips of negative words into this 10sec clip Because you had synthesized the word “activate” into the background clip, you know exactly when in the 10sec clip the “activate” makes its appearance. You’ll see later that this makes it easier to generate the labels $y^{\langle t \rangle}$ as well. You will use the pydub package to manipulate audio. Pydub converts raw audio files into lists of Pydub data structures (it is not important to know the details here). Pydub uses 1ms as the discretization interval (1ms is 1 millisecond = 1/1000 seconds) which is why a 10sec clip is always represented using 10,000 steps. 123456# Load audio segments using pydub activates, negatives, backgrounds = load_raw_audio()print("background len: " + str(len(backgrounds[0]))) # Should be 10,000, since it is a 10 sec clipprint("activate[0] len: " + str(len(activates[0]))) # Maybe around 1000, since an "activate" audio clip is usually around 1 sec (but varies a lot)print("activate[1] len: " + str(len(activates[1]))) # Different "activate" clips can have different lengths background len: 10000 activate[0] len: 916 activate[1] len: 1579 Overlaying positive/negative words on the background: Given a 10sec background clip and a short audio clip (positive or negative word), you need to be able to “add” or “insert” the word’s short audio clip onto the background. To ensure audio segments inserted onto the background do not overlap, you will keep track of the times of previously inserted audio clips. You will be inserting multiple clips of positive/negative words onto the background, and you don’t want to insert an “activate” or a random word somewhere that overlaps with another clip you had previously added. For clarity, when you insert a 1sec “activate” onto a 10sec clip of cafe noise, you end up with a 10sec clip that sounds like someone sayng “activate” in a cafe, with “activate” superimposed on the background cafe noise. You do not end up with an 11 sec clip. You’ll see later how pydub allows you to do this. Creating the labels at the same time you overlay: Recall also that the labels $y^{\langle t \rangle}$ represent whether or not someone has just finished saying “activate.” Given a background clip, we can initialize $y^{\langle t \rangle}=0$ for all $t$, since the clip doesn’t contain any “activates.” When you insert or overlay an “activate” clip, you will also update labels for $y^{\langle t \rangle}$, so that 50 steps of the output now have target label 1. You will train a GRU to detect when someone has finished saying “activate”. For example, suppose the synthesized “activate” clip ends at the 5sec mark in the 10sec audio—exactly halfway into the clip. Recall that $T_y = 1375$, so timestep $687 = $ int(1375*0.5) corresponds to the moment at 5sec into the audio. So, you will set $y^{\langle 688 \rangle} = 1$. Further, you would quite satisfied if the GRU detects “activate” anywhere within a short time-internal after this moment, so we actually set 50 consecutive values of the label $y^{\langle t \rangle}$ to 1. Specifically, we have $y^{\langle 688 \rangle} = y^{\langle 689 \rangle} = \cdots = y^{\langle 737 \rangle} = 1$. This is another reason for synthesizing the training data: It’s relatively straightforward to generate these labels $y^{\langle t \rangle}$ as described above. In contrast, if you have 10sec of audio recorded on a microphone, it’s quite time consuming for a person to listen to it and mark manually exactly when “activate” finished. Here’s a figure illustrating the labels $y^{\langle t \rangle}$, for a clip which we have inserted “activate”, “innocent”, activate”, “baby.” Note that the positive labels “1” are associated only with the positive words. Figure 2 To implement the training set synthesis process, you will use the following helper functions. All of these function will use a 1ms discretization interval, so the 10sec of audio is alwsys discretized into 10,000 steps. get_random_time_segment(segment_ms) gets a random time segment in our background audio is_overlapping(segment_time, existing_segments) checks if a time segment overlaps with existing segments insert_audio_clip(background, audio_clip, existing_times) inserts an audio segment at a random time in our background audio using get_random_time_segment and is_overlapping insert_ones(y, segment_end_ms) inserts 1’s into our label vector y after the word “activate” The function get_random_time_segment(segment_ms) returns a random time segment onto which we can insert an audio clip of duration segment_ms. Read through the code to make sure you understand what it is doing. 123456789101112131415def get_random_time_segment(segment_ms): """ Gets a random time segment of duration segment_ms in a 10,000 ms audio clip. Arguments: segment_ms -- the duration of the audio clip in ms ("ms" stands for "milliseconds") Returns: segment_time -- a tuple of (segment_start, segment_end) in ms """ segment_start = np.random.randint(low=0, high=10000-segment_ms) # Make sure segment doesn't run past the 10sec background segment_end = segment_start + segment_ms - 1 return (segment_start, segment_end) Next, suppose you have inserted audio clips at segments (1000,1800) and (3400,4500). I.e., the first segment starts at step 1000, and ends at step 1800. Now, if we are considering inserting a new audio clip at (3000,3600) does this overlap with one of the previously inserted segments? In this case, (3000,3600) and (3400,4500) overlap, so we should decide against inserting a clip here. For the purpose of this function, define (100,200) and (200,250) to be overlapping, since they overlap at timestep 200. However, (100,199) and (200,250) are non-overlapping. Exercise: Implement is_overlapping(segment_time, existing_segments) to check if a new time segment overlaps with any of the previous segments. You will need to carry out 2 steps: Create a “False” flag, that you will later set to “True” if you find that there is an overlap. Loop over the previous_segments’ start and end times. Compare these times to the segment’s start and end times. If there is an overlap, set the flag defined in (1) as True. You can use:123for ....: if ... &lt;= ... and ... &gt;= ...: ... Hint: There is overlap if the segment starts before the previous segment ends, and the segment ends after the previous segment starts. 12345678910111213141516171819202122232425262728# GRADED FUNCTION: is_overlappingdef is_overlapping(segment_time, previous_segments): """ Checks if the time of a segment overlaps with the times of existing segments. Arguments: segment_time -- a tuple of (segment_start, segment_end) for the new segment previous_segments -- a list of tuples of (segment_start, segment_end) for the existing segments Returns: True if the time segment overlaps with any of the existing segments, False otherwise """ segment_start, segment_end = segment_time ### START CODE HERE ### (≈ 4 line) # Step 1: Initialize overlap as a "False" flag. (≈ 1 line) overlap = False; # Step 2: loop over the previous_segments start and end times. # Compare start/end times and set the flag to True if there is an overlap (≈ 3 lines) for previous_start, previous_end in previous_segments: if segment_end &gt;= previous_start and segment_start &lt;= previous_end: overlap = True; ### END CODE HERE ### return overlap 1234overlap1 = is_overlapping((950, 1430), [(2000, 2550), (260, 949)])overlap2 = is_overlapping((2305, 2950), [(824, 1532), (1900, 2305), (3424, 3656)])print("Overlap 1 = ", overlap1)print("Overlap 2 = ", overlap2) Overlap 1 = False Overlap 2 = True Expected Output: Overlap 1 False Overlap 2 True Now, lets use the previous helper functions to insert a new audio clip onto the 10sec background at a random time, but making sure that any newly inserted segment doesn’t overlap with the previous segments. Exercise: Implement insert_audio_clip() to overlay an audio clip onto the background 10sec clip. You will need to carry out 4 steps: Get a random time segment of the right duration in ms. Make sure that the time segment does not overlap with any of the previous time segments. If it is overlapping, then go back to step 1 and pick a new time segment. Add the new time segment to the list of existing time segments, so as to keep track of all the segments you’ve inserted. Overlay the audio clip over the background using pydub. We have implemented this for you. 12345678910111213141516171819202122232425262728293031323334353637# GRADED FUNCTION: insert_audio_clipdef insert_audio_clip(background, audio_clip, previous_segments): """ Insert a new audio segment over the background noise at a random time step, ensuring that the audio segment does not overlap with existing segments. Arguments: background -- a 10 second background audio recording. audio_clip -- the audio clip to be inserted/overlaid. previous_segments -- times where audio segments have already been placed Returns: new_background -- the updated background audio """ # Get the duration of the audio clip in ms segment_ms = len(audio_clip) ### START CODE HERE ### # Step 1: Use one of the helper functions to pick a random time segment onto which to insert # the new audio clip. (≈ 1 line) segment_time = get_random_time_segment(segment_ms); # Step 2: Check if the new segment_time overlaps with one of the previous_segments. If so, keep # picking new segment_time at random until it doesn't overlap. (≈ 2 lines) while is_overlapping(segment_time, previous_segments): segment_time = get_random_time_segment(segment_ms); # Step 3: Add the new segment_time to the list of previous_segments (≈ 1 line) previous_segments.append(segment_time); ### END CODE HERE ### # Step 4: Superpose audio segment and background new_background = background.overlay(audio_clip, position = segment_time[0]) return new_background, segment_time 12345np.random.seed(5)audio_clip, segment_time = insert_audio_clip(backgrounds[0], activates[0], [(3790, 4400)])audio_clip.export("insert_test.wav", format="wav")print("Segment Time: ", segment_time)IPython.display.Audio("insert_test.wav") Segment Time: (2254, 3169) Your browser does not support the audio element. Expected Output Segment Time (2254, 3169) 12# Expected audioIPython.display.Audio("audio_examples/insert_reference.wav") Your browser does not support the audio element. Finally, implement code to update the labels $y^{\langle t \rangle}$, assuming you just inserted an “activate.” In the code below, y is a (1,1375) dimensional vector, since $T_y = 1375$. If the “activate” ended at time step $t$, then set $y^{\langle t+1 \rangle} = 1$ as well as for up to 49 additional consecutive values. However, make sure you don’t run off the end of the array and try to update y[0][1375], since the valid indices are y[0][0] through y[0][1374] because $T_y = 1375$. So if “activate” ends at step 1370, you would get only y[0][1371] = y[0][1372] = y[0][1373] = y[0][1374] = 1 Exercise: Implement insert_ones(). You can use a for loop. (If you are an expert in python’s slice operations, feel free also to use slicing to vectorize this.) If a segment ends at segment_end_ms (using a 10000 step discretization), to convert it to the indexing for the outputs $y$ (using a $1375$ step discretization), we will use this formula:1segment_end_y = int(segment_end_ms * Ty / 10000.0) 12345678910111213141516171819202122232425262728# GRADED FUNCTION: insert_onesdef insert_ones(y, segment_end_ms): """ Update the label vector y. The labels of the 50 output steps strictly after the end of the segment should be set to 1. By strictly we mean that the label of segment_end_y should be 0 while, the 50 followinf labels should be ones. Arguments: y -- numpy array of shape (1, Ty), the labels of the training example segment_end_ms -- the end time of the segment in ms Returns: y -- updated labels """ # duration of the background (in terms of spectrogram time-steps) segment_end_y = int(segment_end_ms * Ty / 10000.0) # Add 1 to the correct index in the background label (y) ### START CODE HERE ### (≈ 3 lines) for i in range(segment_end_y + 1, segment_end_y + 51): if i &lt; Ty: y[0, i] = 1 ### END CODE HERE ### return y 123arr1 = insert_ones(np.zeros((1, Ty)), 9700)plt.plot(insert_ones(arr1, 4251)[0,:])print("sanity checks:", arr1[0][1333], arr1[0][634], arr1[0][635]) sanity checks: 0.0 1.0 0.0 Expected Output sanity checks: 0.0 1.0 0.0 Finally, you can use insert_audio_clip and insert_ones to create a new training example.Exercise: Implement create_training_example(). You will need to carry out the following steps:1. Initialize the label vector $y$ as a numpy array of zeros and shape $(1, T_y)$.2. Initialize the set of existing segments to an empty list.3. Randomly select 0 to 4 “activate” audio clips, and insert them onto the 10sec clip. Also insert labels at the correct position in the label vector $y$.4. Randomly select 0 to 2 negative audio clips, and insert them into the 10sec clip.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# GRADED FUNCTION: create_training_exampledef create_training_example(background, activates, negatives): """ Creates a training example with a given background, activates, and negatives. Arguments: background -- a 10 second background audio recording activates -- a list of audio segments of the word "activate" negatives -- a list of audio segments of random words that are not "activate" Returns: x -- the spectrogram of the training example y -- the label at each time step of the spectrogram """ # Set the random seed np.random.seed(18) # Make background quieter background = background - 20 ### START CODE HERE ### # Step 1: Initialize y (label vector) of zeros (≈ 1 line) y = np.zeros((1, Ty)); # Step 2: Initialize segment times as empty list (≈ 1 line) previous_segments = []; ### END CODE HERE ### # Select 0-4 random "activate" audio clips from the entire list of "activates" recordings number_of_activates = np.random.randint(0, 5) random_indices = np.random.randint(len(activates), size=number_of_activates) random_activates = [activates[i] for i in random_indices] ### START CODE HERE ### (≈ 3 lines) # Step 3: Loop over randomly selected "activate" clips and insert in background for random_activate in random_activates: # Insert the audio clip on the background background, segment_time = insert_audio_clip(background, random_activate, previous_segments); # Retrieve segment_start and segment_end from segment_time segment_start, segment_end = segment_time; # Insert labels in "y" y = insert_ones(y, segment_end); ### END CODE HERE ### # Select 0-2 random negatives audio recordings from the entire list of "negatives" recordings number_of_negatives = np.random.randint(0, 3) random_indices = np.random.randint(len(negatives), size=number_of_negatives) random_negatives = [negatives[i] for i in random_indices] ### START CODE HERE ### (≈ 2 lines) # Step 4: Loop over randomly selected negative clips and insert in background for random_negative in random_negatives: # Insert the audio clip on the background background, _ = insert_audio_clip(background, random_negative, previous_segments); ### END CODE HERE ### # Standardize the volume of the audio clip background = match_target_amplitude(background, -20.0) # Export new training example file_handle = background.export("train" + ".wav", format="wav") print("File (train.wav) was saved in your directory.") # Get and plot spectrogram of the new recording (background with superposition of positive and negatives) x = graph_spectrogram("train.wav") return x, y1x, y = create_training_example(backgrounds[0], activates, negatives) File (train.wav) was saved in your directory.Expected OutputNow you can listen to the training example you created and compare it to the spectrogram generated above.1IPython.display.Audio("train.wav") Your browser does not support the audio element.Expected Output1IPython.display.Audio("audio_examples/train_reference.wav") Your browser does not support the audio element.Finally, you can plot the associated labels for the generated training example.1plt.plot(y[0]) [&lt;matplotlib.lines.Line2D at 0x7efcb81a8da0&gt;]Expected Output## 1.4 - Full training setYou’ve now implemented the code needed to generate a single training example. We used this process to generate a large training set. To save time, we’ve already generated a set of training examples.123# Load preprocessed training examplesX = np.load("./XY_train/X.npy")Y = np.load("./XY_train/Y.npy")## 1.5 - Development setTo test our model, we recorded a development set of 25 examples. While our training data is synthesized, we want to create a development set using the same distribution as the real inputs. Thus, we recorded 25 10-second audio clips of people saying “activate” and other random words, and labeled them by hand. This follows the principle described in Course 3 that we should create the dev set to be as similar as possible to the test set distribution; that’s why our dev set uses real rather than synthesized audio.123# Load preprocessed dev set examplesX_dev = np.load("./XY_dev/X_dev.npy")Y_dev = np.load("./XY_dev/Y_dev.npy")# 2 - ModelNow that you’ve built a dataset, lets write and train a trigger word detection model!The model will use 1-D convolutional layers, GRU layers, and dense layers. Let’s load the packages that will allow you to use these layers in Keras. This might take a minute to load.12345from keras.callbacks import ModelCheckpointfrom keras.models import Model, load_model, Sequentialfrom keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1Dfrom keras.layers import GRU, Bidirectional, BatchNormalization, Reshapefrom keras.optimizers import Adam Using TensorFlow backend.## 2.1 - Build the modelHere is the architecture we will use. Take some time to look over the model and see if it makes sense. Figure 3 One key step of this model is the 1D convolutional step (near the bottom of Figure 3). It inputs the 5511 step spectrogram, and outputs a 1375 step output, which is then further processed by multiple layers to get the final $T_y = 1375$ step output. This layer plays a role similar to the 2D convolutions you saw in Course 4, of extracting low-level features and then possibly generating an output of a smaller dimension.Computationally, the 1-D conv layer also helps speed up the model because now the GRU has to process only 1375 timesteps rather than 5511 timesteps. The two GRU layers read the sequence of inputs from left to right, then ultimately uses a dense+sigmoid layer to make a prediction for $y^{\langle t \rangle}$. Because $y$ is binary valued (0 or 1), we use a sigmoid output at the last layer to estimate the chance of the output being 1, corresponding to the user having just said “activate.”Note that we use a uni-directional RNN rather than a bi-directional RNN. This is really important for trigger word detection, since we want to be able to detect the trigger word almost immediately after it is said. If we used a bi-directional RNN, we would have to wait for the whole 10sec of audio to be recorded before we could tell if “activate” was said in the first second of the audio clip.Implementing the model can be done in four steps:Step 1: CONV layer. Use Conv1D() to implement this, with 196 filters,a filter size of 15 (kernel_size=15), and stride of 4. [See documentation.]Step 2: First GRU layer. To generate the GRU layer, use:1X = GRU(units = 128, return_sequences = True)(X)Setting return_sequences=True ensures that all the GRU’s hidden states are fed to the next layer. Remember to follow this with Dropout and BatchNorm layers.Step 3: Second GRU layer. This is similar to the previous GRU layer (remember to use return_sequences=True), but has an extra dropout layer.Step 4: Create a time-distributed dense layer as follows:1X = TimeDistributed(Dense(1, activation = "sigmoid"))(X)This creates a dense layer followed by a sigmoid, so that the parameters used for the dense layer are the same for every time step. [See documentation.]Exercise: Implement model(), the architecture is presented in Figure 3.123456789101112131415161718192021222324252627282930313233343536373839404142# GRADED FUNCTION: modeldef model(input_shape): """ Function creating the model's graph in Keras. Argument: input_shape -- shape of the model's input data (using Keras conventions) Returns: model -- Keras model instance """ X_input = Input(shape = input_shape) ### START CODE HERE ### # Step 1: CONV layer (≈4 lines) X = Conv1D(196, 15, strides = 4)(X_input); # CONV1D X = BatchNormalization()(X); # Batch normalization X = Activation('relu')(X); # ReLu activation X = Dropout(0.8)(X); # dropout (use 0.8) # Step 2: First GRU Layer (≈4 lines) X = GRU(128, return_sequences = True)(X); # GRU (use 128 units and return the sequences) X = Dropout(0.8)(X); # dropout (use 0.8) X = BatchNormalization()(X); # Batch normalization # Step 3: Second GRU Layer (≈4 lines) X = GRU(128, return_sequences = True)(X); # GRU (use 128 units and return the sequences) X = Dropout(0.8)(X); # dropout (use 0.8) X = BatchNormalization()(X); # Batch normalization X = Dropout(0.8)(X); # dropout (use 0.8) # Step 4: Time-distributed dense layer (≈1 line) X = TimeDistributed(Dense(1, activation = "sigmoid"))(X) # time distributed (sigmoid) ### END CODE HERE ### model = Model(inputs = X_input, outputs = X) return model1model = model(input_shape = (Tx, n_freq))Let’s print the model summary to keep track of the shapes.1model.summary() _ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 5511, 101) 0 _ conv1d_1 (Conv1D) (None, 1375, 196) 297136 _ batch_normalization_1 (Batch (None, 1375, 196) 784 _ activation_1 (Activation) (None, 1375, 196) 0 _ dropout_1 (Dropout) (None, 1375, 196) 0 _ gru_1 (GRU) (None, 1375, 128) 124800 _ dropout_2 (Dropout) (None, 1375, 128) 0 _ batch_normalization_2 (Batch (None, 1375, 128) 512 _ gru_2 (GRU) (None, 1375, 128) 98688 _ dropout_3 (Dropout) (None, 1375, 128) 0 _ batch_normalization_3 (Batch (None, 1375, 128) 512 _ dropout_4 (Dropout) (None, 1375, 128) 0 _ time_distributed_1 (TimeDist (None, 1375, 1) 129 ================================================================= Total params: 522,561 Trainable params: 521,657 Non-trainable params: 904 _Expected Output: Total params 522,561 Trainable params 521,657 Non-trainable params 904 The output of the network is of shape (None, 1375, 1) while the input is (None, 5511, 101). The Conv1D has reduced the number of steps from 5511 at spectrogram to 1375. 2.2 - Fit the modelTrigger word detection takes a long time to train. To save time, we’ve already trained a model for about 3 hours on a GPU using the architecture you built above, and a large training set of about 4000 examples. Let’s load the model. 1model = load_model('./models/tr_model.h5') You can train the model further, using the Adam optimizer and binary cross entropy loss, as follows. This will run quickly because we are training just for one epoch and with a small training set of 26 examples. 12opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)model.compile(loss='binary_crossentropy', optimizer=opt, metrics=["accuracy"]) 1model.fit(X, Y, batch_size = 5, epochs=1) Epoch 1/1 26/26 [==============================] - 23s - loss: 0.0727 - acc: 0.9806 &lt;keras.callbacks.History at 0x7efc4e3727f0&gt; 2.3 - Test the modelFinally, let’s see how your model performs on the dev set. 12loss, acc = model.evaluate(X_dev, Y_dev)print("Dev set accuracy = ", acc) 25/25 [==============================] - 4s Dev set accuracy = 0.946036338806 This looks pretty good! However, accuracy isn’t a great metric for this task, since the labels are heavily skewed to 0’s, so a neural network that just outputs 0’s would get slightly over 90% accuracy. We could define more useful metrics such as F1 score or Precision/Recall. But let’s not bother with that here, and instead just empirically see how the model does. 3 - Making PredictionsNow that you have built a working model for trigger word detection, let’s use it to make predictions. This code snippet runs audio (saved in a wav file) through the network. 1234567891011121314def detect_triggerword(filename): plt.subplot(2, 1, 1) x = graph_spectrogram(filename) # the spectogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model x = x.swapaxes(0,1) x = np.expand_dims(x, axis=0) predictions = model.predict(x) plt.subplot(2, 1, 2) plt.plot(predictions[0,:,0]) plt.ylabel('probability') plt.show() return predictions Once you’ve estimated the probability of having detected the word “activate” at each output step, you can trigger a “chiming” sound to play when the probability is above a certain threshold. Further, $y^{\langle t \rangle}$ might be near 1 for many values in a row after “activate” is said, yet we want to chime only once. So we will insert a chime sound at most once every 75 output steps. This will help prevent us from inserting two chimes for a single instance of “activate”. (This plays a role similar to non-max suppression from computer vision.) 12345678910111213141516171819chime_file = "audio_examples/chime.wav"def chime_on_activate(filename, predictions, threshold): audio_clip = AudioSegment.from_wav(filename) chime = AudioSegment.from_wav(chime_file) Ty = predictions.shape[1] # Step 1: Initialize the number of consecutive output steps to 0 consecutive_timesteps = 0 # Step 2: Loop over the output steps in the y for i in range(Ty): # Step 3: Increment consecutive output steps consecutive_timesteps += 1 # Step 4: If prediction is higher than the threshold and more than 75 consecutive output steps have passed if predictions[0,i,0] &gt; threshold and consecutive_timesteps &gt; 75: # Step 5: Superpose audio and background using pydub audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds)*1000) # Step 6: Reset consecutive output steps to 0 consecutive_timesteps = 0 audio_clip.export("chime_output.wav", format='wav') 3.3 - Test on dev examplesLet’s explore how our model performs on two unseen audio clips from the development set. Lets first listen to the two dev set clips. 1IPython.display.Audio("./raw_data/dev/1.wav") Your browser does not support the audio element. 1IPython.display.Audio("./raw_data/dev/2.wav") Your browser does not support the audio element. Now lets run the model on these audio clips and see if it adds a chime after “activate”! 1234filename = "./raw_data/dev/1.wav"prediction = detect_triggerword(filename)chime_on_activate(filename, prediction, 0.5)IPython.display.Audio("./chime_output.wav") Your browser does not support the audio element. 1234filename = "./raw_data/dev/2.wav"prediction = detect_triggerword(filename)chime_on_activate(filename, prediction, 0.5)IPython.display.Audio("./chime_output.wav") Your browser does not support the audio element. CongratulationsYou’ve come to the end of this assignment! Here’s what you should remember: Data synthesis is an effective way to create a large training set for speech problems, specifically trigger word detection. Using a spectrogram and optionally a 1D conv layer is a common pre-processing step prior to passing audio data to an RNN, GRU or LSTM. An end-to-end deep learning approach can be used to built a very effective trigger word detection system. Congratulations on finishing the fimal assignment! Thank you for sticking with us through the end and for all the hard work you’ve put into learning deep learning. We hope you have enjoyed the course! 4 - Try your own example! (OPTIONAL/UNGRADED)In this optional and ungraded portion of this notebook, you can try your model on your own audio clips! Record a 10 second audio clip of you saying the word “activate” and other random words, and upload it to the Coursera hub as myaudio.wav. Be sure to upload the audio as a wav file. If your audio is recorded in a different format (such as mp3) there is free software that you can find online for converting it to wav. If your audio recording is not 10 seconds, the code below will either trim or pad it as needed to make it 10 seconds. 12345678910# Preprocess the audio to the correct formatdef preprocess_audio(filename): # Trim or pad audio segment to 10000ms padding = AudioSegment.silent(duration=10000) segment = AudioSegment.from_wav(filename)[:10000] segment = padding.overlay(segment) # Set frame rate to 44100 segment = segment.set_frame_rate(44100) # Export as wav segment.export(filename, format='wav') Once you’ve uploaded your audio file to Coursera, put the path to your file in the variable below. 1your_filename = "audio_examples/my_audio.wav" 12preprocess_audio(your_filename)IPython.display.Audio(your_filename) # listen to the audio you uploaded Your browser does not support the audio element. Finally, use the model to predict when you say activate in the 10 second audio clip, and trigger a chime. If beeps are not being added appropriately, try to adjust the chime_threshold. 1234chime_threshold = 0.5prediction = detect_triggerword(your_filename)chime_on_activate(your_filename, prediction, chime_threshold)IPython.display.Audio("./chime_output.wav") Your browser does not support the audio element.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural machine translation with attention]]></title>
    <url>%2F2018%2F06%2F05%2FNeural%2Bmachine%2Btranslation%2Bwith%2Battention%2B-%2Bv4%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 3rd week and the copyright belongs to deeplearning.ai. Neural Machine TranslationWelcome to your first programming assignment for this week! You will build a Neural Machine Translation (NMT) model to translate human readable dates (“25th of June, 2009”) into machine readable dates (“2009-06-25”). You will do this using an attention model, one of the most sophisticated sequence to sequence models. This notebook was produced together with NVIDIA’s Deep Learning Institute. Let’s load all the packages you will need for this assignment. 123456789101112131415from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiplyfrom keras.layers import RepeatVector, Dense, Activation, Lambdafrom keras.optimizers import Adamfrom keras.utils import to_categoricalfrom keras.models import load_model, Modelimport keras.backend as Kimport numpy as npfrom faker import Fakerimport randomfrom tqdm import tqdmfrom babel.dates import format_datefrom nmt_utils import *import matplotlib.pyplot as plt%matplotlib inline Using TensorFlow backend. 1 - Translating human readable dates into machine readable datesThe model you will build here could be used to translate from one language to another, such as translating from English to Hindi. However, language translation requires massive datasets and usually takes days of training on GPUs. To give you a place to experiment with these models even without using massive datasets, we will instead use a simpler “date translation” task. The network will input a date written in a variety of possible formats (e.g. “the 29th of August 1958”, “03/30/1968”, “24 JUNE 1987”) and translate them into standardized, machine readable dates (e.g. “1958-08-29”, “1968-03-30”, “1987-06-24”). We will have the network learn to output dates in the common machine-readable format YYYY-MM-DD. 1.1 - DatasetWe will train the model on a dataset of 10000 human readable dates and their equivalent, standardized, machine readable dates. Let’s run the following cells to load the dataset and print some examples. 12m = 10000dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m) 100%|██████████| 10000/10000 [00:01&lt;00:00, 8435.76it/s] 1dataset[:10] [(&apos;9 may 1998&apos;, &apos;1998-05-09&apos;), (&apos;10.09.70&apos;, &apos;1970-09-10&apos;), (&apos;4/28/90&apos;, &apos;1990-04-28&apos;), (&apos;thursday january 26 1995&apos;, &apos;1995-01-26&apos;), (&apos;monday march 7 1983&apos;, &apos;1983-03-07&apos;), (&apos;sunday may 22 1988&apos;, &apos;1988-05-22&apos;), (&apos;tuesday july 8 2008&apos;, &apos;2008-07-08&apos;), (&apos;08 sep 1999&apos;, &apos;1999-09-08&apos;), (&apos;1 jan 1981&apos;, &apos;1981-01-01&apos;), (&apos;monday may 22 1995&apos;, &apos;1995-05-22&apos;)] You’ve loaded: dataset: a list of tuples of (human readable date, machine readable date) human_vocab: a python dictionary mapping all characters used in the human readable dates to an integer-valued index machine_vocab: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. These indices are not necessarily consistent with human_vocab. inv_machine_vocab: the inverse dictionary of machine_vocab, mapping from indices back to characters. Let’s preprocess the data and map the raw text data into the index values. We will also use Tx=30 (which we assume is the maximum length of the human readable date; if we get a longer input, we would have to truncate it) and Ty=10 (since “YYYY-MM-DD” is 10 characters long). 12345678Tx = 30Ty = 10X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)print("X.shape:", X.shape)print("Y.shape:", Y.shape)print("Xoh.shape:", Xoh.shape)print("Yoh.shape:", Yoh.shape) X.shape: (10000, 30) Y.shape: (10000, 10) Xoh.shape: (10000, 30, 37) Yoh.shape: (10000, 10, 11) You now have: X: a processed version of the human readable dates in the training set, where each character is replaced by an index mapped to the character via human_vocab. Each date is further padded to $T_x$ values with a special character (&lt; pad &gt;). X.shape = (m, Tx) Y: a processed version of the machine readable dates in the training set, where each character is replaced by the index it is mapped to in machine_vocab. You should have Y.shape = (m, Ty). Xoh: one-hot version of X, the “1” entry’s index is mapped to the character thanks to human_vocab. Xoh.shape = (m, Tx, len(human_vocab)) Yoh: one-hot version of Y, the “1” entry’s index is mapped to the character thanks to machine_vocab. Yoh.shape = (m, Tx, len(machine_vocab)). Here, len(machine_vocab) = 11 since there are 11 characters (‘-‘ as well as 0-9). Lets also look at some examples of preprocessed training examples. Feel free to play with index in the cell below to navigate the dataset and see how source/target dates are preprocessed. 123456789index = 0print("Source date:", dataset[index][0])print("Target date:", dataset[index][1])print()print("Source after preprocessing (indices):", X[index])print("Target after preprocessing (indices):", Y[index])print()print("Source after preprocessing (one-hot):", Xoh[index])print("Target after preprocessing (one-hot):", Yoh[index]) Source date: 9 may 1998 Target date: 1998-05-09 Source after preprocessing (indices): [12 0 24 13 34 0 4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36] Target after preprocessing (indices): [ 2 10 10 9 0 1 6 0 1 10] Source after preprocessing (one-hot): [[ 0. 0. 0. ..., 0. 0. 0.] [ 1. 0. 0. ..., 0. 0. 0.] [ 0. 0. 0. ..., 0. 0. 0.] ..., [ 0. 0. 0. ..., 0. 0. 1.] [ 0. 0. 0. ..., 0. 0. 1.] [ 0. 0. 0. ..., 0. 0. 1.]] Target after preprocessing (one-hot): [[ 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [ 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [ 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 2 - Neural machine translation with attentionIf you had to translate a book’s paragraph from French to English, you would not read the whole paragraph, then close the book and translate. Even during the translation process, you would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English you are writing down. The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. 2.1 - Attention mechanismIn this part, you will implement the attention mechanism presented in the lecture videos. Here is a figure to remind you how the model works. The diagram on the left shows the attention model. The diagram on the right shows what one “Attention” step does to calculate the attention variables $\alpha^{\langle t, t’ \rangle}$, which are used to compute the context variable $context^{\langle t \rangle}$ for each timestep in the output ($t=1, \ldots, T_y$). Figure 1: Neural machine translation with attentionHere are some properties of the model that you may notice:1. There are two separate LSTMs in this model (see diagram on the left). Because the one at the bottom of the picture is a Bi-directional LSTM and comes before the attention mechanism, we will call it pre-attention Bi-LSTM. The LSTM at the top of the diagram comes after the attention mechanism, so we will call it the post-attention LSTM. The pre-attention Bi-LSTM goes through $T_x$ time steps; the post-attention LSTM goes through $T_y$ time steps.2. The post-attention LSTM passes $s^{\langle t \rangle}, c^{\langle t \rangle}$ from one time step to the next. In the lecture videos, we were using only a basic RNN for the post-activation sequence model, so the state captured by the RNN output activations $s^{\langle t\rangle}$. But since we are using an LSTM here, the LSTM has both the output activation $s^{\langle t\rangle}$ and the hidden cell state $c^{\langle t\rangle}$. However, unlike previous text generation examples (such as Dinosaurus in week 1), in this model the post-activation LSTM at time $t$ does will not take the specific generated $y^{\langle t-1 \rangle}$ as input; it only takes $s^{\langle t\rangle}$ and $c^{\langle t\rangle}$ as input. We have designed the model this way, because (unlike language generation where adjacent characters are highly correlated) there isn’t as strong a dependency between the previous character and the next character in a YYYY-MM-DD date.3. We use $a^{\langle t \rangle} = [\overrightarrow{a}^{\langle t \rangle}; \overleftarrow{a}^{\langle t \rangle}]$ to represent the concatenation of the activations of both the forward-direction and backward-directions of the pre-attention Bi-LSTM.4. The diagram on the right uses a RepeatVector node to copy $s^{\langle t-1 \rangle}$’s value $T_x$ times, and then Concatenation to concatenate $s^{\langle t-1 \rangle}$ and $a^{\langle t \rangle}$ to compute $e^{\langle t, t’}$, which is then passed through a softmax to compute $\alpha^{\langle t, t’ \rangle}$. We’ll explain how to use RepeatVector and Concatenation in Keras below.Lets implement this model. You will start by implementing two functions: one_step_attention() and model().1) one_step_attention(): At step $t$, given all the hidden states of the Bi-LSTM ($[a^{},a^{}, …, a^{&lt;T_x&gt;}]$) and the previous hidden state of the second LSTM ($s^{}$), one_step_attention() will compute the attention weights ($[\alpha^{&lt;t,1&gt;},\alpha^{&lt;t,2&gt;}, …, \alpha^{&lt;t,T_x&gt;}]$) and output the context vector (see Figure 1 (right) for details): $$context^{} = \sum_{t' = 0}^{T_x} \alpha^{}\tag{1}$$ Note that we are denoting the attention in this notebook $context^{\langle t \rangle}$. In the lecture videos, the context was denoted $c^{\langle t \rangle}$, but here we are calling it $context^{\langle t \rangle}$ to avoid confusion with the (post-attention) LSTM’s internal memory cell variable, which is sometimes also denoted $c^{\langle t \rangle}$.2) model(): Implements the entire model. It first runs the input through a Bi-LSTM to get back $[a^{},a^{}, …, a^{&lt;T_x&gt;}]$. Then, it calls one_step_attention() $T_y$ times (for loop). At each iteration of this loop, it gives the computed context vector $c^{}$ to the second LSTM, and runs the output of the LSTM through a dense layer with softmax activation to generate a prediction $\hat{y}^{}$.Exercise: Implement one_step_attention(). The function model() will call the layers in one_step_attention() $T_y$ using a for-loop, and it is important that all $T_y$ copies have the same weights. I.e., it should not re-initiaiize the weights every time. In other words, all $T_y$ steps should have shared weights. Here’s how you can implement layers with shareable weights in Keras:1. Define the layer objects (as global variables for examples).2. Call these objects when propagating the input.We have defined the layers you need as global variables. Please run the following cells to create them. Please check the Keras documentation to make sure you understand what these layers are: RepeatVector(), Concatenate(), Dense(), Activation(), Dot().1234567# Defined shared layers as global variablesrepeator = RepeatVector(Tx)concatenator = Concatenate(axis=-1)densor1 = Dense(10, activation = "tanh")densor2 = Dense(1, activation = "relu")activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebookdotor = Dot(axes = 1)Now you can use these layers to implement one_step_attention(). In order to propagate a Keras tensor object X through one of these layers, use layer(X) (or layer([X,Y]) if it requires multiple inputs.), e.g. densor(X) will propagate X through the Dense(1) layer defined above.12345678910111213141516171819202122232425262728293031# GRADED FUNCTION: one_step_attentiondef one_step_attention(a, s_prev): """ Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights "alphas" and the hidden states "a" of the Bi-LSTM. Arguments: a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a) s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s) Returns: context -- context vector, input of the next (post-attetion) LSTM cell """ ### START CODE HERE ### # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states "a" (≈ 1 line) s_prev = repeator(s_prev); # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line) concat = concatenator([a, s_prev]); # Use densor1 to propagate concat through a small fully-connected neural network to compute the "intermediate energies" variable e. (≈1 lines) e = densor1(concat); # Use densor2 to propagate e through a small fully-connected neural network to compute the "energies" variable energies. (≈1 lines) energies = densor2(e); # Use "activator" on "energies" to compute the attention weights "alphas" (≈ 1 line) alphas = activator(energies); # Use dotor together with "alphas" and "a" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line) context = dotor([alphas, a]); ### END CODE HERE ### return contextYou will be able to check the expected output of one_step_attention() after you’ve coded the model() function.Exercise: Implement model() as explained in figure 2 and the text above. Again, we have defined global layers that will share weights to be used in model().1234n_a = 32n_s = 64post_activation_LSTM_cell = LSTM(n_s, return_state = True)output_layer = Dense(len(machine_vocab), activation=softmax)Now you can use these layers $T_y$ times in a for loop to generate the outputs, and their parameters will not be reinitialized. You will have to carry out the following steps:1. Propagate the input into a Bidirectional LSTM2. Iterate for $t = 0, \dots, T_y-1$: 1. Call one_step_attention() on $[\alpha^{&lt;t,1&gt;},\alpha^{&lt;t,2&gt;}, …, \alpha^{&lt;t,T_x&gt;}]$ and $s^{}$ to get the context vector $context^{}$. 2. Give $context^{}$ to the post-attention LSTM cell. Remember pass in the previous hidden-state $s^{\langle t-1\rangle}$ and cell-states $c^{\langle t-1\rangle}$ of this LSTM using initial_state= [previous hidden state, previous cell state]. Get back the new hidden state $s^{}$ and the new cell state $c^{}$. 3. Apply a softmax layer to $s^{}$, get the output. 4. Save the output by adding it to the list of outputs.3. Create your Keras model instance, it should have three inputs (“inputs”, $s^{}$ and $c^{}$) and output the list of “outputs”.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# GRADED FUNCTION: modeldef model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size): """ Arguments: Tx -- length of the input sequence Ty -- length of the output sequence n_a -- hidden state size of the Bi-LSTM n_s -- hidden state size of the post-attention LSTM human_vocab_size -- size of the python dictionary "human_vocab" machine_vocab_size -- size of the python dictionary "machine_vocab" Returns: model -- Keras model instance """ # Define the inputs of your model with a shape (Tx,) # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,) X = Input(shape=(Tx, human_vocab_size)) s0 = Input(shape=(n_s,), name='s0') c0 = Input(shape=(n_s,), name='c0') s = s0 c = c0 # Initialize empty list of outputs outputs = [] ### START CODE HERE ### # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line) a = Bidirectional(LSTM(n_a, return_sequences = True))(X); # Step 2: Iterate for Ty steps for t in range(Ty): # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line) context = one_step_attention(a, s); # Step 2.B: Apply the post-attention LSTM cell to the "context" vector. # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line) s, _, c = post_activation_LSTM_cell(context, initial_state= [s, c]); # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line) out = output_layer(s); # Step 2.D: Append "out" to the "outputs" list (≈ 1 line) outputs.append(out); # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line) model = Model(inputs = [X, s0, c0], outputs = outputs); ### END CODE HERE ### return modelRun the following cell to create your model.1model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))Let’s get a summary of the model to check if it matches the expected output.1model.summary() ____ Layer (type) Output Shape Param # Connected to ==================================================================================================== input_1 (InputLayer) (None, 30, 37) 0 ____ s0 (InputLayer) (None, 64) 0 ____ bidirectional_1 (Bidirectional) (None, 30, 64) 17920 input_1[0][0] ____ repeat_vector_1 (RepeatVector) (None, 30, 64) 0 s0[0][0] lstm_1[0][0] lstm_1[1][0] lstm_1[2][0] lstm_1[3][0] lstm_1[4][0] lstm_1[5][0] lstm_1[6][0] lstm_1[7][0] lstm_1[8][0] ____ concatenate_1 (Concatenate) (None, 30, 128) 0 bidirectional_1[0][0] repeat_vector_1[0][0] bidirectional_1[0][0] repeat_vector_1[1][0] bidirectional_1[0][0] repeat_vector_1[2][0] bidirectional_1[0][0] repeat_vector_1[3][0] bidirectional_1[0][0] repeat_vector_1[4][0] bidirectional_1[0][0] repeat_vector_1[5][0] bidirectional_1[0][0] repeat_vector_1[6][0] bidirectional_1[0][0] repeat_vector_1[7][0] bidirectional_1[0][0] repeat_vector_1[8][0] bidirectional_1[0][0] repeat_vector_1[9][0] ____ dense_1 (Dense) (None, 30, 10) 1290 concatenate_1[0][0] concatenate_1[1][0] concatenate_1[2][0] concatenate_1[3][0] concatenate_1[4][0] concatenate_1[5][0] concatenate_1[6][0] concatenate_1[7][0] concatenate_1[8][0] concatenate_1[9][0] ____ dense_2 (Dense) (None, 30, 1) 11 dense_1[0][0] dense_1[1][0] dense_1[2][0] dense_1[3][0] dense_1[4][0] dense_1[5][0] dense_1[6][0] dense_1[7][0] dense_1[8][0] dense_1[9][0] ____ attention_weights (Activation) (None, 30, 1) 0 dense_2[0][0] dense_2[1][0] dense_2[2][0] dense_2[3][0] dense_2[4][0] dense_2[5][0] dense_2[6][0] dense_2[7][0] dense_2[8][0] dense_2[9][0] ____ dot_1 (Dot) (None, 1, 64) 0 attention_weights[0][0] bidirectional_1[0][0] attention_weights[1][0] bidirectional_1[0][0] attention_weights[2][0] bidirectional_1[0][0] attention_weights[3][0] bidirectional_1[0][0] attention_weights[4][0] bidirectional_1[0][0] attention_weights[5][0] bidirectional_1[0][0] attention_weights[6][0] bidirectional_1[0][0] attention_weights[7][0] bidirectional_1[0][0] attention_weights[8][0] bidirectional_1[0][0] attention_weights[9][0] bidirectional_1[0][0] ____ c0 (InputLayer) (None, 64) 0 ____ lstm_1 (LSTM) [(None, 64), (None, 6 33024 dot_1[0][0] s0[0][0] c0[0][0] dot_1[1][0] lstm_1[0][0] lstm_1[0][2] dot_1[2][0] lstm_1[1][0] lstm_1[1][2] dot_1[3][0] lstm_1[2][0] lstm_1[2][2] dot_1[4][0] lstm_1[3][0] lstm_1[3][2] dot_1[5][0] lstm_1[4][0] lstm_1[4][2] dot_1[6][0] lstm_1[5][0] lstm_1[5][2] dot_1[7][0] lstm_1[6][0] lstm_1[6][2] dot_1[8][0] lstm_1[7][0] lstm_1[7][2] dot_1[9][0] lstm_1[8][0] lstm_1[8][2] ____ dense_3 (Dense) (None, 11) 715 lstm_1[0][0] lstm_1[1][0] lstm_1[2][0] lstm_1[3][0] lstm_1[4][0] lstm_1[5][0] lstm_1[6][0] lstm_1[7][0] lstm_1[8][0] lstm_1[9][0] ==================================================================================================== Total params: 52,960 Trainable params: 52,960 Non-trainable params: 0 ____Expected Output:Here is the summary you should see Total params: 52,960 Trainable params: 52,960 Non-trainable params: 0 bidirectional_1’s output shape (None, 30, 64) repeat_vector_1’s output shape (None, 30, 64) concatenate_1’s output shape (None, 30, 128) attention_weights’s output shape (None, 30, 1) dot_1’s output shape (None, 1, 64) dense_3’s output shape (None, 11) As usual, after creating your model in Keras, you need to compile it and define what loss, optimizer and metrics your are want to use. Compile your model using categorical_crossentropy loss, a custom Adam optimizer (learning rate = 0.005, $\beta_1 = 0.9$, $\beta_2 = 0.999$, decay = 0.01) and [&#39;accuracy&#39;] metrics: 1234### START CODE HERE ### (≈2 lines)opt = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, decay=0.01);model.compile(loss = 'categorical_crossentropy',optimizer=opt, metrics = ['accuracy']);### END CODE HERE ### The last step is to define all your inputs and outputs to fit the model: You already have X of shape $(m = 10000, T_x = 30)$ containing the training examples. You need to create s0 and c0 to initialize your post_activation_LSTM_cell with 0s. Given the model() you coded, you need the “outputs” to be a list of 11 elements of shape (m, T_y). So that: outputs[i][0], ..., outputs[i][Ty] represent the true labels (characters) corresponding to the $i^{th}$ training example (X[i]). More generally, outputs[i][j] is the true label of the $j^{th}$ character in the $i^{th}$ training example. 123s0 = np.zeros((m, n_s))c0 = np.zeros((m, n_s))outputs = list(Yoh.swapaxes(0,1)) Let’s now fit the model and run it for one epoch. 1model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100) Epoch 1/1 10000/10000 [==============================] - 31s - loss: 22.1424 - dense_3_loss_1: 2.3001 - dense_3_loss_2: 2.2528 - dense_3_loss_3: 2.3607 - dense_3_loss_4: 2.5894 - dense_3_loss_5: 1.6743 - dense_3_loss_6: 1.9239 - dense_3_loss_7: 2.6330 - dense_3_loss_8: 1.5383 - dense_3_loss_9: 2.0970 - dense_3_loss_10: 2.7730 - dense_3_acc_1: 0.0035 - dense_3_acc_2: 0.0309 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 0.0045 - dense_3_acc_5: 0.9581 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 0.0027 - dense_3_acc_8: 0.9599 - dense_3_acc_9: 0.0051 - dense_3_acc_10: 0.0088 &lt;keras.callbacks.History at 0x7f8fbd556f60&gt; While training you can see the loss as well as the accuracy on each of the 10 positions of the output. The table below gives you an example of what the accuracies could be if the batch had 2 examples: Thus, dense_2_acc_8: 0.89 means that you are predicting the 7th character of the output correctly 89% of the time in the current batch of data. We have run this model for longer, and saved the weights. Run the next cell to load our weights. (By training a model for several minutes, you should be able to obtain a model of similar accuracy, but loading our model will save you time.) 1model.load_weights('models/model.h5') You can now see the results on new examples. 1234567891011EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']for example in EXAMPLES: source = string_to_int(example, Tx, human_vocab) source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1) prediction = model.predict([source, s0, c0]) prediction = np.argmax(prediction, axis = -1) output = [inv_machine_vocab[int(i)] for i in prediction] print("source:", example) print("output:", ''.join(output)) source: 3 May 1979 output: 1979-05-03 source: 5 April 09 output: 2009-05-05 source: 21th of August 2016 output: 2016-08-21 source: Tue 10 Jul 2007 output: 2007-07-10 source: Saturday May 9 2018 output: 2018-05-09 source: March 3 2001 output: 2001-03-03 source: March 3rd 2001 output: 2001-03-03 source: 1 March 2001 output: 2001-03-01 You can also change these examples to test with your own examples. The next part will give you a better sense on what the attention mechanism is doing–i.e., what part of the input the network is paying attention to when generating a particular output character. 3 - Visualizing Attention (Optional / Ungraded)Since the problem has a fixed output length of 10, it is also possible to carry out this task using 10 different softmax units to generate the 10 characters of the output. But one advantage of the attention model is that each part of the output (say the month) knows it needs to depend only on a small part of the input (the characters in the input giving the month). We can visualize what part of the output is looking at what part of the input. Consider the task of translating “Saturday 9 May 2018” to “2018-05-09”. If we visualize the computed $\alpha^{\langle t, t’ \rangle}$ we get this: Figure 8: Full Attention Map Notice how the output ignores the “Saturday” portion of the input. None of the output timesteps are paying much attention to that portion of the input. We see also that 9 has been translated as 09 and May has been correctly translated into 05, with the output paying attention to the parts of the input it needs to to make the translation. The year mostly requires it to pay attention to the input’s “18” in order to generate “2018.” 3.1 - Getting the activations from the networkLets now visualize the attention values in your network. We’ll propagate an example through the network, then visualize the values of $\alpha^{\langle t, t’ \rangle}$. To figure out where the attention values are located, let’s start by printing a summary of the model . 1model.summary() ____________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ==================================================================================================== input_1 (InputLayer) (None, 30, 37) 0 ____________________________________________________________________________________________________ s0 (InputLayer) (None, 64) 0 ____________________________________________________________________________________________________ bidirectional_1 (Bidirectional) (None, 30, 64) 17920 input_1[0][0] ____________________________________________________________________________________________________ repeat_vector_1 (RepeatVector) (None, 30, 64) 0 s0[0][0] lstm_1[0][0] lstm_1[1][0] lstm_1[2][0] lstm_1[3][0] lstm_1[4][0] lstm_1[5][0] lstm_1[6][0] lstm_1[7][0] lstm_1[8][0] ____________________________________________________________________________________________________ concatenate_1 (Concatenate) (None, 30, 128) 0 bidirectional_1[0][0] repeat_vector_1[0][0] bidirectional_1[0][0] repeat_vector_1[1][0] bidirectional_1[0][0] repeat_vector_1[2][0] bidirectional_1[0][0] repeat_vector_1[3][0] bidirectional_1[0][0] repeat_vector_1[4][0] bidirectional_1[0][0] repeat_vector_1[5][0] bidirectional_1[0][0] repeat_vector_1[6][0] bidirectional_1[0][0] repeat_vector_1[7][0] bidirectional_1[0][0] repeat_vector_1[8][0] bidirectional_1[0][0] repeat_vector_1[9][0] ____________________________________________________________________________________________________ dense_1 (Dense) (None, 30, 10) 1290 concatenate_1[0][0] concatenate_1[1][0] concatenate_1[2][0] concatenate_1[3][0] concatenate_1[4][0] concatenate_1[5][0] concatenate_1[6][0] concatenate_1[7][0] concatenate_1[8][0] concatenate_1[9][0] ____________________________________________________________________________________________________ dense_2 (Dense) (None, 30, 1) 11 dense_1[0][0] dense_1[1][0] dense_1[2][0] dense_1[3][0] dense_1[4][0] dense_1[5][0] dense_1[6][0] dense_1[7][0] dense_1[8][0] dense_1[9][0] ____________________________________________________________________________________________________ attention_weights (Activation) (None, 30, 1) 0 dense_2[0][0] dense_2[1][0] dense_2[2][0] dense_2[3][0] dense_2[4][0] dense_2[5][0] dense_2[6][0] dense_2[7][0] dense_2[8][0] dense_2[9][0] ____________________________________________________________________________________________________ dot_1 (Dot) (None, 1, 64) 0 attention_weights[0][0] bidirectional_1[0][0] attention_weights[1][0] bidirectional_1[0][0] attention_weights[2][0] bidirectional_1[0][0] attention_weights[3][0] bidirectional_1[0][0] attention_weights[4][0] bidirectional_1[0][0] attention_weights[5][0] bidirectional_1[0][0] attention_weights[6][0] bidirectional_1[0][0] attention_weights[7][0] bidirectional_1[0][0] attention_weights[8][0] bidirectional_1[0][0] attention_weights[9][0] bidirectional_1[0][0] ____________________________________________________________________________________________________ c0 (InputLayer) (None, 64) 0 ____________________________________________________________________________________________________ lstm_1 (LSTM) [(None, 64), (None, 6 33024 dot_1[0][0] s0[0][0] c0[0][0] dot_1[1][0] lstm_1[0][0] lstm_1[0][2] dot_1[2][0] lstm_1[1][0] lstm_1[1][2] dot_1[3][0] lstm_1[2][0] lstm_1[2][2] dot_1[4][0] lstm_1[3][0] lstm_1[3][2] dot_1[5][0] lstm_1[4][0] lstm_1[4][2] dot_1[6][0] lstm_1[5][0] lstm_1[5][2] dot_1[7][0] lstm_1[6][0] lstm_1[6][2] dot_1[8][0] lstm_1[7][0] lstm_1[7][2] dot_1[9][0] lstm_1[8][0] lstm_1[8][2] ____________________________________________________________________________________________________ dense_3 (Dense) (None, 11) 715 lstm_1[0][0] lstm_1[1][0] lstm_1[2][0] lstm_1[3][0] lstm_1[4][0] lstm_1[5][0] lstm_1[6][0] lstm_1[7][0] lstm_1[8][0] lstm_1[9][0] ==================================================================================================== Total params: 52,960 Trainable params: 52,960 Non-trainable params: 0 ____________________________________________________________________________________________________ Navigate through the output of model.summary() above. You can see that the layer named attention_weights outputs the alphas of shape (m, 30, 1) before dot_2 computes the context vector for every time step $t = 0, \ldots, T_y-1$. Lets get the activations from this layer. The function attention_map() pulls out the attention values from your model and plots them. 1attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, "Tuesday 09 Oct 1993", num = 7, n_s = 64) &lt;matplotlib.figure.Figure at 0x7f8fcf43c748&gt; On the generated plot you can observe the values of the attention weights for each character of the predicted output. Examine this plot and check that where the network is paying attention makes sense to you. In the date translation application, you will observe that most of the time attention helps predict the year, and hasn’t much impact on predicting the day/month. Congratulations!You have come to the end of this assignment Here’s what you should remember from this notebook: Machine translation models can be used to map from one sequence to another. They are useful not just for translating human languages (like French-&gt;English) but also for tasks like date format translation. An attention mechanism allows a network to focus on the most relevant parts of the input when producing a specific part of the output. A network using an attention mechanism can translate from inputs of length $T_x$ to outputs of length $T_y$, where $T_x$ and $T_y$ can be different. You can visualize attention weights $\alpha^{\langle t,t’ \rangle}$ to see what the network is paying attention to while generating each output. Congratulations on finishing this assignment! You are now able to implement an attention model and use it to learn complex mappings from one sequence to another.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sequence models attention mechanism]]></title>
    <url>%2F2018%2F06%2F03%2F03_sequence-models-attention-mechanism%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal lecture note after studying the course nlp sequence models at the 3rd week and the copyright belongs to deeplearning.ai.## 01_various-sequence-to-sequence-architectures 01_basic-modelsHello, and welcome to this final week of this course, as well as to the final week of this sequence of five courses in the deep learning specialization. You’re nearly at the finish line. In this week, you hear about sequence-to-sequence models, which are useful for everything from machine translation to speech recognition. Let’s start with the basic models and then later this week you, hear about beam search, the attention model, and we’ll wrap up the discussion of models for audio data, like speech. Let’s get started. Let’s say you want to input a French sentence like Jane visite l’Afrique en septembre, and you want to translate it to the English sentence, Jane is visiting Africa in September. As usual, let’s use x through x, in this case , to represent the words in the input sequence, and we’ll use y through y to represent the words in the output sequence. So, how can you train a new network to input the sequence x and output the sequence y? Well, here’s something you could do, and the ideas I’m about to present are mainly from these two papers due to Sutskever, Oriol Vinyals, and Quoc Le, and that one by Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwen, and Yoshua Bengio. First, let’s have a network, which we’re going to call the encoder network be built as a RNN, and this could be a GRU and LSTM, feed in the input French words one word at a time. And after ingesting the input sequence, the RNN then offers a vector that represents the input sentence. After that, you can build a decoder network which I’m going to draw here, which takes as input the encoding output by the encoder network shown in black on the left, and then can be trained to output the translation one word at a time until eventually it outputs say, the end of sequence or end the sentence token upon which the decoder stops and as usual we could take the generated tokens and feed them to the next [inaudible] in the sequence like we ‘re doing before when synthesizing text using the language model. One of the most remarkable recent results in deep learning is that this model works, given enough pairs of French and English sentences. If you train the model to input a French sentence and output the corresponding English translation, this will actually work decently well. And this model simply uses an encoder network, whose job it is to find an encoding of the input French sentence and then use a decoder network to then generate the corresponding English translation. An architecture very similar to this also works for image captioning so given an image like the one shown here, maybe wanted to be captioned automatically as a cat sitting on a chair. So how do you train a new network to input an image and output a caption like that phrase up there? Here’s what you can do. From the earlier course on ConvNet you’ve seen how you can input an image into a convolutional network, maybe a pre-trained AlexNet, and have that learn an encoding or learn a set of features of the input image. So, this is actually the AlexNet architecture and if we get rid of this final Softmax unit, the pre-trained AlexNet can give you a 4096-dimensional feature vector of which to represent this picture of a cat. And so this pre-trained network can be the encoder network for the image and you now have a 4096-dimensional vector that represents the image. You can then take this and feed it to an RNN, whose job it is to generate the caption one word at a time. So similar to what we saw with machine translation translating from French to English, you can now input a feature vector describing the input and then have it generate an output sequence or output set of words one word at a time. And this actually works pretty well for image captioning, especially if the caption you want to generate is not too long. As far as I know, this type of model was first proposed by Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille, although it turns out there were multiple groups coming up with very similar models independently and at about the same time. So two other groups that had done very similar work at about the same time and I think independently of Mao et al were Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan, as well as Andrej Karpathy and Fei-Fei Yi. So, you’ve now seen how a basic sequence-to-sequence model works, or how a basic image-to-sequence or image captioning model works, but there are some differences between how you would run a model like this, so generating a sequence compared to how you were synthesizing novel text using a language model. One of the key differences is, you don’t want a randomly chosen translation, you maybe want the most likely translation, or you don’t want a randomly chosen caption, maybe not, but you might want the best caption and most likely caption. So let’s see in the next video how you go about generating that. 02_picking-the-most-likely-sentenceThere are some similarities between the sequence to sequence machine translation model and the language models that you have worked within the first week of this course, but there are some significant differences as well. Let’s take a look. So, you can think of machine translation as building a conditional language model. Here’s what I mean, in language modeling, this was the network we had built in the first week. And this model allows you to estimate the probability of a sentence. That’s what a language model does. And you can also use this to generate novel sentences, and sometimes when you are writing x1 and x2 here, where in this example, x2 would be equal to y1 or equal to y and one is just a feedback. But x1, x2, and so on were not important. So just to clean this up for this slide, I’m going to just cross these off. X1 could be the vector of all zeros and x2, x3 are just the previous output you are generating. So that was the language model. The machine translation model looks as follows, and I am going to use a couple different colors, green and purple, to denote respectively the coded network in green and the decoded network in purple. And you notice that the decoded network looks pretty much identical to the language model that we had up there. So what the machine translation model is, is very similar to the language model, except that instead of always starting along with the vector of all zeros, it instead has an encoded network that figures out some representation for the input sentence, and it takes that input sentence and starts off the decoded network with representation of the input sentence rather than with the representation of all zeros. So, that’s why I call this a conditional language model, and instead of modeling the probability of any sentence, it is now modeling the probability of, say, the output English translation, conditions on some input French sentence. So in other words, you’re trying to estimate the probability of an English translation. Like, what’s the chance that the translation is “Jane is visiting Africa in September,” but conditions on the input French censors like, “Jane visite I’Afrique en septembre.” So, this is really the probability of an English sentence conditions on an input French sentence which is why it is a conditional language model. Now, if you want to apply this model to actually translate a sentence from French into English, given this input French sentence, the model might tell you what is the probability of difference in corresponding English translations. So, x is the French sentence, “Jane visite l’Afrique en septembre.” And, this now tells you what is the probability of different English translations of that French input. And, what you do not want is to sample outputs at random. If you sample words from this distribution, p of y given x, maybe one time you get a pretty good translation, “Jane is visiting Africa in September.” But, maybe another time you get a different translation, “Jane is going to be visiting Africa in September. “ Which sounds a little awkward but is not a terrible translation, just not the best one. And sometimes, just by chance, you get, say, others: “In September, Jane will visit Africa.” And maybe, just by chance, sometimes you sample a really bad translation: “Her African friend welcomed Jane in September.” So, when you’re using this model for machine translation, you’re not trying to sample at random from this distribution. Instead, what you would like is to find the English sentence, y, that maximizes that conditional probability. So in developing a machine translation system, one of the things you need to do is come up with an algorithm that can actually find the value of y that maximizes this term over here. The most common algorithm for doing this is called beam search, and it’s something you’ll see in the next video. But, before moving on to describe beam search, you might wonder, why not just use greedy search? So, what is greedy search? Well, greedy search is an algorithm from computer science which says to generate the first word just pick whatever is the most likely first word according to your conditional language model. Going to your machine translation model and then after having picked the first word, you then pick whatever is the second word that seems most likely, then pick the third word that seems most likely. This algorithm is called greedy search. And, what you would really like is to pick the entire sequence of words, $y^{}, y^{}$ , up to $y^{&lt;T_y&gt;}$, that’s there, that maximizes the joint probability of that whole thing. And it turns out that the greedy approach, where you just pick the best first word, and then, after having picked the best first word, try to pick the best second word, and then, after that, try to pick the best third word, that approach doesn’t really work. To demonstrate that, let’s consider the following two translations. The first one is a better translation, so hopefully, in our machine translation model, it will say that p of y given x is higher for the first sentence. It’s just a better, more succinct translation of the French input. The second one is not a bad translation, it’s just more verbose, it has more unnecessary words. But, if the algorithm has picked “Jane is” as the first two words, because “going” is a more common English word, probably the chance of “Jane is going,” given the French input, this might actually be higher than the chance of “Jane is visiting,” given the French sentence. So, it’s quite possible that if you just pick the third word based on whatever maximizes the probability of just the first three words, you end up choosing option number two. But, this ultimately ends up resulting in a less optimal sentence, in a less good sentence as measured by this model for p of y given x. I know this was may be a slightly hand-wavey argument, but, this is an example of a broader phenomenon, where if you want to find the sequence of words, y1, y2, all the way up to the final word that together maximize the probability, it’s not always optimal to just pick one word at a time. And, of course, the total number of combinations of words in the English sentence is exponentially larger. So, if you have just 10,000 words in a dictionary and if you’re contemplating translations that are up to ten words long, then there are 10000 to the tenth possible sentences that are ten words long. Picking words from the vocabulary size, the dictionary size of 10000 words. So, this is just a huge space of possible sentences, and it’s impossible to rate them all, which is why the most common thing to do is use an approximate search out of them. And, what an approximate search algorithm does, is it will try, it won’t always succeed, but it will to pick the sentence, y, that maximizes that conditional probability. And, even though it’s not guaranteed to find the value of y that maximizes this, it usually does a good enough job. So, to summarize, in this video, you saw how machine translation can be posed as a conditional language modeling problem. But one major difference between this and the earlier language modeling problems is rather than wanting to generate a sentence at random, you may want to try to find the most likely English sentence, most likely English translation. But the set of all English sentences of a certain length is too large to exhaustively enumerate. So, we have to resort to a search algorithm. So, with that, let’s go onto the next video where you’ll learn about beam search algorithm. 03_beam-searchIn this video, you learn about the beam search algorithm. In the last video, you remember how for machine translation given an input French sentence, you don’t want to output a random English translation, you want to output the best and the most likely English translation. The same is also true for speech recognition where given an input audio clip, you don’t want to output a random text transcript of that audio, you want to output the best, maybe the most likely, text transcript. Beam search is the most widely used algorithm to do this. And in this video, you see how to get beam search to work for yourself. Let’s just try Beam Search using our running example of the French sentence, “Jane, visite l’Afrique en Septembre”. Hopefully being translated into, “Jane, visits Africa in September”. The first thing Beam search has to do is try to pick the first words of the English translation, that’s going to operate. So here I’ve listed, say, 10,000 words into vocabulary. And to simplify the problem a bit, I’m going to ignore capitalization. So I’m just listing all the words in lower case. So, in the first step of Beam Search, I use this network fragment with the coalition in green and decoalition in purple, to try to evaluate what is the probability of that for a square. So, what’s the probability of the first output y, given the input sentence x gives the French input. So, whereas greedy search will pick only the one most likely words and move on, Beam Search instead can consider multiple alternatives. So, the Beam Search algorithm has a parameter called B, which is called the beam width and for this example I’m going to set the beam width to be with the three. And what this means is Beam search will cause that not just one possibility but consider three at the time. So in particular, let’s say evaluating this probability over different choices the first words, it finds that the choices in, Jane and September are the most likely three possibilities for the first words in the English outputs. Then Beam search will stowaway in computer memory that it wants to try all of three of these words, and if the beam width parameter were said differently, the beam width parameter was 10, then we keep track of not just three but of the ten, most likely possible choices for the first word. So, to be clear in order to perform this first step of Beam search, what you need to do is run the input French sentence through this encoder network and then this first step will then decode the network, this is a softmax output overall 10,000 possibilities. Then you would take those 10,000 possible outputs and keep in memory which were the top three. Let’s go into the second step of Beam search. Having picked in, Jane and September as the three most likely choice of the first word, what Beam search will do now, is for each of these three choices consider what should be the second word, so after “in” maybe a second word is “a” or maybe as Aaron, I’m just listing words from the vocabulary, from the dictionary or somewhere down the list will be September, somewhere down the list there’s visit and then all the way to z and then the last word is zulu. So, to evaluate the probability of second word, it will use this new network fragments where is coder in green and for the decoder portion when trying to decide what comes after in. Remember the decoder first outputs, y hat one. So, I’m going to set to this y hat one to the word “in” as it goes back in. So there’s the word “in” because it decided for now. That’s because It trying to figure out that the first word was “in”, what is the second word, and then this will output I guess y hat two. And so by hard wiring y hat one here, really the inputs here to be the first words “in” this time were fragment can be used to evaluate whether it’s the probability of the second word given the input french sentence and that the first words of the translation has been the word “in”. Now notice that what we also need help out in this second step would be assertions to find the pair of the first and second words that is most likely it’s not just a second where is most likely that the pair of the first and second whereas the most likely and by the rules of conditional probability. This can be expressed as P of the first words times P of probability of the second words. Which you are getting from this network fragment and so if for each of the three words you’ve chosen “in”, “Jane,” and “September” you save away this probability then you can multiply them by this second probabilities to get the probability of the first and second words. So now you’ve seen how if the first word was “in” how you can evaluate the probability of the second word. Now at first it was “Jane” you do the same thing. The sentence could be “Jane a”,” Jane Aaron”, and so on down to “Jane is”, “Jane visits” and so on. And you will use this in your network fragments let me draw this in as well where here you will hardwire, Y hat One to be Jane. And so with the First word y one hat’s hard wired as Jane than just the network fragments can tell you what’s the probability of the second words to me. And given that the first word is “Jane”. And then same as above you can multiply with P of Y1 to get the probability of Y1 and Y2 for each of these 10,000 different possible choices for the second word. And then finally do the same thing for September although words from a down to Zulu and use this network fragment. That just goes in as well to see if the first word was September. What was the most likely options for the second words. So for this second step of beam search because we’re continuing to use a beam width of three and because there are 10,000 words in the vocabulary you’d end up considering three times 10000 or thirty thousand possibilities because there are 10,000 here, 10,000 here, 10,000 here as the beam width times the number of words in the vocabulary and what you do is you evaluate all of these 30000 options according to the probably the first and second words and then pick the top three. So with a cut down, these 30,000 possibilities down to three again down the beam width rounded again so let’s say that 30,000 choices, the most likely were in September and say Jane is, and Jane visits sorry this bit messy but those are the most likely three out of the 30,000 choices then that’s what Beam’s search would memorize away and take on to the next step being surge. So notice one thing if beam search decides that the most likely choices are the first and second words are in September, or Jane is, or Jane visits. Then what that means is that it is now rejecting September as a candidate for the first words of the output English translation so we’re now down to two possibilities for the first words but we still have a beam width of three keeping track of three choices for pairs of Y1, Y2 before going onto the third step of beam search. Just want to notice that because of beam width is equal to three, every step you instantiate three copies of the network to evaluate these partial sentence fragments and the output. And it’s because of beam width is equal to three that you have three copies of the network with different choices for the first words, but these three copies of the network can be very efficiently used to evaluate all 30,000 options for the second word. So just don’t instantiate 30,000 copies of the network or three copies of the network to very quickly evaluate all 10,000 possible outputs at that softmax output say for Y2. Let’s just quickly illustrate one more step of beam search. So said that the most likely choices for first two words were in September, Jane is, and Jane visits and for each of these pairs of words which we should have saved the way in computer memory the probability of Y1 and Y2 given the input X given the French sentence X. So similar to before, we now want to consider what is the third word. So in September a? In September Aaron? All the way down to is in September Zulu and to evaluate possible choices for the third word, you use this network fragments where you Hardwire the first word here to be in the second word to be September. And so this network fragment allows you to evaluate what’s the probability of the third word given the input French sentence X and given that the first two words are in September and English output. And then you do the same thing for the second fragment. So like so. And same thing for Jane visits and so beam search will then once again pick the top three possibilities may be that things in September. Jane is a likely outcome or Jane is visiting is likely or maybe Jane visits Africa is likely for that first three words and then it keeps going and then you go onto the fourth step of beam search hat one more word and on it goes. And the outcome of this process hopefully will be that adding one word at a time that Beam search will decide that. Jane visits Africa in September will be terminated by the end of sentence symbol using that system is quite common. They’ll find that this is a likely output English sentence and you’ll see more details of this yourself. In this week’s exercise as well where you get to play with beam search yourself. So with a beam of three being searched considers three possibilities at a time. Notice that if the beam width was said to be equal to one, say cause there’s only one, then this essentially becomes the greedy search algorithm which we had discussed in the last video but by considering multiple possibilities say three or ten or some other number at the same time beam search will usually find a much better output sentence than greedy search. You’ve now seen how Beam Search works but it turns out there’s some additional tips and tricks refinements that help you to make beam search work even better. Let’s go onto the next video to take a look. 04_refinements-to-beam-searchIn the last video, you saw the basic beam search algorithm. In this video, you’ll learn some little changes that make it work even better. Length normalization is a small change to the beam search algorithm that can help you get much better results. Here’s what it is. Beam search is maximizing this probability. And this product here is just expressing the observation that P(y1) up to y(Ty), given x, can be expressed as P(y1) given x times P(y2), given x and y1 times dot dot dot, up to I guess p of y Ty given x and y1 up to y t1-1. Maybe this notation is a bit more scary and more intimidating than it needs to be, but this is that probabilities that you see previously. Now, if you’re implementing these, these probabilities are all numbers less than 1. Often they’re much less than 1. And multiplying a lot of numbers less than 1 will result in a tiny, tiny, tiny number, which can result in numerical underflow. Meaning that it’s too small for the floating part representation in your computer to store accurately. So in practice, instead of maximizing this product, we will take logs. And if you insert a log there, then log of a product becomes a sum of a log, and maximizing this sum of log probabilities should give you the same results in terms of selecting the most likely sentence y. So by taking logs, you end up with a more numerically stable algorithm that is less prone to rounding errors, numerical rounding errors, or to really numerical underflow. And because the log function, that’s the logarithmic function, this is strictly monotonically increasing function, maximizing P(y). And because the logarithmic function, here’s the log function, is a strictly monotonically increasing function, we know that maximizing log P(y) given x should give you the same result as maximizing P(y) given x. As in the same value of y that maximizes this should also maximize that. So in most implementations, you keep track of the sum of logs of the probabilities rather than the protocol of probabilities. Now, there’s one other change to this objective function that makes the machine translation algorithm work even better. Which is that, if you referred to this original objective up here, if you have a very long sentence, the probability of that sentence is going to be low, because you’re multiplying as many terms here. Lots of numbers are less than 1 to estimate the probability of that sentence. And so if you multiply all the numbers that are less than 1 together, you just tend to end up with a smaller probability. And so this objective function has an undesirable effect, that maybe it unnaturally tends to prefer very short translations. It tends to prefer very short outputs. Because the probability of a short sentence is determined just by multiplying fewer of these numbers are less than 1. And so the product would just be not quite as small. And by the way, the same thing is true for this. The log of our probability is always less than or equal to 1. You’re actually in this range of the log. So the more terms you have together, the more negative this thing becomes. So there’s one other change to the algorithm that makes it work better, which is instead of using this as the objective you’re trying to maximize, one thing you could do is normalize this by the number of words in your translation. And so this takes the average of the log of the probability of each word. And this significantly reduces the penalty for outputting longer translations. And in practice, as a heuristic instead of dividing by Ty, by the number of words in the output sentence, sometimes you use a softer approach. We have Ty to the power of alpha, where maybe alpha is equal to 0.7. So if alpha was equal to 1, then yeah, completely normalizing by length. If alpha was equal to 0, then, well, Ty to the 0 would be 1, then you’re just not normalizing at all. And this is somewhat in between full normalization, and no normalization, and alpha’s another hyper parameter you have within that you can tune to try to get the best results. And have to admit, using alpha this way, this is a heuristic or this is a hack. There isn’t a great theoretical justification for it, but people have found this works well. People have found that it works well in practice, so many groups will do this. And you can try out different values of alpha and see which one gives you the best result. So just to wrap up how you run beam search, as you run beam search you see a lot of sentences with length equal 1, a lot of sentences with length equal 2, a lot of sentences with length equals 3. And so on, and maybe you run beam search for 30 steps and you consider output sentences up to length 30, let’s say. And so with beam with a 3, you will be keeping track of the top three possibilities for each of these possible sentence lengths, 1, 2, 3, 4 and so on, up to 30. Then, you would look at all of the output sentences and score them against this score. And so you can take your top sentences and just compute this objective function onto sentences that you have seen through the beam search process. And then finally, of all of these sentences that you validate this way, you pick the one that achieves the highest value on this normalized log probability objective. Sometimes it’s called a normalized log likelihood objective. And then that would be the final translation, your outputs. So that’s how you implement beam search, and you get to play this yourself in this week’s problem exercise. Finally, a few implementational details, how do you choose the beam width B? The larger B is, the more possibilities you’re considering, and does the better the sentence you probably find. But the larger B is, the more computationally expensive your algorithm is, because you’re also keeping a lot more possibilities around. All right, so finally, let’s just wrap up with some thoughts on how to choose the beam width B. So here are the pros and cons of setting B to be very large versus very small. If the beam width is very large, then you consider a lot of possibilities, and so you tend to get a better result because you are consuming a lot of different options, but it will be slower. And the memory requirements will also grow, will also be compositionally slower. Whereas if you use a very small beam width, then you get a worse result because you’re just keeping less possibilities in mind as the algorithm is running. But you get a result faster and the memory requirements will also be lower. So in the previous video, we used in our running example a beam width of three, so we’re keeping three possibilities in mind. In practice, that is on the small side. In production systems, it’s not uncommon to see a beam width maybe around 10, and I think beam width of 100 would be considered very large for a production system, depending on the application. But for research systems where people want to squeeze out every last drop of performance in order to publish the paper with the best possible result. It’s not uncommon to see people use beam widths of 1,000 or 3,000, but this is very application, that’s why it’s a domain dependent. So I would say try other variety of values of B as you work through your application. But when B gets very large, there is often diminishing returns. So for many applications, I would expect to see a huge gain as you go from a beam widht of 1, which is very greedy search, to 3, to maybe 10. But the gains as you go from 1,000 to 3,000 in beam width might not be as big. And for those of you that have taken maybe a lot of computer science courses before, if you’re familiar with computer science search algorithms like BFS, Breadth First Search, or DFS, Depth First Search. The way to think about beam search is that, unlike those other algorithms which you have learned about in a computer science algorithms course, and don’t worry about it if you’ve not heard of these algorithms. But if you’ve heard of Breadth First Search and Depth First Search then unlike those algorithms, which are exact search algorithms. Beam search runs much faster but does not guarantee to find the exact maximum for this argmax that you would like to find. If you haven’t heard of breadth first search or depth first search, don’t worry about it, it’s not important for our purposes. But if you have, this is how beam search relates to those algorithms. So that’s it for beam search, which is a widely used algorithm in many production systems, or in many commercial systems. Now, in the circles in the sequence of courses of deep learning, we talked a lot about error analysis. It turns out, one of the most useful tools I’ve found is to be able to do error analysis on beam search. So you sometimes wonder, should I increase my beam width? Is my beam width working well enough? And there’s some simple things you can compute to give you guidance on whether you need to work on improving your search algorithm. Let’s talk about that in the next video. 05_error-analysis-in-beam-searchIn the third course of this sequence of five courses, you saw how error analysis can help you focus your time on doing the most useful work for your project. Now, beam search is an approximate search algorithm, also called a heuristic search algorithm. And so it doesn’t always output the most likely sentence. It’s only keeping track of B equals 3 or 10 or 100 top possibilities. So what if beam search makes a mistake? In this video, you’ll learn how error analysis interacts with beam search and how you can figure out whether it is the beam search algorithm that’s causing problems and worth spending time on. Or whether it might be your RNN model that is causing problems and worth spending time on. Let’s take a look at how to do error analysis with beam search. Let’s use this example of Jane visite l’Afrique en septembre. So let’s say that in your machine translation dev set, your development set, the human provided this translation and Jane visits Africa in September, and I’m going to call this y. So it is a pretty good translation written by a human. Then let’s say that when you run beam search on your learned RNN model and your learned translation model, it ends up with this translation, which we will call y-hat, Jane visited Africa last September, which is a much worse translation of the French sentence. It actually changes the meaning, so it’s not a good translation. Now, your model has two main components. There is a neural network model, the sequence to sequence model. We shall just call this your RNN model. It’s really an encoder and a decoder. And you have your beam search algorithm, which you’re running with some beam width b. And wouldn’t it be nice if you could attribute this error, this not very good translation, to one of these two components? Was it the RNN or really the neural network that is more to blame, or is it the beam search algorithm, that is more to blame? And what you saw in the third course of the sequence is that it’s always tempting to collect more training data that never hurts. So in similar way, it’s always tempting to increase the beam width that never hurts or pretty much never hurts. But just as getting more training data by itself might not get you to the level of performance you want. In the same way, increasing the beam width by itself might not get you to where you want to go. But how do you decide whether or not improving the search algorithm is a good use of your time? So just how you can break the problem down and figure out what’s actually a good use of your time. Now, the RNN, the neural network, what was called RNN really means the encoder and the decoder. It computes P(y given x). So for example, for a sentence, Jane visits Africa in September, you plug in Jane visits Africa. Again, I’m ignoring upper versus lowercase now, right, and so on. And this computes P(y given x). So it turns out that the most useful thing for you to do at this point is to compute using this model to compute P(y given x) as well as to compute P(y-hat given x) using your RNN model. And then to see which of these two is bigger. So it’s possible that the left side is bigger than the right hand side. It’s also possible that P(y*) is less than P(y-hat) actually, or less than or equal to, right? Depending on which of these two cases hold true, you’d be able to more clearly ascribe this particular error, this particular bad translation to one of the RNN or the beam search algorithm being had greater fault. So let’s take out the logic behind this. Here are the two sentences from the previous slide. And remember, we’re going to compute P(y given x) and P(y-hat given x) and see which of these two is bigger. So there are going to be two cases. In case 1, P(y given x) as output by the RNN model is greater than P(y-hat given x). What does this mean? Well, the beam search algorithm chose y-hat, right? The way you got y-hat was you had an RNN that was computing P(y given x). And beam search’s job was to try to find a value of y that gives that arg max. But in this case, y actually attains a higher value for P(y given x) than the y-hat. So what this allows you to conclude is beam search is failing to actually give you the value of y that maximizes P(y given x) because the one job that beam search had was to find the value of y that makes this really big. But it chose y-hat, the y actually gets a much bigger value. So in this case, you could conclude that beam search is at fault. Now, how about the other case? In case 2, P(y given x) is less than or equal to P(y-hat given x), right? And then either this or this has gotta be true. So either case 1 or case 2 has to hold true. What do you conclude under case 2? Well, in our example, y is a better translation than y-hat. But according to the RNN, P(y) is less than P(y-hat), so saying that y is a less likely output than y-hat. So in this case, it seems that the RNN model is at fault and it might be worth spending more time working on the RNN. There’s some subtleties here pertaining to length normalizations that I’m glossing over. There’s some subtleties pertaining to length normalizations that I’m glossing over. And if you are using some sort of length normalization, instead of evaluating these probabilities, you should be evaluating the optimization objective that takes into account length normalization. But ignoring that complication for now, in this case, what this tells you is that even though y is a better translation, the RNN ascribed y in lower probability than the inferior translation. So in this case, I will say the RNN model is at fault. So the error analysis process looks as follows. You go through the development set and find the mistakes that the algorithm made in the development set. And so in this example, let’s say that P(y given x) was 2 x 10 to the -10, whereas, P(y-hat given x) was 1 x 10 to the -10. Using the logic from the previous slide, in this case, we see that beam search actually chose y-hat, which has a lower probability than y. So I will say beam search is at fault. So I’ll abbreviate that B. And then you go through a second mistake or second bad output by the algorithm, look at these probabilities. And maybe for the second example, you think the model is at fault. I’m going to abbreviate the RNN model with R. And you go through more examples. And sometimes the beam search is at fault, sometimes the model is at fault, and so on. And through this process, you can then carry out error analysis to figure out what fraction of errors are due to beam search versus the RNN model. And with an error analysis process like this, for every example in your dev sets, where the algorithm gives a much worse output than the human translation, you can try to ascribe the error to either the search algorithm or to the objective function, or to the RNN model that generates the objective function that beam search is supposed to be maximizing. And through this, you can try to figure out which of these two components is responsible for more errors. And only if you find that beam search is responsible for a lot of errors, then maybe is we’re working hard to increase the beam width. Whereas in contrast, if you find that the RNN model is at fault, then you could do a deeper layer of analysis to try to figure out if you want to add regularization, or get more training data, or try a different network architecture, or something else. And so a lot of the techniques that you saw in the third course in the sequence will be applicable there. So that’s it for error analysis using beam search. I found this particular error analysis process very useful whenever you have an approximate optimization algorithm, such as beam search that is working to optimize some sort of objective, some sort of cost function that is output by a learning algorithm, such as a sequence-to-sequence model or a sequence-to-sequence RNN that we’ve been discussing in these lectures. So with that, I hope that you’ll be more efficient at making these types of models work well for your applications. 06_bleu-score-optionalOne of the challenges of machine translation is that, given a French sentence, there could be multiple English translations that are equally good translations of that French sentence. So how do you evaluate a machine translation system if there are multiple equally good answers, unlike, say, image recognition where there’s one right answer? You just measure accuracy. If there are multiple great answers, how do you measure accuracy? The way this is done conventionally is through something called the BLEU score. So, in this optional video, I want to share with you, I want to give you a sense of how the BLEU score works. Let’s say you are given a French sentence Le chat est sur le tapis. And you are given a reference, human generated translation of this, which is the the cat is on the mat. But there are multiple, pretty good translations of this. So a different human, different person might translate it as there is a cat on the mat. And both of these are actually just perfectly fine translations of the French sentence. What the BLEU score does is given a machine generated translation, it allows you to automatically compute a score that measures how good is that machine translation. And the intuition is so long as the machine generated translation is pretty close to any of the references provided by humans, then it will get a high BLEU score. BLEU, by the way, stands for bilingual evaluation, Understudy. So in the theater world, an understudy is someone that learns the role of a more senior actor so they can take over the role of the more senior actor, if necessary. And motivation for BLEU is that, whereas you could ask human evaluators to evaluate the machine translation system, the BLEU score is an understudy, could be a substitute for having humans evaluate every output of a machine translation system. So the BLEU score was due to Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. This paper has been incredibly influential, and is, actually, quite a readable paper. So I encourage you to take a look if you have time. So, the intuition behind the BLEU score is we’re going to look at the machine generated output and see if the types of words it generates appear in at least one of the human generated references. And so these human generated references would be provided as part of the depth set or as part of the test set. Now, let’s look at a somewhat extreme example. Let’s say that the machine translation system abbreviating machine translation is MT. So the machine translation, or the MT output, is the the the the the the the. So this is clearly a pretty terrible translation. So one way to measure how good the machine translation output is, is to look at each the words in the output and see if it appears in the references. And so, this would be called a precision of the machine translation output. And in this case, there are seven words in the machine translation output. And every one of these 7 words appears in either Reference 1 or Reference 2, right? So the word the appears in both references. So each of these words looks like a pretty good word to include. So this will have a precision of 7 over 7. It looks like it was a great precision. So this is why the basic precision measure of what fraction of the words in the MT output also appear in the references. This is not a particularly useful measure, because it seems to imply that this MT output has very high precision. So instead, what we’re going to use is a modified precision measure in which we will give each word credit only up to the maximum number of times it appears in the reference sentences. So in Reference 1, the word, the, appears twice. In Reference 2, the word, the, appears just once. So 2 is bigger than 1, and so we’re going to say that the word, the, gets credit up to twice. So, with a modified precision, we will say that, it gets a score of 2 out of 7, because out of 7 words, we’ll give it a 2 credits for appearing. So here, the denominator is the count of the number of times the word, the, appears of 7 words in total. And the numerator is the count of the number of times the word, the, appears. We clip this count, we take a max, or we clip this count, at 2. So this gives us the modified precision measure. Now, so far, we’ve been looking at words in isolation. In the BLEU score, you don’t want to just look at isolated words. You maybe want to look at pairs of words as well. Let’s define a portion of the BLEU score on bigrams. And bigrams just means pairs of words appearing next to each other. So now, let’s see how we could use bigrams to define the BLEU score. And this will just be a portion of the final BLEU score. And we’ll take unigrams, or single words, as well as bigrams, which means pairs of words into account as well as maybe even longer sequences of words, such as trigrams, which means three words pairing together. So, let’s continue our example from before. We have to same Reference 1 and Reference 2. But now let’s say the machine translation or the MT System has a slightly better output. The cat the cat on the mat. Still not a great translation, but maybe better than the last one. So here, the possible bigrams are, well there’s the cat, but ignore case. And then there’s cat the, that’s another bigram. And then there’s the cat again, but I’ve already had that, so let’s skip that. And then cat on is the next one. And then on the, and the mat. So these are the bigrams in the machine translation output. And so let’s count up, How many times each of these bigrams appear. The cat appears twice, cat the appears once, and the others all appear just once. And then finally, let’s define the clipped count, so count, and then subscript clip. And to define that, let’s take this column of numbers, but give our algorithm credit only up to the maximum number of times that that bigram appears in either Reference 1 or Reference 2. So the cat appears a maximum of once in either of the references. So I’m going to clip that count to 1. Cat the, well, it doesn’t appear in Reference 1 or Reference 2, so I clip that to 0. Cat on, yep, that appears once. We give it credit for once. On the appears once, give that credit for once, and the mat appears once. So these are the clipped counts. We’re taking all the counts and clipping them, really reducing them to be no more than the number of times that bigram appears in at least one of the references. And then, finally, our modified bigram precision will be the sum of the count clipped. So that’s 1, 2, 3, 4 divided by the total number of bigrams. That’s 2, 3, 4, 5, 6, so 4 out of 6 or two-thirds is the modified precision on bigrams. So let’s just formalize this a little bit further. With what we had developed on unigrams, we defined this modified precision computed on unigrams as P subscript 1. The P stands for precision and the subscript 1 here means that we’re referring to unigrams. But that is defined as sum over the unigrams. So that just means sum over the words that appear in the machine translation output. So this is called y hat of count clip, Of that unigram. Divided by sum of our unigrams in the machine translation output of count, number of counts of that unigram, right? And so this is what we had gotten I guess is 2 out of 7, 2 slides back. So the 1 here refers to unigram, meaning we’re looking at single words in isolation. You can also define Pn as the n-gram version, Instead of unigram, for n-gram. So this would be sum over the n-grams in the machine translation output of count clip of that n-gram divided by sum over n-grams of the count of that n-gram. And so these precisions, or these modified precision scores, measured on unigrams or on bigrams, which we did on a previous slide, or on trigrams, which are triples of words, or even higher values of n for other n-grams. This allows you to measure the degree to which the machine translation output is similar or maybe overlaps with the references. And one thing that you could probably convince yourself of is if the MT output is exactly the same as either Reference 1 or Reference 2, then all of these values P1, and P2 and so on, they’ll all be equal to 1.0. So to get a modified precision of 1.0, you just have to be exactly equal to one of the references. And sometimes it’s possible to achieve this even if you aren’t exactly the same as any of the references. But you kind of combine them in a way that hopefully still results in a good translation. Finally, Finally, let’s put this together to form the final BLEU score. So P subscript n is the BLEU score computed on n-grams only. Also the modified precision computed on n-grams only. And by convention to compute one number, you compute P1, P2, P3 and P4, and combine them together using the following formula. It’s going to be the average, so sum from n = 1 to 4 of Pn and divide that by 4. So basically taking the average. By convention the BLEU score is defined as, e to the this, then exponentiations, and linear operate, exponentiation is strictly monotonically increasing operation and then we actually adjust this with one more factor called the, BP penalty. So BP, Stands for brevity penalty. The details maybe aren’t super important. But to just give you a sense, it turns out that if you output very short translations, it’s easier to get high precision. Because probably most of the words you output appear in the references. But we don’t want translations that are very short. So the BP, or the brevity penalty, is an adjustment factor that penalizes translation systems that output translations that are too short. So the formula for the brevity penalty is the following. $${P_n}{\rm{ = }}\frac{{\sum\limits_{n - gram \in \widehat y} {Coun{t_{clip}}(n - gram)} }}{{\sum\limits_{n - gram \in \widehat y} {Count(n - gram)} }}$$ $$BP \exp(\dfrac{1}{4}\sum_{n=1}^{4}P_{n})$$ $$BP = \left\{ \begin{array}{l} 1, if{\kern 1pt} {\kern 1pt} MT\_length > reference\_length{\kern 1pt} {\kern 1pt} \\ \exp (1 - MT\_length/reference\_length), otherwise \end{array} \right.$$ It’s equal to 1 if your machine translation system actually outputs things that are longer than the human generated reference outputs. And otherwise is some formula like that that overall penalizes shorter translations. So, in the details you can find in this paper. So, once again, earlier in this set of courses, you saw the importance of having a single real number evaluation metric. Because it allows you to try out two ideas, see which one achieves a higher score, and then try to stick with the one that achieved the higher score. So the reason the BLEU score was revolutionary for machine translation was because this gave a pretty good, by no means perfect, but pretty good single real number evaluation metric. And so that accelerated the progress of the entire field of machine translation. I hope this video gave you a sense of how the BLEU score works. In practice, few people would implement a BLEU score from scratch. There are open source implementations that you can download and just use to evaluate your own system. But today, BLEU score is used to evaluate many systems that generate text, such as machine translation systems, as well as the example I showed briefly earlier of image captioning systems where you would have a system, have a neural network generated image caption. And then use the BLEU score to see how much that overlaps with maybe a reference caption or multiple reference captions that were generated by people. So the BLEU score is a useful single real number evaluation metric to use whenever you want your algorithm to generate a piece of text. And you want to see whether it has similar meaning as a reference piece of text generated by humans. This is not used for speech recognition, because in speech recognition, there’s usually one ground truth. And you just use other measures to see if you got the speech transcription on pretty much, exactly word for word correct. But for things like image captioning, and multiple captions for a picture, it could be about equally good or for machine translations. There are multiple translations, but equally good. The BLEU score gives you a way to evaluate that automatically and therefore speed up your development. So with that, I hope you have a sense of how the BLEU score works. 07_attention-model-intuitionFor most of this week, you’ve been using a Encoder-Decoder architecture for machine translation. Where one RNN reads in a sentence and then different one outputs a sentence. There’s a modification to this called the Attention Model, that makes all this work much better. The attention algorithm, the attention idea has been one of the most influential ideas in deep learning. Let’s take a look at how that works. Get a very long French sentence like this. What we are asking this green encoder in your network to do is, to read in the whole sentence and then memorize the whole sentences and store it in the activations conveyed here. Then for the purple network, the decoder network till then generate the English translation. Jane went to Africa last September and enjoyed the culture and met many wonderful people; she came back raving about how wonderful her trip was, and is tempting me to go too. Now, the way a human translator would translate this sentence is not to first read the whole French sentence and then memorize the whole thing and then regurgitate an English sentence from scratch. Instead, what the human translator would do is read the first part of it, maybe generate part of the translation. Look at the second part, generate a few more words, look at a few more words, generate a few more words and so on. You kind of work part by part through the sentence, because it’s just really difficult to memorize the whole long sentence like that. What you see for the Encoder-Decoder architecture above is that, it works quite well for short sentences, so we might achieve a relatively high Bleu score, but for very long sentences, maybe longer than 30 or 40 words, the performance comes down. The Bleu score might look like this as the sentence that varies and short sentences are just hard to translate, hard to get all the words, right? Long sentences, it doesn’t do well on because it’s just difficult to get in your network to memorize a super long sentence. In this and the next video, you’ll see the Attention Model which translates maybe a bit more like humans might, looking at part of the sentence at a time and with an Attention Model, machine translation systems performance can look like this, because by working one part of the sentence at a time, you don’t see this huge dip which is really measuring the ability of a neural network to memorize a long sentence which maybe isn’t what we most badly need a neural network to do. In this video, I want to just give you some intuition about how attention works and then we’ll flesh out the details in the next video. The Attention Model was due to Dimitri, Bahdanau, Camcrun Cho, Yoshe Bengio and even though it was obviously developed for machine translation, it spread to many other application areas as well. This is really a very influential, I think very seminal paper in the deep learning literature. Let’s illustrate this with a short sentence, even though these ideas were maybe developed more for long sentences, but it’ll be easier to illustrate these ideas with a simpler example. We have our usual sentence, Jane visite l’Afrique en Septembre. Let’s say that we use a R and N, and in this case, I’m going to use a bidirectional R and N, in order to compute some set of features for each of the input words and you have to understand it, bidirectional RNN with outputs Y1 to Y3 and so on up to Y5 but we’re not doing a word for word translation, let me get rid of the Y’s on top. But using a bidirectional R and N, what we’ve done is for each other words, really for each of the five positions into sentence, you can compute a very rich set of features about the words in the sentence and maybe surrounding words in every position. Now, let’s go ahead and generate the English translation. We’re going to use another RNN to generate the English translations. Here’s my RNN note as usual and instead of using A to denote the activation, in order to avoid confusion with the activations down here, I’m just going to use a different notation, I’m going to use S to denote the hidden state in this RNN up here, so instead of writing A1 I’m going to right S1 and so we hope in this model that the first word it generates will be Jane, to generate Jane visits Africa in September. Now, the question is, when you’re trying to generate this first word, this output, what part of the input French sentence should you be looking at? Seems like you should be looking primarily at this first word, maybe a few other words close by, but you don’t need to be looking way at the end of the sentence. What the Attention Model would be computing is a set of attention weights and we’re going to use $\alpha^{&lt;1, 1&gt;}$ to denote when you’re generating the first words, how much should you be paying attention to this first piece of information here. And then we’ll also come up with a second that’s called Attention Weight, $\alpha^{&lt;1, 2&gt;}$ which tells us what we’re trying to compute the first work of Jane, how much attention we’re paying to this second work from the inputs and so on and the $\alpha^{&lt;1, 3&gt;}$ and so on, and together this will tell us what is exactly the context from denoter C that we should be paying attention to, and that is input to this RNN unit to then try to generate the first words. That’s one step of the R and N, we will flesh out all these details in the next video. For the second step of this R and N, we’re going to have a new hidden state S two and we’re going to have a new set of the attention weights. We’re going to have $\alpha^{&lt;2, 1&gt;}$ to tell us when we generate in the second word. I guess this will be visits maybe that being the ground trip label. How much should we paying attention to the first word in the french input and also, $\alpha^{&lt;2, 2&gt;}$ and so on. How much should we paying attention the word visite, how much should we pay attention to the free and so on. And of course, the first word we generate in Jane is also an input to this, and then we have some context that we’re paying attention to and the second step, there’s also an input and that together will generate the second word and that leads us to the third step, S three, where this is an input and we have some new context C that depends on the various $\alpha^{&lt;3, t&gt;}$ for the different time sets, that tells us how much should we be paying attention to the different words from the input French sentence and so on. So, some things I haven’t specified yet, but that will go further into detail in the next video of this, how exactly this context defines and the goal of the context is for the third word is really should capture that maybe we should be looking around this part of the sentence. The formula you use to do that will defer to the next video as well as how do you compute these attention weights. And you see in the next video that $\alpha^{&lt;3, t&gt;}$, which is, when you’re trying to generate the third word, I guess this would be the Africa, just getting the right output. The amounts that this RNN step should be paying attention to the French word that time T, that depends on the activations of the bidirectional RNN at time T, I guess it depends on the fourth activations and the, backward activations at time T and it will depend on the state from the previous steps, it will depend on S two, and these things together will influence, how much you pay attention to a specific word in the input French sentence. But we’ll flesh out all these details in the next video. But the key intuition to take away is that this way the RNN marches forward generating one word at a time, until eventually it generates maybe the EOS and at every step, there are these attention weighs. $\alpha^{&lt;t, t’&gt;}$ that tells it, when you’re trying to generate the T, English word, how much should you be paying attention to the T prime French words.And this allows it on every time step to look only maybe within a local window of the French sentence to pay attention to, when generating a specific English word. I hope this video conveys some intuition about Attention Model and that we now have a rough sense of, maybe how the algorithm works. Let’s go to the next video to flesh out the details of the Attention Model. 08_attention-modelIn the last video, you saw how the attention model allows a neural network to pay attention to only part of an input sentence while it’s generating a translation, much like a human translator might. Let’s now formalize that intuition into the exact details of how you would implement an attention model. So same as in the previous video, let’s assume you have an input sentence and you use a bidirectional RNN, or bidirectional GRU, or bidirectional LSTM to compute features on every word. In practice, GRUs and LSTMs are often used for this, with maybe LSTMs be more common. And so for the forward occurrence, you have a forward occurrence first time step. Activation backward occurrence, first time step. Activation forward occurrence, second time step. Activation backward and so on. For all of them in just a forward fifth time step a backwards fifth time step. We had a zero here technically we can also have I guess a backwards sixth as a factor of all zero, actually that’s a factor of all zeroes. And then to simplify the notation going forwards at every time step, even though you have the features computed from the forward occurrence and from the backward occurrence in the bidirectional RNN. I’m just going to use $a^{t’}$ to represent both of these concatenated together, $a^{&lt;t^{&lt;\prime&gt;}&gt;}=({\overrightarrow a^{&lt;t^{\prime}&gt;}},{\overleftarrow a^{&lt;t^{\prime}&gt;}})$. So a of t is going to be a feature vector for time step t. Although to be consistent with notation, we’re using second, I’m going to call this $t^\prime$. Actually, I’m going to use $t^{\prime}$ to index into the words in the French sentence. Next, we have our forward only, so it’s a single direction RNN with state s to generate the translation. And so the first time step, it should generate $y^{}$ and just will have as input some context C. And if you want to index it with time I guess you could write a $C^{}$ but sometimes I just right C without the superscript one. And this will depend on the attention parameters so $\alpha^{&lt;1,1&gt;}$, $\alpha^{&lt;1,2&gt;}$ and so on tells us how much attention. And so these alpha parameters tells us how much the context would depend on the features we’re getting or the activations we’re getting from the different time steps. And so the way we define the context is actually be a way to some of the features from the different time steps weighted by these attention weights. So more formally the attention weights will satisfy this that they are all be non-negative, so it will be a zero positive and they’ll sum to one. We’ll see later how to make sure this is true. And we will have the context or the context at time one often drop that superscript that’s going to be sum over $t^{\prime}$, all the values of $t^{\prime}$ of this weighted sum of these activations $c^{} = \sum\alpha^{&lt;1, t^{\prime}&gt;}a^{&lt;t^{\prime}&gt;}$. So this term, $\alpha^{&lt;1, t^{\prime}&gt;}$, here are the attention weights and this term, $a^{&lt;t^{\prime}&gt;}$, here comes from here $a^{&lt;t^{\prime}&gt;}=({\overrightarrow a^{&lt;t^{\prime}&gt;}},{\overleftarrow a ^{&lt;t^{\prime}&gt;}})$. So $\alpha^{&lt;t, t^{\prime}&gt;}$ is the amount of attention that’s $y^t$ should pay to $a^{t^{\prime}}$. So in other words, when you’re generating the t of the output words, how much you should be paying attention to the $t^{\prime}$th input to word. So that’s one step of generating the output and then at the next time step, you generate the second output and is again done some of where now you have a new set of attention weights on they to find a new way to sum. That generates a new context. This, $y^{}$, is also input and that allows you to generate the second word. Only now just this way to sum becomes the context of the second time step is $c^{} = \sum\alpha^{&lt;2, t^{\prime}&gt;}a^{&lt;t^{\prime}&gt;}$. So using these context vectors. $c^{}$ right there back, $c^{}$, and so on. This network uo here, which circled in purple color, here looks like a pretty standard RNN sequence with the context vectors as output and we can just generate the translation one word at a time. We have also define how to compute the context vectors in terms of these attention ways and those features of the input sentence. So the only remaining thing to do is to define how to actually compute these attention weights. Let’s do that on the next slide. So just to recap, $\alpha^{&lt;t, t^{\prime}&gt;}$ is the amount of attention you should paid to $a^{&lt;t^{\prime}&gt;}$ when you’re trying to generate the $t^{th}$ words in the output translation. So let me just write down the formula and we talk of how this works. This is formula you could use the compute $\alpha^{&lt;t, t^{\prime}&gt;}$ which is going to compute these terms $e^{&lt;t, t^{\prime}&gt;}$ and then use essentially a softmax to make sure that these weights sum to one if you sum over $t^{\prime}$. So for every fix value of t, these things, ${\alpha^{}} =\frac{{\exp({e^{}})}}{{\sum\limits_{t^{\prime} = 1}^{{T_x}} {\exp({e^{}})}}}$ , sum to one if you’re summing over $t^{\prime}$. And using this softmax prioritization, just ensures this properly sums to one. Now how do we compute these factors e. Well, one way to do so is to use a small neural network as follows. So $s^{}$ was the neural network state from the previous time step. So here is the network we have.If you’re trying to generate $y^t$ then $s^{}$ was the hidden state from the previous step that just fell into $s^t$ and that’s one input to very small neural network. Usually, one hidden layer in neural network because you need to compute these a lot. And then $a^{&lt;t^{\prime}&gt;}$ the features from time step $t^{\prime}$ is the other inputs. And the intuition is, if you want to decide how much attention to pay to the activation of $t^{\prime}$. Well, the things that seems like it should depend the most on is what is your own hidden state activation from the previous time step. You don’t have the current state activation yet because of context feeds into this so you haven’t computed that. But look at whatever you’re hidden stages of this RNN generating the upper translation and then for each of the positions, each of the words look at their features. So it seems pretty natural that $\alpha^{&lt;t, t^{\prime}&gt;}$ and $e^{&lt;t, t^{\prime}&gt;}$ should depend on these two quantities. But we don’t know what the function is. So one thing you could do is just train a very small neural network to learn whatever this function should be. And trust that back propagation trust gradient descent to learn the right function. And it turns out that if you implemented this whole model and train it with gradient descent, the whole thing actually works. This little neural network does a pretty decent job telling you how much attention $y^t$ should pay to $a^{&lt;t^{\prime}&gt;}$and this formula ${\alpha^{}} =\frac{{\exp({e^{}})}}{{\sum\limits_{t^{\prime} = 1}^{{T_x}} {\exp({e^{}})}}}$ makes sure that the attention weights sum to one and then as you chug along generating one word at a time, this neural network actually pays attention to the right parts of the input sentence that learns all this automatically using gradient descent.Now, one downside to this algorithm is that it does take quadratic time or quadratic cost to run this algorithm. If you have $T_x$ words in the input and $T_y$ words in the output then the total number of these attention parameters are going to be $T_x$ times $T_y$. And so this algorithm runs in quadratic cost. Although in machine translation applications where neither input nor output sentences is usually that long maybe quadratic cost is actually acceptable. Although, there is some research work on trying to reduce costs as well. Now, so far up in describing the attention idea in the context of machine translation. Without going too much into detail this idea has been applied to other problems as well. So just image captioning. So in the image capturing problem the task is to look at the picture and write a caption for that picture. So in this paper set to the bottom by Kevin Chu, Jimmy Barr, Ryan Kiros, Kelvin Shaw, Aaron Korver, Russell Zarkutnov, Virta Zemo, and Andrew Benjo they also showed that you could have a very similar architecture. Look at the picture and pay attention only to parts of the picture at a time while you’re writing a caption for a picture. So if you’re interested, then I encourage you to take a look at that paper as well. And you get to play with all this and more in the programming exercise.Whereas machine translation is a very complicated problem in the prior exercise you get to implement and play of the attention while you yourself for the date normalization problem. So the problem inputting a date like this. This actually has a date of the Apollo Moon landing and normalizing it into standard formats or a date like this and having a neural network a sequence, sequence model normalize it to this format. This by the way is the birthday of William Shakespeare. Also it’s believed to be. And what you see in prior exercises as you can train a neural network to input dates in any of these formats and have it use an attention model to generate a normalized format for these dates. One other thing that sometimes fun to do is to look at the visualizations of the attention weights. So here’s a machine translation example and here were plotted in different colors. the magnitude of the different attention weights. I don’t want to spend too much time on this but you find that the corresponding input and output words you find that the attention weights will tend to be high. Thus, suggesting that when it’s generating a specific word in output is, usually paying attention to the correct words in the input and all this including learning where to pay attention when was all learned using propagation with an attention model. So that’s it for the attention model really one of the most powerful ideas in deep learning. I hope you enjoy implementing and playing with these ideas yourself later in this week’s programming exercises. 02_speech-recognition-audio-data01_speech-recognitionOne of the most exciting developments were sequence-to-sequence models has been the rise of very accurate speech recognition. We’re nearing the end of the course, we want to take just a couple of videos to give you a sense of how these sequence-to-sequence models are applied to audio data, such as the speech. So, what is the speech recognition problem? You’re given an audio clip, x, and your job is to automatically find a text transcript, y. So, an audio clip, if you plot it looks like this, the horizontal axis here is time, and what a microphone does is it really measures minuscule changes in air pressure, and the way you’re hearing my voice right now is that your ear is detecting little changes in air pressure, probably generated either by your speakers or by a headset. And some audio clips like this plots with the air pressure against time. And, if this audio clip is of me saying, “the quick brown fox”, then hopefully, a speech recognition algorithm can input that audio clip and output that transcript. And because even the human ear doesn’t process raw wave forms, but the human ear has physical structures that measures the amounts of intensity of different frequencies, there is, a common pre-processing step for audio data is to run your raw audio clip and generate a spectrogram. So, this is the plots where the horizontal axis is time, and the vertical axis is frequencies, and intensity of different colors shows the amount of energy. So, how loud is the sound at different frequencies? At different times? And so, these types of spectrograms, or you might also hear people talk about false blank outputs, is often commonly applied pre-processing step before audio is pass into in the running algorithm. And the human ear does a computation pretty similar to this pre-processing step. So, one of the most exciting trends in speech recognition is that, once upon a time, speech recognition systems used to be built using phonemes and this where, I want to say hand-engineered basic units of cells. So, the quick brown fox represented as phonemes. I’m going to simplify a bit, let say, “The” has a “de” and “e” sound and Quick, has a “ku” and “wu”, “ik”, “k” sound, and linguist used to write off these basic units of sound, and try the Greek language down to these basic units of sound. So, brown, this aren’t the official phonemes which are written with more complicated notation, but linguists use to hypothesize that writing down audio in terms of these basic units of sound called phonemes would be the best way to do speech recognition. But with end-to-end deep learning, we’re finding that phonemes representations are no longer necessary. But instead, you can built systems that input an audio clip and directly output a transcript without needing to use hand-engineered representations like these. One of the things that made this possible was going to much larger data sets. So, academic data sets on speech recognition might be as a 300 hours, and in academia, 3000 hour data sets of transcribed audio would be considered reasonable size, so lot of research has been done, a lot of research papers that are written on data sets there are several thousand voice. But, the best commercial systems are now trains on over 10,000 hours and sometimes over a 100,000 hours of audio. And, it’s really moving to a much larger audio data sets, transcribe audio data sets were both x and y, together with deep learning algorithm, that has driven a lot of progress is speech recognition. So, how do you build a speech recognition system? In the last video, we’re talking about the attention model. So, one thing you could do is actually do that, where on the horizontal axis, you take in different time frames of the audio input, and then you have an attention model try to output the transcript like, “the quick brown fox”, or what it was said. One other method that seems to work well is to use the CTC cost for speech recognition. CTC stands for Connection is Temporal Classification and is due to Alex Graves, Santiago Fernandes, Faustino Gomez, and Jürgen Schmidhuber. So, here’s the idea. Let’s say the audio clip was someone saying, “the quick brown fox”. We’re going to use a new network structured like this with an equal number of input x’s and output y’s, and I have drawn a simple of what uni-directional for the RNN for this, but in practice, this will usually be a bidirectional LSTM and bidirectional GRU and usually, a deeper model. But notice that the number of time steps here is very large and in speech recognition, usually the number of input time steps is much bigger than the number of output time steps. So, for example, if you have 10 seconds of audio and your features come at a 100 hertz so 100 samples per second, then a 10 second audio clip would end up with a thousand inputs. Right, so it’s 100 hertz times 10 seconds, and so with a thousand inputs. But your output might not have a thousand alphabets, might not have a thousand characters. So, what do you do? The CTC cost function allows the RNN to generate an output like this ttt, there’s a special character called the blank character, which we’re going to write as an underscore here, h_eee___, and then maybe a space, we’re going to write like this, so that a space and then _ qqq. And, this is considered a correct output for the first parts of the space, quick with the Q, and the basic rule for the CTC cost function is to collapse repeated characters not separated by “blank”. So, to be clear, I’m using this underscore to denote a special blank character and that’s different than the space character. So, there is a space here between the and quick, so I should output a space. But, by collapsing repeated characters, not separated by blank, it actually collapse the sequence into t, h, e, and then space, and q, and this allows your network to have a thousand outputs by repeating characters allow the times. So, inserting a bunch of blank characters and still ends up with a much shorter output text transcript. So, this phrase here “the quick brown fox” including spaces actually has 19 characters, and if somehow, the newer network is forced upwards of a thousand characters by allowing the network to insert blanks and repeated characters and can still represent this 19 character upwards with this 1000 outputs of values of Y. So, this paper by Alex Grace, as well as by those deep speech recognition system, which I was involved in, used this idea to build effective Speech recognition systems. So, I hope that gives you a rough sense of how speech recognition models work. Attention like models work and CTC models work and present two different options of how to go about building these systems. Now, today, building effective where production skills speech recognition system is a pretty significant effort and requires a very large data set. But, what I like to do in the next video is share you, how you can build a trigger word detection system, where keyword detection system which is actually much easier and can be done with even a smaller or more reasonable amount of data. So, let’s talk about that in the next video. 02_trigger-word-detectionyou’ve now learned so much about deep learning and sequence models that we can actually describe a trigger word system quite simply just on one slide as you see in this video but when the rise of speech recognition have been more and more devices you can wake up with your voice and those are sometimes called trigger word detection systems so let’s see how you can build a trigger word system. Examples of triggering systems include Amazon echo which is broken out with that word Alexa. The Baidu DuerOs part devices woken up with face xiaodunihao. Apple Siri working out with hey Siri and Google home woken up with Ok Google. So stands the trigger word detection that if you have say an Amazon echo in your living room, you can walk the living room and just say: “Alexa what time is it” and have it wake up. It’ll be triggered by the words of Alexa and answer your voice query. So if you can build a trigger word detection system maybe you can make your computer do something by telling it computer activate. One of my friends also works on turning on an offer particular lamp using a trigger word kind of as a fun project but what I want to show you is how you can build a trigger word detection system. Now the trigger word detection literature is still evolving so there actually isn’t a single universally agreed on algorithm for trigger word detection yet the literature on trigger word detection algorithm is still evolving so there isn’t wide consensus yet on what’s the best algorithm for trigger word detection so I’m just going to show you one example of an algorithm you can use. now you’ve seen our ends like this and what we really do is take an audio clip maybe compute spectrogram features and that generates features $x^{} x^{} x^{}$ or audio features $x^{} x^{} x^{}$ that you pass through an RNN and so all that remains to be done is to define the target labels Y so if this point in the audio clip is when someone just finished saying the trigger word such as “Alexa”, “nihaobaidu” or “hey Siri” or “Okay Google” then in the training sets you can set the target labels to be zero for everything before that point and right after that to set the target label of one and then if a little bit later on you know the trigger word was set again and the trigger word said at this point then you can again set the target label to be one right after that now this type of labeling scheme for an RNN you know could work actually this won’t actually work reasonably well. One slight disadvantage of this is it creates a very imbalanced training set so if a lot more zeros than ones. So one other thing you could do that it’s getting a little bit of a hack but could make them all the little bit easy to train is instead of setting only a single time step to output one you can actually make an output a few ones for several times or for a fixed period of time before reverting back to zero so and that slightly evens out the ratio of ones to zeros but this is a little bit of a hack. But if this is when in the audio clipper trigger where the set then right after that you can set the target label to one and if this is the trigger words said again, then right after that just when you want the RNN to output one so you get to play more of this as well in the programming exercise but so I think you should feel quite proud of yourself we’ve learned enough about the learning that it just takes one picture at one slide to this to describe something as complicated as trigger word detection and based on this I hope you’d be able to implement something that works and allows you to detect trigger words but you see more of this in the program exercise. So that’s it for trigger words and I hope you feel quite proud of yourself for how much you’ve learned about deep learning that you can now describe trigger words in just one slide in a few minutes and that you’ve been hopeful II implemented and get it to work maybe even make it do something fun in your house that I’m like turn on or turn off um you could do something like a computer when you’re when someone else says they trigger words on this is the last technical video of this course and to wrap up in this course on sequence models you learned about rnns including both gr use and LS TMS and then in the second week you learned a lot about word embeddings and how they learn representations of words and then in this week you learned about the attention model as well as how to use it to process audio data and I hope you have fun implementing all of these ideas in this beast program sighs let’s go on to the last video. conclusion-and-thank-youcongratulations on making it this far I just wanna wrap up and leave you with a few final thoughts we’ve been on quite a journey together but if you’ve taken the whole specialization then you’ve learned about new networks and deep learning how to improve deep neural networks of the structure machine learning projects convolutional neural networks and then in this most recent course sequence models and I know you work really hard and I also hope you feel very proud of yourself for your hard work and for how much you’ve done.so I want to leave you one maybe important thought which is that I think deep learning is a superpower with deep learning algorithms you can make a computer see you can have a computer synthesize novel art or synthesized music or you can have a computer translate from one language to another maybe have it locally radiology image and render a medical diagnosis or build pieces of a car that can drive itself and if that isn’t a superpower I don’t know what is and as we wrap up this sequence of courses as we wrap up this specialization I hope that you will find ways to use these ideas to further your career to pursue your dreams but perhaps most important to do whatever you think is the best work you can do our humanity the world today has challenges but with the power of a on power of deep learning I think we can make it a much better place and now that you have this superpower I hope you will use it to go out there and make life better for yourself but also for other people and of course I also hope you feel very proud of your accomplishments in the power far you’ve come and of all that you’ve learned and when you complete this sequence of causes you should also share it on social media like Twitter or Facebook and let your friends know. and finally the very last thing I want to say to you is congratulations on Nikolas I hope you feel great about your accomplishments but also I want to thank you very much I know that you have a busy life but despite that spends a lot of time watching these videos and maybe spent a long time also working on the quizzes and the programming exercises I hope you enjoyed it and you got a lot out of the process but I’m also very grateful for all your time you spend and for all your hard work you put into learning these materials so thank you very much.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Operations on word vectors]]></title>
    <url>%2F2018%2F06%2F03%2FOperations%2Bon%2Bword%2Bvectors%2B-%2Bv2%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 2nd week and the copyright belongs to deeplearning.ai. Operations on word vectorsWelcome to your first assignment of this week! Because word embeddings are very computionally expensive to train, most ML practitioners will load a pre-trained set of embeddings. After this assignment you will be able to: Load pre-trained word vectors, and measure similarity using cosine similarity Use word embeddings to solve word analogy problems such as Man is to Woman as King is to __. Modify word embeddings to reduce their gender bias Let’s get started! Run the following cell to load the packages you will need. 12import numpy as npfrom w2v_utils import * Using TensorFlow backend. Next, lets load the word vectors. For this assignment, we will use 50-dimensional GloVe vectors to represent words. Run the following cell to load the word_to_vec_map. 1words, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt') You’ve loaded: words: set of words in the vocabulary. word_to_vec_map: dictionary mapping words to their GloVe vector representation. You’ve seen that one-hot vectors do not do a good job cpaturing what words are similar. GloVe vectors provide much more useful information about the meaning of individual words. Lets now see how you can use GloVe vectors to decide how similar two words are. 1 - Cosine similarityTo measure how similar two words are, we need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows: $$\text{CosineSimilarity(u, v)} = \frac {u . v} {||u||_2 ||v||_2} = cos(\theta) \tag{1}$$ where $u.v$ is the dot product (or inner product) of two vectors, $||u||_2$ is the norm (or length) of the vector $u$, and $\theta$ is the angle between $u$ and $v$. This similarity depends on the angle between $u$ and $v$. If $u$ and $v$ are very similar, their cosine similarity will be close to 1; if they are dissimilar, the cosine similarity will take a smaller value. Figure 1: The cosine of the angle between two vectors is a measure of how similar they are Exercise: Implement the function cosine_similarity() to evaluate similarity between word vectors. Reminder: The norm of $u$ is defined as $ ||u||_2 = \sqrt{\sum_{i=1}^{n} u_i^2}$ 1234567891011121314151617181920212223242526272829# GRADED FUNCTION: cosine_similaritydef cosine_similarity(u, v): """ Cosine similarity reflects the degree of similariy between u and v Arguments: u -- a word vector of shape (n,) v -- a word vector of shape (n,) Returns: cosine_similarity -- the cosine similarity between u and v defined by the formula above. """ distance = 0.0 ### START CODE HERE ### # Compute the dot product between u and v (≈1 line) dot = np.dot(u, v); # Compute the L2 norm of u (≈1 line) norm_u = np.linalg.norm(u); # Compute the L2 norm of v (≈1 line) norm_v = np.linalg.norm(v); # Compute the cosine similarity defined by formula (1) (≈1 line) cosine_similarity = dot / norm_u / norm_v; ### END CODE HERE ### return cosine_similarity 123456789101112father = word_to_vec_map["father"]mother = word_to_vec_map["mother"]ball = word_to_vec_map["ball"]crocodile = word_to_vec_map["crocodile"]france = word_to_vec_map["france"]italy = word_to_vec_map["italy"]paris = word_to_vec_map["paris"]rome = word_to_vec_map["rome"]print("cosine_similarity(father, mother) = ", cosine_similarity(father, mother))print("cosine_similarity(ball, crocodile) = ",cosine_similarity(ball, crocodile))print("cosine_similarity(france - paris, rome - italy) = ",cosine_similarity(france - paris, rome - italy)) cosine_similarity(father, mother) = 0.890903844289 cosine_similarity(ball, crocodile) = 0.274392462614 cosine_similarity(france - paris, rome - italy) = -0.675147930817 Expected Output: cosine_similarity(father, mother) = 0.890903844289 cosine_similarity(ball, crocodile) = 0.274392462614 cosine_similarity(france - paris, rome - italy) = -0.675147930817 After you get the correct expected output, please feel free to modify the inputs and measure the cosine similarity between other pairs of words! Playing around the cosine similarity of other inputs will give you a better sense of how word vectors behave. 2 - Word analogy taskIn the word analogy task, we complete the sentence “a is to b as c is to ____“. An example is ‘man is to woman as king is to queen‘ . In detail, we are trying to find a word d, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner: $e_b - e_a \approx e_d - e_c$. We will measure the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity. Exercise: Complete the code below to be able to perform word analogies! 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# GRADED FUNCTION: complete_analogydef complete_analogy(word_a, word_b, word_c, word_to_vec_map): """ Performs the word analogy task as explained above: a is to b as c is to ____. Arguments: word_a -- a word, string word_b -- a word, string word_c -- a word, string word_to_vec_map -- dictionary that maps words to their corresponding vectors. Returns: best_word -- the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity """ # convert words to lower case word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower() ### START CODE HERE ### # Get the word embeddings v_a, v_b and v_c (≈1-3 lines) e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]; ### END CODE HERE ### words = word_to_vec_map.keys() max_cosine_sim = -100 # Initialize max_cosine_sim to a large negative number best_word = None # Initialize best_word with None, it will help keep track of the word to output # loop over the whole word vector set for w in words: # to avoid best_word being one of the input words, pass on them. if w in [word_a, word_b, word_c] : continue ### START CODE HERE ### # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c) (≈1 line) cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c); # If the cosine_sim is more than the max_cosine_sim seen so far, # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines) if cosine_sim &gt; max_cosine_sim: max_cosine_sim = cosine_sim; best_word = w; ### END CODE HERE ### return best_word Run the cell below to test your code, this may take 1-2 minutes. 123triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]for triad in triads_to_try: print ('&#123;&#125; -&gt; &#123;&#125; :: &#123;&#125; -&gt; &#123;&#125;'.format( *triad, complete_analogy(*triad,word_to_vec_map))) italy -&gt; italian :: spain -&gt; spanish india -&gt; delhi :: japan -&gt; tokyo man -&gt; woman :: boy -&gt; girl small -&gt; smaller :: large -&gt; larger Expected Output: italy -&gt; italian :: spain -&gt; spanish india -&gt; delhi :: japan -&gt; tokyo man -&gt; woman :: boy -&gt; girl small -&gt; smaller :: large -&gt; larger Once you get the correct expected output, please feel free to modify the input cells above to test your own analogies. Try to find some other analogy pairs that do work, but also find some where the algorithm doesn’t give the right answer: For example, you can try small-&gt;smaller as big-&gt;?. Congratulations!You’ve come to the end of this assignment. Here are the main points you should remember: Cosine similarity a good way to compare similarity between pairs of word vectors. (Though L2 distance works too.) For NLP applications, using a pre-trained set of word vectors from the internet is often a good way to get started. Even though you have finished the graded portions, we recommend you take a look too at the rest of this notebook. Congratulations on finishing the graded portions of this notebook! 3 - Debiasing word vectors (OPTIONAL/UNGRADED)In the following exercise, you will examine gender biases that can be reflected in a word embedding, and explore algorithms for reducing the bias. In addition to learning about the topic of debiasing, this exercise will also help hone your intuition about what word vectors are doing. This section involves a bit of linear algebra, though you can probably complete it even without being expert in linear algebra, and we encourage you to give it a shot. This portion of the notebook is optional and is not graded. Lets first see how the GloVe word embeddings relate to gender. You will first compute a vector $g = e_{woman}-e_{man}$, where $e_{woman}$ represents the word vector corresponding to the word woman, and $e_{man}$ corresponds to the word vector corresponding to the word man. The resulting vector $g$ roughly encodes the concept of “gender”. (You might get a more accurate representation if you compute $g_1 = e_{mother}-e_{father}$, $g_2 = e_{girl}-e_{boy}$, etc. and average over them. But just using $e_{woman}-e_{man}$ will give good enough results for now.) 12g = word_to_vec_map['woman'] - word_to_vec_map['man']print(g) [-0.087144 0.2182 -0.40986 -0.03922 -0.1032 0.94165 -0.06042 0.32988 0.46144 -0.35962 0.31102 -0.86824 0.96006 0.01073 0.24337 0.08193 -1.02722 -0.21122 0.695044 -0.00222 0.29106 0.5053 -0.099454 0.40445 0.30181 0.1355 -0.0606 -0.07131 -0.19245 -0.06115 -0.3204 0.07165 -0.13337 -0.25068714 -0.14293 -0.224957 -0.149 0.048882 0.12191 -0.27362 -0.165476 -0.20426 0.54376 -0.271425 -0.10245 -0.32108 0.2516 -0.33455 -0.04371 0.01258 ] Now, you will consider the cosine similarity of different words with $g$. Consider what a positive value of similarity means vs a negative cosine similarity. 1234567print ('List of names and their similarities with constructed vector:')# girls and boys namename_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']for w in name_list: print (w, cosine_similarity(word_to_vec_map[w], g)) List of names and their similarities with constructed vector: john -0.23163356146 marie 0.315597935396 sophie 0.318687898594 ronaldo -0.312447968503 priya 0.17632041839 rahul -0.169154710392 danielle 0.243932992163 reza -0.079304296722 katy 0.283106865957 yasmin 0.233138577679 As you can see, female first names tend to have a positive cosine similarity with our constructed vector $g$, while male first names tend to have a negative cosine similarity. This is not suprising, and the result seems acceptable. But let’s try with some other words. 12345print('Other words and their similarities:')word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', 'technology', 'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']for w in word_list: print (w, cosine_similarity(word_to_vec_map[w], g)) Other words and their similarities: lipstick 0.276919162564 guns -0.18884855679 science -0.0608290654093 arts 0.00818931238588 literature 0.0647250443346 warrior -0.209201646411 doctor 0.118952894109 tree -0.0708939917548 receptionist 0.330779417506 technology -0.131937324476 fashion 0.0356389462577 teacher 0.179209234318 engineer -0.0803928049452 pilot 0.00107644989919 computer -0.103303588739 singer 0.185005181365 Do you notice anything surprising? It is astonishing how these results reflect certain unhealthy gender stereotypes. For example, “computer” is closer to “man” while “literature” is closer to “woman”. Ouch! We’ll see below how to reduce the bias of these vectors, using an algorithm due to Boliukbasi et al., 2016. Note that some word pairs such as “actor”/“actress” or “grandmother”/“grandfather” should remain gender specific, while other words such as “receptionist” or “technology” should be neutralized, i.e. not be gender-related. You will have to treat these two type of words differently when debiasing. 3.1 - Neutralize bias for non-gender specific wordsThe figure below should help you visualize what neutralizing does. If you’re using a 50-dimensional word embedding, the 50 dimensional space can be split into two parts: The bias-direction $g$, and the remaining 49 dimensions, which we’ll call $g_{\perp}$. In linear algebra, we say that the 49 dimensional $g_{\perp}$ is perpendicular (or “othogonal”) to $g$, meaning it is at 90 degrees to $g$. The neutralization step takes a vector such as $e_{receptionist}$ and zeros out the component in the direction of $g$, giving us $e_{receptionist}^{debiased}$. Even though $g_{\perp}$ is 49 dimensional, given the limitations of what we can draw on a screen, we illustrate it using a 1 dimensional axis below. Figure 2: The word vector for “receptionist” represented before and after applying the neutralize operation. Exercise: Implement neutralize() to remove the bias of words such as “receptionist” or “scientist”. Given an input embedding $e$, you can use the following formulas to compute $e^{debiased}$: $$e^{bias_component} = \frac{e \cdot g}{||g||_2^2} * g\tag{2}$$$$e^{debiased} = e - e^{bias_component}\tag{3}$$ If you are an expert in linear algebra, you may recognize $e^{bias_component}$ as the projection of $e$ onto the direction $g$. If you’re not an expert in linear algebra, don’t worry about this. 123456789101112131415161718192021222324252627def neutralize(word, g, word_to_vec_map): """ Removes the bias of "word" by projecting it on the space orthogonal to the bias axis. This function ensures that gender neutral words are zero in the gender subspace. Arguments: word -- string indicating the word to debias g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender) word_to_vec_map -- dictionary mapping words to their corresponding vectors. Returns: e_debiased -- neutralized word vector representation of the input "word" """ ### START CODE HERE ### # Select word vector representation of "word". Use word_to_vec_map. (≈ 1 line) e = word_to_vec_map[word]; # Compute e_biascomponent using the formula give above. (≈ 1 line) e_biascomponent = np.dot(e, g) / np.dot(g, g) * g; # Neutralize e by substracting e_biascomponent from it # e_debiased should be equal to its orthogonal projection. (≈ 1 line) e_debiased = e - e_biascomponent; ### END CODE HERE ### return e_debiased 12345e = "receptionist"print("cosine similarity between " + e + " and g, before neutralizing: ", cosine_similarity(word_to_vec_map["receptionist"], g))e_debiased = neutralize("receptionist", g, word_to_vec_map)print("cosine similarity between " + e + " and g, after neutralizing: ", cosine_similarity(e_debiased, g)) cosine similarity between receptionist and g, before neutralizing: 0.330779417506 cosine similarity between receptionist and g, after neutralizing: -5.60374039375e-17 Expected Output: The second result is essentially 0, up to numerical roundof (on the order of $10^{-17}$). cosine similarity between receptionist and g, before neutralizing: : 0.330779417506 cosine similarity between receptionist and g, after neutralizing: : -3.26732746085e-17 3.2 - Equalization algorithm for gender-specific wordsNext, lets see how debiasing can also be applied to word pairs such as “actress” and “actor.” Equalization is applied to pairs of words that you might want to have differ only through the gender property. As a concrete example, suppose that “actress” is closer to “babysit” than “actor.” By applying neutralizing to “babysit” we can reduce the gender-stereotype associated with babysitting. But this still does not guarantee that “actor” and “actress” are equidistant from “babysit.” The equalization algorithm takes care of this. The key idea behind equalization is to make sure that a particular pair of words are equi-distant from the 49-dimensional $g_\perp$. The equalization step also ensures that the two equalized steps are now the same distance from $e_{receptionist}^{debiased}$, or from any other work that has been neutralized. In pictures, this is how equalization works: The derivation of the linear algebra to do this is a bit more complex. (See Bolukbasi et al., 2016 for details.) But the key equations are: $$ \mu = \frac{e_{w1} + e_{w2}}{2}\tag{4}$$ $$ \mu_{B} = \frac {\mu \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis}\tag{5}$$ $$\mu_{\perp} = \mu - \mu_{B} \tag{6}$$ $$ e_{w1B} = \frac {e_{w1} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} \text{bias_axis}\tag{7}$$$$ e_{w2B} = \frac {e_{w2} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} \text{bias_axis}\tag{8}$$ $$e_{w1B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w1B}} - \mu_B} {|(e_{w1} - \mu_{\perp}) - \mu_B)|} \tag{9}$$ $$e_{w2B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w2B}} - \mu_B} {|(e_{w2} - \mu_{\perp}) - \mu_B)|} \tag{10}$$ $$e_1 = e_{w1B}^{corrected} + \mu_{\perp} \tag{11}$$$$e_2 = e_{w2B}^{corrected} + \mu_{\perp} \tag{12}$$ Exercise: Implement the function below. Use the equations above to get the final equalized version of the pair of words. Good luck! 1234567891011121314151617181920212223242526272829303132333435363738394041def equalize(pair, bias_axis, word_to_vec_map): """ Debias gender specific words by following the equalize method described in the figure above. Arguments: pair -- pair of strings of gender specific words to debias, e.g. ("actress", "actor") bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender word_to_vec_map -- dictionary mapping words to their corresponding vectors Returns e_1 -- word vector corresponding to the first word e_2 -- word vector corresponding to the second word """ ### START CODE HERE ### # Step 1: Select word vector representation of "word". Use word_to_vec_map. (≈ 2 lines) w1, w2 = pair; e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]; # Step 2: Compute the mean of e_w1 and e_w2 (≈ 1 line) mu = (e_w1 + e_w2) / 2; # Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines) mu_B = np.dot(mu, bias_axis) / np.dot(bias_axis, bias_axis) * bias_axis; mu_orth = mu - mu_B; # Step 4: Use equations (7) and (8) to compute e_w1B and e_w2B (≈2 lines) e_w1B = np.dot(e_w1, bias_axis) / np.dot(bias_axis, bias_axis) * bias_axis; e_w2B = np.dot(e_w2, bias_axis) / np.dot(bias_axis, bias_axis) * bias_axis; # Step 5: Adjust the Bias part of e_w1B and e_w2B using the formulas (9) and (10) given above (≈2 lines) corrected_e_w1B = np.sqrt(np.absolute(1 - np.linalg.norm(mu_orth) ** 2)) * (e_w1B - mu_B) / np.linalg.norm(e_w1 - mu_orth - mu_B); corrected_e_w2B = np.sqrt(np.absolute(1 - np.linalg.norm(mu_orth) ** 2)) * (e_w2B - mu_B) / np.linalg.norm(e_w2 - mu_orth - mu_B); # Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines) e1 = corrected_e_w1B + mu_orth; e2 = corrected_e_w2B + mu_orth; ### END CODE HERE ### return e1, e2 12345678print("cosine similarities before equalizing:")print("cosine_similarity(word_to_vec_map[\"man\"], gender) = ", cosine_similarity(word_to_vec_map["man"], g))print("cosine_similarity(word_to_vec_map[\"woman\"], gender) = ", cosine_similarity(word_to_vec_map["woman"], g))print()e1, e2 = equalize(("man", "woman"), g, word_to_vec_map)print("cosine similarities after equalizing:")print("cosine_similarity(e1, gender) = ", cosine_similarity(e1, g))print("cosine_similarity(e2, gender) = ", cosine_similarity(e2, g)) cosine similarities before equalizing: cosine_similarity(word_to_vec_map[&quot;man&quot;], gender) = -0.117110957653 cosine_similarity(word_to_vec_map[&quot;woman&quot;], gender) = 0.356666188463 cosine similarities after equalizing: cosine_similarity(e1, gender) = -0.700436428931 cosine_similarity(e2, gender) = 0.700436428931 Expected Output: cosine similarities before equalizing: cosine_similarity(word_to_vec_map[“man”], gender) = -0.117110957653 cosine_similarity(word_to_vec_map[“woman”], gender) = 0.356666188463 cosine similarities after equalizing: cosine_similarity(u1, gender) = -0.700436428931 cosine_similarity(u2, gender) = 0.700436428931 Please feel free to play with the input words in the cell above, to apply equalization to other pairs of words. These debiasing algorithms are very helpful for reducing bias, but are not perfect and do not eliminate all traces of bias. For example, one weakness of this implementation was that the bias direction $g$ was defined using only the pair of words _woman_ and _man_. As discussed earlier, if $g$ were defined by computing $g_1 = e_{woman} - e_{man}$; $g_2 = e_{mother} - e_{father}$; $g_3 = e_{girl} - e_{boy}$; and so on and averaging over them, you would obtain a better estimate of the “gender” dimension in the 50 dimensional word embedding space. Feel free to play with such variants as well. CongratulationsYou have come to the end of this notebook, and have seen a lot of the ways that word vectors can be used as well as modified. Congratulations on finishing this notebook! References: The debiasing algorithm is from Bolukbasi et al., 2016, Man is to Computer Programmer as Woman is toHomemaker? Debiasing Word Embeddings The GloVe word embeddings were due to Jeffrey Pennington, Richard Socher, and Christopher D. Manning. (https://nlp.stanford.edu/projects/glove/)]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Emojify]]></title>
    <url>%2F2018%2F06%2F03%2FEmojify%2B-%2Bv2%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 2nd week and the copyright belongs to deeplearning.ai. Emojify!Welcome to the second assignment of Week 2. You are going to use word vector representations to build an Emojifier. Have you ever wanted to make your text messages more expressive? Your emojifier app will help you do that. So rather than writing “Congratulations on the promotion! Lets get coffee and talk. Love you!” the emojifier can automatically turn this into “Congratulations on the promotion! 👍 Lets get coffee and talk. ☕️ Love you! ❤️” You will implement a model which inputs a sentence (such as “Let’s go see the baseball game tonight!”) and finds the most appropriate emoji to be used with this sentence (⚾️). In many emoji interfaces, you need to remember that ❤️ is the “heart” symbol rather than the “love” symbol. But using word vectors, you’ll see that even if your training set explicitly relates only a few words to a particular emoji, your algorithm will be able to generalize and associate words in the test set to the same emoji even if those words don’t even appear in the training set. This allows you to build an accurate classifier mapping from sentences to emojis, even using a small training set. In this exercise, you’ll start with a baseline model (Emojifier-V1) using word embeddings, then build a more sophisticated model (Emojifier-V2) that further incorporates an LSTM. Lets get started! Run the following cell to load the package you are going to use. 123456import numpy as npfrom emo_utils import *import emojiimport matplotlib.pyplot as plt%matplotlib inline 1 - Baseline model: Emojifier-V11.1 - Dataset EMOJISETLet’s start by building a simple baseline classifier. You have a tiny dataset (X, Y) where: X contains 127 sentences (strings) Y contains a integer label between 0 and 4 corresponding to an emoji for each sentence Figure 1: EMOJISET - a classification problem with 5 classes. A few examples of sentences are given here. Let’s load the dataset using the code below. We split the dataset between training (127 examples) and testing (56 examples). 12X_train, Y_train = read_csv('data/train_emoji.csv')X_test, Y_test = read_csv('data/tesss.csv') 1maxLen = len(max(X_train, key=len).split()) Run the following cell to print sentences from X_train and corresponding labels from Y_train. Change index to see different examples. Because of the font the iPython notebook uses, the heart emoji may be colored black rather than red. 12index = 1print(X_train[index], label_to_emoji(Y_train[index])) I am proud of your achievements 😄 1.2 - Overview of the Emojifier-V1In this part, you are going to implement a baseline model called “Emojifier-v1”. Figure 2: Baseline model (Emojifier-V1). The input of the model is a string corresponding to a sentence (e.g. “I love you). In the code, the output will be a probability vector of shape (1,5), that you then pass in an argmax layer to extract the index of the most likely emoji output. To get our labels into a format suitable for training a softmax classifier, lets convert $Y$ from its current shape current shape $(m, 1)$ into a “one-hot representation” $(m, 5)$, where each row is a one-hot vector giving the label of one example, You can do so using this next code snipper. Here, Y_oh stands for “Y-one-hot” in the variable names Y_oh_train and Y_oh_test: 12Y_oh_train = convert_to_one_hot(Y_train, C = 5)Y_oh_test = convert_to_one_hot(Y_test, C = 5) Let’s see what convert_to_one_hot() did. Feel free to change index to print out different values. 12index = 50print(Y_train[index], "is converted into one hot", Y_oh_train[index]) 0 is converted into one hot [ 1. 0. 0. 0. 0.] All the data is now ready to be fed into the Emojify-V1 model. Let’s implement the model! 1.3 - Implementing Emojifier-V1As shown in Figure (2), the first step is to convert an input sentence into the word vector representation, which then get averaged together. Similar to the previous exercise, we will use pretrained 50-dimensional GloVe embeddings. Run the following cell to load the word_to_vec_map, which contains all the vector representations. 1word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt') You’ve loaded: word_to_index: dictionary mapping from words to their indices in the vocabulary (400,001 words, with the valid indices ranging from 0 to 400,000) index_to_word: dictionary mapping from indices to their corresponding words in the vocabulary word_to_vec_map: dictionary mapping words to their GloVe vector representation. Run the following cell to check if it works. 1234word = "cucumber"index = 289846print("the index of", word, "in the vocabulary is", word_to_index[word])print("the", str(index) + "th word in the vocabulary is", index_to_word[index]) the index of cucumber in the vocabulary is 113317 the 289846th word in the vocabulary is potatos Exercise: Implement sentence_to_avg(). You will need to carry out two steps: Convert every sentence to lower-case, then split the sentence into a list of words. X.lower() and X.split() might be useful. For each word in the sentence, access its GloVe representation. Then, average all these values. 123456789101112131415161718192021222324252627282930# GRADED FUNCTION: sentence_to_avgdef sentence_to_avg(sentence, word_to_vec_map): """ Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word and averages its value into a single vector encoding the meaning of the sentence. Arguments: sentence -- string, one training example from X word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation Returns: avg -- average vector encoding information about the sentence, numpy-array of shape (50,) """ ### START CODE HERE ### # Step 1: Split sentence into list of lower case words (≈ 1 line) words = sentence.lower().split(); # Initialize the average word vector, should have the same shape as your word vectors. avg = np.zeros((word_to_vec_map[words[0]].shape)); # Step 2: average the word vectors. You can loop over the words in the list "words". for w in words: avg += word_to_vec_map[w]; avg = avg / len(words); ### END CODE HERE ### return avg 12avg = sentence_to_avg("Morrocan couscous is my favorite dish", word_to_vec_map)print("avg = ", avg) avg = [-0.008005 0.56370833 -0.50427333 0.258865 0.55131103 0.03104983 -0.21013718 0.16893933 -0.09590267 0.141784 -0.15708967 0.18525867 0.6495785 0.38371117 0.21102167 0.11301667 0.02613967 0.26037767 0.05820667 -0.01578167 -0.12078833 -0.02471267 0.4128455 0.5152061 0.38756167 -0.898661 -0.535145 0.33501167 0.68806933 -0.2156265 1.797155 0.10476933 -0.36775333 0.750785 0.10282583 0.348925 -0.27262833 0.66768 -0.10706167 -0.283635 0.59580117 0.28747333 -0.3366635 0.23393817 0.34349183 0.178405 0.1166155 -0.076433 0.1445417 0.09808667] Expected Output: avg= [-0.008005 0.56370833 -0.50427333 0.258865 0.55131103 0.03104983 -0.21013718 0.16893933 -0.09590267 0.141784 -0.15708967 0.18525867 0.6495785 0.38371117 0.21102167 0.11301667 0.02613967 0.26037767 0.05820667 -0.01578167 -0.12078833 -0.02471267 0.4128455 0.5152061 0.38756167 -0.898661 -0.535145 0.33501167 0.68806933 -0.2156265 1.797155 0.10476933 -0.36775333 0.750785 0.10282583 0.348925 -0.27262833 0.66768 -0.10706167 -0.283635 0.59580117 0.28747333 -0.3366635 0.23393817 0.34349183 0.178405 0.1166155 -0.076433 0.1445417 0.09808667] ModelYou now have all the pieces to finish implementing the model() function. After using sentence_to_avg() you need to pass the average through forward propagation, compute the cost, and then backpropagate to update the softmax’s parameters. Exercise: Implement the model() function described in Figure (2). Assuming here that $Yoh$ (“Y one hot”) is the one-hot encoding of the output labels, the equations you need to implement in the forward pass and to compute the cross-entropy cost are:$$ z^{(i)} = W . avg^{(i)} + b$$$$ a^{(i)} = softmax(z^{(i)})$$$$ \mathcal{L}^{(i)} = - \sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)$$ It is possible to come up with a more efficient vectorized implementation. But since we are using a for-loop to convert the sentences one at a time into the avg^{(i)} representation anyway, let’s not bother this time. We provided you a function softmax(). 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# GRADED FUNCTION: modeldef model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400): """ Model to train word vector representations in numpy. Arguments: X -- input data, numpy array of sentences as strings, of shape (m, 1) Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1) word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation learning_rate -- learning_rate for the stochastic gradient descent algorithm num_iterations -- number of iterations Returns: pred -- vector of predictions, numpy-array of shape (m, 1) W -- weight matrix of the softmax layer, of shape (n_y, n_h) b -- bias of the softmax layer, of shape (n_y,) """ np.random.seed(1) # Define number of training examples m = Y.shape[0] # number of training examples n_y = 5 # number of classes n_h = 50 # dimensions of the GloVe vectors # Initialize parameters using Xavier initialization W = np.random.randn(n_y, n_h) / np.sqrt(n_h) b = np.zeros((n_y,)) # Convert Y to Y_onehot with n_y classes Y_oh = convert_to_one_hot(Y, C = n_y) # Optimization loop for t in range(num_iterations): # Loop over the number of iterations for i in range(m): # Loop over the training examples ### START CODE HERE ### (≈ 4 lines of code) # Average the word vectors of the words from the i'th training example avg = sentence_to_avg(X[i], word_to_vec_map); # Forward propagate the avg through the softmax layer z = np.dot(W, avg) + b; a = softmax(z); # Compute cost using the i'th training label's one hot representation and "A" (the output of the softmax) cost = np.sum(-Y_oh[i] * np.log(a)); ### END CODE HERE ### # Compute gradients dz = a - Y_oh[i] dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h)) db = dz # Update parameters with Stochastic Gradient Descent W = W - learning_rate * dW b = b - learning_rate * db if t % 100 == 0: print("Epoch: " + str(t) + " --- cost = " + str(cost)) pred = predict(X, Y, W, b, word_to_vec_map) return pred, W, b 123456789101112131415161718192021print(X_train.shape)print(Y_train.shape)print(np.eye(5)[Y_train.reshape(-1)].shape)print(X_train[0])print(type(X_train))Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])print(Y.shape)X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear', 'Lets go party and drinks','Congrats on the new job','Congratulations', 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you', 'You totally deserve this prize', 'Let us go play football', 'Are you down for football this afternoon', 'Work hard play harder', 'It is suprising how people can be dumb sometimes', 'I am very disappointed','It is the best day in my life', 'I think I will end up alone','My life is so boring','Good job', 'Great so awesome'])print(X.shape)print(np.eye(5)[Y_train.reshape(-1)].shape)print(type(X_train)) (132,) (132,) (132, 5) never talk to me again &lt;class &apos;numpy.ndarray&apos;&gt; (20,) (20,) (132, 5) &lt;class &apos;numpy.ndarray&apos;&gt; Run the next cell to train your model and learn the softmax parameters (W,b). 12pred, W, b = model(X_train, Y_train, word_to_vec_map)print(pred) Epoch: 0 --- cost = 1.95204988128 Accuracy: 0.348484848485 Epoch: 100 --- cost = 0.0797181872601 Accuracy: 0.931818181818 Epoch: 200 --- cost = 0.0445636924368 Accuracy: 0.954545454545 Epoch: 300 --- cost = 0.0343226737879 Accuracy: 0.969696969697 [[ 3.] [ 2.] [ 3.] [ 0.] [ 4.] [ 0.] [ 3.] [ 2.] [ 3.] [ 1.] [ 3.] [ 3.] [ 1.] [ 3.] [ 2.] [ 3.] [ 2.] [ 3.] [ 1.] [ 2.] [ 3.] [ 0.] [ 2.] [ 2.] [ 2.] [ 1.] [ 4.] [ 3.] [ 3.] [ 4.] [ 0.] [ 3.] [ 4.] [ 2.] [ 0.] [ 3.] [ 2.] [ 2.] [ 3.] [ 4.] [ 2.] [ 2.] [ 0.] [ 2.] [ 3.] [ 0.] [ 3.] [ 2.] [ 4.] [ 3.] [ 0.] [ 3.] [ 3.] [ 3.] [ 4.] [ 2.] [ 1.] [ 1.] [ 1.] [ 2.] [ 3.] [ 1.] [ 0.] [ 0.] [ 0.] [ 3.] [ 4.] [ 4.] [ 2.] [ 2.] [ 1.] [ 2.] [ 0.] [ 3.] [ 2.] [ 2.] [ 0.] [ 3.] [ 3.] [ 1.] [ 2.] [ 1.] [ 2.] [ 2.] [ 4.] [ 3.] [ 3.] [ 2.] [ 4.] [ 0.] [ 0.] [ 3.] [ 3.] [ 3.] [ 3.] [ 2.] [ 0.] [ 1.] [ 2.] [ 3.] [ 0.] [ 2.] [ 2.] [ 2.] [ 3.] [ 2.] [ 2.] [ 2.] [ 4.] [ 1.] [ 1.] [ 3.] [ 3.] [ 4.] [ 1.] [ 2.] [ 1.] [ 1.] [ 3.] [ 1.] [ 0.] [ 4.] [ 0.] [ 3.] [ 3.] [ 4.] [ 4.] [ 1.] [ 4.] [ 3.] [ 0.] [ 2.]] Expected Output (on a subset of iterations): Epoch: 0 cost = 1.95204988128 Accuracy: 0.348484848485 Epoch: 100 cost = 0.0797181872601 Accuracy: 0.931818181818 Epoch: 200 cost = 0.0445636924368 Accuracy: 0.954545454545 Epoch: 300 cost = 0.0343226737879 Accuracy: 0.969696969697 Great! Your model has pretty high accuracy on the training set. Lets now see how it does on the test set. 1.4 - Examining test set performance1234print("Training set:")pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)print('Test set:')pred_test = predict(X_test, Y_test, W, b, word_to_vec_map) Training set: Accuracy: 0.977272727273 Test set: Accuracy: 0.857142857143 Expected Output: Train set accuracy 97.7 Test set accuracy 85.7 Random guessing would have had 20% accuracy given that there are 5 classes. This is pretty good performance after training on only 127 examples. In the training set, the algorithm saw the sentence “I love you“ with the label ❤️. You can check however that the word “adore” does not appear in the training set. Nonetheless, lets see what happens if you write “I adore you.” 12345X_my_sentences = np.array(["i adore you", "i love you", "funny lol", "lets play with a ball", "food is ready", "not feeling happy"])Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)print_predictions(X_my_sentences, pred) Accuracy: 0.833333333333 i adore you ❤️ i love you ❤️ funny lol 😄 lets play with a ball ⚾ food is ready 🍴 not feeling happy 😄 Amazing! Because adore has a similar embedding as love, the algorithm has generalized correctly even to a word it has never seen before. Words such as heart, dear, beloved or adore have embedding vectors similar to love, and so might work too—feel free to modify the inputs above and try out a variety of input sentences. How well does it work? Note though that it doesn’t get “not feeling happy” correct. This algorithm ignores word ordering, so is not good at understanding phrases like “not happy.” Printing the confusion matrix can also help understand which classes are more difficult for your model. A confusion matrix shows how often an example whose label is one class (“actual” class) is mislabeled by the algorithm with a different class (“predicted” class). 1234print(Y_test.shape)print(' '+ label_to_emoji(0)+ ' ' + label_to_emoji(1) + ' ' + label_to_emoji(2)+ ' ' + label_to_emoji(3)+' ' + label_to_emoji(4))print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))plot_confusion_matrix(Y_test, pred_test) (56,) ❤️ ⚾ 😄 😞 🍴 Predicted 0.0 1.0 2.0 3.0 4.0 All Actual 0 6 0 0 1 0 7 1 0 8 0 0 0 8 2 2 0 16 0 0 18 3 1 1 2 12 0 16 4 0 0 1 0 6 7 All 9 9 19 13 6 56 What you should remember from this part: Even with a 127 training examples, you can get a reasonably good model for Emojifying. This is due to the generalization power word vectors gives you. Emojify-V1 will perform poorly on sentences such as “This movie is not good and not enjoyable” because it doesn’t understand combinations of words–it just averages all the words’ embedding vectors together, without paying attention to the ordering of words. You will build a better algorithm in the next part. 2 - Emojifier-V2: Using LSTMs in Keras:Let’s build an LSTM model that takes as input word sequences. This model will be able to take word ordering into account. Emojifier-V2 will continue to use pre-trained word embeddings to represent words, but will feed them into an LSTM, whose job it is to predict the most appropriate emoji. Run the following cell to load the Keras packages. 12345678import numpy as npnp.random.seed(0)from keras.models import Modelfrom keras.layers import Dense, Input, Dropout, LSTM, Activationfrom keras.layers.embeddings import Embeddingfrom keras.preprocessing import sequencefrom keras.initializers import glorot_uniformnp.random.seed(1) Using TensorFlow backend. 2.1 - Overview of the modelHere is the Emojifier-v2 you will implement: Figure 3: Emojifier-V2. A 2-layer LSTM sequence classifier. 2.2 Keras and mini-batchingIn this exercise, we want to train Keras using mini-batches. However, most deep learning frameworks require that all sequences in the same mini-batch have the same length. This is what allows vectorization to work: If you had a 3-word sentence and a 4-word sentence, then the computations needed for them are different (one takes 3 steps of an LSTM, one takes 4 steps) so it’s just not possible to do them both at the same time. The common solution to this is to use padding. Specifically, set a maximum sequence length, and pad all sequences to the same length. For example, of the maximum sequence length is 20, we could pad every sentence with “0”s so that each input sentence is of length 20. Thus, a sentence “i love you” would be represented as $(e_{i}, e_{love}, e_{you}, \vec{0}, \vec{0}, \ldots, \vec{0})$. In this example, any sentences longer than 20 words would have to be truncated. One simple way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set. 2.3 - The Embedding layerIn Keras, the embedding matrix is represented as a “layer”, and maps positive integers (indices corresponding to words) into dense vectors of fixed size (the embedding vectors). It can be trained or initialized with a pretrained embedding. In this part, you will learn how to create an Embedding() layer in Keras, initialize it with the GloVe 50-dimensional vectors loaded earlier in the notebook. Because our training set is quite small, we will not update the word embeddings but will instead leave their values fixed. But in the code below, we’ll show you how Keras allows you to either train or leave fixed this layer. The Embedding() layer takes an integer matrix of size (batch size, max input length) as input. This corresponds to sentences converted into lists of indices (integers), as shown in the figure below. Figure 4: Embedding layer. This example shows the propagation of two examples through the embedding layer. Both have been zero-padded to a length of max_len=5. The final dimension of the representation is (2,max_len,50) because the word embeddings we are using are 50 dimensional. The largest integer (i.e. word index) in the input should be no larger than the vocabulary size. The layer outputs an array of shape (batch size, max input length, dimension of word vectors). The first step is to convert all your training sentences into lists of indices, and then zero-pad all these lists so that their length is the length of the longest sentence. Exercise: Implement the function below to convert X (array of sentences as strings) into an array of indices corresponding to words in the sentences. The output shape should be such that it can be given to Embedding() (described in Figure 4). 12345678910111213141516171819202122232425262728293031323334353637383940# GRADED FUNCTION: sentences_to_indicesdef sentences_to_indices(X, word_to_index, max_len): """ Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences. The output shape should be such that it can be given to `Embedding()` (described in Figure 4). Arguments: X -- array of sentences (strings), of shape (m, 1) word_to_index -- a dictionary containing the each word mapped to its index max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. Returns: X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len) """ m = X.shape[0] # number of training examples ### START CODE HERE ### # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line) X_indices = np.zeros((m, max_len)); for i in range(m): # loop over training examples # Convert the ith training sentence in lower case and split is into words. You should get a list of words. sentence_words = X[i].lower().split(); # Initialize j to 0 j = 0 # Loop over the words of sentence_words for w in sentence_words: # Set the (i,j)th entry of X_indices to the index of the correct word. X_indices[i, j] = word_to_index[w]; # Increment j to j + 1 j = j + 1; ### END CODE HERE ### return X_indices Run the following cell to check what sentences_to_indices() does, and check your results. 1234X1 = np.array(["funny lol", "lets play baseball", "food is ready for you"])X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)print("X1 =", X1)print("X1_indices =", X1_indices) X1 = [&apos;funny lol&apos; &apos;lets play baseball&apos; &apos;food is ready for you&apos;] X1_indices = [[ 155345. 225122. 0. 0. 0.] [ 220930. 286375. 69714. 0. 0.] [ 151204. 192973. 302254. 151349. 394475.]] Expected Output: X1 = [‘funny lol’ ‘lets play football’ ‘food is ready for you’] X1_indices = [[ 155345. 225122. 0. 0. 0.] [ 220930. 286375. 151266. 0. 0.] [ 151204. 192973. 302254. 151349. 394475.]] Let’s build the Embedding() layer in Keras, using pre-trained word vectors. After this layer is built, you will pass the output of sentences_to_indices() to it as an input, and the Embedding() layer will return the word embeddings for a sentence. Exercise: Implement pretrained_embedding_layer(). You will need to carry out the following steps: Initialize the embedding matrix as a numpy array of zeroes with the correct shape. Fill in the embedding matrix with all the word embeddings extracted from word_to_vec_map. Define Keras embedding layer. Use Embedding(). Be sure to make this layer non-trainable, by setting trainable = False when calling Embedding(). If you were to set trainable = True, then it will allow the optimization algorithm to modify the values of the word embeddings. Set the embedding weights to be equal to the embedding matrix 123456789101112131415161718192021222324252627282930313233343536# GRADED FUNCTION: pretrained_embedding_layerdef pretrained_embedding_layer(word_to_vec_map, word_to_index): """ Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors. Arguments: word_to_vec_map -- dictionary mapping words to their GloVe vector representation. word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words) Returns: embedding_layer -- pretrained layer Keras instance """ vocab_len = len(word_to_index) + 1 # adding 1 to fit Keras embedding (requirement) emb_dim = word_to_vec_map["cucumber"].shape[0] # define dimensionality of your GloVe word vectors (= 50) ### START CODE HERE ### # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim) emb_matrix = np.zeros((vocab_len, emb_dim)); # Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary for word, index in word_to_index.items(): emb_matrix[index, :] = word_to_vec_map[word]; # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. embedding_layer = Embedding(vocab_len, emb_dim, trainable = False); ### END CODE HERE ### # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None". embedding_layer.build((None,)) # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained. embedding_layer.set_weights([emb_matrix]) return embedding_layer 12embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)print("weights[0][1][3] =", embedding_layer.get_weights()[0][1][3]) weights[0][1][3] = -0.3403 Expected Output: weights[0][1][3] = -0.3403 2.3 Building the Emojifier-V2Lets now build the Emojifier-V2 model. You will do so using the embedding layer you have built, and feed its output to an LSTM network. Figure 3: Emojifier-v2. A 2-layer LSTM sequence classifier. Exercise: Implement Emojify_V2(), which builds a Keras graph of the architecture shown in Figure 3. The model takes as input an array of sentences of shape (m, max_len, ) defined by input_shape. It should output a softmax probability vector of shape (m, C = 5). You may need Input(shape = ..., dtype = &#39;...&#39;), LSTM(), Dropout(), Dense(), and Activation(). 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# GRADED FUNCTION: Emojify_V2def Emojify_V2(input_shape, word_to_vec_map, word_to_index): """ Function creating the Emojify-v2 model's graph. Arguments: input_shape -- shape of the input, usually (max_len,) word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words) Returns: model -- a model instance in Keras """ ### START CODE HERE ### # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices). sentence_indices = Input(shape = input_shape, dtype = 'int32'); # Create the embedding layer pretrained with GloVe Vectors (≈1 line) embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index); # Propagate sentence_indices through your embedding layer, you get back the embeddings embeddings = embedding_layer(sentence_indices); # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state # Be careful, the returned output should be a batch of sequences. X = LSTM(128, return_sequences = True)(embeddings); # Add dropout with a probability of 0.5 X = Dropout(0.5)(X); # Propagate X trough another LSTM layer with 128-dimensional hidden state # Be careful, the returned output should be a single hidden state, not a batch of sequences. X = LSTM(128, return_sequences = False)(X); # Add dropout with a probability of 0.5 X = Dropout(0.5)(X); # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors. X = Dense(5, activation = 'softmax')(X); # Add a softmax activation X = Activation('softmax')(X); # Create Model instance which converts sentence_indices into X. model = Model(inputs = sentence_indices, outputs = X); ### END CODE HERE ### return model Run the following cell to create your model and check its summary. Because all sentences in the dataset are less than 10 words, we chose max_len = 10. You should see your architecture, it uses “20,223,927” parameters, of which 20,000,050 (the word embeddings) are non-trainable, and the remaining 223,877 are. Because our vocabulary size has 400,001 words (with valid indices from 0 to 400,000) there are 400,001*50 = 20,000,050 non-trainable parameters. 12model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 10) 0 _________________________________________________________________ embedding_2 (Embedding) (None, 10, 50) 20000050 _________________________________________________________________ lstm_1 (LSTM) (None, 10, 128) 91648 _________________________________________________________________ dropout_1 (Dropout) (None, 10, 128) 0 _________________________________________________________________ lstm_2 (LSTM) (None, 128) 131584 _________________________________________________________________ dropout_2 (Dropout) (None, 128) 0 _________________________________________________________________ dense_1 (Dense) (None, 5) 645 _________________________________________________________________ activation_1 (Activation) (None, 5) 0 ================================================================= Total params: 20,223,927 Trainable params: 223,877 Non-trainable params: 20,000,050 _________________________________________________________________ As usual, after creating your model in Keras, you need to compile it and define what loss, optimizer and metrics your are want to use. Compile your model using categorical_crossentropy loss, adam optimizer and [&#39;accuracy&#39;] metrics: 1model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) It’s time to train your model. Your Emojifier-V2 model takes as input an array of shape (m, max_len) and outputs probability vectors of shape (m, number of classes). We thus have to convert X_train (array of sentences as strings) to X_train_indices (array of sentences as list of word indices), and Y_train (labels as indices) to Y_train_oh (labels as one-hot vectors). 12X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)Y_train_oh = convert_to_one_hot(Y_train, C = 5) Fit the Keras model on X_train_indices and Y_train_oh. We will use epochs = 50 and batch_size = 32. 1model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True) Epoch 1/50 132/132 [==============================] - 0s - loss: 1.6086 - acc: 0.1818 Epoch 2/50 132/132 [==============================] - 0s - loss: 1.5867 - acc: 0.3409 Epoch 3/50 132/132 [==============================] - 0s - loss: 1.5721 - acc: 0.2652 Epoch 4/50 132/132 [==============================] - 0s - loss: 1.5540 - acc: 0.3485 Epoch 5/50 132/132 [==============================] - 0s - loss: 1.5413 - acc: 0.3030 Epoch 6/50 132/132 [==============================] - 0s - loss: 1.5195 - acc: 0.3712 Epoch 7/50 132/132 [==============================] - 0s - loss: 1.5275 - acc: 0.3258 Epoch 8/50 132/132 [==============================] - 0s - loss: 1.4633 - acc: 0.4545 Epoch 9/50 132/132 [==============================] - 0s - loss: 1.4320 - acc: 0.4924 Epoch 10/50 132/132 [==============================] - 0s - loss: 1.3712 - acc: 0.6136 Epoch 11/50 132/132 [==============================] - 0s - loss: 1.3441 - acc: 0.6136 Epoch 12/50 132/132 [==============================] - 0s - loss: 1.2784 - acc: 0.6894 Epoch 13/50 132/132 [==============================] - 0s - loss: 1.2723 - acc: 0.6364 Epoch 14/50 132/132 [==============================] - 0s - loss: 1.2651 - acc: 0.6667 Epoch 15/50 132/132 [==============================] - 0s - loss: 1.2106 - acc: 0.6970 Epoch 16/50 132/132 [==============================] - 0s - loss: 1.2334 - acc: 0.7197 Epoch 17/50 132/132 [==============================] - 0s - loss: 1.2150 - acc: 0.7045 Epoch 18/50 132/132 [==============================] - 0s - loss: 1.1613 - acc: 0.7803 Epoch 19/50 132/132 [==============================] - 0s - loss: 1.1587 - acc: 0.7576 Epoch 20/50 132/132 [==============================] - 0s - loss: 1.1129 - acc: 0.8182 Epoch 21/50 132/132 [==============================] - 0s - loss: 1.1016 - acc: 0.8030 Epoch 22/50 132/132 [==============================] - 0s - loss: 1.1939 - acc: 0.6970 Epoch 23/50 132/132 [==============================] - 0s - loss: 1.2618 - acc: 0.6288 Epoch 24/50 132/132 [==============================] - 0s - loss: 1.2123 - acc: 0.6818 Epoch 25/50 132/132 [==============================] - 0s - loss: 1.1606 - acc: 0.7652 Epoch 26/50 132/132 [==============================] - 0s - loss: 1.1066 - acc: 0.8030 Epoch 27/50 132/132 [==============================] - 0s - loss: 1.1312 - acc: 0.7727 Epoch 28/50 132/132 [==============================] - 0s - loss: 1.1400 - acc: 0.7652 Epoch 29/50 132/132 [==============================] - 0s - loss: 1.1107 - acc: 0.8030 Epoch 30/50 132/132 [==============================] - 0s - loss: 1.0676 - acc: 0.8485 Epoch 31/50 132/132 [==============================] - 0s - loss: 1.0660 - acc: 0.8258 Epoch 32/50 132/132 [==============================] - 0s - loss: 1.0450 - acc: 0.8712 Epoch 33/50 132/132 [==============================] - 0s - loss: 1.0246 - acc: 0.8939 Epoch 34/50 132/132 [==============================] - 0s - loss: 1.0163 - acc: 0.8939 Epoch 35/50 132/132 [==============================] - 0s - loss: 1.0080 - acc: 0.9015 Epoch 36/50 132/132 [==============================] - 0s - loss: 1.0144 - acc: 0.9015 Epoch 37/50 132/132 [==============================] - 0s - loss: 1.0861 - acc: 0.8106 Epoch 38/50 132/132 [==============================] - 0s - loss: 1.0484 - acc: 0.8561 Epoch 39/50 132/132 [==============================] - 0s - loss: 1.1126 - acc: 0.7955 Epoch 40/50 132/132 [==============================] - 0s - loss: 1.0712 - acc: 0.8561 Epoch 41/50 132/132 [==============================] - 0s - loss: 1.0277 - acc: 0.8864 Epoch 42/50 132/132 [==============================] - 0s - loss: 1.0459 - acc: 0.8561 Epoch 43/50 132/132 [==============================] - 0s - loss: 1.0214 - acc: 0.8864 Epoch 44/50 132/132 [==============================] - 0s - loss: 1.0012 - acc: 0.9091 Epoch 45/50 132/132 [==============================] - 0s - loss: 0.9877 - acc: 0.9242 Epoch 46/50 132/132 [==============================] - 0s - loss: 0.9827 - acc: 0.9167 Epoch 47/50 132/132 [==============================] - 0s - loss: 0.9835 - acc: 0.9167 Epoch 48/50 132/132 [==============================] - 0s - loss: 0.9817 - acc: 0.9242 Epoch 49/50 132/132 [==============================] - 0s - loss: 0.9894 - acc: 0.9167 Epoch 50/50 132/132 [==============================] - 0s - loss: 0.9780 - acc: 0.9318 &lt;keras.callbacks.History at 0x7f49ffd55e48&gt; Your model should perform close to 100% accuracy on the training set. The exact accuracy you get may be a little different. Run the following cell to evaluate your model on the test set. 12345X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)Y_test_oh = convert_to_one_hot(Y_test, C = 5)loss, acc = model.evaluate(X_test_indices, Y_test_oh)print()print("Test accuracy = ", acc) 32/56 [================&gt;.............] - ETA: 0s Test accuracy = 0.839285714286 You should get a test accuracy between 80% and 95%. Run the cell below to see the mislabelled examples. 12345678910# This code allows you to see the mislabelled examplesC = 5y_test_oh = np.eye(C)[Y_test.reshape(-1)]X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)pred = model.predict(X_test_indices)for i in range(len(X_test)): x = X_test_indices num = np.argmax(pred[i]) if(num != Y_test[i]): print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip()) Expected emoji:😄 prediction: she got me a nice present ❤️ Expected emoji:😞 prediction: work is hard 😄 Expected emoji:😞 prediction: This girl is messing with me ❤️ Expected emoji:😞 prediction: work is horrible 😄 Expected emoji:😄 prediction: you brighten my day ❤️ Expected emoji:😞 prediction: she is a bully 😄 Expected emoji:😞 prediction: My life is so boring ❤️ Expected emoji:😄 prediction: will you be my valentine 😞 Expected emoji:😄 prediction: What you did was awesome 😞 Now you can try it on your own example. Write your own sentence below. 1234# Change the sentence below to see your prediction. Make sure all the words are in the Glove embeddings. x_test = np.array(['not feeling happy'])X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)print(x_test[0] +' '+ label_to_emoji(np.argmax(model.predict(X_test_indices)))) not feeling happy 😄 Previously, Emojify-V1 model did not correctly label “not feeling happy,” but our implementation of Emojiy-V2 got it right. (Keras’ outputs are slightly random each time, so you may not have obtained the same result.) The current model still isn’t very robust at understanding negation (like “not happy”) because the training set is small and so doesn’t have a lot of examples of negation. But if the training set were larger, the LSTM model would be much better than the Emojify-V1 model at understanding such complex sentences. Congratulations!You have completed this notebook! ❤️❤️❤️ What you should remember: If you have an NLP task where the training set is small, using word embeddings can help your algorithm significantly. Word embeddings allow your model to work on words in the test set that may not even have appeared in your training set. Training sequence models in Keras (and in most other deep learning frameworks) requires a few important details: To use mini-batches, the sequences need to be padded so that all the examples in a mini-batch have the same length. An Embedding() layer can be initialized with pretrained values. These values can be either fixed or trained further on your dataset. If however your labeled dataset is small, it’s usually not worth trying to train a large pre-trained set of embeddings. LSTM() has a flag called return_sequences to decide if you would like to return every hidden states or only the last one. You can use Dropout() right after LSTM() to regularize your network. Congratulations on finishing this assignment and building an Emojifier. We hope you’re happy with what you’ve accomplished in this notebook! 😀😀😀😀😀😀AcknowledgmentsThanks to Alison Darcy and the Woebot team for their advice on the creation of this assignment. Woebot is a chatbot friend that is ready to speak with you 24/7. As part of Woebot’s technology, it uses word embeddings to understand the emotions of what you say. You can play with it by going to http://woebot.io]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[natural language processing word embeddings]]></title>
    <url>%2F2018%2F06%2F02%2F02_natural-language-processing-word-embeddings%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal lecture note after studying the course nlp sequence models at the 2nd week and the copyright belongs to deeplearning.ai. 01_introduction-to-word-embeddings01_word-representationHello, and welcome back. Last week, we learned about RNNs, GRUs, and LSTMs. In this week, you see how many of these ideas can be applied to NLP, to Natural Language Processing, which is one of the features of AI because it’s really being revolutionized by deep learning. One of the key ideas you learn about is word embeddings, which is a way of representing words. That let your algorithms automatically understand analogies like that, man is to woman, as king is to queen, and many other examples. And through these ideas of word embeddings, you’ll be able to build NPL applications, even with models the size of, usually of relatively small label training sets. Finally towards the end of the week, you’ll see how to debias word embeddings. That’s to reduce undesirable gender or ethnicity or other types of bias that learning algorithms can sometimes pick up. So with that, let’s get started with a discussion on word representation. So far, we’ve been representing words using a vocabulary of words, and a vocabulary from the previous week might be say, 10,000 words. And we’ve been representing words using a one-hot vector. So for example, if man is word number 5391 in this dictionary, then you represent him with a vector with one in position 5391. And I’m also going to use O subscript 5391 to represent this factor, where O here stands for one-hot. And then, if woman is word number 9853, then you represent it with O subscript 9853 which just has a one in position 9853 and zeros elsewhere. And then other words king, queen, apple, orange will be similarly represented with one-hot vector. One of the weaknesses of this representation is that it treats each word as a thing unto itself, and it doesn’t allow an algorithm to easily generalize the cross words. For example, let’s say you have a language model that has learned that when you see “I want a glass of orange “. Well, what do you think the next word will be? Very likely, it’ll be “juice”. But even if the learning algorithm has learned that “I want a glass of orange juice” is a likely sentence, if it sees “I want a glass of apple _“. As far as it knows the relationship between apple and orange is not any closer as the relationship between any of the other words man, woman, king, queen, and orange. And so, it’s not easy for the learning algorithm to generalize from knowing that orange juice is a popular thing, to recognizing that apple juice might also be a popular thing or a popular phrase. And this is because the any product between any two different one-hot vector is zero. If you take any two vectors say, queen and king and any product of them, the end product is zero. If you take apple and orange and any product of them, the end product is zero. And you couldn’t get distance between any pair of these vectors, which is also the same. So it just doesn’t know that somehow apple and orange are much more similar than king and orange or queen and orange. So, won’t it be nice if instead of a one-hot presentation we can instead learn a featurized representation with each of these words, a man, woman, king, queen, apple, orange or really for every word in the dictionary, we could learn a set of features and values for each of them. So for example, each of these words, we want to know what is the gender associated with each of these things. So, if gender goes from minus one for male to plus one for female, then the gender associated with man might be minus one, for woman might be plus one. And then eventually, learning these things maybe for king you get minus 0.95, for queen plus 0.97, and for apple and orange sort of genderless. Another feature might be, well how royal are these things. And so the terms, man and woman are not really royal, so they might have feature values close to zero. Whereas king and queen are highly royal. And apple and orange are not really loyal. How about age? Well, man and woman doesn’t connotes much about age. Maybe men and woman implies that they’re adults, but maybe neither necessarily young nor old. So maybe values close to zero. Whereas kings and queens are always almost always adults. And apple and orange might be more neutral with respect to age. And then, another feature for here, is this is a food? Well, man is not a food, woman is not a food, neither are kings and queens, but apples and oranges are foods. And they can be many other features as well ranging from, what is the size of this? What is the cost? Is this something that is a live? Is this an action, or is this a noun, or is this a verb, or is it something else? And so on. So you can imagine coming up with many features. And for the sake of the illustration let’s say, 300 different features, and what that does is, it allows you to take this list of numbers, I’ve only written four here, but this could be a list of 300 numbers, that then becomes a 300 dimensional vector for representing the word man. And I’m going to use the notation e subscript 5391 to denote a representation like this. And similarly, this vector, this 300 dimensional vector or 300 dimensional vector like this, I would denote e9853 to denote a 300 dimensional vector we could use to represent the word woman. And similarly, for the other examples here. Now, if you use this representation to represent the words orange and apple, then notice that the representations for orange and apple are now quite similar. Some of the features will differ because of the color of an orange, the color an apple, the taste, or some of the features would differ. But by a large, a lot of the features of apple and orange are actually the same, or take on very similar values. And so, this increases the odds of the learning algorithm that has figured out that orange juice is a thing, to also quickly figure out that apple juice is a thing. So this allows it to generalize better across different words. So over the next few videos, we’ll find a way to learn words embeddings. We just need you to learn high dimensional feature vectors like these, that gives a better representation than one-hot vectors for representing different words. And the features we’ll end up learning, won’t have a easy to interpret interpretation like that component one is gender, component two is royal, component three is age and so on. Exactly what they’re representing will be a bit harder to figure out. But nonetheless, the featurized representations we will learn, will allow an algorithm to quickly figure out that apple and orange are more similar than say, king and orange or queen and orange. If we’re able to learn a 300 dimensional feature vector or 300 dimensional embedding for each words, one of the popular things to do is also to take this 300 dimensional data and embed it say, in a two dimensional space so that you can visualize them. And so, one common algorithm for doing this is the t-SNE algorithm due to Laurens van der Maaten and Geoff Hinton. And if you look at one of these embeddings, one of these representations, you find that words like man and woman tend to get grouped together, king and queen tend to get grouped together, and these are the people which tends to get grouped together. Those are animals who can get grouped together. Fruits will tend to be close to each other. Numbers like one, two, three, four, will be close to each other. And then, maybe the animate objects as whole will also tend to be grouped together. But you see plots like these sometimes on the internet to visualize some of these 300 or higher dimensional embeddings. And maybe this gives you a sense that, word embeddings algorithms like this can learn similar features for concepts that feel like they should be more related, as visualized by that concept that seem to you and me like they should be more similar, end up getting mapped to a more similar feature vectors. And these representations will use these sort of featurized representations in maybe a 300 dimensional space, these are called embeddings. And the reason we call them embeddings is, you can think of a 300 dimensional space. And again, they can’t draw out here in two dimensional space because it’s a 3D one. And what you do is you take every words like orange, and have a three dimensional feature vector so that word orange gets embedded to a point in this 300 dimensional space. And the word apple, gets embedded to a different point in this 300 dimensional space. And of course to visualize it, algorithms like t-SNE, map this to a much lower dimensional space, you can actually plot the 2D data and look at it. But that’s what the term embedding comes from. Word embeddings has been one of the most important ideas in NLP, in Natural Language Processing. In this video, you saw why you might want to learn or use word embeddings. In the next video, let’s take a deeper look at how you’ll be able to use these algorithms, to build NLP algorithims. 02_using-word-embeddingsIn the last video, you saw what it might mean to learn a featurized representations of different words. In this video, you see how we can take these representations and plug them into NLP applications. Let’s start with an example. Continuing with the named entity recognition example, if you’re trying to detect people’s names. Given a sentence like Sally Johnson is an orange farmer, hopefully, you’ll figure out that Sally Johnson is a person’s name, hence, the outputs 1 like that. And one way to be sure that Sally Johnson has to be a person, rather than say the name of the corporation is that you know orange farmer is a person. So previously, we had talked about one hot representations to represent these words, x(1), x(2), and so on. But if you can now use the featurized representations, the embedding vectors that we talked about in the last video. Then after having trained a model that uses word embeddings as the inputs, if you now see a new input, Robert Lin is an apple farmer. Knowing that orange and apple are very similar will make it easier for your learning algorithm to generalize to figure out that Robert Lin is also a human, is also a person’s name. One of the most interesting cases will be, what if in your test set you see not Robert Lin is an apple farmer, but you see much less common words? What if you see Robert Lin is a durian cultivator? A durian is a rare type of fruit, popular in Singapore and a few other countries. But if you have a small label training set for the named entity recognition task, you might not even have seen the word durian or seen the word cultivator in your training set. I guess technically, this should be a durian cultivator. But if you have learned a word embedding that tells you that durian is a fruit, so it’s like an orange, and a cultivator, someone that cultivates is like a farmer, then you might still be generalize from having seen an orange farmer in your training set to knowing that a durian cultivator is also probably a person. So one of the reasons that word embeddings will be able to do this is the algorithms to learning word embeddings can examine very large text corpuses, maybe found off the Internet. So you can examine very large data sets, maybe a billion words, maybe even up to 100 billion words would be quite reasonable. So very large training sets of just unlabeled text. And by examining tons of unlabeled text, which you can download more or less for free, you can figure out that orange and durian are similar. And farmer and cultivator are similar, and therefore, learn embeddings, that groups them together. Now having discovered that orange and durian are both fruits by reading massive amounts of Internet text, what you can do is then take this word embedding and apply it to your named entity recognition task, for which you might have a much smaller training set, maybe just 100,000 words in your training set, or even much smaller. And so this allows you to carry out transfer learning, where you take information you’ve learned from huge amounts of unlabeled text that you can suck down essentially for free off the Internet to figure out that orange, apple, and durian are fruits. And then transfer that knowledge to a task, such as named entity recognition, for which you may have a relatively small labeled training set. And, of course, for simplicity, l drew this for it only as a unidirectional RNN. If you actually want to carry out the named entity recognition task, you should, of course, use a bidirectional RNN rather than a simpler one I’ve drawn here. But to summarize, this is how you can carry out transfer learning using word embeddings. Step 1 is to learn word embeddings from a large text corpus, a very large text corpus or you can also download pre-trained word embeddings online. There are several word embeddings that you can find online under very permissive licenses. And you can then take these word embeddings and transfer the embedding to new task, where you have a much smaller labeled training sets. And use this, let’s say, 300 dimensional embedding, to represent your words. One nice thing also about this is you can now use relatively lower dimensional feature vectors. So rather than using a 10,000 dimensional one-hot vector, you can now instead use maybe a 300 dimensional dense vector. Although the one-hot vector is fast and the 300 dimensional vector that you might learn for your embedding will be a dense vector. And then, finally, as you train your model on your new task, on your named entity recognition task with a smaller label data set, one thing you can optionally do is to continue to fine tune, continue to adjust the word embeddings with the new data. In practice, you would do this only if this task 2 has a pretty big data set. If your label data set for step 2 is quite small, then usually, I would not bother to continue to fine tune the word embeddings. So word embeddings tend to make the biggest difference when the task you’re trying to carry out has a relatively smaller training set. So it has been useful for many NLP tasks. And I’ll just name a few. Don’t worry if you don’t know these terms. It has been useful for named entity recognition, for text summarization, for co-reference resolution, for parsing. These are all maybe pretty standard NLP tasks. It has been less useful for language modeling, machine translation, especially if you’re accessing a language modeling or machine translation task for which you have a lot of data just dedicated to that task. So as seen in other transfer learning settings, if you’re trying to transfer from some task A to some task B, the process of transfer learning is just most useful when you happen to have a ton of data for A and a relatively smaller data set for B. And so that’s true for a lot of NLP tasks, and just less true for some language modeling and machine translation settings. Finally, word embeddings has a interesting relationship to the face encoding ideas that you learned about in the previous course, if you took the convolutional neural networks course. So you will remember that for face recognition, we train this Siamese network architecture that would learn, say, a 128 dimensional representation for different faces. And then you can compare these encodings in order to figure out if these two pictures are of the same face. The words encoding and embedding mean fairly similar things. So in the face recognition literature, people also use the term encoding to refer to these vectors, f(x(i)) and f(x(j)). One difference between the face recognition literature and what we do in word embeddings is that, for face recognition, you wanted to train a neural network that can take as input any face picture, even a picture you’ve never seen before, and have a neural network compute an encoding for that new picture. Whereas what we’ll do, and you’ll understand this better when we go through the next few videos, whereas what we’ll do for learning word embeddings is that we’ll have a fixed vocabulary of, say, 10,000 words. And we’ll learn a vector e1 through, say, e10,000 that just learns a fixed encoding or learns a fixed embedding for each of the words in our vocabulary. So that’s one difference between the set of ideas you saw for face recognition versus what the algorithms we’ll discuss in the next few videos. But the terms encoding and embedding are used somewhat interchangeably. So the difference I just described is not represented by the difference in terminologies. It’s just a difference in how we need to use these algorithms in face recognition, where there’s unlimited sea of pictures you could see in the future. Versus natural language processing, where there might be just a fixed vocabulary, and everything else like that we’ll just declare as an unknown word. So in this video, you saw how using word embeddings allows you to implement this type of transfer learning. And how, by replacing the one-hot vectors we’re using previously with the embedding vectors, you can allow your algorithms to generalize much better, or you can learn from much less label data. Next, I want to show you just a few more properties of these word embeddings. And then after that, we will talk about algorithms for actually learning these word embeddings. Let’s go on to the next video, where you’ll see how word embeddings can help with reasoning about analogies. 03_properties-of-word-embeddingsBy now, you should have a sense of how word embeddings can help you build NLP applications. One of the most fascinating properties of word embeddings is that they can also help with analogy reasoning. And while reasonable analogies may not be by itself the most important NLP application, they might also help convey a sense of what these word embeddings are doing, what these word embeddings can do. Let me show you what I mean here are the featurized representations of a set of words that you might hope a word embedding could capture. Let’s say I pose a question, man is to woman as king is to what? Many of you will say, man is to woman as king is to queen. But is it possible to have an algorithm figure this out automatically? Well, here’s how you could do it, let’s say that you’re using this four dimensional vector to represent man. So this will be your E5391, although just for this video, let me call this e subscript man. And let’s say that’s the embedding vector for woman, so I’m going to call that e subscript woman, and similarly for king and queen. And for this example, I’m just going to assume you’re using four dimensional embeddings, rather than anywhere from 50 to 1,000 dimensional, which would be more typical. One interesting property of these vectors is that if you take the vector, e man, and subtract the vector e woman, then, You end up with approximately -1, negative another 1 is -2, decimal 0- 0, 0- 0, close to 0- 0, so you get roughly -2 0 0 0. And similarly if you take e king minus e queen, then that’s approximately the same thing. That’s about -1- 0.97, it’s about -2. This is about 1- 1, since kings and queens are both about equally royal. So that’s 0, and then age difference, food difference, 0. And so what this is capturing is that the main difference between man and woman is the gender. And the main difference between king and queen, as represented by these vectors, is also the gender. Which is why the difference e man- e woman, and the difference e king- e queen, are about the same. So one way to carry out this analogy reasoning is, if the algorithm is asked, man is to woman as king is to what? What it can do is compute e man- e woman, and try to find a vector, try to find a word so that e man- e woman is close to e king- e of that new word. And it turns out that when queen is the word plugged in here, then the left hand side is close to the the right hand side. So these ideas were first pointed out by Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. And it’s been one of the most remarkable and surprisingly influential results about word embeddings. And I think has helped the whole community get better intuitions about what word embeddings are doing. So let’s formalize how you can turn this into an algorithm. In pictures, the word embeddings live in maybe a 300 dimensional space. And so the word man is represented as a point in the space, and the word woman is represented as a point in the space. And the word king is represented as another point, and the word queen is represented as another point. And what we pointed out really on the last slide is that the vector difference between man and woman is very similar to the vector difference between king and queen. And this arrow I just drew is really the vector that represents a difference in gender. And remember, these are points we’re plotting in a 300 dimensional space. So in order to carry out this kind of analogical reasoning to figure out, man is to woman is king is to what, what you can do is try to find the word w, So that, This equation holds true, so you want there to be, A high degree of a similarity, between I’m going to use s, And so what you want is to find the word w that maximizes the similarity between, e w compared to e king- e man + e woman Right, so what I did is, I took this e question mark, and replaced that with ew, and then brought ew to just one side of the equation. And then the other three terms to the right hand side of this equation. So we have some appropriate similarity function for measuring how similar is the embedding of some word w to this quantity of the right. Then finding the word that maximizes the similarity should hopefully let you pick out the word queen. And the remarkable thing is, this actually works. If you learn a set of word embeddings and find a word w that maximizes this type of similarity, you can actually get the exact right answer. Depending on the details of the task, but if you look at research papers, it’s not uncommon for research papers to report anywhere from, say, 30% to 75% accuracy on analogy using tasks like these. Where you count an anology attempt as correct only if it guesses the exact word right. So only if, in this case, it picks out the word queen. Before moving on, I just want to clarify what this plot on the left is. Previously, we talked about using algorithms like t-SNE to visualize words. What t-SNE does is, it takes 300-D data, and it maps it in a very non-linear way to a 2D space. And so the mapping that t-SNE learns, this is a very complicated and very non-linear mapping. So after the t-SNE mapping, you should not expect these types of parallelogram relationships, like the one we saw on the left, to hold true. And it’s really in this original 300 dimensional space that you can more reliably count on these types of parallelogram relationships in analogy pairs to hold true. And it may hold true after a mapping through t-SNE, but in most cases, because of t-SNE’s non-linear mapping, you should not count on that. And many of the parallelogram analogy relationships will be broken by t-SNE. Now, before moving on, let me just quickly describe the similarity function that is most commonly used. So the most commonly used similarity function is called cosine similarity. So this is the equation we had from the previous slide. So in cosine similarity, you define the similarity between two vectors u and v as u transpose v divided by the lengths by the Euclidean lengths. So ignoring the denominator for now, this is basically the inner product between u and v. And so if u and v are very similar, their inner product will tend to be large. And this is called cosine similarity because this is actually the cosine of the angle between the two vectors, u and v. So that’s the angle phi, so this formula is actually the cosine between them. And so you remember from calculus that if this phi, then the cosine of phi looks like this. So if the angle between them is 0, then the cosine similarity is equal to 1. And if their angle is 90 degrees, the cosine similarity is 0. And then if they’re 180 degrees, or pointing in completely opposite directions, it ends up being -1. So that’s where the term cosine similarity comes from, and it works quite well for these analogy reasoning tasks. If you want, you can also use square distance or Euclidian distance, u-v squared. Technically, this would be a measure of dissimilarity rather than a measure of similarity. So we need to take the negative of this, and this will work okay as well. Although I see cosine similarity being used a bit more often. And the main difference between these is how it normalizes the lengths of the vectors u and v. So one of the remarkable results about word embeddings is the generality of analogy relationships they can learn. So for example, it can learn that man is to woman as boy is to girl, because the vector difference between man and woman, similar to king and queen and boy and girl, is primarily just the gender. It can learn that Ottawa, which is the capital of Canada, that Ottawa is to Canada as Nairobi is to Kenya. So that’s the city capital is to the name of the country. It can learn that big is to bigger as tall is to taller, and it can learn things like that. Yen is to Japan, since yen is the currency of Japan, as ruble is to Russia. And all of these things can be learned just by running a word embedding learning algorithm on the large text corpus. It can spot all of these patterns by itself, just by running from very large bodies of text. So in this video, you saw how word embeddings can be used for analogy reasoning. And while you might not be trying to build an analogy reasoning system yourself as an application, this I hope conveys some intuition about the types of feature-like representations that these representations can learn. And you also saw how cosine similarity can be a way to measure the similarity between two different word embeddings. Now, we talked a lot about properties of these embeddings and how you can use them. Next, let’s talk about how you’d actually learn these word embeddings, let’s go on to the next video. 04_embedding-matrixLet’s start to formalize the problem of learning a good word embedding. When you implement an algorithm to learn a word embedding, what you end up learning is an embedding matrix. Let’s take a look at what I means. Let’s say, as usual we’re using our 10,000-word vocabulary. So, the vocabulary has A, Aaron, Orange, Zulu, maybe also unknown word as a token. What we’re going to do is learn embedding matrix E, which is going to be a 300 dimensional by 10,000 dimensional matrix, if you have 10,000 words vocabulary or maybe 10,001 is our word token, there’s one extra token. And the columns of this matrix would be the different embeddings for the 10,000 different words you have in your vocabulary. So, Orange was word number 6257 in our vocabulary of 10,000 words. So, one piece of notation we’ll use is that 06257 was the one-hot vector with zeros everywhere and a one in position 6257. And so, this will be a 10,000-dimensional vector with a one in just one position. So, this isn’t quite a drawn scale. Yes, this should be as tall as the embedding matrix on the left is wide. And if the embedding matrix is called capital E then notice that if you take E and multiply it by just one-hot vector by 0 of 6257, then this will be a 300-dimensional vector. So, E is 300 by 10,000 and 0 is 10,000 by 1. So, the product will be 300 by 1, so with 300-dimensional vector and notice that to compute the first element of this vector, of this 300-dimensional vector, what you do is you will multiply the first row of the matrix E with this. But all of these elements are zero except for element 6257 and so you end up with zero times this, zero times this, zero times this, and so on. And then, 1 times whatever this is, and zero times this, zero times this, zero times and so on. And so, you end up with the first element as whatever is that elements up there, under the Orange column. And then, for the second element of this 300-dimensional vector we’re computing, you would take the vector 0657 and multiply it by the second row with the matrix E. So again, you have zero times this, plus zero times this, plus zero times all of these are the elements and then one times this, and then zero times everything else and add that together. So you end up with this and so on as you go down the rest of this column. So, that’s why the embedding matrix E times this one-hot vector here winds up selecting out this 300-dimensional column corresponding to the word Orange. So, this is going to be equal to E 6257 which is the notation we’re going to use to represent the embedding vector that 300 by one dimensional vector for the word Orange. And more generally, E for a specific word W, this is going to be embedding for a word W. And more generally, E times O substitute J, one-hot vector with one that position J, this is going to be E_J and that’s going to be the embedding for word J in the vocabulary. So, the thing to remember from this slide is that our goal will be to learn an embedding matrix E and what you see in the next video is you initialize E randomly and you’re straight in the sense to learn all the parameters of this 300 by 10,000 dimensional matrix and E times this one-hot vector gives you the embedding vector. Now just one note, when we’re writing the equation, it’ll be convenient to write this type of notation where you take the matrix E and multiply it by the one-hot vector O. But if when you’re implementing this, it is not efficient to actually implement this as a mass matrix vector multiplication because the one-hot vectors, now this is a relatively high dimensional vector and most of these elements are zero. So, it’s actually not efficient to use a matrix vector multiplication to implement this because if we multiply a whole bunch of things by zeros and so the practice, you would actually use a specialized function to just look up a column of the Matrix E rather than do this with the matrix multiplication. But writing of the map, it is just convenient to write it out this way. So, in Keras’s for example there is a embedding layer and we use the embedding layer then it more efficiently just pulls out the column you want from the embedding matrix rather than does it with a much slower matrix vector multiplication. So, in this video you saw the notations were used to describe algorithms to learning these embeddings and the key terminology is this matrix capital E which contain all the embeddings for the words of the vocabulary. In the next video, we’ll start to talk about specific algorithms for learning this matrix E. Let’s go onto the next video. 02_learning-word-embeddings-word2vec-glove01_learning-word-embeddingsIn this video, you’ll start to learn some concrete algorithms for learning word embeddings. In the history of deep learning as applied to learning word embeddings, people actually started off with relatively complex algorithms. And then over time, researchers discovered they can use simpler and simpler and simpler algorithms and still get very good results especially for a large dataset. But what happened is, some of the algorithms that are most popular today, they are so simple that if I present them first, it might seem almost a little bit magical, how can something this simple work? So, what I’m going to do is start off with some of the slightly more complex algorithms because I think it’s actually easier to develop intuition about why they should work, and then we’ll move on to simplify these algorithms and show you some of the simple algorithms that also give very good results. So, let’s get started. Let’s say you’re building a language model and you do it with a neural network. So, during training, you might want your neural network to do something like input, I want a glass of orange, and then predict the next word in the sequence. And below each of these words, I have also written down the index in the vocabulary of the different words. So it turns out that building a neural language model is the small way to learn a set of embeddings. And the ideas I present on this slide were due to Yoshua Bengio, Rejean Ducharme, Pascals Vincent, and Christian Jauvin. So, here’s how you can build a neural network to predict the next word in the sequence. Let me take the list of words, I want a glass of orange, and let’s start with the first word I. So I’m going to construct one add vector corresponding to the word I. So there’s a one add vector with a one in position, 4343. So this is going to be 10,000 dimensional vector. And what we’re going to do is then have a matrix of parameters E, and take E times O to get an embedding vector e4343, and this step really means that e4343 is obtained by the matrix E times the one add vector 43. And then we’ll do the same for all of the other words. So the word want, is where 9665 one add vector, multiply by E to get the embedding vector. And similarly, for all the other words. A, is a first word in dictionary, alphabetic comes first, so there is O one, gets this E one. And similarly, for the other words in this phrase. So now you have a bunch of three dimensional embedding, so each of this is a 300 dimensional embedding vector. And what we can do, is fill all of them into a neural network. So here is the neural network layer. And then this neural network feeds to a softmax, which has it’s own parameters as well. And a softmax classifies among the 10,000 possible outputs in the vocab for those final word we’re trying to predict. And so, if in the training slide we saw the word juice then, the target for the softmax in training repeat that it should predict the other word juice was what came after this. So this hidden name here will have his own parameters. So have some, I’m going to call this W1 and there’s also B1. The softmax there was this own parameters W2, B2, and they’re using 300 dimensional word embeddings, then here we have six words. So, this would be six times 300. So this layer or this input will be a 1,800 dimensional vector obtained by taking your six embedding vectors and stacking them together. Well, what’s actually more commonly done is to have a fixed historical window. So for example, you might decide that you always want to predict the next word given say the previous four words, where four here is a hyperparameter of the algorithm. So this is how you adjust to either very long or very short sentences or you decide to always just look at the previous four words, so you say, I will still use those four words. And so, let’s just get rid of these. And so, if you’re always using a four word history, this means that your neural network will input a 1,200 dimensional feature vector, go into this layer, then have a softmax and try to predict the output. And again, variety of choices. And using a fixed history, just means that you can deal with even arbitrarily long sentences because the input sizes are always fixed. So, the parameters of this model will be this matrix E, and use the same matrix E for all the words. So you don’t have different matrices for different positions in the proceedings four words, is the same matrix E. And then, these weights are also parameters of the algorithm and you can use backprop to perform gradient descent to maximize the likelihood of your training set to just repeatedly predict given four words in a sequence, what is the next word in your text corpus? And it turns out that this algorithm we’ll learn pretty decent word embeddings. And the reason is, if you remember our orange juice, apple juice example, is in the algorithm’s incentive to learn pretty similar word embeddings for orange and apple because doing so allows it to fit the training set better because it’s going to see orange juice sometimes, or see apple juice sometimes, and so, if you have only a 300 dimensional feature vector to represent all of these words, the algorithm will find that it fits the training set fast. If apples, oranges, and grapes, and pears, and so on and maybe also durians which is a very rare fruit and that with similar feature vectors. So, this is one of the earlier and pretty successful algorithms for learning word embeddings, for learning this matrix E. But now let’s generalize this algorithm and see how we can derive even simpler algorithms. So, I want to illustrate the other algorithms using a more complex sentence as our example. Let’s say that in your training set, you have this longer sentence, I want a glass of orange juice to go along with my cereal. So, what we saw on the last slide was that the job of the algorithm was to predict some word juice, which we are going to call the target words, and it was given some context which was the last four words. And so, if your goal is to learn a embedding of researchers I’ve experimented with many different types of context. If it goes to build a language model then is natural for the context to be a few words right before the target word. But if your goal is into learn the language model per se, then you can choose other contexts. For example, you can pose a learning problem where the context is the four words on the left and right. So, you can take the four words on the left and right as the context, and what that means is that we’re posing a learning problem where the algorithm is given four words on the left. So, a glass of orange, and four words on the right, to go along with, and this has to predict the word in the middle. And posing a learning problem like this where you have the embeddings of the left four words and the right four words feed into a neural network, similar to what you saw in the previous slide, to try to predict the word in the middle, try to put it target word in the middle, this can also be used to learn word embeddings. Or if you want to use a simpler context, maybe you’ll just use the last one word. So given just the word orange, what comes after orange? So this will be different learning problem where you tell it one word, orange, and will say well, what do you think is the next word. And you can construct a neural network that just fits in the word, the one previous word or the embedding of the one previous word to a neural network as you try to predict the next word. Or, one thing that works surprisingly well is to take a nearby one word. Some might tell you that, well, take the word glass, is somewhere close by. Some might say, I saw the word glass and then there’s another words somewhere close to glass, what do you think that word is? So, that’ll be using nearby one word as the context. And we’ll formalize this in the next video but this is the idea of a Skip-Gram model, and just an example of a simpler algorithm where the context is now much simpler, is just one word rather than four words, but this works remarkably well. So what researchers found was that if you really want to build a language model, it’s natural to use the last few words as a context. But if your main goal is really to learn a word embedding, then you can use all of these other contexts and they will result in very meaningful work embeddings as well. I will formalize the details of this in the next video where we talk about the Word2Vec model. To summarize, in this video you saw how the language modeling problem which causes the pose of machines learning problem where you input the context like the last four words and predicts some target words, how posing that problem allows you to learn input word embedding. In the next video, you’ll see how using even simpler context and even simpler learning algorithms to mark from context to target word, can also allow you to learn a good word embedding. Let’s go on to the next video where we’ll discuss the Walter VEC. 02_word2vecIn the last video, you saw how you can learn a neural language model in order to get good word embeddings. In this video, you see the Word2Vec algorithm which is simple and comfortably more efficient way to learn this types of embeddings. Lets take a look. Most of the ideas I’ll present in this video are due to Tomas Mikolov, Kai Chen, Greg Corrado, and Jeff Dean. Let’s say you’re given this sentence in your training set. In the skip-gram model, what we’re going to do is come up with a few context to target pairs to create our supervised learning problem. So rather than having the context be always the last four words or the last end words immediately before the target word, what I’m going to do is, say, randomly pick a word to be the context word. And let’s say we chose the word orange. And what we’re going to do is randomly pick another word within some window. Say plus minus five words or plus minus ten words of the context word and we choose that to be target word. So maybe just by chance you might pick juice to be a target word, that’s just one word later. Or you might choose two words before. So you have another pair where the target could be glass or, Maybe just by chance you choose the word my as the target. And so we’ll set up a supervised learning problem where given the context word, you’re asked to predict what is a randomly chosen word within say, a plus minus ten word window, or plus minus five or ten word window of that input context word. And obviously, this is not a very easy learning problem, because within plus minus 10 words of the word orange, it could be a lot of different words. But a goal of setting up this supervised learning problem, isn’t to do well on the supervised learning problem per se, it is that we want to use this learning problem to learn good word embeddings. So, here are the details of the model. Let’s say that we’ll continue to our vocab of 10,000 words. And some have been on vocab sizes that exceeds a million words. But the basic supervised learning problem we’re going to solve is that we want to learn the mapping from some Context c, such as the word orange to some target, which we will call t, which might be the word juice or the word glass or the word my, if we use the example from the previous slide. So in our vocabulary, orange is word 6257, and the word juice is the word 4834 in our vocab of 10,000 words. And so that’s the input x that you want to learn to map to that open y. So to represent the input such as the word orange, you can start out with some one hot vector which is going to be write as $o_c$, so there’s a one hot vector for the context words. And then similar to what you saw on the last video you can take the embedding matrix E, multiply E by the vector $o_c$, and this gives you your embedding vector for the input context word, so here $e_c$ is equal to capital E times that one hot vector. Then in this new network that we formed we’re going to take this vector $e_c$ and feed it to a softmax unit. So I’ve been drawing softmax unit as a node in a neural network. That’s not an o, that’s a softmax unit. And then there’s a drop in the softmax unit to output $\hat{y}$. So to write out this model in detail. This is the model, the softmax model, probability of different tanka words given the input context word as e to the e, theta t transpose,$e_c$. Divided by some over all words, so we’re going to say, sum from J equals one to all 10,000 words of e to the theta j transposed $e_c$. So here theta T is the parameter associated with, I’ll put t, but really there’s a chance of a particular word, t, being the label. So I’ve left off the biased term to solve mass but we could include that too if we wish. And then finally the loss function for softmax will be the usual. So we use y to represent the target word. And we use a one-hot representation for y hat and y here. Then the lost would be The negative log liklihood, so sum from i equals 1 to 10,000 of $y_ilog(\hat{y}_i)$. So that’s a usual loss for softmax where we’re representing the target y as a one hot vector. So this would be a one hot vector with just 1 1 and the rest zeros. And if the target word is juice, then it’d be element 4834 from up here. That is equal to 1 and the rest will be equal to 0. And similarly Y hat will be a 10,000 dimensional vector output by the softmax unit with probabilities for all 10,000 possible targets words. So to summarize, this is the overall little model, little neural network with basically looking up the embedding and then just a soft max unit. And the matrix E will have a lot of parameters, so the matrix E has parameters corresponding to all of these embedding vectors, $e_c$. And then the softmax unit also has parameters that gives the theta T parameters but if you optimize this loss function with respect to the all of these parameters, you actually get a pretty good set of embedding vectors. So this is called the skip-gram model because is taking as input one word like orange and then tr$y_i$ng to predict some words skipping a few words from the left or the right side. To predict what comes little bit before little bit after the context words. Now, it turns out there are a couple problems with using this algorithm. And the primary problem is computational speed. In particular, for the softmax model, every time you want to evaluate this probability, you need to carry out a sum over all 10,000 words in your vocabulary. And maybe 10,000 isn’t too bad, but if you’re using a vocabulary of size 100,000 or a 1,000,000, it gets really slow to sum up over this denominator every single time. And, in fact, 10,000 is actually already that will be quite slow, but it makes even harder to scale to larger vocabularies. So there are a few solutions to this, one which you see in the literature is to use a hierarchical softmax classifier. And what that means is, instead of trying to categorize something into all 10,000 carries on one go. Imagine if you have one classifier, it tells you is the target word in the first 5,000 words in the vocabulary? Or is in the second 5,000 words in the vocabulary? And lets say this binary cost that it tells you this is in the first 5,000 words, think of second class to tell you that this in the first 2,500 words of vocab or in the second 2,500 words vocab and so on. Until eventually you get down to classify exactly what word it is, so that the leaf of this tree, and so having a tree of classifiers like this, means that each of the retriever nodes of the tree can be just a binding classifier. And so you don’t need to sum over all 10,000 words or else it will capsize in order to make a single classification. In fact, the computational classifying tree like this scales like log of the vocab size rather than linear in vocab size. So this is called a hierarchical softmax classifier. I should mention in practice, the hierarchical softmax classifier doesn’t use a perfectly balanced tree or this perfectly symmetric tree, with equal numbers of words on the left and right sides of each branch. In practice, the hierarchical softmax classifier can be developed so that the common words tend to be on top, whereas the less common words like durian can be buried much deeper in the tree. Because you see the more common words more often, and so you might need only a few traversals to get to common words like the and of. Whereas you see less frequent words like durian much less often, so it says okay that are buried deep in the tree because you don’t need to go that deep. So there are various heuristics for building the tree how you used to build the hierarchical software spire. So this is one idea you see in the literature, the speeding up the softmax classification. But I won’t spend too much more time. And you can read more details of this on the paper that I referenced by Thomas and others, on the first slide. But I won’t spend too much more time on this. Because in the next video, where she talk about a different method, called nectar sampling, which I think is even simpler. And also works really well for speeding up the softmax classifier and the problem of needing the sum over the entire cap size in the denominator. So you see more of that in the next video. But before moving on, one quick Topic I want you to understand is how to sample the context C. So once you sample the context C, the target T can be sampled within, say, a plus minus ten word window of the context C, but how do you choose the context C? One thing you could do is just sample uniformly, at random, from your training corpus. When we do that, you find that there are some words like the, of, a, and, to and so on that appear extremely frequently. And so, if you do that, you find that in your context to target mapping pairs just get these these types of words extremely frequently, whereas there are other words like orange, apple, and also durian that don’t appear that often. And maybe you don’t want your training site to be dominated by these extremely frequently or current words, because then you spend almost all the effort updating $e_c$, for those frequently occurring words. But you want to make sure that you spend some time updating the embedding, even for these less common words like e durian. So in practice the distribution of words $P(c)$ isn’t taken just entirely uniformly at random for the training set purpose, but instead there are different heuristics that you could use in order to balance out something from the common words together with the less common words. So that’s it for the Word2Vec skip-gram model. If you read the original paper by that I referenced earlier, you find that that paper actually had two versions of this Word2Vec model, the skip gram was one. And the other one is called the CBow, the continuous backwards model, which takes the surrounding contexts from middle word, and uses the surrounding words to try to predict the middle word, and that algorithm also works, it has some advantages and disadvantages. But the key problem with this algorithm with the skip-gram model as presented so far is that the softmax step is very expensive to calculate because needing to sum over your entire vocabulary size into the denominator of the soft packs. In the next video I show you an algorithm that modifies the training objective that makes it run much more efficiently therefore lets you apply this in a much bigger fitting set as well and therefore learn much better word embeddings. Lets go onto the next video. 03_negative-samplingIn the last video, you saw how the Skip-Gram model allows you to construct a supervised learning task. So we map from context to target and how that allows you to learn a useful word embedding. But the downside of that was the Softmax objective was slow to compute. In this video, you’ll see a modified learning problem called negative sampling that allows you to do something similar to the Skip-Gram model you saw just now, but with a much more efficient learning algorithm. Let’s see how you can do this. Most of the ideas presented in this video are due to Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeff Dean. So what we’re going to do in this algorithm is create a new supervised learning problem. And the problem is, given a pair of words like orange and juice, we’re going to predict, is this a context-target pair? So in this example, orange juice was a positive example. And how about orange and king? Well, that’s a negative example, so I’m going to write 0 for the target. So what we’re going to do is we’re actually going to sample a context and a target word. So in this case, we have orange and juice and we’ll associate that with a label of 1, so just put words in the middle. And then having generated a positive example, so the positive example is generated exactly how we generated it in the previous videos. Sample a context word, look around a window of say, plus-minus ten words and pick a target word. So that’s how you generate the first row of this table with orange, juice, 1. And then to generate a negative example, you’re going to take the same context word and then just pick a word at random from the dictionary. So in this case, I chose the word king at random and we will label that as 0. And then let’s take orange and let’s pick another random word from the dictionary. Under the assumption that if we pick a random word, it probably won’t be associated with the word orange, so orange, book, 0. And let’s pick a few others, orange, maybe just by chance, we’ll pick the 0 and then orange. And then orange, and maybe just by chance, we’ll pick the word of and we’ll put a 0 there. And notice that all of these are labeled as 0 even though the word of actually appears next to orange as well. So to summarize, the way we generated this data set is, we’ll pick a context word and then pick a target word and that is the first row of this table. That gives us a positive example. So context, target, and then give that a label of 1. And then what we’ll do is for some number of times say, k times, we’re going to take the same context word and then pick random words from the dictionary, king, book, the, of, whatever comes out at random from the dictionary and label all those 0, and those will be our negative examples. And it’s okay if just by chance, one of those words we picked at random from the dictionary happens to appear in the window, in a plus-minus ten word window say, next to the context word, orange. Then we’re going to create a supervised learning problem where the learning algorithm inputs x, inputs this pair of words, and it has to predict the target label to predict the output y. So the problem is really given a pair of words like orange and juice, do you think they appear together? Do you think I got these two words by sampling two words close to each other? Or do you think I got them as one word from the text and one word chosen at random from the dictionary? It’s really to try to distinguish between these two types of distributions from which you might sample a pair of words. So this is how you generate the training set. How do you choose k, Mikolov et al, recommend that maybe k is 5 to 20 for smaller data sets. And if you have a very large data set, then chose k to be smaller. So k equals 2 to 5 for larger data sets, and large values of k for smaller data sets. Okay, and in this example, I’ll just use k = 4. Next, let’s describe the supervised learning model for learning a mapping from x to y. So here was the Softmax model you saw from the previous video. And here is the training set we got from the previous slide where again, this is going to be the new input x and this is going to be the value of y you’re trying to predict. So to define the model, I’m going to use this to denote, this was c for the context word, this to denote the possible target word, t, and this, I’ll use y to denote 0, 1, this is a context target pair. So what we’re going to do is define a logistic regression model. Say, that the chance of y = 1, given the input c, t pair, we’re going to model this as basically a regression model, but the specific formula we’ll use s sigma applied to theta transpose, theta t transpose, e c. So the parameters are similar as before, you have one parameter vector theta for each possible target word. And a separate parameter vector, really the embedding vector, for each possible context word. And we’re going to use this formula to estimate the probability that y is equal to 1. So if you have k examples here, then you can think of this as having a k to 1 ratio of negative to positive examples. So for every positive examples, you have k negative examples with which to train this logistic regression-like model. And so to draw this as a neural network, if the input word is orange, Which is word 6257, then what you do is, you input the one hop vector passing through e, do the multiplication to get the embedding vector 6257. And then what you have is really 10,000 possible logistic regression classification problems. Where one of these will be the classifier corresponding to, well, is the target word juice or not? And then there will be other words, for example, there might be ones somewhere down here which is predicting, is the word king or not and so on, for these possible words in your vocabulary. So think of this as having 10,000 binary logistic regression classifiers, but instead of training all 10,000 of them on every iteration, we’re only going to train five of them. We’re going to train the one responding to the actual target word we got and then train four randomly chosen negative examples. And this is for the case where k is equal to 4. So instead of having one giant 10,000 way Softmax, which is very expensive to compute, we’ve instead turned it into 10,000 binary classification problems, each of which is quite cheap to compute. And on every iteration, we’re only going to train five of them or more generally, k + 1 of them, of k negative examples and one positive examples. And this is why the computation cost of this algorithm is much lower because you’re updating k + 1, let’s just say units, k + 1 binary classification problems. Which is relatively cheap to do on every iteration rather than updating a 10,000 way Softmax classifier. So you get to play with this algorithm in the problem exercise for this week as well. So this technique is called negative sampling because what you’re doing is, you have a positive example, the orange and then juice. And then you will go and deliberately generate a bunch of negative examples, negative samplings, hence, the name negative sampling, with which to train four more of these binary classifiers. And on every iteration, you choose four different random negative words with which to train your algorithm on. Now, before wrapping up, one more important detail with this algorithm is, how do you choose the negative examples? So after having chosen the context word orange, how do you sample these words to generate the negative examples? So one thing you could do is sample the words in the middle, the candidate target words. One thing you could do is sample it according to the empirical frequency of words in your corpus. So just sample it according to how often different words appears. But the problem with that is that you end up with a very high representation of words like the, of, and, and so on. One other extreme would be to say, you use 1 over the vocab size, sample the negative examples uniformly at random, but that’s also very non-representative of the distribution of English words. So the authors, Mikolov et al, reported that empirically, what they found to work best was to take this heuristic value, which is a little bit in between the two extremes of sampling from the empirical frequencies, meaning from whatever’s the observed distribution in English text to the uniform distribution. And what they did was they sampled proportional to their frequency of a word to the power of three-fourths. So if f of wi is the observed frequency of a particular word in the English language or in your training set corpus, then by taking it to the power of three-fourths, this is somewhere in-between the extreme of taking uniform distribution. And the other extreme of just taking whatever was the observed distribution in your training set. And so I’m not sure this is very theoretically justified, but multiple researchers are now using this heuristic, and it seems to work decently well. So to summarize, you’ve seen how you can learn word vectors in a Softmax classier, but it’s very computationally expensive. And in this video, you saw how by changing that to a bunch of binary classification problems, you can very efficiently learn words vectors. And if you run this algorithm, you will be able to learn pretty good word vectors. Now of course, as is the case in other areas of deep learning as well, there are open source implementations. And there are also pre-trained word vectors that others have trained and released online under permissive licenses. And so if you want to get going quickly on a NLP problem, it’d be reasonable to download someone else’s word vectors and use that as a starting point. So that’s it for the Skip-Gram model. In the next video, I want to share with you yet another version of a word embedding learning algorithm that is maybe even simpler than what you’ve seen so far. So in the next video, let’s learn about the Glove algorithm. 04_glove-word-vectorsYou learn about several algorithms for computing words embeddings. Another algorithm that has some momentum in the NLP community is the GloVe algorithm. This is not used as much as the Word2Vec or the skip-gram models, but it has some enthusiasts. Because I think, in part of its simplicity. Let’s take a look. The GloVe algorithm was created by Jeffrey Pennington, Richard Socher, and Chris Manning. And GloVe stands for global vectors for word representation. So, previously, we were sampling pairs of words, context and target words, by picking two words that appear in close proximity to each other in our text corpus. So, what the GloVe algorithm does is, it starts off just by making that explicit. So, let’s say $X_{ij}$ be the number of times that a word i appears in the context of j. And so, here i and j play the role of t and c, so you can think of $X_{ij}$ as being x subscript tc. But, you can go through your training corpus and just count up how many words does a word i appear in the context of a different word j. How many times does the word t appear in context of different words c. And depending on the definition of context and target words, you might have that $X_{ij}$ equals $X_{ji}$. And in fact, if you’re defining context and target in terms of whether or not they appear within plus minus 10 words of each other, then it would be a symmetric relationship. Although, if your choice of context was that, the context is always the word immediately before the target word, then $X_{ij}$ and $X_{ji}$ may not be symmetric like this. But for the purposes of the GloVe algorithm, we can define context and target as whether or not the two words appear in close proximity, say within plus or minus 10 words of each other. So, $X_{ij}$ is a count that captures how often do words i and j appear with each other, or close to each other. So what the GloVe model does is, it optimizes the following. We’re going to minimize the difference between theta i transpose e_j minus log of $X_{ij}$ squared. I’m going to fill in some of the parts of this equation. But again, think of i and j as playing the role of t and c. So this is a bit like what you saw previously with theta t transpose e_c. And what you want is, for this to tell you how related are those two words? How related are words t and c? How related are words i and j as measured by how often they occur with each other? Which is affected by this $X_{ij}$. And so, what we’re going to do is, solve for parameters theta and e using gradient descent to minimize the sum over i equals one to 10,000 sum over j from one to 10,000 of this difference. So you just want to learn vectors, so that their end product is a good predictor for how often the two words occur together. Now, just some additional details, if $X_{ij}$ is equal to zero, then log of 0 is undefined, is negative infinity. And so, what we do is, we want sum over the terms where $X_{ij}$ is equal to zero. And so, what we’re going to do is, add an extra weighting term. So this is going to be a weighting term, and this will be equal to zero if $X_{ij}$ is equal to zero. And we’re going to use a convention that zero log zero is equal to zero. So what this means is, that if $X_{ij}$ is equal to zero, just don’t bother to sum over that $X_{ij}$ pair. So then this log of zero term is not relevant. So this means the sum is sum only over the pairs of words that have co-occurred at least once in that context-target relationship. The other thing that $F(X_{ij})$ does is that, there are some words they just appear very often in the English language like, this, is, of, a, and so on. Sometimes we used to call them stop words but there’s really a continuum between frequent and infrequent words. And then there are also some infrequent words like durion, which you actually still want to take into account, but not as frequently as the more common words. And so, the weighting factor can be a function that gives a meaningful amount of computation, even to the less frequent words like durion, and gives more weight but not an unduly large amount of weight to words like, this, is, of, a, which just appear lost in language. And so, there are various heuristics for choosing this weighting function F that need or gives these words too much weight nor gives the infrequent words too little weight. You can take a look at the GloVe paper, they are referenced in the previous slide, if you want the details of how F can be chosen to be a heuristic to accomplish this. And then, finally, one funny thing about this algorithm is that the roles of theta and e are now completely symmetric. So, theta i and e_j are symmetric in that, if you look at the math, they play pretty much the same role and you could reverse them or sort them around, and they actually end up with the same optimization objective. One way to train the algorithm is to initialize theta and e both uniformly around gradient descent to minimize its objective, and then when you’re done for every word, to then take the average. For a given words w, you can have e final to be equal to the embedding that was trained through this gradient descent procedure, plus theta trained through this gradient descent procedure divided by two, because theta and e in this particular formulation play symmetric roles unlike the earlier models we saw in the previous videos, where theta and e actually play different roles and couldn’t just be averaged like that. That’s it for the GloVe algorithm. I think one confusing part of this algorithm is, if you look at this equation, it seems almost too simple. How could it be that just minimizing a square cost function like this allows you to learn meaningful word embeddings? But it turns out that this works. And the way that the inventors end up with this algorithm was, they were building on the history of much more complicated algorithms like the newer language model, and then later, there came the Word2Vec skip-gram model, and then this came later. And we really hope to simplify all of the earlier algorithms. Before concluding our discussion of algorithms concerning word embeddings, there’s one more property of them that we should discuss briefly. Which is that? We started off with this featurization view as the motivation for learning word vectors. We said, “Well, maybe the first component of the embedding vector to represent gender, the second component to represent how royal it is, then the age and then whether it’s a food, and so on.” But when you learn a word embedding using one of the algorithms that we’ve seen, such as the GloVe algorithm that we just saw on the previous slide, what happens is, you cannot guarantee that the individual components of the embeddings are interpretable. Why is that? Well, let’s say that there is some space where the first axis is gender and the second axis is royal. What you can do is guarantee that the first axis of the embedding vector is aligned with this axis of meaning, of gender, royal, age and food. And in particular, the learning algorithm might choose this to be axis of the first dimension. So, given maybe a context of words, so the first dimension might be this axis and the second dimension might be this. Or it might not even be orthogonal, maybe it’ll be a second non-orthogonal axis, could be the second component of the word embeddings you actually learn. And when we see this, if you have a subsequent understanding of linear algebra is that, if there was some invertible matrix A, then this could just as easily be replaced with A times theta i transpose A inverse transpose e_j. Because we expand this out, this is equal to theta i transpose A transpose A inverse transpose times e_j. And so, the middle term cancels out and we’re left with theta i transpose e_j, same as before. Don’t worry if you didn’t follow the linear algebra, but that’s a brief proof that shows that with an algorithm like this, you can’t guarantee that the axis used to represent the features will be well-aligned with what might be easily humanly interpretable axis. In particular, the first feature might be a combination of gender, and royal, and age, and food, and cost, and size, is it a noun or an action verb, and all the other features. It’s very difficult to look at individual components, individual rows of the embedding matrix and assign the human interpretation to that. But despite this type of linear transformation, the parallelogram map that we worked out when we were describing analogies, that still works. And so, despite this potentially arbitrary linear transformation of the features, you end up learning the parallelogram map for figure analogies still works. So, that’s it for learning word embeddings. You’ve now seen a variety of algorithms for learning these word embeddings and you get to play them more in this week’s programming exercise as well. Next, I’d like to show you how you can use these algorithms to carry out sentiment classification. Let’s go onto the next video. 03_applications-using-word-embeddings01_sentiment-classificationSentiment classification is the task of looking at a piece of text and telling if someone likes or dislikes the thing they’re talking about. It is one of the most important building blocks in NLP and is used in many applications. One of the challenges of sentiment classification is you might not have a huge label training set for it. But with word embeddings, you’re able to build good sentiment classifiers even with only modest-size label training sets. Let’s see how you can do that. So here’s an example of a sentiment classification problem. The input X is a piece of text and the output Y that you want to predict is what is the sentiment, such as the star rating of, let’s say, a restaurant review. So if someone says, “The dessert is excellent” and they give it a four-star review, “Service was quite slow” two-star review, “Good for a quick meal but nothing special” three-star review. And this is a pretty harsh review, “Completely lacking in good taste, good service, and good ambiance.” That’s a one-star review. So if you can train a system to map from X or Y based on a label data set like this, then you could use it to monitor comments that people are saying about maybe a restaurant that you run. So people might also post messages about your restaurant on social media, on Twitter, or Facebook, or Instagram, or other forms of social media. And if you have a sentiment classifier, they can look just a piece of text and figure out how positive or negative is the sentiment of the poster toward your restaurant. Then you can also be able to keep track of whether or not there are any problems or if your restaurant is getting better or worse over time. So one of the challenges of sentiment classification is you might not have a huge label data set. So for sentimental classification task, training sets with maybe anywhere from 10,000 to maybe 100,000 words would not be uncommon. Sometimes even smaller than 10,000 words and word embeddings that you can take can help you to much better understand especially when you have a small training set. So here’s what you can do. We’ll go for a couple different algorithms in this video. Here’s a simple sentiment classification model. You can take a sentence like “dessert is excellent” and look up those words in your dictionary. We use a 10,000-word dictionary as usual. And let’s build a classifier to map it to the output Y that this was four stars. So given these four words, as usual, we can take these four words and look up the one-hot vector. So there’s 0 8 9 2 8 which is a one-hot vector multiplied by the embedding matrix E, which can learn from a much larger text corpus. It can learn in embedding from, say, a billion words or a hundred billion words, and use that to extract out the embedding vector for the word “the”, and then do the same for “dessert”, do the same for “is” and do the same for “excellent”. And if this was trained on a very large data set, like a hundred billion words, then this allows you to take a lot of knowledge even from infrequent words and apply them to your problem, even words that weren’t in your labeled training set. Now here’s one way to build a classifier, which is that you can take these vectors, let’s say these are 300-dimensional vectors, and you could then just sum or average them. And I’m just going to put a bigger average operator here and you could use sum or average. And this gives you a 300-dimensional feature vector that you then pass to a soft-max classifier which then outputs Y-hat. And so the softmax can output what are the probabilities of the five possible outcomes from one-star up to five-star. So this will be assortment of the five possible outcomes to predict what is Y. So notice that by using the average operation here, this particular algorithm works for reviews that are short or long because even if a review that is 100 words long, you can just sum or average all the feature vectors for all hundred words and so that gives you a representation, a 300-dimensional feature representation, that you can then pass into your sentiment classifier. So this average will work decently well. And what it does is it really averages the meanings of all the words or sums the meaning of all the words in your example. And this will work to [inaudible]. So one of the problems with this algorithm is it ignores word order. In particular, this is a very negative review, “Completely lacking in good taste, good service, and good ambiance”. But the word good appears a lot. This is a lot. Good, good, good. So if you use an algorithm like this that ignores word order and just sums or averages all of the embeddings for the different words, then you end up having a lot of the representation of good in your final feature vector and your classifier will probably think this is a good review even though this is actually very harsh. This is a one-star review. So here’s a more sophisticated model which is that, instead of just summing all of your word embeddings, you can instead use a RNN for sentiment classification. So here’s what you can do. You can take that review, “Completely lacking in good taste, good service, and good ambiance”, and find for each of them, the one-hot vector. And so I’m going to just skip the one-hot vector representation but take the one-hot vectors, multiply it by the embedding matrix E as usual, then this gives you the embedding vectors and then you can feed these into an RNN. And the job of the RNN is to then compute the representation at the last time step that allows you to predict Y-hat. So this is an example of a many-to-one RNN architecture which we saw in the previous week. And with an algorithm like this, it will be much better at taking word sequence into account and realize that “things are lacking in good taste” is a negative review and “not good” a negative review unlike the previous algorithm, which just sums everything together into a big-word vector mush and doesn’t realize that “not good” has a very different meaning than the words “good” or “lacking in good taste” and so on. And so if you train this algorithm, you end up with a pretty decent sentiment classification algorithm and because your word embeddings can be trained from a much larger data set, this will do a better job generalizing to maybe even new words now that you’ll see in your training set, such as if someone else says, “Completely absent of good taste, good service, and good ambiance” or something, then even if the word “absent” is not in your label training set, if it was in your 1 billion or 100 billion word corpus used to train the word embeddings, it might still get this right and generalize much better even to words that were in the training set used to train the word embeddings but not necessarily in the label training set that you had for specifically the sentiment classification problem. So that’s it for sentiment classification, and I hope this gives you a sense of how once you’ve learned or downloaded from online a word embedding, this allows you to quite quickly build pretty effective NLP systems. 02_debiasing-word-embeddingsMachine learning and AI algorithms are increasingly trusted to help with, or to make, extremely important decisions. And so we like to make sure that as much as possible that they’re free of undesirable forms of bias, such as gender bias, ethnicity bias and so on. What I want to do in this video is show you some of the ideas for diminishing or eliminating these forms of bias in word embeddings. When I use the term bias in this video, I don’t mean the bias variants or sense of the bias, instead I mean gender, ethnicity, sexual orientation bias. That’s a different sense of bias then is typically used in the technical discussion on machine learning. But mostly the problem, we talked about how word embeddings can learn analogies like man is to woman as king is to queen. But what if you ask it, man is to computer programmer as woman is to what? And so the authors of this paper Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai found a somewhat horrifying result where a learned word embedding might output Man:Computer_Programmer as Woman:Homemaker. And that just seems wrong and it enforces a very unhealthy gender stereotype. It’d be much more preferable to have algorithm output man is to computer programmer as a woman is to computer programmer. And they found also, Father:Doctor as Mother is to what? And the really unfortunate result is that some learned word embeddings would output Mother:Nurse. So word embeddings can reflect the gender, ethnicity, age, sexual orientation, and other biases of the text used to train the model. One that I’m especially passionate about is bias relating to socioeconomic status. I think that every person, whether you come from a wealthy family, or a low income family, or anywhere in between, I think everyone should have great opportunities. And because machine learning algorithms are being used to make very important decisions. They’re influencing everything ranging from college admissions, to the way people find jobs, to loan applications, whether your application for a loan gets approved, to in the criminal justice system, even sentencing guidelines. Learning algorithms are making very important decisions and so I think it’s important that we try to change learning algorithms to diminish as much as is possible, or, ideally, eliminate these types of undesirable biases. Now in the case of word embeddings, they can pick up the biases of the text used to train the model and so the biases they pick up or tend to reflect the biases in text as is written by people. Over many decades, over many centuries, I think humanity has made progress in reducing these types of bias. And I think maybe fortunately for AI, I think we actually have better ideas for quickly reducing the bias in AI than for quickly reducing the bias in the human race. Although I think we’re by no means done for AI as well and there’s still a lot of research and hard work to be done to reduce these types of biases in our learning algorithms. But what I want to do in this video is share with you one example of a set of ideas due to the paper referenced at the bottom by Bolukbasi and others on reducing the bias in word embeddings. So here’s the idea. Let’s say that we’ve already learned a word embedding, so the word babysitter is here, the word doctor is here. We have grandmother here, and grandfather here. Maybe the word girl is embedded there, the word boy is embedded there. And maybe she is embedded here, and he is embedded there. So the first thing we’re going to do it is identify the direction corresponding to a particular bias we want to reduce or eliminate. And, for illustration, I’m going to focus on gender bias but these ideas are applicable to all of the other types of bias that I mention on the previous slide as well. And so how do you identify the direction corresponding to the bias? For the case of gender, what we can do is take the embedding vector for he and subtract the embedding vector for she, because that differs by gender. And take e male, subtract e female, and take a few of these and average them, right? And take a few of these differences and basically average them. And this will allow you to figure out in this case that what looks like this direction(the horizontal direction in the slide) is the gender direction, or the bias direction. Whereas this direction(the vertical direction in the slide) is unrelated to the particular bias we’re trying to address. So this is the non-bias direction. And in this case, the bias direction, think of this as a 1D subspace whereas a non-bias direction, this will be 299-dimensional subspace. Okay, and I’ve simplified the description a little bit in the original paper. The bias direction can be higher than 1-dimensional, and rather than take an average, as I’m describing it here, it’s actually found using a more complicated algorithm called a SVD, a singular value decomposition. Which is closely related to, if you’re familiar with principle component analysis, it uses ideas similar to the pca or the principle component analysis algorithm. After that, the next step is a neutralization step. So for every word that’s not definitional, project it to get rid of bias. So there are some words that intrinsically capture gender. So words like grandmother, grandfather, girl, boy, she, he, a gender is intrinsic in the definition. Whereas there are other word like doctor and babysitter that we want to be gender neutral. And really, in the more general case, you might want words like doctor or babysitter to be ethnicity neutral or sexual orientation neutral, and so on, but we’ll just use gender as the illustrating example here. But so for every word that is not definitional, this basically means not words like grandmother and grandfather, which really have a very legitimate gender component, because, by definition, grandmothers are female, and grandfathers are male. So for words like doctor and babysitter, let’s just project them onto this axis to reduce their components, or to eliminate their component, in the bias direction. So reduce their component in this horizontal direction. So that’s the second neutralize step. And then the final step is called equalization in which you might have pairs of words such as grandmother and grandfather, or girl and boy, where you want the only difference in their embedding to be the gender. And so, why do you want that? Well in this example, the distance, or the similarity, between babysitter and grandmother is actually smaller than the distance between babysitter and grandfather. And so this maybe reinforces an unhealthy, or maybe undesirable, bias that grandmothers end up babysitting more than grandfathers. So in the final equalization step, what we’d like to do is to make sure that words like grandmother and grandfather are both exactly the same similarity, or exactly the same distance, from words that should be gender neutral, such as babysitter or such as doctor. So there are a few linear algebra steps for that. But what it will basically do is move grandmother and grandfather to a pair of points that are equidistant from this axis in the middle. And so the effect of that is that now the distance between babysitter, compared to these two words, will be exactly the same. And so, in general, there are many pairs of words like this grandmother-grandfather, boy-girl, sorority-fraternity, girlhood-boyhood, sister-brother, niece-nephew, daughter-son, that you might want to carry out through this equalization step. So the final detail is, how do you decide what word to neutralize? So for example, the word doctor seems like a word you should neutralize to make it non-gender-specific or non-ethnicity-specific. Whereas the words grandmother and grandmother should not be made non-gender-specific. And there are also words like beard, right, that it’s just a statistical fact that men are much more likely to have beards than women, so maybe beards should be closer to male than female. And so what the authors did is train a classifier to try to figure out what words are definitional, what words should be gender-specific and what words should not be. And it turns out that most words in the English language are not definitional, meaning that gender is not part of the definition. And it’s such a relatively small subset of words like this, grandmother-grandfather, girl-boy, sorority-fraternity, and so on that should not be neutralized. And so a linear classifier can tell you what words to pass through the neutralization step to project out this bias direction, to project it on to this essentially 299-dimensional subspace. And then, finally, the number of pairs you want to equalize, that’s actually also relatively small, and is, at least for the gender example, it is quite feasible to hand-pick most of the pairs you want to equalize. So the full algorithm is a bit more complicated than I present it here, you can take a look at the paper for the full details. And you also get to play with a few of these ideas in the programming exercises as well. So to summarize, I think that reducing or eliminating bias of our learning algorithms is a very important problem because these algorithms are being asked to help with or to make more and more important decisions in society. In this video I shared just one set of ideas for how to go about trying to address this problem, but this is still a very much an ongoing area of active research by many researchers. So that’s it for this week’s videos. Best of luck with this week’s programming exercises and I look forward to seeing you next week.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Improvise a Jazz Solo with an LSTM Network]]></title>
    <url>%2F2018%2F06%2F02%2FImprovise%2Ba%2BJazz%2BSolo%2Bwith%2Ban%2BLSTM%2BNetwork%2B-%2Bv3%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 1st week and the copyright belongs to deeplearning.ai. Improvise a Jazz Solo with an LSTM NetworkWelcome to your final programming assignment of this week! In this notebook, you will implement a model that uses an LSTM to generate music. You will even be able to listen to your own music at the end of the assignment. You will learn to: Apply an LSTM to music generation. Generate your own jazz music with deep learning. Please run the following cell to load all the packages required in this assignment. This may take a few minutes. 12345678910111213141516from __future__ import print_functionimport IPythonimport sysfrom music21 import *import numpy as npfrom grammar import *from qa import *from preprocess import * from music_utils import *from data_utils import *from keras.models import load_model, Modelfrom keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVectorfrom keras.initializers import glorot_uniformfrom keras.utils import to_categoricalfrom keras.optimizers import Adamfrom keras import backend as K Using TensorFlow backend. 1 - Problem statementYou would like to create a jazz music piece specially for a friend’s birthday. However, you don’t know any instruments or music composition. Fortunately, you know deep learning and will solve this problem using an LSTM netwok. You will train a network to generate novel jazz solos in a style representative of a body of performed work. 1.1 - DatasetYou will train your algorithm on a corpus of Jazz music. Run the cell below to listen to a snippet of the audio from the training set: 1IPython.display.Audio('./data/30s_seq.mp3') &lt;audio controls=&quot;controls&quot; &gt; &lt;source src=&quot;http://pt8q6wt5q.bkt.clouddn.com/gitpage/deeplearning.ai/nlp-sequence-models/jupter/week1/Jazz_improvisation_with_LSTM/data/30s_seq.mp3&quot; type=&quot;audio/mpeg&quot; /&gt; Your browser does not support the audio element. &lt;/audio&gt; We have taken care of the preprocessing of the musical data to render it in terms of musical “values.” You can informally think of each “value” as a note, which comprises a pitch and a duration. For example, if you press down a specific piano key for 0.5 seconds, then you have just played a note. In music theory, a “value” is actually more complicated than this–specifically, it also captures the information needed to play multiple notes at the same time. For example, when playing a music piece, you might press down two piano keys at the same time (playng multiple notes at the same time generates what’s called a “chord”). But we don’t need to worry about the details of music theory for this assignment. For the purpose of this assignment, all you need to know is that we will obtain a dataset of values, and will learn an RNN model to generate sequences of values. Our music generation system will use 78 unique values. Run the following code to load the raw music data and preprocess it into values. This might take a few minutes. 123456X, Y, n_values, indices_values = load_music_utils()print('shape of X:', X.shape)print('number of training examples:', X.shape[0])print('Tx (length of sequence):', X.shape[1])print('total # of unique values:', n_values)print('Shape of Y:', Y.shape) shape of X: (60, 30, 78) number of training examples: 60 Tx (length of sequence): 30 total # of unique values: 78 Shape of Y: (30, 60, 78) You have just loaded the following: X: This is an (m, $T_x$, 78) dimensional array. We have m training examples, each of which is a snippet of $T_x =30$ musical values. At each time step, the input is one of 78 different possible values, represented as a one-hot vector. Thus for example, X[i,t,:] is a one-hot vector representating the value of the i-th example at time t. Y: This is essentially the same as X, but shifted one step to the left (to the past). Similar to the dinosaurus assignment, we’re interested in the network using the previous values to predict the next value, so our sequence model will try to predict $y^{\langle t \rangle}$ given $x^{\langle 1\rangle}, \ldots, x^{\langle t \rangle}$. However, the data in Y is reordered to be dimension $(T_y, m, 78)$, where $T_y = T_x$. This format makes it more convenient to feed to the LSTM later. n_values: The number of unique values in this dataset. This should be 78. indices_values: python dictionary mapping from 0-77 to musical values. 1.2 - Overview of our modelHere is the architecture of the model we will use. This is similar to the Dinosaurus model you had used in the previous notebook, except that in you will be implementing it in Keras. The architecture is as follows: We will be training the model on random snippets of 30 values taken from a much longer piece of music. Thus, we won’t bother to set the first input $x^{\langle 1 \rangle} = \vec{0}$, which we had done previously to denote the start of a dinosaur name, since now most of these snippets of audio start somewhere in the middle of a piece of music. We are setting each of the snippts to have the same length $T_x = 30$ to make vectorization easier. 2 - Building the modelIn this part you will build and train a model that will learn musical patterns. To do so, you will need to build a model that takes in X of shape $(m, T_x, 78)$ and Y of shape $(T_y, m, 78)$. We will use an LSTM with 64 dimensional hidden states. Lets set n_a = 64. 1n_a = 64 Here’s how you can create a Keras model with multiple inputs and outputs. If you’re building an RNN where even at test time entire input sequence $x^{\langle 1 \rangle}, x^{\langle 2 \rangle}, \ldots, x^{\langle T_x \rangle}$ were given in advance, for example if the inputs were words and the output was a label, then Keras has simple built-in functions to build the model. However, for sequence generation, at test time we don’t know all the values of $x^{\langle t\rangle}$ in advance; instead we generate them one at a time using $x^{\langle t\rangle} = y^{\langle t-1 \rangle}$. So the code will be a bit more complicated, and you’ll need to implement your own for-loop to iterate over the different time steps. The function djmodel() will call the LSTM layer $T_x$ times using a for-loop, and it is important that all $T_x$ copies have the same weights. I.e., it should not re-initiaiize the weights every time—the $T_x$ steps should have shared weights. The key steps for implementing layers with shareable weights in Keras are: Define the layer objects (we will use global variables for this). Call these objects when propagating the input. We have defined the layers objects you need as global variables. Please run the next cell to create them. Please check the Keras documentation to make sure you understand what these layers are: Reshape(), LSTM(), Dense(). 123reshapor = Reshape((1, 78)) # Used in Step 2.B of djmodel(), belowLSTM_cell = LSTM(n_a, return_state = True) # Used in Step 2.Cdensor = Dense(n_values, activation='softmax') # Used in Step 2.D Each of reshapor, LSTM_cell and densor are now layer objects, and you can use them to implement djmodel(). In order to propagate a Keras tensor object X through one of these layers, use layer_object(X) (or layer_object([X,Y]) if it requires multiple inputs.). For example, reshapor(X) will propagate X through the Reshape((1,78)) layer defined above. Exercise: Implement djmodel(). You will need to carry out 2 steps: Create an empty list “outputs” to save the outputs of the LSTM Cell at every time step. Loop for $t \in 1, \ldots, T_x$: A. Select the “t”th time-step vector from X. The shape of this selection should be (78,). To do so, create a custom Lambda layer in Keras by using this line of code: 123456789 x = Lambda(lambda x: X[:,t,:])(X)``` Look over the Keras documentation to figure out what this does. It is creating a "temporary" or "unnamed" function (that's what Lambda functions are) that extracts out the appropriate one-hot vector, and making this function a Keras `Layer` object to apply to `X`. B. Reshape x to be (1,78). You may find the `reshapor()` layer (defined below) helpful. C. Run x through one step of LSTM_cell. Remember to initialize the LSTM_cell with the previous step's hidden state $a$ and cell state $c$. Use the following formatting:```pythona, _, c = LSTM_cell(input_x, initial_state=[previous hidden state, previous cell state]) D. Propagate the LSTM’s output activation value through a dense+softmax layer using densor. E. Append the predicted value to the list of “outputs” 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# GRADED FUNCTION: djmodeldef djmodel(Tx, n_a, n_values): """ Implement the model Arguments: Tx -- length of the sequence in a corpus n_a -- the number of activations used in our model n_values -- number of unique values in the music data Returns: model -- a keras model with the """ # Define the input of your model with a shape X = Input(shape=(Tx, n_values)) # Define s0, initial hidden state for the decoder LSTM a0 = Input(shape=(n_a,), name='a0') c0 = Input(shape=(n_a,), name='c0') a = a0 c = c0 ### START CODE HERE ### # Step 1: Create empty list to append the outputs while you iterate (≈1 line) outputs = []; # Step 2: Loop for t in range(Tx): # Step 2.A: select the "t"th time step vector from X. x = Lambda(lambda x: X[:,t,:])(X); # Step 2.B: Use reshapor to reshape x to be (1, n_values) (≈1 line) x = reshapor(x); # Step 2.C: Perform one step of the LSTM_cell a, _, c = LSTM_cell(x, initial_state=[a, c]); # Step 2.D: Apply densor to the hidden state output of LSTM_Cell out = densor(a); # Step 2.E: add the output to "outputs" p = outputs.append(out); # Step 3: Create model instance model = Model(input=[X, a0, c0], outputs = outputs); ### END CODE HERE ### return model Run the following cell to define your model. We will use Tx=30, n_a=64 (the dimension of the LSTM activations), and n_values=78. This cell may take a few seconds to run. 1model = djmodel(Tx = 30 , n_a = 64, n_values = 78) /opt/conda/lib/python3.6/site-packages/ipykernel/__main__.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[&lt;tf.Tenso..., inputs=[&lt;tf.Tenso...)` You now need to compile your model to be trained. We will Adam and a categorical cross-entropy loss. 123opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) Finally, lets initialize a0 and c0 for the LSTM’s initial state to be zero. 123m = 60a0 = np.zeros((m, n_a))c0 = np.zeros((m, n_a)) Lets now fit the model! We will turn Y to a list before doing so, since the cost function expects Y to be provided in this format (one list item per time-step). So list(Y) is a list with 30 items, where each of the list items is of shape (60,78). Lets train for 100 epochs. This will take a few minutes. 1model.fit([X, a0, c0], list(Y), epochs=100) Epoch 1/100 60/60 [==============================] - 5s - loss: 125.8264 - dense_1_loss_1: 4.3545 - dense_1_loss_2: 4.3464 - dense_1_loss_3: 4.3425 - dense_1_loss_4: 4.3442 - dense_1_loss_5: 4.3421 - dense_1_loss_6: 4.3446 - dense_1_loss_7: 4.3401 - dense_1_loss_8: 4.3457 - dense_1_loss_9: 4.3314 - dense_1_loss_10: 4.3323 - dense_1_loss_11: 4.3423 - dense_1_loss_12: 4.3389 - dense_1_loss_13: 4.3364 - dense_1_loss_14: 4.3380 - dense_1_loss_15: 4.3371 - dense_1_loss_16: 4.3311 - dense_1_loss_17: 4.3417 - dense_1_loss_18: 4.3396 - dense_1_loss_19: 4.3346 - dense_1_loss_20: 4.3342 - dense_1_loss_21: 4.3366 - dense_1_loss_22: 4.3406 - dense_1_loss_23: 4.3338 - dense_1_loss_24: 4.3317 - dense_1_loss_25: 4.3376 - dense_1_loss_26: 4.3340 - dense_1_loss_27: 4.3329 - dense_1_loss_28: 4.3416 - dense_1_loss_29: 4.3399 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0500 - dense_1_acc_3: 0.0500 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0333 - dense_1_acc_7: 0.0500 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.1000 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0667 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0667 - dense_1_acc_15: 0.0667 - dense_1_acc_16: 0.0500 - dense_1_acc_17: 0.0500 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.1000 - dense_1_acc_20: 0.0667 - dense_1_acc_21: 0.0500 - dense_1_acc_22: 0.0667 - dense_1_acc_23: 0.1167 - dense_1_acc_24: 0.1000 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.1000 - dense_1_acc_27: 0.0500 - dense_1_acc_28: 0.0500 - dense_1_acc_29: 0.0833 - dense_1_acc_30: 0.0000e+00 Epoch 2/100 60/60 [==============================] - 0s - loss: 122.6142 - dense_1_loss_1: 4.3317 - dense_1_loss_2: 4.2991 - dense_1_loss_3: 4.2729 - dense_1_loss_4: 4.2763 - dense_1_loss_5: 4.2523 - dense_1_loss_6: 4.2653 - dense_1_loss_7: 4.2464 - dense_1_loss_8: 4.2352 - dense_1_loss_9: 4.2288 - dense_1_loss_10: 4.2197 - dense_1_loss_11: 4.2248 - dense_1_loss_12: 4.2489 - dense_1_loss_13: 4.2078 - dense_1_loss_14: 4.2074 - dense_1_loss_15: 4.2073 - dense_1_loss_16: 4.1991 - dense_1_loss_17: 4.2009 - dense_1_loss_18: 4.2387 - dense_1_loss_19: 4.1921 - dense_1_loss_20: 4.2132 - dense_1_loss_21: 4.2112 - dense_1_loss_22: 4.1933 - dense_1_loss_23: 4.1941 - dense_1_loss_24: 4.2164 - dense_1_loss_25: 4.2240 - dense_1_loss_26: 4.1728 - dense_1_loss_27: 4.2027 - dense_1_loss_28: 4.2063 - dense_1_loss_29: 4.2258 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.1333 - dense_1_acc_3: 0.1500 - dense_1_acc_4: 0.1667 - dense_1_acc_5: 0.2000 - dense_1_acc_6: 0.1167 - dense_1_acc_7: 0.1667 - dense_1_acc_8: 0.1167 - dense_1_acc_9: 0.1833 - dense_1_acc_10: 0.1667 - dense_1_acc_11: 0.2000 - dense_1_acc_12: 0.0667 - dense_1_acc_13: 0.1333 - dense_1_acc_14: 0.1333 - dense_1_acc_15: 0.1167 - dense_1_acc_16: 0.1833 - dense_1_acc_17: 0.2000 - dense_1_acc_18: 0.0667 - dense_1_acc_19: 0.1333 - dense_1_acc_20: 0.1667 - dense_1_acc_21: 0.1333 - dense_1_acc_22: 0.1000 - dense_1_acc_23: 0.1167 - dense_1_acc_24: 0.1333 - dense_1_acc_25: 0.1167 - dense_1_acc_26: 0.1833 - dense_1_acc_27: 0.1000 - dense_1_acc_28: 0.1833 - dense_1_acc_29: 0.0833 - dense_1_acc_30: 0.0000e+00 Epoch 3/100 60/60 [==============================] - 0s - loss: 116.8061 - dense_1_loss_1: 4.3093 - dense_1_loss_2: 4.2449 - dense_1_loss_3: 4.1836 - dense_1_loss_4: 4.1745 - dense_1_loss_5: 4.1156 - dense_1_loss_6: 4.1481 - dense_1_loss_7: 4.0958 - dense_1_loss_8: 4.0446 - dense_1_loss_9: 3.9897 - dense_1_loss_10: 3.8988 - dense_1_loss_11: 3.8989 - dense_1_loss_12: 4.1165 - dense_1_loss_13: 3.8994 - dense_1_loss_14: 3.8898 - dense_1_loss_15: 3.9828 - dense_1_loss_16: 3.9182 - dense_1_loss_17: 3.8867 - dense_1_loss_18: 4.2104 - dense_1_loss_19: 3.8670 - dense_1_loss_20: 4.0711 - dense_1_loss_21: 4.0630 - dense_1_loss_22: 3.9217 - dense_1_loss_23: 3.9589 - dense_1_loss_24: 4.0469 - dense_1_loss_25: 4.0823 - dense_1_loss_26: 3.7266 - dense_1_loss_27: 3.9689 - dense_1_loss_28: 3.9623 - dense_1_loss_29: 4.1299 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.1500 - dense_1_acc_3: 0.2000 - dense_1_acc_4: 0.1333 - dense_1_acc_5: 0.1833 - dense_1_acc_6: 0.1000 - dense_1_acc_7: 0.1167 - dense_1_acc_8: 0.0833 - dense_1_acc_9: 0.1167 - dense_1_acc_10: 0.1167 - dense_1_acc_11: 0.0833 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.1000 - dense_1_acc_14: 0.1000 - dense_1_acc_15: 0.0500 - dense_1_acc_16: 0.0833 - dense_1_acc_17: 0.1000 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.1000 - dense_1_acc_20: 0.0667 - dense_1_acc_21: 0.0667 - dense_1_acc_22: 0.0500 - dense_1_acc_23: 0.0833 - dense_1_acc_24: 0.0833 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.1167 - dense_1_acc_27: 0.0500 - dense_1_acc_28: 0.0667 - dense_1_acc_29: 0.0333 - dense_1_acc_30: 0.0000e+00 Epoch 4/100 60/60 [==============================] - 0s - loss: 112.2963 - dense_1_loss_1: 4.2889 - dense_1_loss_2: 4.1981 - dense_1_loss_3: 4.0962 - dense_1_loss_4: 4.0810 - dense_1_loss_5: 3.9790 - dense_1_loss_6: 4.0129 - dense_1_loss_7: 3.9439 - dense_1_loss_8: 3.7697 - dense_1_loss_9: 3.8046 - dense_1_loss_10: 3.6386 - dense_1_loss_11: 3.7236 - dense_1_loss_12: 3.9783 - dense_1_loss_13: 3.7060 - dense_1_loss_14: 3.7075 - dense_1_loss_15: 3.7358 - dense_1_loss_16: 3.7286 - dense_1_loss_17: 3.8079 - dense_1_loss_18: 3.9018 - dense_1_loss_19: 3.6729 - dense_1_loss_20: 3.9865 - dense_1_loss_21: 3.9529 - dense_1_loss_22: 3.8378 - dense_1_loss_23: 3.7695 - dense_1_loss_24: 3.7576 - dense_1_loss_25: 3.9597 - dense_1_loss_26: 3.6666 - dense_1_loss_27: 3.6978 - dense_1_loss_28: 3.8733 - dense_1_loss_29: 4.0193 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.1500 - dense_1_acc_3: 0.2167 - dense_1_acc_4: 0.1833 - dense_1_acc_5: 0.2667 - dense_1_acc_6: 0.1333 - dense_1_acc_7: 0.1667 - dense_1_acc_8: 0.1833 - dense_1_acc_9: 0.1667 - dense_1_acc_10: 0.1667 - dense_1_acc_11: 0.1667 - dense_1_acc_12: 0.1000 - dense_1_acc_13: 0.1500 - dense_1_acc_14: 0.2167 - dense_1_acc_15: 0.1000 - dense_1_acc_16: 0.1167 - dense_1_acc_17: 0.1000 - dense_1_acc_18: 0.1000 - dense_1_acc_19: 0.1500 - dense_1_acc_20: 0.0833 - dense_1_acc_21: 0.0667 - dense_1_acc_22: 0.1167 - dense_1_acc_23: 0.0833 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.1000 - dense_1_acc_26: 0.1000 - dense_1_acc_27: 0.0833 - dense_1_acc_28: 0.1167 - dense_1_acc_29: 0.0667 - dense_1_acc_30: 0.0000e+00 Epoch 5/100 60/60 [==============================] - 0s - loss: 110.0390 - dense_1_loss_1: 4.2729 - dense_1_loss_2: 4.1581 - dense_1_loss_3: 4.0292 - dense_1_loss_4: 4.0164 - dense_1_loss_5: 3.8981 - dense_1_loss_6: 3.9318 - dense_1_loss_7: 3.8775 - dense_1_loss_8: 3.6710 - dense_1_loss_9: 3.7225 - dense_1_loss_10: 3.5653 - dense_1_loss_11: 3.6287 - dense_1_loss_12: 3.8595 - dense_1_loss_13: 3.6459 - dense_1_loss_14: 3.6176 - dense_1_loss_15: 3.7001 - dense_1_loss_16: 3.6384 - dense_1_loss_17: 3.7419 - dense_1_loss_18: 3.7274 - dense_1_loss_19: 3.6644 - dense_1_loss_20: 3.8134 - dense_1_loss_21: 3.8085 - dense_1_loss_22: 3.7113 - dense_1_loss_23: 3.6167 - dense_1_loss_24: 3.6441 - dense_1_loss_25: 3.9445 - dense_1_loss_26: 3.7134 - dense_1_loss_27: 3.6405 - dense_1_loss_28: 3.8265 - dense_1_loss_29: 3.9533 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.1333 - dense_1_acc_3: 0.2167 - dense_1_acc_4: 0.2000 - dense_1_acc_5: 0.1333 - dense_1_acc_6: 0.0333 - dense_1_acc_7: 0.1000 - dense_1_acc_8: 0.1500 - dense_1_acc_9: 0.0833 - dense_1_acc_10: 0.1000 - dense_1_acc_11: 0.1000 - dense_1_acc_12: 0.0667 - dense_1_acc_13: 0.1000 - dense_1_acc_14: 0.1500 - dense_1_acc_15: 0.0833 - dense_1_acc_16: 0.0500 - dense_1_acc_17: 0.0500 - dense_1_acc_18: 0.0833 - dense_1_acc_19: 0.0333 - dense_1_acc_20: 0.0500 - dense_1_acc_21: 0.0833 - dense_1_acc_22: 0.0833 - dense_1_acc_23: 0.1667 - dense_1_acc_24: 0.0500 - dense_1_acc_25: 0.0500 - dense_1_acc_26: 0.0500 - dense_1_acc_27: 0.0833 - dense_1_acc_28: 0.0167 - dense_1_acc_29: 0.0167 - dense_1_acc_30: 0.0000e+00 Epoch 6/100 60/60 [==============================] - 0s - loss: 106.1460 - dense_1_loss_1: 4.2571 - dense_1_loss_2: 4.1230 - dense_1_loss_3: 3.9604 - dense_1_loss_4: 3.9405 - dense_1_loss_5: 3.8132 - dense_1_loss_6: 3.8401 - dense_1_loss_7: 3.7750 - dense_1_loss_8: 3.5455 - dense_1_loss_9: 3.5752 - dense_1_loss_10: 3.4639 - dense_1_loss_11: 3.5982 - dense_1_loss_12: 3.7733 - dense_1_loss_13: 3.5049 - dense_1_loss_14: 3.4641 - dense_1_loss_15: 3.5221 - dense_1_loss_16: 3.5189 - dense_1_loss_17: 3.5414 - dense_1_loss_18: 3.5307 - dense_1_loss_19: 3.5341 - dense_1_loss_20: 3.6316 - dense_1_loss_21: 3.6324 - dense_1_loss_22: 3.5577 - dense_1_loss_23: 3.5073 - dense_1_loss_24: 3.5296 - dense_1_loss_25: 3.8212 - dense_1_loss_26: 3.4278 - dense_1_loss_27: 3.4614 - dense_1_loss_28: 3.5999 - dense_1_loss_29: 3.6956 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.1667 - dense_1_acc_3: 0.2000 - dense_1_acc_4: 0.2000 - dense_1_acc_5: 0.2500 - dense_1_acc_6: 0.0833 - dense_1_acc_7: 0.0833 - dense_1_acc_8: 0.1667 - dense_1_acc_9: 0.1000 - dense_1_acc_10: 0.2000 - dense_1_acc_11: 0.1333 - dense_1_acc_12: 0.1000 - dense_1_acc_13: 0.1833 - dense_1_acc_14: 0.2167 - dense_1_acc_15: 0.1167 - dense_1_acc_16: 0.1167 - dense_1_acc_17: 0.1333 - dense_1_acc_18: 0.1667 - dense_1_acc_19: 0.1833 - dense_1_acc_20: 0.1167 - dense_1_acc_21: 0.1500 - dense_1_acc_22: 0.1500 - dense_1_acc_23: 0.1833 - dense_1_acc_24: 0.1167 - dense_1_acc_25: 0.0667 - dense_1_acc_26: 0.2000 - dense_1_acc_27: 0.1000 - dense_1_acc_28: 0.1500 - dense_1_acc_29: 0.0667 - dense_1_acc_30: 0.0000e+00 Epoch 7/100 60/60 [==============================] - 0s - loss: 102.2579 - dense_1_loss_1: 4.2413 - dense_1_loss_2: 4.0875 - dense_1_loss_3: 3.8934 - dense_1_loss_4: 3.8654 - dense_1_loss_5: 3.7056 - dense_1_loss_6: 3.7368 - dense_1_loss_7: 3.6732 - dense_1_loss_8: 3.4290 - dense_1_loss_9: 3.4259 - dense_1_loss_10: 3.3381 - dense_1_loss_11: 3.4889 - dense_1_loss_12: 3.6443 - dense_1_loss_13: 3.3488 - dense_1_loss_14: 3.3007 - dense_1_loss_15: 3.3981 - dense_1_loss_16: 3.3846 - dense_1_loss_17: 3.3449 - dense_1_loss_18: 3.3858 - dense_1_loss_19: 3.4057 - dense_1_loss_20: 3.4521 - dense_1_loss_21: 3.4389 - dense_1_loss_22: 3.3936 - dense_1_loss_23: 3.4140 - dense_1_loss_24: 3.3620 - dense_1_loss_25: 3.6902 - dense_1_loss_26: 3.2316 - dense_1_loss_27: 3.3343 - dense_1_loss_28: 3.3640 - dense_1_loss_29: 3.4791 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.1333 - dense_1_acc_3: 0.2167 - dense_1_acc_4: 0.1833 - dense_1_acc_5: 0.2667 - dense_1_acc_6: 0.1667 - dense_1_acc_7: 0.1333 - dense_1_acc_8: 0.2333 - dense_1_acc_9: 0.1667 - dense_1_acc_10: 0.2000 - dense_1_acc_11: 0.1833 - dense_1_acc_12: 0.1333 - dense_1_acc_13: 0.1667 - dense_1_acc_14: 0.2667 - dense_1_acc_15: 0.1667 - dense_1_acc_16: 0.1500 - dense_1_acc_17: 0.1833 - dense_1_acc_18: 0.1167 - dense_1_acc_19: 0.1333 - dense_1_acc_20: 0.1833 - dense_1_acc_21: 0.1167 - dense_1_acc_22: 0.1333 - dense_1_acc_23: 0.1333 - dense_1_acc_24: 0.1500 - dense_1_acc_25: 0.0667 - dense_1_acc_26: 0.2167 - dense_1_acc_27: 0.1000 - dense_1_acc_28: 0.1500 - dense_1_acc_29: 0.1833 - dense_1_acc_30: 0.0000e+00 Epoch 8/100 60/60 [==============================] - 0s - loss: 98.0187 - dense_1_loss_1: 4.2277 - dense_1_loss_2: 4.0477 - dense_1_loss_3: 3.8258 - dense_1_loss_4: 3.7791 - dense_1_loss_5: 3.6089 - dense_1_loss_6: 3.6218 - dense_1_loss_7: 3.5396 - dense_1_loss_8: 3.2991 - dense_1_loss_9: 3.2584 - dense_1_loss_10: 3.1349 - dense_1_loss_11: 3.2992 - dense_1_loss_12: 3.4534 - dense_1_loss_13: 3.1133 - dense_1_loss_14: 3.0906 - dense_1_loss_15: 3.2273 - dense_1_loss_16: 3.2308 - dense_1_loss_17: 3.1062 - dense_1_loss_18: 3.2503 - dense_1_loss_19: 3.2314 - dense_1_loss_20: 3.2470 - dense_1_loss_21: 3.2910 - dense_1_loss_22: 3.2553 - dense_1_loss_23: 3.2761 - dense_1_loss_24: 3.2245 - dense_1_loss_25: 3.5042 - dense_1_loss_26: 3.0631 - dense_1_loss_27: 3.2291 - dense_1_loss_28: 3.2519 - dense_1_loss_29: 3.3307 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.1167 - dense_1_acc_3: 0.1667 - dense_1_acc_4: 0.1667 - dense_1_acc_5: 0.2667 - dense_1_acc_6: 0.1833 - dense_1_acc_7: 0.1500 - dense_1_acc_8: 0.2667 - dense_1_acc_9: 0.1667 - dense_1_acc_10: 0.2333 - dense_1_acc_11: 0.1667 - dense_1_acc_12: 0.1167 - dense_1_acc_13: 0.2833 - dense_1_acc_14: 0.2333 - dense_1_acc_15: 0.1500 - dense_1_acc_16: 0.2000 - dense_1_acc_17: 0.2167 - dense_1_acc_18: 0.1333 - dense_1_acc_19: 0.1667 - dense_1_acc_20: 0.2833 - dense_1_acc_21: 0.1667 - dense_1_acc_22: 0.1500 - dense_1_acc_23: 0.1500 - dense_1_acc_24: 0.1333 - dense_1_acc_25: 0.1167 - dense_1_acc_26: 0.2500 - dense_1_acc_27: 0.1000 - dense_1_acc_28: 0.1500 - dense_1_acc_29: 0.1667 - dense_1_acc_30: 0.0000e+00 Epoch 9/100 60/60 [==============================] - 0s - loss: 93.9753 - dense_1_loss_1: 4.2159 - dense_1_loss_2: 4.0105 - dense_1_loss_3: 3.7472 - dense_1_loss_4: 3.6921 - dense_1_loss_5: 3.4942 - dense_1_loss_6: 3.4897 - dense_1_loss_7: 3.4181 - dense_1_loss_8: 3.1503 - dense_1_loss_9: 3.1051 - dense_1_loss_10: 2.9563 - dense_1_loss_11: 3.1541 - dense_1_loss_12: 3.2926 - dense_1_loss_13: 2.9499 - dense_1_loss_14: 2.9662 - dense_1_loss_15: 3.0675 - dense_1_loss_16: 3.1146 - dense_1_loss_17: 2.9696 - dense_1_loss_18: 3.1479 - dense_1_loss_19: 3.0151 - dense_1_loss_20: 3.0469 - dense_1_loss_21: 3.0955 - dense_1_loss_22: 3.0573 - dense_1_loss_23: 3.1520 - dense_1_loss_24: 3.0335 - dense_1_loss_25: 3.3512 - dense_1_loss_26: 2.8350 - dense_1_loss_27: 3.1168 - dense_1_loss_28: 3.1114 - dense_1_loss_29: 3.2186 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.1167 - dense_1_acc_3: 0.2000 - dense_1_acc_4: 0.1500 - dense_1_acc_5: 0.2500 - dense_1_acc_6: 0.1833 - dense_1_acc_7: 0.1500 - dense_1_acc_8: 0.1833 - dense_1_acc_9: 0.2667 - dense_1_acc_10: 0.2500 - dense_1_acc_11: 0.1667 - dense_1_acc_12: 0.1500 - dense_1_acc_13: 0.3333 - dense_1_acc_14: 0.2333 - dense_1_acc_15: 0.2000 - dense_1_acc_16: 0.2333 - dense_1_acc_17: 0.3000 - dense_1_acc_18: 0.1333 - dense_1_acc_19: 0.2000 - dense_1_acc_20: 0.3333 - dense_1_acc_21: 0.1833 - dense_1_acc_22: 0.1500 - dense_1_acc_23: 0.2000 - dense_1_acc_24: 0.2000 - dense_1_acc_25: 0.1167 - dense_1_acc_26: 0.2667 - dense_1_acc_27: 0.1667 - dense_1_acc_28: 0.1500 - dense_1_acc_29: 0.2000 - dense_1_acc_30: 0.0000e+00 Epoch 10/100 60/60 [==============================] - 0s - loss: 89.7720 - dense_1_loss_1: 4.2048 - dense_1_loss_2: 3.9711 - dense_1_loss_3: 3.6677 - dense_1_loss_4: 3.6035 - dense_1_loss_5: 3.3800 - dense_1_loss_6: 3.3506 - dense_1_loss_7: 3.2899 - dense_1_loss_8: 3.0100 - dense_1_loss_9: 2.9501 - dense_1_loss_10: 2.7743 - dense_1_loss_11: 3.0100 - dense_1_loss_12: 3.0628 - dense_1_loss_13: 2.8252 - dense_1_loss_14: 2.8456 - dense_1_loss_15: 2.9193 - dense_1_loss_16: 2.9354 - dense_1_loss_17: 2.7749 - dense_1_loss_18: 3.0148 - dense_1_loss_19: 2.8805 - dense_1_loss_20: 2.8963 - dense_1_loss_21: 2.9775 - dense_1_loss_22: 2.8919 - dense_1_loss_23: 2.9468 - dense_1_loss_24: 2.8604 - dense_1_loss_25: 3.1973 - dense_1_loss_26: 2.6616 - dense_1_loss_27: 2.9519 - dense_1_loss_28: 2.9121 - dense_1_loss_29: 3.0059 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.0667 - dense_1_acc_3: 0.2000 - dense_1_acc_4: 0.1667 - dense_1_acc_5: 0.2000 - dense_1_acc_6: 0.2000 - dense_1_acc_7: 0.2167 - dense_1_acc_8: 0.1833 - dense_1_acc_9: 0.3000 - dense_1_acc_10: 0.2500 - dense_1_acc_11: 0.1667 - dense_1_acc_12: 0.1500 - dense_1_acc_13: 0.2833 - dense_1_acc_14: 0.2500 - dense_1_acc_15: 0.2500 - dense_1_acc_16: 0.2667 - dense_1_acc_17: 0.3000 - dense_1_acc_18: 0.1000 - dense_1_acc_19: 0.2167 - dense_1_acc_20: 0.2833 - dense_1_acc_21: 0.2167 - dense_1_acc_22: 0.1500 - dense_1_acc_23: 0.2333 - dense_1_acc_24: 0.1667 - dense_1_acc_25: 0.1167 - dense_1_acc_26: 0.2833 - dense_1_acc_27: 0.1667 - dense_1_acc_28: 0.2167 - dense_1_acc_29: 0.2167 - dense_1_acc_30: 0.0000e+00 Epoch 11/100 60/60 [==============================] - 0s - loss: 85.6615 - dense_1_loss_1: 4.1942 - dense_1_loss_2: 3.9323 - dense_1_loss_3: 3.5914 - dense_1_loss_4: 3.5058 - dense_1_loss_5: 3.2599 - dense_1_loss_6: 3.2069 - dense_1_loss_7: 3.1536 - dense_1_loss_8: 2.8428 - dense_1_loss_9: 2.8446 - dense_1_loss_10: 2.6600 - dense_1_loss_11: 2.8793 - dense_1_loss_12: 2.8746 - dense_1_loss_13: 2.6513 - dense_1_loss_14: 2.6880 - dense_1_loss_15: 2.7775 - dense_1_loss_16: 2.8001 - dense_1_loss_17: 2.6575 - dense_1_loss_18: 2.8262 - dense_1_loss_19: 2.6729 - dense_1_loss_20: 2.7437 - dense_1_loss_21: 2.7738 - dense_1_loss_22: 2.7370 - dense_1_loss_23: 2.8320 - dense_1_loss_24: 2.6954 - dense_1_loss_25: 2.9728 - dense_1_loss_26: 2.5801 - dense_1_loss_27: 2.7190 - dense_1_loss_28: 2.7862 - dense_1_loss_29: 2.8024 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.0667 - dense_1_acc_3: 0.2000 - dense_1_acc_4: 0.2167 - dense_1_acc_5: 0.2833 - dense_1_acc_6: 0.2167 - dense_1_acc_7: 0.2333 - dense_1_acc_8: 0.2833 - dense_1_acc_9: 0.2667 - dense_1_acc_10: 0.3167 - dense_1_acc_11: 0.1333 - dense_1_acc_12: 0.2000 - dense_1_acc_13: 0.3333 - dense_1_acc_14: 0.2333 - dense_1_acc_15: 0.2333 - dense_1_acc_16: 0.2500 - dense_1_acc_17: 0.2333 - dense_1_acc_18: 0.1500 - dense_1_acc_19: 0.2500 - dense_1_acc_20: 0.2833 - dense_1_acc_21: 0.2333 - dense_1_acc_22: 0.2000 - dense_1_acc_23: 0.2333 - dense_1_acc_24: 0.2000 - dense_1_acc_25: 0.1667 - dense_1_acc_26: 0.3333 - dense_1_acc_27: 0.2000 - dense_1_acc_28: 0.2167 - dense_1_acc_29: 0.3500 - dense_1_acc_30: 0.0000e+00 Epoch 12/100 60/60 [==============================] - 0s - loss: 81.9096 - dense_1_loss_1: 4.1837 - dense_1_loss_2: 3.8924 - dense_1_loss_3: 3.5047 - dense_1_loss_4: 3.4058 - dense_1_loss_5: 3.1285 - dense_1_loss_6: 3.0528 - dense_1_loss_7: 3.0213 - dense_1_loss_8: 2.6764 - dense_1_loss_9: 2.6832 - dense_1_loss_10: 2.5371 - dense_1_loss_11: 2.7424 - dense_1_loss_12: 2.7007 - dense_1_loss_13: 2.5169 - dense_1_loss_14: 2.5984 - dense_1_loss_15: 2.5748 - dense_1_loss_16: 2.6452 - dense_1_loss_17: 2.5546 - dense_1_loss_18: 2.6831 - dense_1_loss_19: 2.6039 - dense_1_loss_20: 2.6078 - dense_1_loss_21: 2.6546 - dense_1_loss_22: 2.5963 - dense_1_loss_23: 2.6691 - dense_1_loss_24: 2.6460 - dense_1_loss_25: 2.8278 - dense_1_loss_26: 2.3809 - dense_1_loss_27: 2.6169 - dense_1_loss_28: 2.5561 - dense_1_loss_29: 2.6480 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.1167 - dense_1_acc_3: 0.2167 - dense_1_acc_4: 0.2167 - dense_1_acc_5: 0.3000 - dense_1_acc_6: 0.2333 - dense_1_acc_7: 0.2833 - dense_1_acc_8: 0.3167 - dense_1_acc_9: 0.3167 - dense_1_acc_10: 0.3000 - dense_1_acc_11: 0.1667 - dense_1_acc_12: 0.2500 - dense_1_acc_13: 0.4000 - dense_1_acc_14: 0.2833 - dense_1_acc_15: 0.2500 - dense_1_acc_16: 0.2500 - dense_1_acc_17: 0.2500 - dense_1_acc_18: 0.1500 - dense_1_acc_19: 0.2833 - dense_1_acc_20: 0.3167 - dense_1_acc_21: 0.2667 - dense_1_acc_22: 0.2333 - dense_1_acc_23: 0.2333 - dense_1_acc_24: 0.2500 - dense_1_acc_25: 0.1333 - dense_1_acc_26: 0.3833 - dense_1_acc_27: 0.3000 - dense_1_acc_28: 0.3167 - dense_1_acc_29: 0.3167 - dense_1_acc_30: 0.0000e+00 Epoch 13/100 60/60 [==============================] - 0s - loss: 77.9424 - dense_1_loss_1: 4.1726 - dense_1_loss_2: 3.8520 - dense_1_loss_3: 3.4239 - dense_1_loss_4: 3.3049 - dense_1_loss_5: 3.0094 - dense_1_loss_6: 2.9040 - dense_1_loss_7: 2.8879 - dense_1_loss_8: 2.5388 - dense_1_loss_9: 2.5642 - dense_1_loss_10: 2.3932 - dense_1_loss_11: 2.5736 - dense_1_loss_12: 2.5587 - dense_1_loss_13: 2.3320 - dense_1_loss_14: 2.4560 - dense_1_loss_15: 2.4168 - dense_1_loss_16: 2.5107 - dense_1_loss_17: 2.3550 - dense_1_loss_18: 2.4863 - dense_1_loss_19: 2.4692 - dense_1_loss_20: 2.4468 - dense_1_loss_21: 2.5056 - dense_1_loss_22: 2.4056 - dense_1_loss_23: 2.4519 - dense_1_loss_24: 2.6144 - dense_1_loss_25: 2.6999 - dense_1_loss_26: 2.1690 - dense_1_loss_27: 2.4230 - dense_1_loss_28: 2.4840 - dense_1_loss_29: 2.5331 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.1167 - dense_1_acc_3: 0.2500 - dense_1_acc_4: 0.2000 - dense_1_acc_5: 0.3167 - dense_1_acc_6: 0.2167 - dense_1_acc_7: 0.3000 - dense_1_acc_8: 0.3500 - dense_1_acc_9: 0.3333 - dense_1_acc_10: 0.3333 - dense_1_acc_11: 0.2833 - dense_1_acc_12: 0.2500 - dense_1_acc_13: 0.3667 - dense_1_acc_14: 0.2500 - dense_1_acc_15: 0.2667 - dense_1_acc_16: 0.2333 - dense_1_acc_17: 0.3167 - dense_1_acc_18: 0.1833 - dense_1_acc_19: 0.2667 - dense_1_acc_20: 0.3333 - dense_1_acc_21: 0.3000 - dense_1_acc_22: 0.3000 - dense_1_acc_23: 0.2833 - dense_1_acc_24: 0.2500 - dense_1_acc_25: 0.1833 - dense_1_acc_26: 0.4167 - dense_1_acc_27: 0.2500 - dense_1_acc_28: 0.2167 - dense_1_acc_29: 0.3167 - dense_1_acc_30: 0.0000e+00 Epoch 14/100 60/60 [==============================] - 0s - loss: 74.5680 - dense_1_loss_1: 4.1639 - dense_1_loss_2: 3.8115 - dense_1_loss_3: 3.3439 - dense_1_loss_4: 3.1987 - dense_1_loss_5: 2.8879 - dense_1_loss_6: 2.7570 - dense_1_loss_7: 2.7657 - dense_1_loss_8: 2.4219 - dense_1_loss_9: 2.4471 - dense_1_loss_10: 2.2721 - dense_1_loss_11: 2.4152 - dense_1_loss_12: 2.4041 - dense_1_loss_13: 2.1848 - dense_1_loss_14: 2.3034 - dense_1_loss_15: 2.2661 - dense_1_loss_16: 2.3730 - dense_1_loss_17: 2.2420 - dense_1_loss_18: 2.3084 - dense_1_loss_19: 2.3039 - dense_1_loss_20: 2.3927 - dense_1_loss_21: 2.3191 - dense_1_loss_22: 2.2784 - dense_1_loss_23: 2.3497 - dense_1_loss_24: 2.4033 - dense_1_loss_25: 2.6364 - dense_1_loss_26: 2.1220 - dense_1_loss_27: 2.3866 - dense_1_loss_28: 2.4073 - dense_1_loss_29: 2.4020 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.1500 - dense_1_acc_3: 0.2500 - dense_1_acc_4: 0.2500 - dense_1_acc_5: 0.3333 - dense_1_acc_6: 0.2833 - dense_1_acc_7: 0.2667 - dense_1_acc_8: 0.3167 - dense_1_acc_9: 0.3333 - dense_1_acc_10: 0.3667 - dense_1_acc_11: 0.3167 - dense_1_acc_12: 0.2333 - dense_1_acc_13: 0.4000 - dense_1_acc_14: 0.3500 - dense_1_acc_15: 0.3500 - dense_1_acc_16: 0.2833 - dense_1_acc_17: 0.3667 - dense_1_acc_18: 0.2000 - dense_1_acc_19: 0.2833 - dense_1_acc_20: 0.3000 - dense_1_acc_21: 0.2833 - dense_1_acc_22: 0.2833 - dense_1_acc_23: 0.3333 - dense_1_acc_24: 0.2333 - dense_1_acc_25: 0.1667 - dense_1_acc_26: 0.3833 - dense_1_acc_27: 0.2667 - dense_1_acc_28: 0.2333 - dense_1_acc_29: 0.2833 - dense_1_acc_30: 0.0000e+00 Epoch 15/100 60/60 [==============================] - 0s - loss: 70.7818 - dense_1_loss_1: 4.1566 - dense_1_loss_2: 3.7716 - dense_1_loss_3: 3.2704 - dense_1_loss_4: 3.1003 - dense_1_loss_5: 2.7766 - dense_1_loss_6: 2.6157 - dense_1_loss_7: 2.6423 - dense_1_loss_8: 2.3160 - dense_1_loss_9: 2.3343 - dense_1_loss_10: 2.1863 - dense_1_loss_11: 2.3080 - dense_1_loss_12: 2.2695 - dense_1_loss_13: 2.0643 - dense_1_loss_14: 2.1616 - dense_1_loss_15: 2.2092 - dense_1_loss_16: 2.2644 - dense_1_loss_17: 2.1717 - dense_1_loss_18: 2.1806 - dense_1_loss_19: 2.1495 - dense_1_loss_20: 2.2528 - dense_1_loss_21: 2.0959 - dense_1_loss_22: 2.1184 - dense_1_loss_23: 2.2349 - dense_1_loss_24: 2.2799 - dense_1_loss_25: 2.4104 - dense_1_loss_26: 1.9154 - dense_1_loss_27: 2.1144 - dense_1_loss_28: 2.2116 - dense_1_loss_29: 2.1992 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.1500 - dense_1_acc_3: 0.2833 - dense_1_acc_4: 0.2667 - dense_1_acc_5: 0.3167 - dense_1_acc_6: 0.3167 - dense_1_acc_7: 0.3167 - dense_1_acc_8: 0.3500 - dense_1_acc_9: 0.4000 - dense_1_acc_10: 0.4000 - dense_1_acc_11: 0.2333 - dense_1_acc_12: 0.2333 - dense_1_acc_13: 0.4667 - dense_1_acc_14: 0.4167 - dense_1_acc_15: 0.2833 - dense_1_acc_16: 0.3167 - dense_1_acc_17: 0.3667 - dense_1_acc_18: 0.3167 - dense_1_acc_19: 0.3500 - dense_1_acc_20: 0.2833 - dense_1_acc_21: 0.3500 - dense_1_acc_22: 0.3667 - dense_1_acc_23: 0.4000 - dense_1_acc_24: 0.3000 - dense_1_acc_25: 0.2000 - dense_1_acc_26: 0.5167 - dense_1_acc_27: 0.3833 - dense_1_acc_28: 0.4167 - dense_1_acc_29: 0.4333 - dense_1_acc_30: 0.0000e+00 Epoch 16/100 60/60 [==============================] - 0s - loss: 67.6264 - dense_1_loss_1: 4.1490 - dense_1_loss_2: 3.7330 - dense_1_loss_3: 3.1997 - dense_1_loss_4: 2.9972 - dense_1_loss_5: 2.6689 - dense_1_loss_6: 2.4691 - dense_1_loss_7: 2.4959 - dense_1_loss_8: 2.2321 - dense_1_loss_9: 2.2149 - dense_1_loss_10: 2.0676 - dense_1_loss_11: 2.1944 - dense_1_loss_12: 2.0894 - dense_1_loss_13: 1.9174 - dense_1_loss_14: 2.0482 - dense_1_loss_15: 2.0521 - dense_1_loss_16: 2.1589 - dense_1_loss_17: 2.0443 - dense_1_loss_18: 2.0343 - dense_1_loss_19: 2.0277 - dense_1_loss_20: 2.0924 - dense_1_loss_21: 2.0356 - dense_1_loss_22: 2.0433 - dense_1_loss_23: 2.1854 - dense_1_loss_24: 2.1334 - dense_1_loss_25: 2.2683 - dense_1_loss_26: 1.8710 - dense_1_loss_27: 2.0543 - dense_1_loss_28: 2.0875 - dense_1_loss_29: 2.0611 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.1833 - dense_1_acc_3: 0.3000 - dense_1_acc_4: 0.2667 - dense_1_acc_5: 0.3333 - dense_1_acc_6: 0.3167 - dense_1_acc_7: 0.3500 - dense_1_acc_8: 0.3833 - dense_1_acc_9: 0.3833 - dense_1_acc_10: 0.3500 - dense_1_acc_11: 0.2500 - dense_1_acc_12: 0.3667 - dense_1_acc_13: 0.4167 - dense_1_acc_14: 0.4167 - dense_1_acc_15: 0.3333 - dense_1_acc_16: 0.3833 - dense_1_acc_17: 0.4000 - dense_1_acc_18: 0.4000 - dense_1_acc_19: 0.3833 - dense_1_acc_20: 0.4167 - dense_1_acc_21: 0.3833 - dense_1_acc_22: 0.3333 - dense_1_acc_23: 0.3000 - dense_1_acc_24: 0.3333 - dense_1_acc_25: 0.2167 - dense_1_acc_26: 0.4667 - dense_1_acc_27: 0.3500 - dense_1_acc_28: 0.4333 - dense_1_acc_29: 0.4333 - dense_1_acc_30: 0.0000e+00 Epoch 17/100 60/60 [==============================] - 0s - loss: 64.3102 - dense_1_loss_1: 4.1432 - dense_1_loss_2: 3.6922 - dense_1_loss_3: 3.1260 - dense_1_loss_4: 2.9039 - dense_1_loss_5: 2.5473 - dense_1_loss_6: 2.3139 - dense_1_loss_7: 2.3524 - dense_1_loss_8: 2.1075 - dense_1_loss_9: 2.1829 - dense_1_loss_10: 1.9446 - dense_1_loss_11: 2.1464 - dense_1_loss_12: 2.0344 - dense_1_loss_13: 1.8492 - dense_1_loss_14: 1.8603 - dense_1_loss_15: 1.9291 - dense_1_loss_16: 2.0644 - dense_1_loss_17: 1.9326 - dense_1_loss_18: 1.8428 - dense_1_loss_19: 1.9004 - dense_1_loss_20: 1.9474 - dense_1_loss_21: 1.9269 - dense_1_loss_22: 1.9244 - dense_1_loss_23: 1.9607 - dense_1_loss_24: 2.0257 - dense_1_loss_25: 2.1022 - dense_1_loss_26: 1.7460 - dense_1_loss_27: 1.8937 - dense_1_loss_28: 1.9563 - dense_1_loss_29: 1.9534 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.2000 - dense_1_acc_3: 0.3500 - dense_1_acc_4: 0.2667 - dense_1_acc_5: 0.3333 - dense_1_acc_6: 0.3500 - dense_1_acc_7: 0.3500 - dense_1_acc_8: 0.3667 - dense_1_acc_9: 0.3667 - dense_1_acc_10: 0.4333 - dense_1_acc_11: 0.3000 - dense_1_acc_12: 0.3500 - dense_1_acc_13: 0.4333 - dense_1_acc_14: 0.4167 - dense_1_acc_15: 0.3667 - dense_1_acc_16: 0.3333 - dense_1_acc_17: 0.3833 - dense_1_acc_18: 0.4833 - dense_1_acc_19: 0.4500 - dense_1_acc_20: 0.4667 - dense_1_acc_21: 0.4167 - dense_1_acc_22: 0.3667 - dense_1_acc_23: 0.4167 - dense_1_acc_24: 0.3667 - dense_1_acc_25: 0.2833 - dense_1_acc_26: 0.5667 - dense_1_acc_27: 0.4500 - dense_1_acc_28: 0.4167 - dense_1_acc_29: 0.4667 - dense_1_acc_30: 0.0000e+00 Epoch 18/100 60/60 [==============================] - 0s - loss: 60.9770 - dense_1_loss_1: 4.1352 - dense_1_loss_2: 3.6501 - dense_1_loss_3: 3.0557 - dense_1_loss_4: 2.8116 - dense_1_loss_5: 2.4615 - dense_1_loss_6: 2.2039 - dense_1_loss_7: 2.2144 - dense_1_loss_8: 1.9720 - dense_1_loss_9: 2.0354 - dense_1_loss_10: 1.8256 - dense_1_loss_11: 1.9682 - dense_1_loss_12: 1.8455 - dense_1_loss_13: 1.7386 - dense_1_loss_14: 1.7591 - dense_1_loss_15: 1.7897 - dense_1_loss_16: 1.9169 - dense_1_loss_17: 1.8054 - dense_1_loss_18: 1.8099 - dense_1_loss_19: 1.7484 - dense_1_loss_20: 1.7715 - dense_1_loss_21: 1.7874 - dense_1_loss_22: 1.8334 - dense_1_loss_23: 1.7951 - dense_1_loss_24: 1.9296 - dense_1_loss_25: 1.9762 - dense_1_loss_26: 1.6691 - dense_1_loss_27: 1.8107 - dense_1_loss_28: 1.8523 - dense_1_loss_29: 1.8047 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.2167 - dense_1_acc_3: 0.3500 - dense_1_acc_4: 0.2500 - dense_1_acc_5: 0.3667 - dense_1_acc_6: 0.3333 - dense_1_acc_7: 0.4000 - dense_1_acc_8: 0.4333 - dense_1_acc_9: 0.3667 - dense_1_acc_10: 0.4500 - dense_1_acc_11: 0.3833 - dense_1_acc_12: 0.4167 - dense_1_acc_13: 0.5333 - dense_1_acc_14: 0.4667 - dense_1_acc_15: 0.4667 - dense_1_acc_16: 0.3333 - dense_1_acc_17: 0.4000 - dense_1_acc_18: 0.3667 - dense_1_acc_19: 0.4000 - dense_1_acc_20: 0.4833 - dense_1_acc_21: 0.3833 - dense_1_acc_22: 0.4500 - dense_1_acc_23: 0.4833 - dense_1_acc_24: 0.3500 - dense_1_acc_25: 0.3500 - dense_1_acc_26: 0.5333 - dense_1_acc_27: 0.4333 - dense_1_acc_28: 0.4333 - dense_1_acc_29: 0.5500 - dense_1_acc_30: 0.0000e+00 Epoch 19/100 60/60 [==============================] - 0s - loss: 58.1739 - dense_1_loss_1: 4.1267 - dense_1_loss_2: 3.6067 - dense_1_loss_3: 2.9783 - dense_1_loss_4: 2.7143 - dense_1_loss_5: 2.3603 - dense_1_loss_6: 2.1084 - dense_1_loss_7: 2.1157 - dense_1_loss_8: 1.8884 - dense_1_loss_9: 1.9336 - dense_1_loss_10: 1.7485 - dense_1_loss_11: 1.9035 - dense_1_loss_12: 1.7516 - dense_1_loss_13: 1.5965 - dense_1_loss_14: 1.6437 - dense_1_loss_15: 1.6844 - dense_1_loss_16: 1.8346 - dense_1_loss_17: 1.7095 - dense_1_loss_18: 1.7362 - dense_1_loss_19: 1.6973 - dense_1_loss_20: 1.6533 - dense_1_loss_21: 1.6370 - dense_1_loss_22: 1.7230 - dense_1_loss_23: 1.7123 - dense_1_loss_24: 1.7885 - dense_1_loss_25: 1.8111 - dense_1_loss_26: 1.6029 - dense_1_loss_27: 1.7325 - dense_1_loss_28: 1.7083 - dense_1_loss_29: 1.6667 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.2167 - dense_1_acc_3: 0.3667 - dense_1_acc_4: 0.2500 - dense_1_acc_5: 0.3500 - dense_1_acc_6: 0.3833 - dense_1_acc_7: 0.4000 - dense_1_acc_8: 0.4167 - dense_1_acc_9: 0.4333 - dense_1_acc_10: 0.4500 - dense_1_acc_11: 0.3167 - dense_1_acc_12: 0.5000 - dense_1_acc_13: 0.6000 - dense_1_acc_14: 0.5000 - dense_1_acc_15: 0.5333 - dense_1_acc_16: 0.3833 - dense_1_acc_17: 0.4833 - dense_1_acc_18: 0.3833 - dense_1_acc_19: 0.4333 - dense_1_acc_20: 0.5000 - dense_1_acc_21: 0.5000 - dense_1_acc_22: 0.4833 - dense_1_acc_23: 0.4833 - dense_1_acc_24: 0.4167 - dense_1_acc_25: 0.4333 - dense_1_acc_26: 0.6333 - dense_1_acc_27: 0.5000 - dense_1_acc_28: 0.5167 - dense_1_acc_29: 0.5667 - dense_1_acc_30: 0.0000e+00 Epoch 20/100 60/60 [==============================] - 0s - loss: 55.4761 - dense_1_loss_1: 4.1189 - dense_1_loss_2: 3.5637 - dense_1_loss_3: 2.9002 - dense_1_loss_4: 2.6177 - dense_1_loss_5: 2.2596 - dense_1_loss_6: 1.9893 - dense_1_loss_7: 2.0135 - dense_1_loss_8: 1.7673 - dense_1_loss_9: 1.8833 - dense_1_loss_10: 1.6986 - dense_1_loss_11: 1.7912 - dense_1_loss_12: 1.6945 - dense_1_loss_13: 1.5218 - dense_1_loss_14: 1.5522 - dense_1_loss_15: 1.6312 - dense_1_loss_16: 1.7254 - dense_1_loss_17: 1.6705 - dense_1_loss_18: 1.5892 - dense_1_loss_19: 1.6236 - dense_1_loss_20: 1.5906 - dense_1_loss_21: 1.5903 - dense_1_loss_22: 1.6394 - dense_1_loss_23: 1.5444 - dense_1_loss_24: 1.6563 - dense_1_loss_25: 1.7084 - dense_1_loss_26: 1.4733 - dense_1_loss_27: 1.5569 - dense_1_loss_28: 1.5812 - dense_1_loss_29: 1.5237 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.2667 - dense_1_acc_3: 0.3833 - dense_1_acc_4: 0.2500 - dense_1_acc_5: 0.3500 - dense_1_acc_6: 0.4000 - dense_1_acc_7: 0.4000 - dense_1_acc_8: 0.4167 - dense_1_acc_9: 0.4167 - dense_1_acc_10: 0.4667 - dense_1_acc_11: 0.4167 - dense_1_acc_12: 0.4000 - dense_1_acc_13: 0.6000 - dense_1_acc_14: 0.5500 - dense_1_acc_15: 0.4833 - dense_1_acc_16: 0.4667 - dense_1_acc_17: 0.4667 - dense_1_acc_18: 0.4833 - dense_1_acc_19: 0.5000 - dense_1_acc_20: 0.5833 - dense_1_acc_21: 0.5833 - dense_1_acc_22: 0.5167 - dense_1_acc_23: 0.6167 - dense_1_acc_24: 0.5333 - dense_1_acc_25: 0.4167 - dense_1_acc_26: 0.6500 - dense_1_acc_27: 0.6000 - dense_1_acc_28: 0.5167 - dense_1_acc_29: 0.6167 - dense_1_acc_30: 0.0000e+00 Epoch 21/100 60/60 [==============================] - 0s - loss: 52.5952 - dense_1_loss_1: 4.1115 - dense_1_loss_2: 3.5210 - dense_1_loss_3: 2.8198 - dense_1_loss_4: 2.5191 - dense_1_loss_5: 2.1622 - dense_1_loss_6: 1.8718 - dense_1_loss_7: 1.8840 - dense_1_loss_8: 1.6437 - dense_1_loss_9: 1.7017 - dense_1_loss_10: 1.5723 - dense_1_loss_11: 1.6463 - dense_1_loss_12: 1.5608 - dense_1_loss_13: 1.3714 - dense_1_loss_14: 1.4084 - dense_1_loss_15: 1.4898 - dense_1_loss_16: 1.5919 - dense_1_loss_17: 1.5521 - dense_1_loss_18: 1.4812 - dense_1_loss_19: 1.4532 - dense_1_loss_20: 1.5159 - dense_1_loss_21: 1.4975 - dense_1_loss_22: 1.5416 - dense_1_loss_23: 1.4791 - dense_1_loss_24: 1.5756 - dense_1_loss_25: 1.6586 - dense_1_loss_26: 1.4051 - dense_1_loss_27: 1.5384 - dense_1_loss_28: 1.5311 - dense_1_loss_29: 1.4903 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.2667 - dense_1_acc_3: 0.4000 - dense_1_acc_4: 0.2667 - dense_1_acc_5: 0.3500 - dense_1_acc_6: 0.4500 - dense_1_acc_7: 0.4500 - dense_1_acc_8: 0.5000 - dense_1_acc_9: 0.5167 - dense_1_acc_10: 0.5000 - dense_1_acc_11: 0.4333 - dense_1_acc_12: 0.4667 - dense_1_acc_13: 0.7167 - dense_1_acc_14: 0.6833 - dense_1_acc_15: 0.5167 - dense_1_acc_16: 0.5500 - dense_1_acc_17: 0.4833 - dense_1_acc_18: 0.5333 - dense_1_acc_19: 0.5333 - dense_1_acc_20: 0.4833 - dense_1_acc_21: 0.6167 - dense_1_acc_22: 0.5667 - dense_1_acc_23: 0.5667 - dense_1_acc_24: 0.4833 - dense_1_acc_25: 0.4500 - dense_1_acc_26: 0.6333 - dense_1_acc_27: 0.5500 - dense_1_acc_28: 0.5833 - dense_1_acc_29: 0.6500 - dense_1_acc_30: 0.0000e+00 Epoch 22/100 60/60 [==============================] - 0s - loss: 50.2160 - dense_1_loss_1: 4.1047 - dense_1_loss_2: 3.4770 - dense_1_loss_3: 2.7407 - dense_1_loss_4: 2.4178 - dense_1_loss_5: 2.0648 - dense_1_loss_6: 1.7635 - dense_1_loss_7: 1.7659 - dense_1_loss_8: 1.5881 - dense_1_loss_9: 1.5796 - dense_1_loss_10: 1.4720 - dense_1_loss_11: 1.5638 - dense_1_loss_12: 1.4441 - dense_1_loss_13: 1.3000 - dense_1_loss_14: 1.3932 - dense_1_loss_15: 1.3870 - dense_1_loss_16: 1.5121 - dense_1_loss_17: 1.4827 - dense_1_loss_18: 1.3958 - dense_1_loss_19: 1.4016 - dense_1_loss_20: 1.4361 - dense_1_loss_21: 1.4005 - dense_1_loss_22: 1.5129 - dense_1_loss_23: 1.3737 - dense_1_loss_24: 1.4531 - dense_1_loss_25: 1.5305 - dense_1_loss_26: 1.3757 - dense_1_loss_27: 1.4563 - dense_1_loss_28: 1.4114 - dense_1_loss_29: 1.4113 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.2667 - dense_1_acc_3: 0.4167 - dense_1_acc_4: 0.3000 - dense_1_acc_5: 0.3667 - dense_1_acc_6: 0.5000 - dense_1_acc_7: 0.4667 - dense_1_acc_8: 0.5167 - dense_1_acc_9: 0.6000 - dense_1_acc_10: 0.5500 - dense_1_acc_11: 0.4333 - dense_1_acc_12: 0.5167 - dense_1_acc_13: 0.7000 - dense_1_acc_14: 0.6333 - dense_1_acc_15: 0.6000 - dense_1_acc_16: 0.5333 - dense_1_acc_17: 0.5500 - dense_1_acc_18: 0.5833 - dense_1_acc_19: 0.6333 - dense_1_acc_20: 0.6833 - dense_1_acc_21: 0.6500 - dense_1_acc_22: 0.6167 - dense_1_acc_23: 0.6833 - dense_1_acc_24: 0.5667 - dense_1_acc_25: 0.5333 - dense_1_acc_26: 0.6500 - dense_1_acc_27: 0.5167 - dense_1_acc_28: 0.7000 - dense_1_acc_29: 0.6667 - dense_1_acc_30: 0.0000e+00 Epoch 23/100 60/60 [==============================] - 0s - loss: 47.6829 - dense_1_loss_1: 4.0972 - dense_1_loss_2: 3.4353 - dense_1_loss_3: 2.6637 - dense_1_loss_4: 2.3184 - dense_1_loss_5: 1.9563 - dense_1_loss_6: 1.6456 - dense_1_loss_7: 1.6569 - dense_1_loss_8: 1.4727 - dense_1_loss_9: 1.5131 - dense_1_loss_10: 1.3883 - dense_1_loss_11: 1.4958 - dense_1_loss_12: 1.3610 - dense_1_loss_13: 1.2473 - dense_1_loss_14: 1.3105 - dense_1_loss_15: 1.3116 - dense_1_loss_16: 1.3763 - dense_1_loss_17: 1.3985 - dense_1_loss_18: 1.3418 - dense_1_loss_19: 1.3085 - dense_1_loss_20: 1.3157 - dense_1_loss_21: 1.3183 - dense_1_loss_22: 1.4045 - dense_1_loss_23: 1.3021 - dense_1_loss_24: 1.3491 - dense_1_loss_25: 1.4308 - dense_1_loss_26: 1.2834 - dense_1_loss_27: 1.3413 - dense_1_loss_28: 1.3298 - dense_1_loss_29: 1.3091 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2667 - dense_1_acc_3: 0.4333 - dense_1_acc_4: 0.3333 - dense_1_acc_5: 0.4000 - dense_1_acc_6: 0.5000 - dense_1_acc_7: 0.5000 - dense_1_acc_8: 0.5667 - dense_1_acc_9: 0.6167 - dense_1_acc_10: 0.6167 - dense_1_acc_11: 0.5667 - dense_1_acc_12: 0.6167 - dense_1_acc_13: 0.7333 - dense_1_acc_14: 0.6167 - dense_1_acc_15: 0.5833 - dense_1_acc_16: 0.6000 - dense_1_acc_17: 0.6333 - dense_1_acc_18: 0.6500 - dense_1_acc_19: 0.6833 - dense_1_acc_20: 0.7667 - dense_1_acc_21: 0.6500 - dense_1_acc_22: 0.6500 - dense_1_acc_23: 0.7333 - dense_1_acc_24: 0.6667 - dense_1_acc_25: 0.5333 - dense_1_acc_26: 0.7333 - dense_1_acc_27: 0.6667 - dense_1_acc_28: 0.7167 - dense_1_acc_29: 0.7000 - dense_1_acc_30: 0.0000e+00 Epoch 24/100 60/60 [==============================] - 0s - loss: 45.3187 - dense_1_loss_1: 4.0901 - dense_1_loss_2: 3.3922 - dense_1_loss_3: 2.5825 - dense_1_loss_4: 2.2373 - dense_1_loss_5: 1.8703 - dense_1_loss_6: 1.5549 - dense_1_loss_7: 1.5355 - dense_1_loss_8: 1.4056 - dense_1_loss_9: 1.3904 - dense_1_loss_10: 1.3019 - dense_1_loss_11: 1.3719 - dense_1_loss_12: 1.2736 - dense_1_loss_13: 1.1612 - dense_1_loss_14: 1.2376 - dense_1_loss_15: 1.2141 - dense_1_loss_16: 1.2789 - dense_1_loss_17: 1.3098 - dense_1_loss_18: 1.2803 - dense_1_loss_19: 1.2623 - dense_1_loss_20: 1.2124 - dense_1_loss_21: 1.2315 - dense_1_loss_22: 1.3501 - dense_1_loss_23: 1.2118 - dense_1_loss_24: 1.2614 - dense_1_loss_25: 1.3370 - dense_1_loss_26: 1.2148 - dense_1_loss_27: 1.2702 - dense_1_loss_28: 1.2633 - dense_1_loss_29: 1.2155 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2833 - dense_1_acc_3: 0.4500 - dense_1_acc_4: 0.3500 - dense_1_acc_5: 0.4000 - dense_1_acc_6: 0.6000 - dense_1_acc_7: 0.5500 - dense_1_acc_8: 0.6333 - dense_1_acc_9: 0.6333 - dense_1_acc_10: 0.6500 - dense_1_acc_11: 0.5333 - dense_1_acc_12: 0.6833 - dense_1_acc_13: 0.8000 - dense_1_acc_14: 0.6000 - dense_1_acc_15: 0.6000 - dense_1_acc_16: 0.6667 - dense_1_acc_17: 0.6833 - dense_1_acc_18: 0.7167 - dense_1_acc_19: 0.7333 - dense_1_acc_20: 0.7500 - dense_1_acc_21: 0.7500 - dense_1_acc_22: 0.7000 - dense_1_acc_23: 0.7667 - dense_1_acc_24: 0.7500 - dense_1_acc_25: 0.5000 - dense_1_acc_26: 0.7167 - dense_1_acc_27: 0.6667 - dense_1_acc_28: 0.7667 - dense_1_acc_29: 0.7667 - dense_1_acc_30: 0.0000e+00 Epoch 25/100 60/60 [==============================] - 0s - loss: 43.0943 - dense_1_loss_1: 4.0826 - dense_1_loss_2: 3.3473 - dense_1_loss_3: 2.5037 - dense_1_loss_4: 2.1542 - dense_1_loss_5: 1.7748 - dense_1_loss_6: 1.4676 - dense_1_loss_7: 1.4258 - dense_1_loss_8: 1.3439 - dense_1_loss_9: 1.2775 - dense_1_loss_10: 1.2072 - dense_1_loss_11: 1.2504 - dense_1_loss_12: 1.1964 - dense_1_loss_13: 1.0910 - dense_1_loss_14: 1.1317 - dense_1_loss_15: 1.1530 - dense_1_loss_16: 1.1781 - dense_1_loss_17: 1.2662 - dense_1_loss_18: 1.2050 - dense_1_loss_19: 1.1581 - dense_1_loss_20: 1.1413 - dense_1_loss_21: 1.1878 - dense_1_loss_22: 1.2693 - dense_1_loss_23: 1.1599 - dense_1_loss_24: 1.1910 - dense_1_loss_25: 1.2612 - dense_1_loss_26: 1.1298 - dense_1_loss_27: 1.2135 - dense_1_loss_28: 1.1730 - dense_1_loss_29: 1.1530 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3000 - dense_1_acc_3: 0.4667 - dense_1_acc_4: 0.3500 - dense_1_acc_5: 0.4500 - dense_1_acc_6: 0.6000 - dense_1_acc_7: 0.5833 - dense_1_acc_8: 0.6500 - dense_1_acc_9: 0.6833 - dense_1_acc_10: 0.6833 - dense_1_acc_11: 0.6167 - dense_1_acc_12: 0.7167 - dense_1_acc_13: 0.8333 - dense_1_acc_14: 0.7167 - dense_1_acc_15: 0.7000 - dense_1_acc_16: 0.7167 - dense_1_acc_17: 0.6333 - dense_1_acc_18: 0.7000 - dense_1_acc_19: 0.8000 - dense_1_acc_20: 0.7833 - dense_1_acc_21: 0.7667 - dense_1_acc_22: 0.7000 - dense_1_acc_23: 0.7833 - dense_1_acc_24: 0.6833 - dense_1_acc_25: 0.5833 - dense_1_acc_26: 0.7500 - dense_1_acc_27: 0.6500 - dense_1_acc_28: 0.7833 - dense_1_acc_29: 0.7667 - dense_1_acc_30: 0.0000e+00 Epoch 26/100 60/60 [==============================] - 0s - loss: 40.8022 - dense_1_loss_1: 4.0748 - dense_1_loss_2: 3.3002 - dense_1_loss_3: 2.4259 - dense_1_loss_4: 2.0715 - dense_1_loss_5: 1.6825 - dense_1_loss_6: 1.3776 - dense_1_loss_7: 1.3323 - dense_1_loss_8: 1.2728 - dense_1_loss_9: 1.2005 - dense_1_loss_10: 1.1305 - dense_1_loss_11: 1.1444 - dense_1_loss_12: 1.1230 - dense_1_loss_13: 1.0321 - dense_1_loss_14: 1.0275 - dense_1_loss_15: 1.0911 - dense_1_loss_16: 1.0935 - dense_1_loss_17: 1.1465 - dense_1_loss_18: 1.1157 - dense_1_loss_19: 1.0581 - dense_1_loss_20: 1.0947 - dense_1_loss_21: 1.1024 - dense_1_loss_22: 1.1725 - dense_1_loss_23: 1.0688 - dense_1_loss_24: 1.0804 - dense_1_loss_25: 1.1933 - dense_1_loss_26: 1.0530 - dense_1_loss_27: 1.1423 - dense_1_loss_28: 1.0956 - dense_1_loss_29: 1.0991 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2833 - dense_1_acc_3: 0.5000 - dense_1_acc_4: 0.3833 - dense_1_acc_5: 0.5000 - dense_1_acc_6: 0.6833 - dense_1_acc_7: 0.7000 - dense_1_acc_8: 0.6167 - dense_1_acc_9: 0.7000 - dense_1_acc_10: 0.7833 - dense_1_acc_11: 0.7167 - dense_1_acc_12: 0.8000 - dense_1_acc_13: 0.8833 - dense_1_acc_14: 0.8667 - dense_1_acc_15: 0.7833 - dense_1_acc_16: 0.8167 - dense_1_acc_17: 0.8000 - dense_1_acc_18: 0.7333 - dense_1_acc_19: 0.8333 - dense_1_acc_20: 0.8167 - dense_1_acc_21: 0.8667 - dense_1_acc_22: 0.7667 - dense_1_acc_23: 0.8500 - dense_1_acc_24: 0.8000 - dense_1_acc_25: 0.7000 - dense_1_acc_26: 0.8667 - dense_1_acc_27: 0.7667 - dense_1_acc_28: 0.8333 - dense_1_acc_29: 0.8333 - dense_1_acc_30: 0.0000e+00 Epoch 27/100 60/60 [==============================] - 0s - loss: 38.8025 - dense_1_loss_1: 4.0663 - dense_1_loss_2: 3.2536 - dense_1_loss_3: 2.3506 - dense_1_loss_4: 1.9905 - dense_1_loss_5: 1.5981 - dense_1_loss_6: 1.3005 - dense_1_loss_7: 1.2525 - dense_1_loss_8: 1.2083 - dense_1_loss_9: 1.1123 - dense_1_loss_10: 1.0546 - dense_1_loss_11: 1.0712 - dense_1_loss_12: 1.0661 - dense_1_loss_13: 0.9613 - dense_1_loss_14: 0.9802 - dense_1_loss_15: 1.0168 - dense_1_loss_16: 1.0461 - dense_1_loss_17: 1.0501 - dense_1_loss_18: 1.0304 - dense_1_loss_19: 1.0029 - dense_1_loss_20: 1.0419 - dense_1_loss_21: 1.0439 - dense_1_loss_22: 1.0489 - dense_1_loss_23: 1.0150 - dense_1_loss_24: 0.9941 - dense_1_loss_25: 1.1266 - dense_1_loss_26: 1.0159 - dense_1_loss_27: 1.0529 - dense_1_loss_28: 1.0263 - dense_1_loss_29: 1.0247 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2833 - dense_1_acc_3: 0.5000 - dense_1_acc_4: 0.4000 - dense_1_acc_5: 0.5167 - dense_1_acc_6: 0.6667 - dense_1_acc_7: 0.7500 - dense_1_acc_8: 0.6500 - dense_1_acc_9: 0.7500 - dense_1_acc_10: 0.8333 - dense_1_acc_11: 0.7667 - dense_1_acc_12: 0.8167 - dense_1_acc_13: 0.8833 - dense_1_acc_14: 0.8667 - dense_1_acc_15: 0.8333 - dense_1_acc_16: 0.9000 - dense_1_acc_17: 0.8500 - dense_1_acc_18: 0.7500 - dense_1_acc_19: 0.8833 - dense_1_acc_20: 0.8500 - dense_1_acc_21: 0.8500 - dense_1_acc_22: 0.8833 - dense_1_acc_23: 0.8333 - dense_1_acc_24: 0.8667 - dense_1_acc_25: 0.7000 - dense_1_acc_26: 0.8500 - dense_1_acc_27: 0.8000 - dense_1_acc_28: 0.8333 - dense_1_acc_29: 0.8333 - dense_1_acc_30: 0.0000e+00 Epoch 28/100 60/60 [==============================] - 0s - loss: 36.8764 - dense_1_loss_1: 4.0580 - dense_1_loss_2: 3.2063 - dense_1_loss_3: 2.2747 - dense_1_loss_4: 1.9115 - dense_1_loss_5: 1.5232 - dense_1_loss_6: 1.2318 - dense_1_loss_7: 1.1687 - dense_1_loss_8: 1.1294 - dense_1_loss_9: 1.0486 - dense_1_loss_10: 0.9668 - dense_1_loss_11: 1.0165 - dense_1_loss_12: 1.0011 - dense_1_loss_13: 0.8906 - dense_1_loss_14: 0.8915 - dense_1_loss_15: 0.9522 - dense_1_loss_16: 0.9425 - dense_1_loss_17: 0.9959 - dense_1_loss_18: 0.9568 - dense_1_loss_19: 0.9452 - dense_1_loss_20: 0.9827 - dense_1_loss_21: 0.9702 - dense_1_loss_22: 0.9839 - dense_1_loss_23: 0.9601 - dense_1_loss_24: 0.9256 - dense_1_loss_25: 1.0594 - dense_1_loss_26: 0.9498 - dense_1_loss_27: 1.0003 - dense_1_loss_28: 0.9784 - dense_1_loss_29: 0.9549 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2833 - dense_1_acc_3: 0.5333 - dense_1_acc_4: 0.4500 - dense_1_acc_5: 0.5500 - dense_1_acc_6: 0.7167 - dense_1_acc_7: 0.8500 - dense_1_acc_8: 0.7167 - dense_1_acc_9: 0.8167 - dense_1_acc_10: 0.8833 - dense_1_acc_11: 0.7667 - dense_1_acc_12: 0.8500 - dense_1_acc_13: 0.9500 - dense_1_acc_14: 0.8833 - dense_1_acc_15: 0.8500 - dense_1_acc_16: 0.9500 - dense_1_acc_17: 0.8333 - dense_1_acc_18: 0.7667 - dense_1_acc_19: 0.8667 - dense_1_acc_20: 0.8333 - dense_1_acc_21: 0.8500 - dense_1_acc_22: 0.9000 - dense_1_acc_23: 0.8833 - dense_1_acc_24: 0.8833 - dense_1_acc_25: 0.7667 - dense_1_acc_26: 0.8833 - dense_1_acc_27: 0.8167 - dense_1_acc_28: 0.8333 - dense_1_acc_29: 0.8833 - dense_1_acc_30: 0.0000e+00 Epoch 29/100 60/60 [==============================] - 0s - loss: 34.8516 - dense_1_loss_1: 4.0505 - dense_1_loss_2: 3.1603 - dense_1_loss_3: 2.2028 - dense_1_loss_4: 1.8335 - dense_1_loss_5: 1.4395 - dense_1_loss_6: 1.1476 - dense_1_loss_7: 1.0989 - dense_1_loss_8: 1.0629 - dense_1_loss_9: 0.9678 - dense_1_loss_10: 0.9031 - dense_1_loss_11: 0.9193 - dense_1_loss_12: 0.9352 - dense_1_loss_13: 0.8223 - dense_1_loss_14: 0.8405 - dense_1_loss_15: 0.8826 - dense_1_loss_16: 0.8818 - dense_1_loss_17: 0.9120 - dense_1_loss_18: 0.8814 - dense_1_loss_19: 0.8926 - dense_1_loss_20: 0.9129 - dense_1_loss_21: 0.8975 - dense_1_loss_22: 0.9191 - dense_1_loss_23: 0.8582 - dense_1_loss_24: 0.8664 - dense_1_loss_25: 0.9873 - dense_1_loss_26: 0.8851 - dense_1_loss_27: 0.9208 - dense_1_loss_28: 0.8919 - dense_1_loss_29: 0.8781 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2833 - dense_1_acc_3: 0.5333 - dense_1_acc_4: 0.4833 - dense_1_acc_5: 0.5833 - dense_1_acc_6: 0.7667 - dense_1_acc_7: 0.9000 - dense_1_acc_8: 0.7500 - dense_1_acc_9: 0.8000 - dense_1_acc_10: 0.8833 - dense_1_acc_11: 0.8500 - dense_1_acc_12: 0.9167 - dense_1_acc_13: 0.9500 - dense_1_acc_14: 0.9500 - dense_1_acc_15: 0.8833 - dense_1_acc_16: 0.9667 - dense_1_acc_17: 0.9000 - dense_1_acc_18: 0.8833 - dense_1_acc_19: 0.8667 - dense_1_acc_20: 0.9000 - dense_1_acc_21: 0.9333 - dense_1_acc_22: 0.9167 - dense_1_acc_23: 0.9167 - dense_1_acc_24: 0.9167 - dense_1_acc_25: 0.8000 - dense_1_acc_26: 0.9167 - dense_1_acc_27: 0.9000 - dense_1_acc_28: 0.8667 - dense_1_acc_29: 0.8833 - dense_1_acc_30: 0.0000e+00 Epoch 30/100 60/60 [==============================] - 0s - loss: 33.0396 - dense_1_loss_1: 4.0425 - dense_1_loss_2: 3.1119 - dense_1_loss_3: 2.1338 - dense_1_loss_4: 1.7691 - dense_1_loss_5: 1.3593 - dense_1_loss_6: 1.0798 - dense_1_loss_7: 1.0260 - dense_1_loss_8: 0.9722 - dense_1_loss_9: 0.8939 - dense_1_loss_10: 0.8502 - dense_1_loss_11: 0.8510 - dense_1_loss_12: 0.8728 - dense_1_loss_13: 0.7624 - dense_1_loss_14: 0.8034 - dense_1_loss_15: 0.8181 - dense_1_loss_16: 0.8146 - dense_1_loss_17: 0.8315 - dense_1_loss_18: 0.8346 - dense_1_loss_19: 0.8512 - dense_1_loss_20: 0.8402 - dense_1_loss_21: 0.8208 - dense_1_loss_22: 0.8591 - dense_1_loss_23: 0.7893 - dense_1_loss_24: 0.8128 - dense_1_loss_25: 0.9424 - dense_1_loss_26: 0.7967 - dense_1_loss_27: 0.8467 - dense_1_loss_28: 0.8232 - dense_1_loss_29: 0.8301 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2833 - dense_1_acc_3: 0.5500 - dense_1_acc_4: 0.4833 - dense_1_acc_5: 0.6333 - dense_1_acc_6: 0.7833 - dense_1_acc_7: 0.9500 - dense_1_acc_8: 0.8000 - dense_1_acc_9: 0.8333 - dense_1_acc_10: 0.9000 - dense_1_acc_11: 0.9000 - dense_1_acc_12: 0.9333 - dense_1_acc_13: 0.9667 - dense_1_acc_14: 0.9500 - dense_1_acc_15: 0.8833 - dense_1_acc_16: 0.9667 - dense_1_acc_17: 0.9333 - dense_1_acc_18: 0.9000 - dense_1_acc_19: 0.9000 - dense_1_acc_20: 0.9167 - dense_1_acc_21: 0.9500 - dense_1_acc_22: 0.9333 - dense_1_acc_23: 0.9333 - dense_1_acc_24: 0.9667 - dense_1_acc_25: 0.8333 - dense_1_acc_26: 0.9500 - dense_1_acc_27: 0.9167 - dense_1_acc_28: 0.9000 - dense_1_acc_29: 0.8833 - dense_1_acc_30: 0.0000e+00 Epoch 31/100 60/60 [==============================] - 0s - loss: 31.2115 - dense_1_loss_1: 4.0347 - dense_1_loss_2: 3.0670 - dense_1_loss_3: 2.0677 - dense_1_loss_4: 1.6894 - dense_1_loss_5: 1.2791 - dense_1_loss_6: 1.0036 - dense_1_loss_7: 0.9697 - dense_1_loss_8: 0.9078 - dense_1_loss_9: 0.8270 - dense_1_loss_10: 0.7842 - dense_1_loss_11: 0.7984 - dense_1_loss_12: 0.7919 - dense_1_loss_13: 0.6998 - dense_1_loss_14: 0.7360 - dense_1_loss_15: 0.7607 - dense_1_loss_16: 0.7495 - dense_1_loss_17: 0.7710 - dense_1_loss_18: 0.7747 - dense_1_loss_19: 0.7774 - dense_1_loss_20: 0.7782 - dense_1_loss_21: 0.7584 - dense_1_loss_22: 0.7907 - dense_1_loss_23: 0.7403 - dense_1_loss_24: 0.7389 - dense_1_loss_25: 0.8684 - dense_1_loss_26: 0.7347 - dense_1_loss_27: 0.7808 - dense_1_loss_28: 0.7643 - dense_1_loss_29: 0.7672 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2833 - dense_1_acc_3: 0.5667 - dense_1_acc_4: 0.4833 - dense_1_acc_5: 0.7000 - dense_1_acc_6: 0.8167 - dense_1_acc_7: 0.9333 - dense_1_acc_8: 0.8667 - dense_1_acc_9: 0.8500 - dense_1_acc_10: 0.9500 - dense_1_acc_11: 0.9000 - dense_1_acc_12: 0.9500 - dense_1_acc_13: 0.9833 - dense_1_acc_14: 0.9500 - dense_1_acc_15: 0.9167 - dense_1_acc_16: 0.9833 - dense_1_acc_17: 0.9333 - dense_1_acc_18: 0.9500 - dense_1_acc_19: 0.9500 - dense_1_acc_20: 0.9500 - dense_1_acc_21: 0.9333 - dense_1_acc_22: 0.9500 - dense_1_acc_23: 0.9833 - dense_1_acc_24: 0.9667 - dense_1_acc_25: 0.8500 - dense_1_acc_26: 0.9500 - dense_1_acc_27: 0.9500 - dense_1_acc_28: 0.9167 - dense_1_acc_29: 0.9000 - dense_1_acc_30: 0.0000e+00 Epoch 32/100 60/60 [==============================] - 0s - loss: 29.5748 - dense_1_loss_1: 4.0282 - dense_1_loss_2: 3.0201 - dense_1_loss_3: 2.0028 - dense_1_loss_4: 1.6073 - dense_1_loss_5: 1.1976 - dense_1_loss_6: 0.9391 - dense_1_loss_7: 0.8996 - dense_1_loss_8: 0.8674 - dense_1_loss_9: 0.7689 - dense_1_loss_10: 0.7085 - dense_1_loss_11: 0.7296 - dense_1_loss_12: 0.7143 - dense_1_loss_13: 0.6478 - dense_1_loss_14: 0.6798 - dense_1_loss_15: 0.7037 - dense_1_loss_16: 0.6957 - dense_1_loss_17: 0.7155 - dense_1_loss_18: 0.7009 - dense_1_loss_19: 0.7276 - dense_1_loss_20: 0.7203 - dense_1_loss_21: 0.7206 - dense_1_loss_22: 0.7310 - dense_1_loss_23: 0.6957 - dense_1_loss_24: 0.6805 - dense_1_loss_25: 0.8066 - dense_1_loss_26: 0.6915 - dense_1_loss_27: 0.7420 - dense_1_loss_28: 0.7182 - dense_1_loss_29: 0.7141 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2833 - dense_1_acc_3: 0.5500 - dense_1_acc_4: 0.5500 - dense_1_acc_5: 0.7333 - dense_1_acc_6: 0.8000 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.8167 - dense_1_acc_9: 0.8833 - dense_1_acc_10: 0.9167 - dense_1_acc_11: 0.9667 - dense_1_acc_12: 0.9667 - dense_1_acc_13: 0.9833 - dense_1_acc_14: 0.9500 - dense_1_acc_15: 0.9500 - dense_1_acc_16: 0.9833 - dense_1_acc_17: 0.9500 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 0.9667 - dense_1_acc_20: 0.9667 - dense_1_acc_21: 0.9667 - dense_1_acc_22: 0.9833 - dense_1_acc_23: 0.9833 - dense_1_acc_24: 0.9833 - dense_1_acc_25: 0.8667 - dense_1_acc_26: 0.9667 - dense_1_acc_27: 0.9500 - dense_1_acc_28: 0.9500 - dense_1_acc_29: 0.9000 - dense_1_acc_30: 0.0000e+00 Epoch 33/100 60/60 [==============================] - 0s - loss: 27.9737 - dense_1_loss_1: 4.0216 - dense_1_loss_2: 2.9721 - dense_1_loss_3: 1.9386 - dense_1_loss_4: 1.5331 - dense_1_loss_5: 1.1200 - dense_1_loss_6: 0.8744 - dense_1_loss_7: 0.8335 - dense_1_loss_8: 0.7904 - dense_1_loss_9: 0.7247 - dense_1_loss_10: 0.6550 - dense_1_loss_11: 0.6820 - dense_1_loss_12: 0.6602 - dense_1_loss_13: 0.6051 - dense_1_loss_14: 0.6329 - dense_1_loss_15: 0.6448 - dense_1_loss_16: 0.6574 - dense_1_loss_17: 0.6564 - dense_1_loss_18: 0.6470 - dense_1_loss_19: 0.6808 - dense_1_loss_20: 0.6682 - dense_1_loss_21: 0.6471 - dense_1_loss_22: 0.6879 - dense_1_loss_23: 0.6490 - dense_1_loss_24: 0.6138 - dense_1_loss_25: 0.7459 - dense_1_loss_26: 0.6281 - dense_1_loss_27: 0.6810 - dense_1_loss_28: 0.6606 - dense_1_loss_29: 0.6623 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3000 - dense_1_acc_3: 0.5500 - dense_1_acc_4: 0.5667 - dense_1_acc_5: 0.7500 - dense_1_acc_6: 0.8333 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.9167 - dense_1_acc_9: 0.9000 - dense_1_acc_10: 0.9833 - dense_1_acc_11: 0.9167 - dense_1_acc_12: 0.9833 - dense_1_acc_13: 0.9833 - dense_1_acc_14: 0.9667 - dense_1_acc_15: 0.9667 - dense_1_acc_16: 0.9833 - dense_1_acc_17: 0.9667 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 0.9667 - dense_1_acc_20: 0.9833 - dense_1_acc_21: 0.9833 - dense_1_acc_22: 0.9833 - dense_1_acc_23: 0.9833 - dense_1_acc_24: 0.9833 - dense_1_acc_25: 0.9000 - dense_1_acc_26: 0.9833 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 0.9833 - dense_1_acc_29: 0.9000 - dense_1_acc_30: 0.0000e+00 Epoch 34/100 60/60 [==============================] - 0s - loss: 26.4537 - dense_1_loss_1: 4.0143 - dense_1_loss_2: 2.9284 - dense_1_loss_3: 1.8764 - dense_1_loss_4: 1.4577 - dense_1_loss_5: 1.0544 - dense_1_loss_6: 0.8088 - dense_1_loss_7: 0.7924 - dense_1_loss_8: 0.7171 - dense_1_loss_9: 0.6815 - dense_1_loss_10: 0.6032 - dense_1_loss_11: 0.6289 - dense_1_loss_12: 0.6122 - dense_1_loss_13: 0.5585 - dense_1_loss_14: 0.5847 - dense_1_loss_15: 0.6003 - dense_1_loss_16: 0.5864 - dense_1_loss_17: 0.6140 - dense_1_loss_18: 0.6007 - dense_1_loss_19: 0.6066 - dense_1_loss_20: 0.6227 - dense_1_loss_21: 0.6074 - dense_1_loss_22: 0.6242 - dense_1_loss_23: 0.5913 - dense_1_loss_24: 0.5784 - dense_1_loss_25: 0.6959 - dense_1_loss_26: 0.5685 - dense_1_loss_27: 0.6152 - dense_1_loss_28: 0.6178 - dense_1_loss_29: 0.6057 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3000 - dense_1_acc_3: 0.5667 - dense_1_acc_4: 0.6167 - dense_1_acc_5: 0.7833 - dense_1_acc_6: 0.8333 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.9167 - dense_1_acc_9: 0.9167 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 0.9667 - dense_1_acc_12: 0.9667 - dense_1_acc_13: 0.9500 - dense_1_acc_14: 0.9833 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 0.9667 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 0.9833 - dense_1_acc_20: 0.9833 - dense_1_acc_21: 0.9833 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 0.9833 - dense_1_acc_24: 0.9833 - dense_1_acc_25: 0.9000 - dense_1_acc_26: 0.9833 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 0.9833 - dense_1_acc_29: 0.9333 - dense_1_acc_30: 0.0000e+00 Epoch 35/100 60/60 [==============================] - 0s - loss: 25.3020 - dense_1_loss_1: 4.0076 - dense_1_loss_2: 2.8809 - dense_1_loss_3: 1.8127 - dense_1_loss_4: 1.3818 - dense_1_loss_5: 0.9883 - dense_1_loss_6: 0.7545 - dense_1_loss_7: 0.7398 - dense_1_loss_8: 0.6632 - dense_1_loss_9: 0.6211 - dense_1_loss_10: 0.5547 - dense_1_loss_11: 0.5828 - dense_1_loss_12: 0.5680 - dense_1_loss_13: 0.5074 - dense_1_loss_14: 0.5488 - dense_1_loss_15: 0.5518 - dense_1_loss_16: 0.5706 - dense_1_loss_17: 0.5637 - dense_1_loss_18: 0.5603 - dense_1_loss_19: 0.5770 - dense_1_loss_20: 0.5882 - dense_1_loss_21: 0.5975 - dense_1_loss_22: 0.5684 - dense_1_loss_23: 0.5613 - dense_1_loss_24: 0.5658 - dense_1_loss_25: 0.6647 - dense_1_loss_26: 0.5433 - dense_1_loss_27: 0.6020 - dense_1_loss_28: 0.5876 - dense_1_loss_29: 0.5882 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3500 - dense_1_acc_3: 0.6000 - dense_1_acc_4: 0.6500 - dense_1_acc_5: 0.7833 - dense_1_acc_6: 0.8667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.9500 - dense_1_acc_9: 0.9000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 0.9667 - dense_1_acc_12: 0.9833 - dense_1_acc_13: 0.9833 - dense_1_acc_14: 0.9667 - dense_1_acc_15: 0.9833 - dense_1_acc_16: 0.9833 - dense_1_acc_17: 0.9833 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 0.9667 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 0.9833 - dense_1_acc_22: 0.9833 - dense_1_acc_23: 0.9833 - dense_1_acc_24: 0.9833 - dense_1_acc_25: 0.8833 - dense_1_acc_26: 0.9833 - dense_1_acc_27: 0.9833 - dense_1_acc_28: 0.9667 - dense_1_acc_29: 0.9000 - dense_1_acc_30: 0.0000e+00 Epoch 36/100 60/60 [==============================] - 0s - loss: 24.0212 - dense_1_loss_1: 4.0018 - dense_1_loss_2: 2.8350 - dense_1_loss_3: 1.7567 - dense_1_loss_4: 1.3129 - dense_1_loss_5: 0.9261 - dense_1_loss_6: 0.7048 - dense_1_loss_7: 0.7056 - dense_1_loss_8: 0.5899 - dense_1_loss_9: 0.5723 - dense_1_loss_10: 0.5290 - dense_1_loss_11: 0.5672 - dense_1_loss_12: 0.5433 - dense_1_loss_13: 0.4691 - dense_1_loss_14: 0.5052 - dense_1_loss_15: 0.5154 - dense_1_loss_16: 0.5191 - dense_1_loss_17: 0.5147 - dense_1_loss_18: 0.5275 - dense_1_loss_19: 0.5322 - dense_1_loss_20: 0.5460 - dense_1_loss_21: 0.5364 - dense_1_loss_22: 0.5149 - dense_1_loss_23: 0.5171 - dense_1_loss_24: 0.5470 - dense_1_loss_25: 0.6014 - dense_1_loss_26: 0.5223 - dense_1_loss_27: 0.5298 - dense_1_loss_28: 0.5323 - dense_1_loss_29: 0.5465 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6167 - dense_1_acc_4: 0.6500 - dense_1_acc_5: 0.8000 - dense_1_acc_6: 0.8833 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.9500 - dense_1_acc_9: 0.9333 - dense_1_acc_10: 0.9833 - dense_1_acc_11: 0.9333 - dense_1_acc_12: 0.9667 - dense_1_acc_13: 0.9833 - dense_1_acc_14: 0.9833 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 0.9833 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 0.9833 - dense_1_acc_19: 0.9833 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 0.9833 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 0.9833 - dense_1_acc_25: 0.9333 - dense_1_acc_26: 0.9667 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 0.9833 - dense_1_acc_29: 0.9333 - dense_1_acc_30: 0.0000e+00 Epoch 37/100 60/60 [==============================] - 0s - loss: 22.7911 - dense_1_loss_1: 3.9958 - dense_1_loss_2: 2.7889 - dense_1_loss_3: 1.6957 - dense_1_loss_4: 1.2449 - dense_1_loss_5: 0.8664 - dense_1_loss_6: 0.6546 - dense_1_loss_7: 0.6433 - dense_1_loss_8: 0.5611 - dense_1_loss_9: 0.5342 - dense_1_loss_10: 0.4752 - dense_1_loss_11: 0.4942 - dense_1_loss_12: 0.4948 - dense_1_loss_13: 0.4327 - dense_1_loss_14: 0.4618 - dense_1_loss_15: 0.4805 - dense_1_loss_16: 0.4725 - dense_1_loss_17: 0.4893 - dense_1_loss_18: 0.4711 - dense_1_loss_19: 0.5226 - dense_1_loss_20: 0.4978 - dense_1_loss_21: 0.4973 - dense_1_loss_22: 0.5085 - dense_1_loss_23: 0.4918 - dense_1_loss_24: 0.4780 - dense_1_loss_25: 0.5687 - dense_1_loss_26: 0.5094 - dense_1_loss_27: 0.4872 - dense_1_loss_28: 0.4759 - dense_1_loss_29: 0.4969 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6167 - dense_1_acc_4: 0.6833 - dense_1_acc_5: 0.8333 - dense_1_acc_6: 0.9167 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.9667 - dense_1_acc_9: 0.9833 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 0.9667 - dense_1_acc_12: 0.9833 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 0.9833 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 0.9833 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 0.9833 - dense_1_acc_20: 0.9667 - dense_1_acc_21: 0.9500 - dense_1_acc_22: 0.9833 - dense_1_acc_23: 0.9833 - dense_1_acc_24: 0.9833 - dense_1_acc_25: 0.9333 - dense_1_acc_26: 0.9500 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 0.9833 - dense_1_acc_29: 0.9667 - dense_1_acc_30: 0.0000e+00 Epoch 38/100 60/60 [==============================] - 0s - loss: 21.5434 - dense_1_loss_1: 3.9901 - dense_1_loss_2: 2.7436 - dense_1_loss_3: 1.6372 - dense_1_loss_4: 1.1774 - dense_1_loss_5: 0.8135 - dense_1_loss_6: 0.6036 - dense_1_loss_7: 0.5891 - dense_1_loss_8: 0.5241 - dense_1_loss_9: 0.4945 - dense_1_loss_10: 0.4275 - dense_1_loss_11: 0.4470 - dense_1_loss_12: 0.4405 - dense_1_loss_13: 0.3920 - dense_1_loss_14: 0.4135 - dense_1_loss_15: 0.4474 - dense_1_loss_16: 0.4103 - dense_1_loss_17: 0.4569 - dense_1_loss_18: 0.4348 - dense_1_loss_19: 0.4566 - dense_1_loss_20: 0.4556 - dense_1_loss_21: 0.4633 - dense_1_loss_22: 0.4516 - dense_1_loss_23: 0.4803 - dense_1_loss_24: 0.4165 - dense_1_loss_25: 0.5328 - dense_1_loss_26: 0.4615 - dense_1_loss_27: 0.4715 - dense_1_loss_28: 0.4605 - dense_1_loss_29: 0.4501 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6167 - dense_1_acc_4: 0.7500 - dense_1_acc_5: 0.9000 - dense_1_acc_6: 0.9333 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.9833 - dense_1_acc_9: 0.9833 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 0.9833 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 0.9833 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 0.9833 - dense_1_acc_25: 0.9667 - dense_1_acc_26: 0.9833 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 0.9833 - dense_1_acc_30: 0.0000e+00 Epoch 39/100 60/60 [==============================] - 0s - loss: 20.4096 - dense_1_loss_1: 3.9847 - dense_1_loss_2: 2.6981 - dense_1_loss_3: 1.5830 - dense_1_loss_4: 1.1062 - dense_1_loss_5: 0.7642 - dense_1_loss_6: 0.5615 - dense_1_loss_7: 0.5547 - dense_1_loss_8: 0.4929 - dense_1_loss_9: 0.4626 - dense_1_loss_10: 0.3892 - dense_1_loss_11: 0.4192 - dense_1_loss_12: 0.4008 - dense_1_loss_13: 0.3720 - dense_1_loss_14: 0.3958 - dense_1_loss_15: 0.4000 - dense_1_loss_16: 0.3839 - dense_1_loss_17: 0.4178 - dense_1_loss_18: 0.4045 - dense_1_loss_19: 0.4057 - dense_1_loss_20: 0.4201 - dense_1_loss_21: 0.4235 - dense_1_loss_22: 0.3947 - dense_1_loss_23: 0.4373 - dense_1_loss_24: 0.3695 - dense_1_loss_25: 0.4961 - dense_1_loss_26: 0.4024 - dense_1_loss_27: 0.4213 - dense_1_loss_28: 0.4212 - dense_1_loss_29: 0.4268 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6167 - dense_1_acc_4: 0.7667 - dense_1_acc_5: 0.9000 - dense_1_acc_6: 0.9500 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 0.9833 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 0.9833 - dense_1_acc_12: 0.9833 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 0.9833 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 0.9833 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 0.9833 - dense_1_acc_30: 0.0000e+00 Epoch 40/100 60/60 [==============================] - 0s - loss: 19.3388 - dense_1_loss_1: 3.9784 - dense_1_loss_2: 2.6535 - dense_1_loss_3: 1.5344 - dense_1_loss_4: 1.0452 - dense_1_loss_5: 0.7121 - dense_1_loss_6: 0.5257 - dense_1_loss_7: 0.5184 - dense_1_loss_8: 0.4365 - dense_1_loss_9: 0.4269 - dense_1_loss_10: 0.3572 - dense_1_loss_11: 0.3919 - dense_1_loss_12: 0.3681 - dense_1_loss_13: 0.3411 - dense_1_loss_14: 0.3671 - dense_1_loss_15: 0.3574 - dense_1_loss_16: 0.3562 - dense_1_loss_17: 0.3790 - dense_1_loss_18: 0.3677 - dense_1_loss_19: 0.3747 - dense_1_loss_20: 0.3929 - dense_1_loss_21: 0.3757 - dense_1_loss_22: 0.3625 - dense_1_loss_23: 0.3857 - dense_1_loss_24: 0.3530 - dense_1_loss_25: 0.4471 - dense_1_loss_26: 0.3708 - dense_1_loss_27: 0.3588 - dense_1_loss_28: 0.3926 - dense_1_loss_29: 0.4082 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3667 - dense_1_acc_3: 0.6167 - dense_1_acc_4: 0.7833 - dense_1_acc_5: 0.9000 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.9833 - dense_1_acc_9: 0.9833 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 0.9833 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 0.9833 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 0.9833 - dense_1_acc_30: 0.0000e+00 Epoch 41/100 60/60 [==============================] - 0s - loss: 18.4427 - dense_1_loss_1: 3.9727 - dense_1_loss_2: 2.6086 - dense_1_loss_3: 1.4855 - dense_1_loss_4: 0.9911 - dense_1_loss_5: 0.6693 - dense_1_loss_6: 0.4911 - dense_1_loss_7: 0.4898 - dense_1_loss_8: 0.3894 - dense_1_loss_9: 0.3934 - dense_1_loss_10: 0.3294 - dense_1_loss_11: 0.3655 - dense_1_loss_12: 0.3469 - dense_1_loss_13: 0.3107 - dense_1_loss_14: 0.3240 - dense_1_loss_15: 0.3409 - dense_1_loss_16: 0.3289 - dense_1_loss_17: 0.3514 - dense_1_loss_18: 0.3306 - dense_1_loss_19: 0.3551 - dense_1_loss_20: 0.3632 - dense_1_loss_21: 0.3502 - dense_1_loss_22: 0.3416 - dense_1_loss_23: 0.3415 - dense_1_loss_24: 0.3464 - dense_1_loss_25: 0.4083 - dense_1_loss_26: 0.3446 - dense_1_loss_27: 0.3314 - dense_1_loss_28: 0.3591 - dense_1_loss_29: 0.3821 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6333 - dense_1_acc_4: 0.7833 - dense_1_acc_5: 0.9167 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.9833 - dense_1_acc_9: 0.9833 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 0.9833 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 0.9833 - dense_1_acc_26: 0.9833 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 0.9833 - dense_1_acc_30: 0.0000e+00 Epoch 42/100 60/60 [==============================] - 0s - loss: 17.5646 - dense_1_loss_1: 3.9672 - dense_1_loss_2: 2.5679 - dense_1_loss_3: 1.4368 - dense_1_loss_4: 0.9286 - dense_1_loss_5: 0.6270 - dense_1_loss_6: 0.4565 - dense_1_loss_7: 0.4589 - dense_1_loss_8: 0.3703 - dense_1_loss_9: 0.3668 - dense_1_loss_10: 0.3025 - dense_1_loss_11: 0.3322 - dense_1_loss_12: 0.3195 - dense_1_loss_13: 0.2819 - dense_1_loss_14: 0.2898 - dense_1_loss_15: 0.3175 - dense_1_loss_16: 0.3034 - dense_1_loss_17: 0.3205 - dense_1_loss_18: 0.3035 - dense_1_loss_19: 0.3269 - dense_1_loss_20: 0.3342 - dense_1_loss_21: 0.3339 - dense_1_loss_22: 0.3222 - dense_1_loss_23: 0.3123 - dense_1_loss_24: 0.3104 - dense_1_loss_25: 0.3722 - dense_1_loss_26: 0.3139 - dense_1_loss_27: 0.3122 - dense_1_loss_28: 0.3210 - dense_1_loss_29: 0.3546 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6333 - dense_1_acc_4: 0.8167 - dense_1_acc_5: 0.9333 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 0.9833 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 0.9833 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 0.9833 - dense_1_acc_30: 0.0000e+00 Epoch 43/100 60/60 [==============================] - 0s - loss: 16.7271 - dense_1_loss_1: 3.9624 - dense_1_loss_2: 2.5229 - dense_1_loss_3: 1.3891 - dense_1_loss_4: 0.8766 - dense_1_loss_5: 0.5865 - dense_1_loss_6: 0.4224 - dense_1_loss_7: 0.4348 - dense_1_loss_8: 0.3382 - dense_1_loss_9: 0.3413 - dense_1_loss_10: 0.2778 - dense_1_loss_11: 0.3062 - dense_1_loss_12: 0.2913 - dense_1_loss_13: 0.2632 - dense_1_loss_14: 0.2670 - dense_1_loss_15: 0.2825 - dense_1_loss_16: 0.2767 - dense_1_loss_17: 0.2871 - dense_1_loss_18: 0.2843 - dense_1_loss_19: 0.2982 - dense_1_loss_20: 0.2984 - dense_1_loss_21: 0.3035 - dense_1_loss_22: 0.2955 - dense_1_loss_23: 0.2923 - dense_1_loss_24: 0.2757 - dense_1_loss_25: 0.3403 - dense_1_loss_26: 0.2941 - dense_1_loss_27: 0.2997 - dense_1_loss_28: 0.2961 - dense_1_loss_29: 0.3230 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6500 - dense_1_acc_4: 0.8500 - dense_1_acc_5: 0.9333 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 0.9833 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 0.9833 - dense_1_acc_30: 0.0000e+00 Epoch 44/100 60/60 [==============================] - 0s - loss: 16.0275 - dense_1_loss_1: 3.9571 - dense_1_loss_2: 2.4818 - dense_1_loss_3: 1.3443 - dense_1_loss_4: 0.8290 - dense_1_loss_5: 0.5499 - dense_1_loss_6: 0.3926 - dense_1_loss_7: 0.4075 - dense_1_loss_8: 0.3038 - dense_1_loss_9: 0.3181 - dense_1_loss_10: 0.2565 - dense_1_loss_11: 0.2853 - dense_1_loss_12: 0.2629 - dense_1_loss_13: 0.2446 - dense_1_loss_14: 0.2522 - dense_1_loss_15: 0.2562 - dense_1_loss_16: 0.2535 - dense_1_loss_17: 0.2654 - dense_1_loss_18: 0.2687 - dense_1_loss_19: 0.2804 - dense_1_loss_20: 0.2723 - dense_1_loss_21: 0.2774 - dense_1_loss_22: 0.2747 - dense_1_loss_23: 0.2851 - dense_1_loss_24: 0.2579 - dense_1_loss_25: 0.3207 - dense_1_loss_26: 0.2677 - dense_1_loss_27: 0.2866 - dense_1_loss_28: 0.2823 - dense_1_loss_29: 0.2931 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6500 - dense_1_acc_4: 0.8667 - dense_1_acc_5: 0.9333 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 45/100 60/60 [==============================] - 0s - loss: 15.2892 - dense_1_loss_1: 3.9528 - dense_1_loss_2: 2.4409 - dense_1_loss_3: 1.2989 - dense_1_loss_4: 0.7792 - dense_1_loss_5: 0.5144 - dense_1_loss_6: 0.3656 - dense_1_loss_7: 0.3803 - dense_1_loss_8: 0.2880 - dense_1_loss_9: 0.2896 - dense_1_loss_10: 0.2389 - dense_1_loss_11: 0.2588 - dense_1_loss_12: 0.2401 - dense_1_loss_13: 0.2234 - dense_1_loss_14: 0.2316 - dense_1_loss_15: 0.2370 - dense_1_loss_16: 0.2391 - dense_1_loss_17: 0.2397 - dense_1_loss_18: 0.2462 - dense_1_loss_19: 0.2602 - dense_1_loss_20: 0.2470 - dense_1_loss_21: 0.2554 - dense_1_loss_22: 0.2459 - dense_1_loss_23: 0.2618 - dense_1_loss_24: 0.2310 - dense_1_loss_25: 0.2938 - dense_1_loss_26: 0.2389 - dense_1_loss_27: 0.2560 - dense_1_loss_28: 0.2606 - dense_1_loss_29: 0.2741 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6500 - dense_1_acc_4: 0.8667 - dense_1_acc_5: 0.9500 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 46/100 60/60 [==============================] - 0s - loss: 14.6790 - dense_1_loss_1: 3.9472 - dense_1_loss_2: 2.3999 - dense_1_loss_3: 1.2582 - dense_1_loss_4: 0.7286 - dense_1_loss_5: 0.4830 - dense_1_loss_6: 0.3464 - dense_1_loss_7: 0.3574 - dense_1_loss_8: 0.2724 - dense_1_loss_9: 0.2679 - dense_1_loss_10: 0.2224 - dense_1_loss_11: 0.2386 - dense_1_loss_12: 0.2260 - dense_1_loss_13: 0.2022 - dense_1_loss_14: 0.2122 - dense_1_loss_15: 0.2253 - dense_1_loss_16: 0.2226 - dense_1_loss_17: 0.2267 - dense_1_loss_18: 0.2232 - dense_1_loss_19: 0.2375 - dense_1_loss_20: 0.2378 - dense_1_loss_21: 0.2357 - dense_1_loss_22: 0.2314 - dense_1_loss_23: 0.2384 - dense_1_loss_24: 0.2123 - dense_1_loss_25: 0.2717 - dense_1_loss_26: 0.2229 - dense_1_loss_27: 0.2288 - dense_1_loss_28: 0.2388 - dense_1_loss_29: 0.2635 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4333 - dense_1_acc_3: 0.6500 - dense_1_acc_4: 0.8667 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 47/100 60/60 [==============================] - 0s - loss: 14.0970 - dense_1_loss_1: 3.9425 - dense_1_loss_2: 2.3600 - dense_1_loss_3: 1.2195 - dense_1_loss_4: 0.6886 - dense_1_loss_5: 0.4519 - dense_1_loss_6: 0.3266 - dense_1_loss_7: 0.3332 - dense_1_loss_8: 0.2453 - dense_1_loss_9: 0.2445 - dense_1_loss_10: 0.2051 - dense_1_loss_11: 0.2236 - dense_1_loss_12: 0.2130 - dense_1_loss_13: 0.1839 - dense_1_loss_14: 0.1961 - dense_1_loss_15: 0.2109 - dense_1_loss_16: 0.2001 - dense_1_loss_17: 0.2142 - dense_1_loss_18: 0.2039 - dense_1_loss_19: 0.2189 - dense_1_loss_20: 0.2237 - dense_1_loss_21: 0.2180 - dense_1_loss_22: 0.2095 - dense_1_loss_23: 0.2203 - dense_1_loss_24: 0.2009 - dense_1_loss_25: 0.2488 - dense_1_loss_26: 0.2097 - dense_1_loss_27: 0.2100 - dense_1_loss_28: 0.2275 - dense_1_loss_29: 0.2467 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4500 - dense_1_acc_3: 0.6667 - dense_1_acc_4: 0.8667 - dense_1_acc_5: 0.9667 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 48/100 60/60 [==============================] - 0s - loss: 13.5650 - dense_1_loss_1: 3.9380 - dense_1_loss_2: 2.3213 - dense_1_loss_3: 1.1828 - dense_1_loss_4: 0.6479 - dense_1_loss_5: 0.4232 - dense_1_loss_6: 0.3065 - dense_1_loss_7: 0.3102 - dense_1_loss_8: 0.2231 - dense_1_loss_9: 0.2256 - dense_1_loss_10: 0.1906 - dense_1_loss_11: 0.2100 - dense_1_loss_12: 0.1964 - dense_1_loss_13: 0.1716 - dense_1_loss_14: 0.1827 - dense_1_loss_15: 0.1923 - dense_1_loss_16: 0.1898 - dense_1_loss_17: 0.1986 - dense_1_loss_18: 0.1855 - dense_1_loss_19: 0.2080 - dense_1_loss_20: 0.2070 - dense_1_loss_21: 0.1993 - dense_1_loss_22: 0.1943 - dense_1_loss_23: 0.2010 - dense_1_loss_24: 0.1894 - dense_1_loss_25: 0.2308 - dense_1_loss_26: 0.1998 - dense_1_loss_27: 0.2002 - dense_1_loss_28: 0.2135 - dense_1_loss_29: 0.2255 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4667 - dense_1_acc_3: 0.6833 - dense_1_acc_4: 0.8667 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 49/100 60/60 [==============================] - 0s - loss: 13.0597 - dense_1_loss_1: 3.9339 - dense_1_loss_2: 2.2832 - dense_1_loss_3: 1.1444 - dense_1_loss_4: 0.6110 - dense_1_loss_5: 0.3977 - dense_1_loss_6: 0.2916 - dense_1_loss_7: 0.2918 - dense_1_loss_8: 0.2103 - dense_1_loss_9: 0.2145 - dense_1_loss_10: 0.1770 - dense_1_loss_11: 0.1955 - dense_1_loss_12: 0.1818 - dense_1_loss_13: 0.1640 - dense_1_loss_14: 0.1702 - dense_1_loss_15: 0.1761 - dense_1_loss_16: 0.1776 - dense_1_loss_17: 0.1823 - dense_1_loss_18: 0.1722 - dense_1_loss_19: 0.1921 - dense_1_loss_20: 0.1922 - dense_1_loss_21: 0.1825 - dense_1_loss_22: 0.1838 - dense_1_loss_23: 0.1810 - dense_1_loss_24: 0.1717 - dense_1_loss_25: 0.2133 - dense_1_loss_26: 0.1796 - dense_1_loss_27: 0.1877 - dense_1_loss_28: 0.1932 - dense_1_loss_29: 0.2074 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4667 - dense_1_acc_3: 0.7167 - dense_1_acc_4: 0.8667 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 50/100 60/60 [==============================] - 0s - loss: 12.6026 - dense_1_loss_1: 3.9298 - dense_1_loss_2: 2.2459 - dense_1_loss_3: 1.1076 - dense_1_loss_4: 0.5701 - dense_1_loss_5: 0.3745 - dense_1_loss_6: 0.2743 - dense_1_loss_7: 0.2741 - dense_1_loss_8: 0.1958 - dense_1_loss_9: 0.2036 - dense_1_loss_10: 0.1632 - dense_1_loss_11: 0.1818 - dense_1_loss_12: 0.1658 - dense_1_loss_13: 0.1562 - dense_1_loss_14: 0.1593 - dense_1_loss_15: 0.1637 - dense_1_loss_16: 0.1626 - dense_1_loss_17: 0.1675 - dense_1_loss_18: 0.1637 - dense_1_loss_19: 0.1737 - dense_1_loss_20: 0.1801 - dense_1_loss_21: 0.1730 - dense_1_loss_22: 0.1703 - dense_1_loss_23: 0.1721 - dense_1_loss_24: 0.1558 - dense_1_loss_25: 0.1999 - dense_1_loss_26: 0.1673 - dense_1_loss_27: 0.1757 - dense_1_loss_28: 0.1815 - dense_1_loss_29: 0.1938 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4667 - dense_1_acc_3: 0.7167 - dense_1_acc_4: 0.9333 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 51/100 60/60 [==============================] - 0s - loss: 12.1969 - dense_1_loss_1: 3.9255 - dense_1_loss_2: 2.2080 - dense_1_loss_3: 1.0726 - dense_1_loss_4: 0.5390 - dense_1_loss_5: 0.3548 - dense_1_loss_6: 0.2577 - dense_1_loss_7: 0.2564 - dense_1_loss_8: 0.1868 - dense_1_loss_9: 0.1882 - dense_1_loss_10: 0.1529 - dense_1_loss_11: 0.1652 - dense_1_loss_12: 0.1568 - dense_1_loss_13: 0.1446 - dense_1_loss_14: 0.1448 - dense_1_loss_15: 0.1528 - dense_1_loss_16: 0.1549 - dense_1_loss_17: 0.1573 - dense_1_loss_18: 0.1558 - dense_1_loss_19: 0.1594 - dense_1_loss_20: 0.1655 - dense_1_loss_21: 0.1656 - dense_1_loss_22: 0.1577 - dense_1_loss_23: 0.1639 - dense_1_loss_24: 0.1475 - dense_1_loss_25: 0.1874 - dense_1_loss_26: 0.1574 - dense_1_loss_27: 0.1644 - dense_1_loss_28: 0.1717 - dense_1_loss_29: 0.1822 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4667 - dense_1_acc_3: 0.7333 - dense_1_acc_4: 0.9333 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 52/100 60/60 [==============================] - 0s - loss: 11.7999 - dense_1_loss_1: 3.9217 - dense_1_loss_2: 2.1720 - dense_1_loss_3: 1.0392 - dense_1_loss_4: 0.5107 - dense_1_loss_5: 0.3348 - dense_1_loss_6: 0.2429 - dense_1_loss_7: 0.2425 - dense_1_loss_8: 0.1760 - dense_1_loss_9: 0.1731 - dense_1_loss_10: 0.1442 - dense_1_loss_11: 0.1543 - dense_1_loss_12: 0.1459 - dense_1_loss_13: 0.1337 - dense_1_loss_14: 0.1360 - dense_1_loss_15: 0.1401 - dense_1_loss_16: 0.1463 - dense_1_loss_17: 0.1466 - dense_1_loss_18: 0.1444 - dense_1_loss_19: 0.1508 - dense_1_loss_20: 0.1517 - dense_1_loss_21: 0.1525 - dense_1_loss_22: 0.1483 - dense_1_loss_23: 0.1505 - dense_1_loss_24: 0.1387 - dense_1_loss_25: 0.1725 - dense_1_loss_26: 0.1485 - dense_1_loss_27: 0.1516 - dense_1_loss_28: 0.1598 - dense_1_loss_29: 0.1704 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4667 - dense_1_acc_3: 0.7333 - dense_1_acc_4: 0.9333 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 53/100 60/60 [==============================] - 0s - loss: 11.4567 - dense_1_loss_1: 3.9172 - dense_1_loss_2: 2.1361 - dense_1_loss_3: 1.0079 - dense_1_loss_4: 0.4832 - dense_1_loss_5: 0.3156 - dense_1_loss_6: 0.2311 - dense_1_loss_7: 0.2329 - dense_1_loss_8: 0.1637 - dense_1_loss_9: 0.1621 - dense_1_loss_10: 0.1354 - dense_1_loss_11: 0.1477 - dense_1_loss_12: 0.1379 - dense_1_loss_13: 0.1248 - dense_1_loss_14: 0.1295 - dense_1_loss_15: 0.1316 - dense_1_loss_16: 0.1364 - dense_1_loss_17: 0.1378 - dense_1_loss_18: 0.1345 - dense_1_loss_19: 0.1434 - dense_1_loss_20: 0.1433 - dense_1_loss_21: 0.1424 - dense_1_loss_22: 0.1385 - dense_1_loss_23: 0.1395 - dense_1_loss_24: 0.1307 - dense_1_loss_25: 0.1613 - dense_1_loss_26: 0.1397 - dense_1_loss_27: 0.1392 - dense_1_loss_28: 0.1497 - dense_1_loss_29: 0.1637 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4500 - dense_1_acc_3: 0.7833 - dense_1_acc_4: 0.9333 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 54/100 60/60 [==============================] - 0s - loss: 11.1297 - dense_1_loss_1: 3.9131 - dense_1_loss_2: 2.1021 - dense_1_loss_3: 0.9777 - dense_1_loss_4: 0.4565 - dense_1_loss_5: 0.2980 - dense_1_loss_6: 0.2192 - dense_1_loss_7: 0.2220 - dense_1_loss_8: 0.1525 - dense_1_loss_9: 0.1522 - dense_1_loss_10: 0.1263 - dense_1_loss_11: 0.1413 - dense_1_loss_12: 0.1293 - dense_1_loss_13: 0.1164 - dense_1_loss_14: 0.1224 - dense_1_loss_15: 0.1257 - dense_1_loss_16: 0.1262 - dense_1_loss_17: 0.1297 - dense_1_loss_18: 0.1259 - dense_1_loss_19: 0.1343 - dense_1_loss_20: 0.1374 - dense_1_loss_21: 0.1325 - dense_1_loss_22: 0.1280 - dense_1_loss_23: 0.1302 - dense_1_loss_24: 0.1229 - dense_1_loss_25: 0.1515 - dense_1_loss_26: 0.1306 - dense_1_loss_27: 0.1295 - dense_1_loss_28: 0.1410 - dense_1_loss_29: 0.1554 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4833 - dense_1_acc_3: 0.7833 - dense_1_acc_4: 0.9333 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 55/100 60/60 [==============================] - 0s - loss: 10.8305 - dense_1_loss_1: 3.9091 - dense_1_loss_2: 2.0676 - dense_1_loss_3: 0.9481 - dense_1_loss_4: 0.4333 - dense_1_loss_5: 0.2845 - dense_1_loss_6: 0.2080 - dense_1_loss_7: 0.2110 - dense_1_loss_8: 0.1461 - dense_1_loss_9: 0.1429 - dense_1_loss_10: 0.1194 - dense_1_loss_11: 0.1314 - dense_1_loss_12: 0.1221 - dense_1_loss_13: 0.1087 - dense_1_loss_14: 0.1134 - dense_1_loss_15: 0.1198 - dense_1_loss_16: 0.1214 - dense_1_loss_17: 0.1213 - dense_1_loss_18: 0.1185 - dense_1_loss_19: 0.1254 - dense_1_loss_20: 0.1298 - dense_1_loss_21: 0.1259 - dense_1_loss_22: 0.1191 - dense_1_loss_23: 0.1219 - dense_1_loss_24: 0.1151 - dense_1_loss_25: 0.1430 - dense_1_loss_26: 0.1220 - dense_1_loss_27: 0.1234 - dense_1_loss_28: 0.1338 - dense_1_loss_29: 0.1443 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4833 - dense_1_acc_3: 0.8000 - dense_1_acc_4: 0.9333 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 56/100 60/60 [==============================] - 0s - loss: 10.5476 - dense_1_loss_1: 3.9054 - dense_1_loss_2: 2.0361 - dense_1_loss_3: 0.9192 - dense_1_loss_4: 0.4097 - dense_1_loss_5: 0.2699 - dense_1_loss_6: 0.1966 - dense_1_loss_7: 0.1976 - dense_1_loss_8: 0.1380 - dense_1_loss_9: 0.1355 - dense_1_loss_10: 0.1130 - dense_1_loss_11: 0.1203 - dense_1_loss_12: 0.1168 - dense_1_loss_13: 0.1029 - dense_1_loss_14: 0.1052 - dense_1_loss_15: 0.1129 - dense_1_loss_16: 0.1181 - dense_1_loss_17: 0.1149 - dense_1_loss_18: 0.1110 - dense_1_loss_19: 0.1188 - dense_1_loss_20: 0.1210 - dense_1_loss_21: 0.1195 - dense_1_loss_22: 0.1123 - dense_1_loss_23: 0.1150 - dense_1_loss_24: 0.1095 - dense_1_loss_25: 0.1334 - dense_1_loss_26: 0.1156 - dense_1_loss_27: 0.1183 - dense_1_loss_28: 0.1270 - dense_1_loss_29: 0.1341 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4833 - dense_1_acc_3: 0.8333 - dense_1_acc_4: 0.9333 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9833 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 57/100 60/60 [==============================] - 0s - loss: 10.2853 - dense_1_loss_1: 3.9014 - dense_1_loss_2: 2.0034 - dense_1_loss_3: 0.8925 - dense_1_loss_4: 0.3899 - dense_1_loss_5: 0.2565 - dense_1_loss_6: 0.1892 - dense_1_loss_7: 0.1875 - dense_1_loss_8: 0.1288 - dense_1_loss_9: 0.1290 - dense_1_loss_10: 0.1055 - dense_1_loss_11: 0.1154 - dense_1_loss_12: 0.1096 - dense_1_loss_13: 0.0982 - dense_1_loss_14: 0.1008 - dense_1_loss_15: 0.1044 - dense_1_loss_16: 0.1075 - dense_1_loss_17: 0.1089 - dense_1_loss_18: 0.1047 - dense_1_loss_19: 0.1123 - dense_1_loss_20: 0.1138 - dense_1_loss_21: 0.1113 - dense_1_loss_22: 0.1068 - dense_1_loss_23: 0.1086 - dense_1_loss_24: 0.1038 - dense_1_loss_25: 0.1242 - dense_1_loss_26: 0.1097 - dense_1_loss_27: 0.1119 - dense_1_loss_28: 0.1217 - dense_1_loss_29: 0.1279 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4833 - dense_1_acc_3: 0.8667 - dense_1_acc_4: 0.9333 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 58/100 60/60 [==============================] - 0s - loss: 10.0500 - dense_1_loss_1: 3.8974 - dense_1_loss_2: 1.9716 - dense_1_loss_3: 0.8691 - dense_1_loss_4: 0.3699 - dense_1_loss_5: 0.2441 - dense_1_loss_6: 0.1818 - dense_1_loss_7: 0.1776 - dense_1_loss_8: 0.1225 - dense_1_loss_9: 0.1224 - dense_1_loss_10: 0.0987 - dense_1_loss_11: 0.1126 - dense_1_loss_12: 0.1019 - dense_1_loss_13: 0.0948 - dense_1_loss_14: 0.0996 - dense_1_loss_15: 0.0984 - dense_1_loss_16: 0.0987 - dense_1_loss_17: 0.1031 - dense_1_loss_18: 0.0995 - dense_1_loss_19: 0.1077 - dense_1_loss_20: 0.1079 - dense_1_loss_21: 0.1046 - dense_1_loss_22: 0.1028 - dense_1_loss_23: 0.1023 - dense_1_loss_24: 0.0975 - dense_1_loss_25: 0.1167 - dense_1_loss_26: 0.1035 - dense_1_loss_27: 0.1052 - dense_1_loss_28: 0.1159 - dense_1_loss_29: 0.1220 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4833 - dense_1_acc_3: 0.8667 - dense_1_acc_4: 0.9500 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 59/100 60/60 [==============================] - 0s - loss: 9.8203 - dense_1_loss_1: 3.8939 - dense_1_loss_2: 1.9422 - dense_1_loss_3: 0.8429 - dense_1_loss_4: 0.3533 - dense_1_loss_5: 0.2323 - dense_1_loss_6: 0.1734 - dense_1_loss_7: 0.1676 - dense_1_loss_8: 0.1176 - dense_1_loss_9: 0.1153 - dense_1_loss_10: 0.0942 - dense_1_loss_11: 0.1060 - dense_1_loss_12: 0.0960 - dense_1_loss_13: 0.0904 - dense_1_loss_14: 0.0930 - dense_1_loss_15: 0.0923 - dense_1_loss_16: 0.0952 - dense_1_loss_17: 0.0974 - dense_1_loss_18: 0.0942 - dense_1_loss_19: 0.1022 - dense_1_loss_20: 0.1018 - dense_1_loss_21: 0.0990 - dense_1_loss_22: 0.0968 - dense_1_loss_23: 0.0966 - dense_1_loss_24: 0.0918 - dense_1_loss_25: 0.1124 - dense_1_loss_26: 0.0959 - dense_1_loss_27: 0.1002 - dense_1_loss_28: 0.1087 - dense_1_loss_29: 0.1179 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4833 - dense_1_acc_3: 0.8667 - dense_1_acc_4: 0.9667 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9833 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 60/100 60/60 [==============================] - 0s - loss: 9.6102 - dense_1_loss_1: 3.8901 - dense_1_loss_2: 1.9130 - dense_1_loss_3: 0.8196 - dense_1_loss_4: 0.3378 - dense_1_loss_5: 0.2224 - dense_1_loss_6: 0.1642 - dense_1_loss_7: 0.1581 - dense_1_loss_8: 0.1135 - dense_1_loss_9: 0.1082 - dense_1_loss_10: 0.0912 - dense_1_loss_11: 0.0977 - dense_1_loss_12: 0.0921 - dense_1_loss_13: 0.0849 - dense_1_loss_14: 0.0853 - dense_1_loss_15: 0.0877 - dense_1_loss_16: 0.0951 - dense_1_loss_17: 0.0930 - dense_1_loss_18: 0.0896 - dense_1_loss_19: 0.0958 - dense_1_loss_20: 0.0967 - dense_1_loss_21: 0.0938 - dense_1_loss_22: 0.0912 - dense_1_loss_23: 0.0920 - dense_1_loss_24: 0.0874 - dense_1_loss_25: 0.1076 - dense_1_loss_26: 0.0912 - dense_1_loss_27: 0.0961 - dense_1_loss_28: 0.1043 - dense_1_loss_29: 0.1108 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4833 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9667 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 61/100 60/60 [==============================] - 0s - loss: 9.4133 - dense_1_loss_1: 3.8863 - dense_1_loss_2: 1.8849 - dense_1_loss_3: 0.7978 - dense_1_loss_4: 0.3220 - dense_1_loss_5: 0.2128 - dense_1_loss_6: 0.1572 - dense_1_loss_7: 0.1521 - dense_1_loss_8: 0.1070 - dense_1_loss_9: 0.1030 - dense_1_loss_10: 0.0857 - dense_1_loss_11: 0.0937 - dense_1_loss_12: 0.0873 - dense_1_loss_13: 0.0807 - dense_1_loss_14: 0.0805 - dense_1_loss_15: 0.0843 - dense_1_loss_16: 0.0901 - dense_1_loss_17: 0.0880 - dense_1_loss_18: 0.0856 - dense_1_loss_19: 0.0903 - dense_1_loss_20: 0.0918 - dense_1_loss_21: 0.0891 - dense_1_loss_22: 0.0861 - dense_1_loss_23: 0.0880 - dense_1_loss_24: 0.0832 - dense_1_loss_25: 0.1017 - dense_1_loss_26: 0.0877 - dense_1_loss_27: 0.0909 - dense_1_loss_28: 0.1005 - dense_1_loss_29: 0.1050 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4833 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9667 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 62/100 60/60 [==============================] - 0s - loss: 9.2328 - dense_1_loss_1: 3.8825 - dense_1_loss_2: 1.8573 - dense_1_loss_3: 0.7768 - dense_1_loss_4: 0.3090 - dense_1_loss_5: 0.2032 - dense_1_loss_6: 0.1514 - dense_1_loss_7: 0.1455 - dense_1_loss_8: 0.1013 - dense_1_loss_9: 0.0986 - dense_1_loss_10: 0.0806 - dense_1_loss_11: 0.0914 - dense_1_loss_12: 0.0821 - dense_1_loss_13: 0.0775 - dense_1_loss_14: 0.0791 - dense_1_loss_15: 0.0815 - dense_1_loss_16: 0.0832 - dense_1_loss_17: 0.0835 - dense_1_loss_18: 0.0818 - dense_1_loss_19: 0.0859 - dense_1_loss_20: 0.0878 - dense_1_loss_21: 0.0853 - dense_1_loss_22: 0.0816 - dense_1_loss_23: 0.0841 - dense_1_loss_24: 0.0788 - dense_1_loss_25: 0.0971 - dense_1_loss_26: 0.0838 - dense_1_loss_27: 0.0861 - dense_1_loss_28: 0.0956 - dense_1_loss_29: 0.1004 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.5333 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 63/100 60/60 [==============================] - 0s - loss: 9.0548 - dense_1_loss_1: 3.8792 - dense_1_loss_2: 1.8308 - dense_1_loss_3: 0.7560 - dense_1_loss_4: 0.2950 - dense_1_loss_5: 0.1932 - dense_1_loss_6: 0.1463 - dense_1_loss_7: 0.1384 - dense_1_loss_8: 0.0973 - dense_1_loss_9: 0.0945 - dense_1_loss_10: 0.0765 - dense_1_loss_11: 0.0868 - dense_1_loss_12: 0.0783 - dense_1_loss_13: 0.0737 - dense_1_loss_14: 0.0760 - dense_1_loss_15: 0.0766 - dense_1_loss_16: 0.0795 - dense_1_loss_17: 0.0796 - dense_1_loss_18: 0.0771 - dense_1_loss_19: 0.0826 - dense_1_loss_20: 0.0834 - dense_1_loss_21: 0.0810 - dense_1_loss_22: 0.0785 - dense_1_loss_23: 0.0788 - dense_1_loss_24: 0.0750 - dense_1_loss_25: 0.0926 - dense_1_loss_26: 0.0799 - dense_1_loss_27: 0.0815 - dense_1_loss_28: 0.0906 - dense_1_loss_29: 0.0961 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.5333 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 64/100 60/60 [==============================] - 0s - loss: 8.8914 - dense_1_loss_1: 3.8755 - dense_1_loss_2: 1.8055 - dense_1_loss_3: 0.7353 - dense_1_loss_4: 0.2827 - dense_1_loss_5: 0.1841 - dense_1_loss_6: 0.1409 - dense_1_loss_7: 0.1301 - dense_1_loss_8: 0.0936 - dense_1_loss_9: 0.0905 - dense_1_loss_10: 0.0738 - dense_1_loss_11: 0.0813 - dense_1_loss_12: 0.0759 - dense_1_loss_13: 0.0702 - dense_1_loss_14: 0.0716 - dense_1_loss_15: 0.0723 - dense_1_loss_16: 0.0781 - dense_1_loss_17: 0.0761 - dense_1_loss_18: 0.0736 - dense_1_loss_19: 0.0786 - dense_1_loss_20: 0.0793 - dense_1_loss_21: 0.0781 - dense_1_loss_22: 0.0746 - dense_1_loss_23: 0.0752 - dense_1_loss_24: 0.0720 - dense_1_loss_25: 0.0887 - dense_1_loss_26: 0.0761 - dense_1_loss_27: 0.0778 - dense_1_loss_28: 0.0867 - dense_1_loss_29: 0.0931 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.5500 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 65/100 60/60 [==============================] - 0s - loss: 8.7391 - dense_1_loss_1: 3.8723 - dense_1_loss_2: 1.7804 - dense_1_loss_3: 0.7166 - dense_1_loss_4: 0.2720 - dense_1_loss_5: 0.1767 - dense_1_loss_6: 0.1358 - dense_1_loss_7: 0.1244 - dense_1_loss_8: 0.0892 - dense_1_loss_9: 0.0861 - dense_1_loss_10: 0.0708 - dense_1_loss_11: 0.0780 - dense_1_loss_12: 0.0725 - dense_1_loss_13: 0.0674 - dense_1_loss_14: 0.0686 - dense_1_loss_15: 0.0695 - dense_1_loss_16: 0.0742 - dense_1_loss_17: 0.0727 - dense_1_loss_18: 0.0706 - dense_1_loss_19: 0.0747 - dense_1_loss_20: 0.0758 - dense_1_loss_21: 0.0747 - dense_1_loss_22: 0.0707 - dense_1_loss_23: 0.0721 - dense_1_loss_24: 0.0693 - dense_1_loss_25: 0.0842 - dense_1_loss_26: 0.0727 - dense_1_loss_27: 0.0748 - dense_1_loss_28: 0.0830 - dense_1_loss_29: 0.0892 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.5500 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 66/100 60/60 [==============================] - 0s - loss: 8.5942 - dense_1_loss_1: 3.8689 - dense_1_loss_2: 1.7559 - dense_1_loss_3: 0.6982 - dense_1_loss_4: 0.2609 - dense_1_loss_5: 0.1701 - dense_1_loss_6: 0.1306 - dense_1_loss_7: 0.1191 - dense_1_loss_8: 0.0851 - dense_1_loss_9: 0.0818 - dense_1_loss_10: 0.0680 - dense_1_loss_11: 0.0746 - dense_1_loss_12: 0.0693 - dense_1_loss_13: 0.0646 - dense_1_loss_14: 0.0660 - dense_1_loss_15: 0.0671 - dense_1_loss_16: 0.0708 - dense_1_loss_17: 0.0694 - dense_1_loss_18: 0.0676 - dense_1_loss_19: 0.0714 - dense_1_loss_20: 0.0726 - dense_1_loss_21: 0.0712 - dense_1_loss_22: 0.0675 - dense_1_loss_23: 0.0694 - dense_1_loss_24: 0.0669 - dense_1_loss_25: 0.0796 - dense_1_loss_26: 0.0696 - dense_1_loss_27: 0.0718 - dense_1_loss_28: 0.0809 - dense_1_loss_29: 0.0853 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.5667 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 67/100 60/60 [==============================] - 0s - loss: 8.4597 - dense_1_loss_1: 3.8653 - dense_1_loss_2: 1.7322 - dense_1_loss_3: 0.6813 - dense_1_loss_4: 0.2504 - dense_1_loss_5: 0.1646 - dense_1_loss_6: 0.1260 - dense_1_loss_7: 0.1150 - dense_1_loss_8: 0.0816 - dense_1_loss_9: 0.0788 - dense_1_loss_10: 0.0654 - dense_1_loss_11: 0.0715 - dense_1_loss_12: 0.0665 - dense_1_loss_13: 0.0620 - dense_1_loss_14: 0.0631 - dense_1_loss_15: 0.0643 - dense_1_loss_16: 0.0678 - dense_1_loss_17: 0.0667 - dense_1_loss_18: 0.0647 - dense_1_loss_19: 0.0686 - dense_1_loss_20: 0.0697 - dense_1_loss_21: 0.0677 - dense_1_loss_22: 0.0649 - dense_1_loss_23: 0.0665 - dense_1_loss_24: 0.0640 - dense_1_loss_25: 0.0765 - dense_1_loss_26: 0.0670 - dense_1_loss_27: 0.0691 - dense_1_loss_28: 0.0773 - dense_1_loss_29: 0.0814 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.5833 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 68/100 60/60 [==============================] - 0s - loss: 8.3313 - dense_1_loss_1: 3.8621 - dense_1_loss_2: 1.7093 - dense_1_loss_3: 0.6644 - dense_1_loss_4: 0.2410 - dense_1_loss_5: 0.1584 - dense_1_loss_6: 0.1217 - dense_1_loss_7: 0.1098 - dense_1_loss_8: 0.0788 - dense_1_loss_9: 0.0759 - dense_1_loss_10: 0.0629 - dense_1_loss_11: 0.0685 - dense_1_loss_12: 0.0640 - dense_1_loss_13: 0.0594 - dense_1_loss_14: 0.0603 - dense_1_loss_15: 0.0613 - dense_1_loss_16: 0.0656 - dense_1_loss_17: 0.0639 - dense_1_loss_18: 0.0619 - dense_1_loss_19: 0.0660 - dense_1_loss_20: 0.0670 - dense_1_loss_21: 0.0648 - dense_1_loss_22: 0.0628 - dense_1_loss_23: 0.0633 - dense_1_loss_24: 0.0610 - dense_1_loss_25: 0.0739 - dense_1_loss_26: 0.0641 - dense_1_loss_27: 0.0664 - dense_1_loss_28: 0.0742 - dense_1_loss_29: 0.0783 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 69/100 60/60 [==============================] - 0s - loss: 8.2080 - dense_1_loss_1: 3.8588 - dense_1_loss_2: 1.6867 - dense_1_loss_3: 0.6497 - dense_1_loss_4: 0.2312 - dense_1_loss_5: 0.1523 - dense_1_loss_6: 0.1170 - dense_1_loss_7: 0.1045 - dense_1_loss_8: 0.0760 - dense_1_loss_9: 0.0730 - dense_1_loss_10: 0.0605 - dense_1_loss_11: 0.0660 - dense_1_loss_12: 0.0614 - dense_1_loss_13: 0.0570 - dense_1_loss_14: 0.0581 - dense_1_loss_15: 0.0590 - dense_1_loss_16: 0.0633 - dense_1_loss_17: 0.0611 - dense_1_loss_18: 0.0595 - dense_1_loss_19: 0.0633 - dense_1_loss_20: 0.0644 - dense_1_loss_21: 0.0625 - dense_1_loss_22: 0.0602 - dense_1_loss_23: 0.0608 - dense_1_loss_24: 0.0586 - dense_1_loss_25: 0.0714 - dense_1_loss_26: 0.0612 - dense_1_loss_27: 0.0638 - dense_1_loss_28: 0.0713 - dense_1_loss_29: 0.0755 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 70/100 60/60 [==============================] - 0s - loss: 8.0932 - dense_1_loss_1: 3.8554 - dense_1_loss_2: 1.6644 - dense_1_loss_3: 0.6344 - dense_1_loss_4: 0.2231 - dense_1_loss_5: 0.1467 - dense_1_loss_6: 0.1132 - dense_1_loss_7: 0.1002 - dense_1_loss_8: 0.0731 - dense_1_loss_9: 0.0704 - dense_1_loss_10: 0.0579 - dense_1_loss_11: 0.0641 - dense_1_loss_12: 0.0590 - dense_1_loss_13: 0.0547 - dense_1_loss_14: 0.0564 - dense_1_loss_15: 0.0570 - dense_1_loss_16: 0.0605 - dense_1_loss_17: 0.0586 - dense_1_loss_18: 0.0574 - dense_1_loss_19: 0.0612 - dense_1_loss_20: 0.0617 - dense_1_loss_21: 0.0602 - dense_1_loss_22: 0.0581 - dense_1_loss_23: 0.0583 - dense_1_loss_24: 0.0565 - dense_1_loss_25: 0.0686 - dense_1_loss_26: 0.0590 - dense_1_loss_27: 0.0614 - dense_1_loss_28: 0.0689 - dense_1_loss_29: 0.0725 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 71/100 60/60 [==============================] - 0s - loss: 7.9833 - dense_1_loss_1: 3.8521 - dense_1_loss_2: 1.6440 - dense_1_loss_3: 0.6199 - dense_1_loss_4: 0.2144 - dense_1_loss_5: 0.1410 - dense_1_loss_6: 0.1097 - dense_1_loss_7: 0.0962 - dense_1_loss_8: 0.0707 - dense_1_loss_9: 0.0678 - dense_1_loss_10: 0.0558 - dense_1_loss_11: 0.0620 - dense_1_loss_12: 0.0569 - dense_1_loss_13: 0.0526 - dense_1_loss_14: 0.0543 - dense_1_loss_15: 0.0549 - dense_1_loss_16: 0.0583 - dense_1_loss_17: 0.0564 - dense_1_loss_18: 0.0554 - dense_1_loss_19: 0.0587 - dense_1_loss_20: 0.0594 - dense_1_loss_21: 0.0580 - dense_1_loss_22: 0.0557 - dense_1_loss_23: 0.0561 - dense_1_loss_24: 0.0546 - dense_1_loss_25: 0.0658 - dense_1_loss_26: 0.0568 - dense_1_loss_27: 0.0590 - dense_1_loss_28: 0.0666 - dense_1_loss_29: 0.0701 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 72/100 60/60 [==============================] - 0s - loss: 7.8793 - dense_1_loss_1: 3.8491 - dense_1_loss_2: 1.6233 - dense_1_loss_3: 0.6055 - dense_1_loss_4: 0.2075 - dense_1_loss_5: 0.1360 - dense_1_loss_6: 0.1063 - dense_1_loss_7: 0.0922 - dense_1_loss_8: 0.0684 - dense_1_loss_9: 0.0655 - dense_1_loss_10: 0.0538 - dense_1_loss_11: 0.0597 - dense_1_loss_12: 0.0548 - dense_1_loss_13: 0.0509 - dense_1_loss_14: 0.0519 - dense_1_loss_15: 0.0528 - dense_1_loss_16: 0.0567 - dense_1_loss_17: 0.0545 - dense_1_loss_18: 0.0533 - dense_1_loss_19: 0.0564 - dense_1_loss_20: 0.0573 - dense_1_loss_21: 0.0558 - dense_1_loss_22: 0.0536 - dense_1_loss_23: 0.0542 - dense_1_loss_24: 0.0528 - dense_1_loss_25: 0.0632 - dense_1_loss_26: 0.0548 - dense_1_loss_27: 0.0568 - dense_1_loss_28: 0.0645 - dense_1_loss_29: 0.0678 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 73/100 60/60 [==============================] - 0s - loss: 7.7824 - dense_1_loss_1: 3.8456 - dense_1_loss_2: 1.6041 - dense_1_loss_3: 0.5923 - dense_1_loss_4: 0.2004 - dense_1_loss_5: 0.1317 - dense_1_loss_6: 0.1029 - dense_1_loss_7: 0.0889 - dense_1_loss_8: 0.0664 - dense_1_loss_9: 0.0632 - dense_1_loss_10: 0.0521 - dense_1_loss_11: 0.0574 - dense_1_loss_12: 0.0529 - dense_1_loss_13: 0.0493 - dense_1_loss_14: 0.0501 - dense_1_loss_15: 0.0510 - dense_1_loss_16: 0.0550 - dense_1_loss_17: 0.0527 - dense_1_loss_18: 0.0514 - dense_1_loss_19: 0.0545 - dense_1_loss_20: 0.0553 - dense_1_loss_21: 0.0539 - dense_1_loss_22: 0.0516 - dense_1_loss_23: 0.0523 - dense_1_loss_24: 0.0509 - dense_1_loss_25: 0.0612 - dense_1_loss_26: 0.0528 - dense_1_loss_27: 0.0548 - dense_1_loss_28: 0.0620 - dense_1_loss_29: 0.0656 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 74/100 60/60 [==============================] - 0s - loss: 7.6862 - dense_1_loss_1: 3.8425 - dense_1_loss_2: 1.5849 - dense_1_loss_3: 0.5793 - dense_1_loss_4: 0.1927 - dense_1_loss_5: 0.1273 - dense_1_loss_6: 0.0993 - dense_1_loss_7: 0.0849 - dense_1_loss_8: 0.0644 - dense_1_loss_9: 0.0610 - dense_1_loss_10: 0.0502 - dense_1_loss_11: 0.0555 - dense_1_loss_12: 0.0509 - dense_1_loss_13: 0.0478 - dense_1_loss_14: 0.0484 - dense_1_loss_15: 0.0494 - dense_1_loss_16: 0.0531 - dense_1_loss_17: 0.0509 - dense_1_loss_18: 0.0496 - dense_1_loss_19: 0.0527 - dense_1_loss_20: 0.0534 - dense_1_loss_21: 0.0521 - dense_1_loss_22: 0.0499 - dense_1_loss_23: 0.0506 - dense_1_loss_24: 0.0491 - dense_1_loss_25: 0.0593 - dense_1_loss_26: 0.0512 - dense_1_loss_27: 0.0528 - dense_1_loss_28: 0.0598 - dense_1_loss_29: 0.0633 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 75/100 60/60 [==============================] - 0s - loss: 7.5985 - dense_1_loss_1: 3.8396 - dense_1_loss_2: 1.5661 - dense_1_loss_3: 0.5669 - dense_1_loss_4: 0.1867 - dense_1_loss_5: 0.1231 - dense_1_loss_6: 0.0963 - dense_1_loss_7: 0.0820 - dense_1_loss_8: 0.0623 - dense_1_loss_9: 0.0592 - dense_1_loss_10: 0.0486 - dense_1_loss_11: 0.0536 - dense_1_loss_12: 0.0492 - dense_1_loss_13: 0.0461 - dense_1_loss_14: 0.0469 - dense_1_loss_15: 0.0479 - dense_1_loss_16: 0.0513 - dense_1_loss_17: 0.0492 - dense_1_loss_18: 0.0480 - dense_1_loss_19: 0.0510 - dense_1_loss_20: 0.0517 - dense_1_loss_21: 0.0506 - dense_1_loss_22: 0.0483 - dense_1_loss_23: 0.0489 - dense_1_loss_24: 0.0476 - dense_1_loss_25: 0.0574 - dense_1_loss_26: 0.0496 - dense_1_loss_27: 0.0511 - dense_1_loss_28: 0.0580 - dense_1_loss_29: 0.0615 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 76/100 60/60 [==============================] - 0s - loss: 7.5125 - dense_1_loss_1: 3.8364 - dense_1_loss_2: 1.5483 - dense_1_loss_3: 0.5546 - dense_1_loss_4: 0.1802 - dense_1_loss_5: 0.1192 - dense_1_loss_6: 0.0934 - dense_1_loss_7: 0.0791 - dense_1_loss_8: 0.0603 - dense_1_loss_9: 0.0576 - dense_1_loss_10: 0.0469 - dense_1_loss_11: 0.0519 - dense_1_loss_12: 0.0477 - dense_1_loss_13: 0.0446 - dense_1_loss_14: 0.0454 - dense_1_loss_15: 0.0463 - dense_1_loss_16: 0.0498 - dense_1_loss_17: 0.0475 - dense_1_loss_18: 0.0464 - dense_1_loss_19: 0.0493 - dense_1_loss_20: 0.0500 - dense_1_loss_21: 0.0490 - dense_1_loss_22: 0.0467 - dense_1_loss_23: 0.0472 - dense_1_loss_24: 0.0460 - dense_1_loss_25: 0.0555 - dense_1_loss_26: 0.0479 - dense_1_loss_27: 0.0494 - dense_1_loss_28: 0.0563 - dense_1_loss_29: 0.0593 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 77/100 60/60 [==============================] - 0s - loss: 7.4341 - dense_1_loss_1: 3.8334 - dense_1_loss_2: 1.5311 - dense_1_loss_3: 0.5435 - dense_1_loss_4: 0.1750 - dense_1_loss_5: 0.1158 - dense_1_loss_6: 0.0910 - dense_1_loss_7: 0.0767 - dense_1_loss_8: 0.0585 - dense_1_loss_9: 0.0560 - dense_1_loss_10: 0.0454 - dense_1_loss_11: 0.0504 - dense_1_loss_12: 0.0463 - dense_1_loss_13: 0.0432 - dense_1_loss_14: 0.0440 - dense_1_loss_15: 0.0450 - dense_1_loss_16: 0.0483 - dense_1_loss_17: 0.0461 - dense_1_loss_18: 0.0450 - dense_1_loss_19: 0.0478 - dense_1_loss_20: 0.0483 - dense_1_loss_21: 0.0475 - dense_1_loss_22: 0.0451 - dense_1_loss_23: 0.0458 - dense_1_loss_24: 0.0446 - dense_1_loss_25: 0.0535 - dense_1_loss_26: 0.0465 - dense_1_loss_27: 0.0480 - dense_1_loss_28: 0.0547 - dense_1_loss_29: 0.0576 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 78/100 60/60 [==============================] - 0s - loss: 7.3557 - dense_1_loss_1: 3.8301 - dense_1_loss_2: 1.5137 - dense_1_loss_3: 0.5326 - dense_1_loss_4: 0.1695 - dense_1_loss_5: 0.1125 - dense_1_loss_6: 0.0882 - dense_1_loss_7: 0.0739 - dense_1_loss_8: 0.0568 - dense_1_loss_9: 0.0543 - dense_1_loss_10: 0.0441 - dense_1_loss_11: 0.0487 - dense_1_loss_12: 0.0448 - dense_1_loss_13: 0.0419 - dense_1_loss_14: 0.0426 - dense_1_loss_15: 0.0435 - dense_1_loss_16: 0.0469 - dense_1_loss_17: 0.0447 - dense_1_loss_18: 0.0435 - dense_1_loss_19: 0.0463 - dense_1_loss_20: 0.0468 - dense_1_loss_21: 0.0460 - dense_1_loss_22: 0.0437 - dense_1_loss_23: 0.0444 - dense_1_loss_24: 0.0434 - dense_1_loss_25: 0.0519 - dense_1_loss_26: 0.0451 - dense_1_loss_27: 0.0468 - dense_1_loss_28: 0.0532 - dense_1_loss_29: 0.0558 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 79/100 60/60 [==============================] - 0s - loss: 7.2792 - dense_1_loss_1: 3.8274 - dense_1_loss_2: 1.4976 - dense_1_loss_3: 0.5203 - dense_1_loss_4: 0.1640 - dense_1_loss_5: 0.1092 - dense_1_loss_6: 0.0854 - dense_1_loss_7: 0.0712 - dense_1_loss_8: 0.0553 - dense_1_loss_9: 0.0525 - dense_1_loss_10: 0.0428 - dense_1_loss_11: 0.0473 - dense_1_loss_12: 0.0433 - dense_1_loss_13: 0.0407 - dense_1_loss_14: 0.0412 - dense_1_loss_15: 0.0423 - dense_1_loss_16: 0.0455 - dense_1_loss_17: 0.0433 - dense_1_loss_18: 0.0422 - dense_1_loss_19: 0.0449 - dense_1_loss_20: 0.0454 - dense_1_loss_21: 0.0445 - dense_1_loss_22: 0.0423 - dense_1_loss_23: 0.0432 - dense_1_loss_24: 0.0420 - dense_1_loss_25: 0.0507 - dense_1_loss_26: 0.0437 - dense_1_loss_27: 0.0455 - dense_1_loss_28: 0.0514 - dense_1_loss_29: 0.0542 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 80/100 60/60 [==============================] - 0s - loss: 7.2088 - dense_1_loss_1: 3.8242 - dense_1_loss_2: 1.4811 - dense_1_loss_3: 0.5108 - dense_1_loss_4: 0.1594 - dense_1_loss_5: 0.1063 - dense_1_loss_6: 0.0832 - dense_1_loss_7: 0.0692 - dense_1_loss_8: 0.0537 - dense_1_loss_9: 0.0511 - dense_1_loss_10: 0.0415 - dense_1_loss_11: 0.0459 - dense_1_loss_12: 0.0420 - dense_1_loss_13: 0.0396 - dense_1_loss_14: 0.0399 - dense_1_loss_15: 0.0412 - dense_1_loss_16: 0.0442 - dense_1_loss_17: 0.0419 - dense_1_loss_18: 0.0410 - dense_1_loss_19: 0.0436 - dense_1_loss_20: 0.0441 - dense_1_loss_21: 0.0431 - dense_1_loss_22: 0.0411 - dense_1_loss_23: 0.0420 - dense_1_loss_24: 0.0408 - dense_1_loss_25: 0.0494 - dense_1_loss_26: 0.0423 - dense_1_loss_27: 0.0441 - dense_1_loss_28: 0.0497 - dense_1_loss_29: 0.0524 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 81/100 60/60 [==============================] - 0s - loss: 7.1396 - dense_1_loss_1: 3.8213 - dense_1_loss_2: 1.4653 - dense_1_loss_3: 0.5001 - dense_1_loss_4: 0.1549 - dense_1_loss_5: 0.1030 - dense_1_loss_6: 0.0809 - dense_1_loss_7: 0.0669 - dense_1_loss_8: 0.0520 - dense_1_loss_9: 0.0497 - dense_1_loss_10: 0.0404 - dense_1_loss_11: 0.0447 - dense_1_loss_12: 0.0409 - dense_1_loss_13: 0.0384 - dense_1_loss_14: 0.0388 - dense_1_loss_15: 0.0400 - dense_1_loss_16: 0.0428 - dense_1_loss_17: 0.0407 - dense_1_loss_18: 0.0398 - dense_1_loss_19: 0.0422 - dense_1_loss_20: 0.0428 - dense_1_loss_21: 0.0420 - dense_1_loss_22: 0.0400 - dense_1_loss_23: 0.0406 - dense_1_loss_24: 0.0398 - dense_1_loss_25: 0.0478 - dense_1_loss_26: 0.0411 - dense_1_loss_27: 0.0429 - dense_1_loss_28: 0.0486 - dense_1_loss_29: 0.0511 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 82/100 60/60 [==============================] - 0s - loss: 7.0758 - dense_1_loss_1: 3.8183 - dense_1_loss_2: 1.4507 - dense_1_loss_3: 0.4906 - dense_1_loss_4: 0.1508 - dense_1_loss_5: 0.1003 - dense_1_loss_6: 0.0791 - dense_1_loss_7: 0.0651 - dense_1_loss_8: 0.0507 - dense_1_loss_9: 0.0485 - dense_1_loss_10: 0.0393 - dense_1_loss_11: 0.0435 - dense_1_loss_12: 0.0398 - dense_1_loss_13: 0.0374 - dense_1_loss_14: 0.0377 - dense_1_loss_15: 0.0388 - dense_1_loss_16: 0.0418 - dense_1_loss_17: 0.0396 - dense_1_loss_18: 0.0386 - dense_1_loss_19: 0.0410 - dense_1_loss_20: 0.0417 - dense_1_loss_21: 0.0407 - dense_1_loss_22: 0.0388 - dense_1_loss_23: 0.0395 - dense_1_loss_24: 0.0387 - dense_1_loss_25: 0.0460 - dense_1_loss_26: 0.0400 - dense_1_loss_27: 0.0417 - dense_1_loss_28: 0.0475 - dense_1_loss_29: 0.0497 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 83/100 60/60 [==============================] - 0s - loss: 7.0105 - dense_1_loss_1: 3.8154 - dense_1_loss_2: 1.4350 - dense_1_loss_3: 0.4811 - dense_1_loss_4: 0.1461 - dense_1_loss_5: 0.0975 - dense_1_loss_6: 0.0767 - dense_1_loss_7: 0.0629 - dense_1_loss_8: 0.0494 - dense_1_loss_9: 0.0473 - dense_1_loss_10: 0.0382 - dense_1_loss_11: 0.0422 - dense_1_loss_12: 0.0388 - dense_1_loss_13: 0.0363 - dense_1_loss_14: 0.0366 - dense_1_loss_15: 0.0375 - dense_1_loss_16: 0.0410 - dense_1_loss_17: 0.0385 - dense_1_loss_18: 0.0375 - dense_1_loss_19: 0.0398 - dense_1_loss_20: 0.0405 - dense_1_loss_21: 0.0396 - dense_1_loss_22: 0.0378 - dense_1_loss_23: 0.0382 - dense_1_loss_24: 0.0377 - dense_1_loss_25: 0.0446 - dense_1_loss_26: 0.0389 - dense_1_loss_27: 0.0406 - dense_1_loss_28: 0.0463 - dense_1_loss_29: 0.0484 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 84/100 60/60 [==============================] - 0s - loss: 6.9501 - dense_1_loss_1: 3.8125 - dense_1_loss_2: 1.4203 - dense_1_loss_3: 0.4719 - dense_1_loss_4: 0.1424 - dense_1_loss_5: 0.0951 - dense_1_loss_6: 0.0747 - dense_1_loss_7: 0.0612 - dense_1_loss_8: 0.0482 - dense_1_loss_9: 0.0460 - dense_1_loss_10: 0.0372 - dense_1_loss_11: 0.0412 - dense_1_loss_12: 0.0377 - dense_1_loss_13: 0.0353 - dense_1_loss_14: 0.0357 - dense_1_loss_15: 0.0366 - dense_1_loss_16: 0.0398 - dense_1_loss_17: 0.0374 - dense_1_loss_18: 0.0365 - dense_1_loss_19: 0.0388 - dense_1_loss_20: 0.0393 - dense_1_loss_21: 0.0385 - dense_1_loss_22: 0.0368 - dense_1_loss_23: 0.0372 - dense_1_loss_24: 0.0367 - dense_1_loss_25: 0.0435 - dense_1_loss_26: 0.0379 - dense_1_loss_27: 0.0395 - dense_1_loss_28: 0.0449 - dense_1_loss_29: 0.0473 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 85/100 60/60 [==============================] - 0s - loss: 6.8908 - dense_1_loss_1: 3.8096 - dense_1_loss_2: 1.4065 - dense_1_loss_3: 0.4622 - dense_1_loss_4: 0.1385 - dense_1_loss_5: 0.0926 - dense_1_loss_6: 0.0726 - dense_1_loss_7: 0.0596 - dense_1_loss_8: 0.0470 - dense_1_loss_9: 0.0448 - dense_1_loss_10: 0.0362 - dense_1_loss_11: 0.0401 - dense_1_loss_12: 0.0367 - dense_1_loss_13: 0.0344 - dense_1_loss_14: 0.0348 - dense_1_loss_15: 0.0358 - dense_1_loss_16: 0.0387 - dense_1_loss_17: 0.0364 - dense_1_loss_18: 0.0355 - dense_1_loss_19: 0.0379 - dense_1_loss_20: 0.0381 - dense_1_loss_21: 0.0375 - dense_1_loss_22: 0.0358 - dense_1_loss_23: 0.0363 - dense_1_loss_24: 0.0357 - dense_1_loss_25: 0.0427 - dense_1_loss_26: 0.0369 - dense_1_loss_27: 0.0384 - dense_1_loss_28: 0.0436 - dense_1_loss_29: 0.0460 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 86/100 60/60 [==============================] - 0s - loss: 6.8355 - dense_1_loss_1: 3.8069 - dense_1_loss_2: 1.3923 - dense_1_loss_3: 0.4541 - dense_1_loss_4: 0.1352 - dense_1_loss_5: 0.0904 - dense_1_loss_6: 0.0708 - dense_1_loss_7: 0.0582 - dense_1_loss_8: 0.0458 - dense_1_loss_9: 0.0436 - dense_1_loss_10: 0.0352 - dense_1_loss_11: 0.0392 - dense_1_loss_12: 0.0357 - dense_1_loss_13: 0.0335 - dense_1_loss_14: 0.0339 - dense_1_loss_15: 0.0349 - dense_1_loss_16: 0.0376 - dense_1_loss_17: 0.0355 - dense_1_loss_18: 0.0347 - dense_1_loss_19: 0.0370 - dense_1_loss_20: 0.0371 - dense_1_loss_21: 0.0366 - dense_1_loss_22: 0.0349 - dense_1_loss_23: 0.0354 - dense_1_loss_24: 0.0348 - dense_1_loss_25: 0.0417 - dense_1_loss_26: 0.0360 - dense_1_loss_27: 0.0373 - dense_1_loss_28: 0.0424 - dense_1_loss_29: 0.0448 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 87/100 60/60 [==============================] - 0s - loss: 6.7820 - dense_1_loss_1: 3.8038 - dense_1_loss_2: 1.3795 - dense_1_loss_3: 0.4460 - dense_1_loss_4: 0.1318 - dense_1_loss_5: 0.0884 - dense_1_loss_6: 0.0693 - dense_1_loss_7: 0.0568 - dense_1_loss_8: 0.0447 - dense_1_loss_9: 0.0426 - dense_1_loss_10: 0.0343 - dense_1_loss_11: 0.0381 - dense_1_loss_12: 0.0348 - dense_1_loss_13: 0.0326 - dense_1_loss_14: 0.0330 - dense_1_loss_15: 0.0340 - dense_1_loss_16: 0.0367 - dense_1_loss_17: 0.0346 - dense_1_loss_18: 0.0337 - dense_1_loss_19: 0.0359 - dense_1_loss_20: 0.0362 - dense_1_loss_21: 0.0356 - dense_1_loss_22: 0.0339 - dense_1_loss_23: 0.0346 - dense_1_loss_24: 0.0339 - dense_1_loss_25: 0.0404 - dense_1_loss_26: 0.0351 - dense_1_loss_27: 0.0364 - dense_1_loss_28: 0.0414 - dense_1_loss_29: 0.0438 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 88/100 60/60 [==============================] - 0s - loss: 6.7287 - dense_1_loss_1: 3.8010 - dense_1_loss_2: 1.3661 - dense_1_loss_3: 0.4374 - dense_1_loss_4: 0.1286 - dense_1_loss_5: 0.0861 - dense_1_loss_6: 0.0677 - dense_1_loss_7: 0.0553 - dense_1_loss_8: 0.0436 - dense_1_loss_9: 0.0417 - dense_1_loss_10: 0.0335 - dense_1_loss_11: 0.0371 - dense_1_loss_12: 0.0341 - dense_1_loss_13: 0.0317 - dense_1_loss_14: 0.0321 - dense_1_loss_15: 0.0331 - dense_1_loss_16: 0.0360 - dense_1_loss_17: 0.0338 - dense_1_loss_18: 0.0329 - dense_1_loss_19: 0.0350 - dense_1_loss_20: 0.0353 - dense_1_loss_21: 0.0347 - dense_1_loss_22: 0.0330 - dense_1_loss_23: 0.0336 - dense_1_loss_24: 0.0331 - dense_1_loss_25: 0.0393 - dense_1_loss_26: 0.0341 - dense_1_loss_27: 0.0356 - dense_1_loss_28: 0.0406 - dense_1_loss_29: 0.0427 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 89/100 60/60 [==============================] - 0s - loss: 6.6773 - dense_1_loss_1: 3.7981 - dense_1_loss_2: 1.3530 - dense_1_loss_3: 0.4293 - dense_1_loss_4: 0.1254 - dense_1_loss_5: 0.0840 - dense_1_loss_6: 0.0658 - dense_1_loss_7: 0.0538 - dense_1_loss_8: 0.0427 - dense_1_loss_9: 0.0406 - dense_1_loss_10: 0.0327 - dense_1_loss_11: 0.0361 - dense_1_loss_12: 0.0332 - dense_1_loss_13: 0.0309 - dense_1_loss_14: 0.0313 - dense_1_loss_15: 0.0324 - dense_1_loss_16: 0.0353 - dense_1_loss_17: 0.0330 - dense_1_loss_18: 0.0321 - dense_1_loss_19: 0.0341 - dense_1_loss_20: 0.0345 - dense_1_loss_21: 0.0339 - dense_1_loss_22: 0.0322 - dense_1_loss_23: 0.0328 - dense_1_loss_24: 0.0324 - dense_1_loss_25: 0.0384 - dense_1_loss_26: 0.0333 - dense_1_loss_27: 0.0348 - dense_1_loss_28: 0.0397 - dense_1_loss_29: 0.0417 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 90/100 60/60 [==============================] - 0s - loss: 6.6297 - dense_1_loss_1: 3.7953 - dense_1_loss_2: 1.3410 - dense_1_loss_3: 0.4219 - dense_1_loss_4: 0.1224 - dense_1_loss_5: 0.0822 - dense_1_loss_6: 0.0643 - dense_1_loss_7: 0.0526 - dense_1_loss_8: 0.0418 - dense_1_loss_9: 0.0397 - dense_1_loss_10: 0.0319 - dense_1_loss_11: 0.0354 - dense_1_loss_12: 0.0324 - dense_1_loss_13: 0.0302 - dense_1_loss_14: 0.0305 - dense_1_loss_15: 0.0317 - dense_1_loss_16: 0.0344 - dense_1_loss_17: 0.0321 - dense_1_loss_18: 0.0313 - dense_1_loss_19: 0.0334 - dense_1_loss_20: 0.0336 - dense_1_loss_21: 0.0331 - dense_1_loss_22: 0.0315 - dense_1_loss_23: 0.0320 - dense_1_loss_24: 0.0315 - dense_1_loss_25: 0.0376 - dense_1_loss_26: 0.0325 - dense_1_loss_27: 0.0340 - dense_1_loss_28: 0.0387 - dense_1_loss_29: 0.0407 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 91/100 60/60 [==============================] - 0s - loss: 6.5828 - dense_1_loss_1: 3.7927 - dense_1_loss_2: 1.3288 - dense_1_loss_3: 0.4146 - dense_1_loss_4: 0.1198 - dense_1_loss_5: 0.0803 - dense_1_loss_6: 0.0626 - dense_1_loss_7: 0.0514 - dense_1_loss_8: 0.0409 - dense_1_loss_9: 0.0387 - dense_1_loss_10: 0.0311 - dense_1_loss_11: 0.0347 - dense_1_loss_12: 0.0316 - dense_1_loss_13: 0.0295 - dense_1_loss_14: 0.0299 - dense_1_loss_15: 0.0311 - dense_1_loss_16: 0.0334 - dense_1_loss_17: 0.0313 - dense_1_loss_18: 0.0306 - dense_1_loss_19: 0.0326 - dense_1_loss_20: 0.0328 - dense_1_loss_21: 0.0323 - dense_1_loss_22: 0.0307 - dense_1_loss_23: 0.0313 - dense_1_loss_24: 0.0308 - dense_1_loss_25: 0.0368 - dense_1_loss_26: 0.0317 - dense_1_loss_27: 0.0332 - dense_1_loss_28: 0.0377 - dense_1_loss_29: 0.0397 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 92/100 60/60 [==============================] - 0s - loss: 6.5360 - dense_1_loss_1: 3.7898 - dense_1_loss_2: 1.3168 - dense_1_loss_3: 0.4072 - dense_1_loss_4: 0.1168 - dense_1_loss_5: 0.0785 - dense_1_loss_6: 0.0611 - dense_1_loss_7: 0.0502 - dense_1_loss_8: 0.0400 - dense_1_loss_9: 0.0379 - dense_1_loss_10: 0.0304 - dense_1_loss_11: 0.0339 - dense_1_loss_12: 0.0309 - dense_1_loss_13: 0.0289 - dense_1_loss_14: 0.0292 - dense_1_loss_15: 0.0304 - dense_1_loss_16: 0.0326 - dense_1_loss_17: 0.0306 - dense_1_loss_18: 0.0298 - dense_1_loss_19: 0.0319 - dense_1_loss_20: 0.0320 - dense_1_loss_21: 0.0315 - dense_1_loss_22: 0.0301 - dense_1_loss_23: 0.0306 - dense_1_loss_24: 0.0301 - dense_1_loss_25: 0.0358 - dense_1_loss_26: 0.0310 - dense_1_loss_27: 0.0324 - dense_1_loss_28: 0.0369 - dense_1_loss_29: 0.0387 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 93/100 60/60 [==============================] - 0s - loss: 6.4926 - dense_1_loss_1: 3.7870 - dense_1_loss_2: 1.3053 - dense_1_loss_3: 0.4006 - dense_1_loss_4: 0.1143 - dense_1_loss_5: 0.0767 - dense_1_loss_6: 0.0598 - dense_1_loss_7: 0.0491 - dense_1_loss_8: 0.0391 - dense_1_loss_9: 0.0371 - dense_1_loss_10: 0.0297 - dense_1_loss_11: 0.0331 - dense_1_loss_12: 0.0302 - dense_1_loss_13: 0.0282 - dense_1_loss_14: 0.0286 - dense_1_loss_15: 0.0296 - dense_1_loss_16: 0.0320 - dense_1_loss_17: 0.0299 - dense_1_loss_18: 0.0292 - dense_1_loss_19: 0.0311 - dense_1_loss_20: 0.0313 - dense_1_loss_21: 0.0307 - dense_1_loss_22: 0.0295 - dense_1_loss_23: 0.0299 - dense_1_loss_24: 0.0295 - dense_1_loss_25: 0.0349 - dense_1_loss_26: 0.0304 - dense_1_loss_27: 0.0317 - dense_1_loss_28: 0.0362 - dense_1_loss_29: 0.0379 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 94/100 60/60 [==============================] - 0s - loss: 6.4500 - dense_1_loss_1: 3.7845 - dense_1_loss_2: 1.2939 - dense_1_loss_3: 0.3931 - dense_1_loss_4: 0.1121 - dense_1_loss_5: 0.0750 - dense_1_loss_6: 0.0586 - dense_1_loss_7: 0.0481 - dense_1_loss_8: 0.0383 - dense_1_loss_9: 0.0363 - dense_1_loss_10: 0.0291 - dense_1_loss_11: 0.0323 - dense_1_loss_12: 0.0296 - dense_1_loss_13: 0.0275 - dense_1_loss_14: 0.0280 - dense_1_loss_15: 0.0289 - dense_1_loss_16: 0.0315 - dense_1_loss_17: 0.0293 - dense_1_loss_18: 0.0285 - dense_1_loss_19: 0.0305 - dense_1_loss_20: 0.0306 - dense_1_loss_21: 0.0300 - dense_1_loss_22: 0.0288 - dense_1_loss_23: 0.0292 - dense_1_loss_24: 0.0289 - dense_1_loss_25: 0.0341 - dense_1_loss_26: 0.0297 - dense_1_loss_27: 0.0311 - dense_1_loss_28: 0.0354 - dense_1_loss_29: 0.0371 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 95/100 60/60 [==============================] - 0s - loss: 6.4081 - dense_1_loss_1: 3.7817 - dense_1_loss_2: 1.2828 - dense_1_loss_3: 0.3861 - dense_1_loss_4: 0.1097 - dense_1_loss_5: 0.0733 - dense_1_loss_6: 0.0572 - dense_1_loss_7: 0.0471 - dense_1_loss_8: 0.0375 - dense_1_loss_9: 0.0355 - dense_1_loss_10: 0.0285 - dense_1_loss_11: 0.0317 - dense_1_loss_12: 0.0289 - dense_1_loss_13: 0.0269 - dense_1_loss_14: 0.0273 - dense_1_loss_15: 0.0283 - dense_1_loss_16: 0.0308 - dense_1_loss_17: 0.0286 - dense_1_loss_18: 0.0279 - dense_1_loss_19: 0.0297 - dense_1_loss_20: 0.0300 - dense_1_loss_21: 0.0294 - dense_1_loss_22: 0.0282 - dense_1_loss_23: 0.0286 - dense_1_loss_24: 0.0283 - dense_1_loss_25: 0.0334 - dense_1_loss_26: 0.0291 - dense_1_loss_27: 0.0304 - dense_1_loss_28: 0.0348 - dense_1_loss_29: 0.0364 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9167 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 96/100 60/60 [==============================] - 0s - loss: 6.3681 - dense_1_loss_1: 3.7790 - dense_1_loss_2: 1.2717 - dense_1_loss_3: 0.3798 - dense_1_loss_4: 0.1075 - dense_1_loss_5: 0.0719 - dense_1_loss_6: 0.0562 - dense_1_loss_7: 0.0461 - dense_1_loss_8: 0.0367 - dense_1_loss_9: 0.0348 - dense_1_loss_10: 0.0279 - dense_1_loss_11: 0.0311 - dense_1_loss_12: 0.0283 - dense_1_loss_13: 0.0264 - dense_1_loss_14: 0.0268 - dense_1_loss_15: 0.0278 - dense_1_loss_16: 0.0301 - dense_1_loss_17: 0.0280 - dense_1_loss_18: 0.0273 - dense_1_loss_19: 0.0291 - dense_1_loss_20: 0.0293 - dense_1_loss_21: 0.0287 - dense_1_loss_22: 0.0275 - dense_1_loss_23: 0.0279 - dense_1_loss_24: 0.0277 - dense_1_loss_25: 0.0327 - dense_1_loss_26: 0.0284 - dense_1_loss_27: 0.0298 - dense_1_loss_28: 0.0339 - dense_1_loss_29: 0.0356 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9167 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 97/100 60/60 [==============================] - 0s - loss: 6.3282 - dense_1_loss_1: 3.7764 - dense_1_loss_2: 1.2606 - dense_1_loss_3: 0.3734 - dense_1_loss_4: 0.1054 - dense_1_loss_5: 0.0702 - dense_1_loss_6: 0.0549 - dense_1_loss_7: 0.0451 - dense_1_loss_8: 0.0359 - dense_1_loss_9: 0.0341 - dense_1_loss_10: 0.0273 - dense_1_loss_11: 0.0305 - dense_1_loss_12: 0.0276 - dense_1_loss_13: 0.0258 - dense_1_loss_14: 0.0263 - dense_1_loss_15: 0.0273 - dense_1_loss_16: 0.0293 - dense_1_loss_17: 0.0274 - dense_1_loss_18: 0.0268 - dense_1_loss_19: 0.0285 - dense_1_loss_20: 0.0287 - dense_1_loss_21: 0.0282 - dense_1_loss_22: 0.0270 - dense_1_loss_23: 0.0274 - dense_1_loss_24: 0.0271 - dense_1_loss_25: 0.0320 - dense_1_loss_26: 0.0278 - dense_1_loss_27: 0.0292 - dense_1_loss_28: 0.0332 - dense_1_loss_29: 0.0348 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9167 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 98/100 60/60 [==============================] - 0s - loss: 6.2921 - dense_1_loss_1: 3.7737 - dense_1_loss_2: 1.2509 - dense_1_loss_3: 0.3680 - dense_1_loss_4: 0.1033 - dense_1_loss_5: 0.0688 - dense_1_loss_6: 0.0540 - dense_1_loss_7: 0.0442 - dense_1_loss_8: 0.0352 - dense_1_loss_9: 0.0335 - dense_1_loss_10: 0.0267 - dense_1_loss_11: 0.0299 - dense_1_loss_12: 0.0271 - dense_1_loss_13: 0.0253 - dense_1_loss_14: 0.0257 - dense_1_loss_15: 0.0267 - dense_1_loss_16: 0.0288 - dense_1_loss_17: 0.0268 - dense_1_loss_18: 0.0263 - dense_1_loss_19: 0.0279 - dense_1_loss_20: 0.0281 - dense_1_loss_21: 0.0276 - dense_1_loss_22: 0.0264 - dense_1_loss_23: 0.0268 - dense_1_loss_24: 0.0265 - dense_1_loss_25: 0.0313 - dense_1_loss_26: 0.0272 - dense_1_loss_27: 0.0286 - dense_1_loss_28: 0.0326 - dense_1_loss_29: 0.0341 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9167 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 99/100 60/60 [==============================] - 0s - loss: 6.2552 - dense_1_loss_1: 3.7710 - dense_1_loss_2: 1.2403 - dense_1_loss_3: 0.3620 - dense_1_loss_4: 0.1015 - dense_1_loss_5: 0.0674 - dense_1_loss_6: 0.0530 - dense_1_loss_7: 0.0433 - dense_1_loss_8: 0.0346 - dense_1_loss_9: 0.0329 - dense_1_loss_10: 0.0262 - dense_1_loss_11: 0.0292 - dense_1_loss_12: 0.0266 - dense_1_loss_13: 0.0247 - dense_1_loss_14: 0.0251 - dense_1_loss_15: 0.0261 - dense_1_loss_16: 0.0285 - dense_1_loss_17: 0.0263 - dense_1_loss_18: 0.0257 - dense_1_loss_19: 0.0273 - dense_1_loss_20: 0.0275 - dense_1_loss_21: 0.0270 - dense_1_loss_22: 0.0258 - dense_1_loss_23: 0.0263 - dense_1_loss_24: 0.0260 - dense_1_loss_25: 0.0307 - dense_1_loss_26: 0.0267 - dense_1_loss_27: 0.0280 - dense_1_loss_28: 0.0320 - dense_1_loss_29: 0.0334 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9167 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 100/100 60/60 [==============================] - 0s - loss: 6.2195 - dense_1_loss_1: 3.7686 - dense_1_loss_2: 1.2304 - dense_1_loss_3: 0.3562 - dense_1_loss_4: 0.0996 - dense_1_loss_5: 0.0661 - dense_1_loss_6: 0.0518 - dense_1_loss_7: 0.0425 - dense_1_loss_8: 0.0339 - dense_1_loss_9: 0.0322 - dense_1_loss_10: 0.0257 - dense_1_loss_11: 0.0285 - dense_1_loss_12: 0.0261 - dense_1_loss_13: 0.0242 - dense_1_loss_14: 0.0246 - dense_1_loss_15: 0.0256 - dense_1_loss_16: 0.0281 - dense_1_loss_17: 0.0257 - dense_1_loss_18: 0.0252 - dense_1_loss_19: 0.0267 - dense_1_loss_20: 0.0269 - dense_1_loss_21: 0.0265 - dense_1_loss_22: 0.0253 - dense_1_loss_23: 0.0257 - dense_1_loss_24: 0.0255 - dense_1_loss_25: 0.0301 - dense_1_loss_26: 0.0261 - dense_1_loss_27: 0.0275 - dense_1_loss_28: 0.0314 - dense_1_loss_29: 0.0328 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6500 - dense_1_acc_3: 0.9167 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 &lt;keras.callbacks.History at 0x7fcff481d908&gt; You should see the model loss going down. Now that you have trained a model, lets go on the the final section to implement an inference algorithm, and generate some music! 3 - Generating musicYou now have a trained model which has learned the patterns of the jazz soloist. Lets now use this model to synthesize new music. 3.1 - Predicting &amp; Sampling At each step of sampling, you will take as input the activation a and cell state c from the previous state of the LSTM, forward propagate by one step, and get a new output activation as well as cell state. The new activation a can then be used to generate the output, using densor as before. To start off the model, we will initialize x0 as well as the LSTM activation and and cell value a0 and c0 to be zeros. Exercise: Implement the function below to sample a sequence of musical values. Here are some of the key steps you’ll need to implement inside the for-loop that generates the $T_y$ output characters: Step 2.A: Use LSTM_Cell, which inputs the previous step’s c and a to generate the current step’s c and a. Step 2.B: Use densor (defined previously) to compute a softmax on a to get the output for the current step. Step 2.C: Save the output you have just generated by appending it to outputs. Step 2.D: Sample x to the be “out”‘s one-hot version (the prediction) so that you can pass it to the next LSTM’s step. We have already provided this line of code, which uses a Lambda function.1x = Lambda(one_hot)(out) [Minor technical note: Rather than sampling a value at random according to the probabilities in out, this line of code actually chooses the single most likely note at each step using an argmax.] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# GRADED FUNCTION: music_inference_modeldef music_inference_model(LSTM_cell, densor, n_values = 78, n_a = 64, Ty = 100): """ Uses the trained "LSTM_cell" and "densor" from model() to generate a sequence of values. Arguments: LSTM_cell -- the trained "LSTM_cell" from model(), Keras layer object densor -- the trained "densor" from model(), Keras layer object n_values -- integer, umber of unique values n_a -- number of units in the LSTM_cell Ty -- integer, number of time steps to generate Returns: inference_model -- Keras model instance """ # Define the input of your model with a shape x0 = Input(shape=(1, n_values)) # Define s0, initial hidden state for the decoder LSTM a0 = Input(shape=(n_a,), name='a0') c0 = Input(shape=(n_a,), name='c0') a = a0 c = c0 x = x0 ### START CODE HERE ### # Step 1: Create an empty list of "outputs" to later store your predicted values (≈1 line) outputs = [] # Step 2: Loop over Ty and generate a value at every time step for t in range(Ty): # Step 2.A: Perform one step of LSTM_cell (≈1 line) a, _, c = LSTM_cell(x, initial_state=[a, c]); # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line) out = densor(a); # Step 2.C: Append the prediction "out" to "outputs". out.shape = (None, 78) (≈1 line) outputs.append(out); # Step 2.D: Select the next value according to "out", and set "x" to be the one-hot representation of the # selected value, which will be passed as the input to LSTM_cell on the next step. We have provided # the line of code you need to do this. x = Lambda(one_hot)(out); # Step 3: Create model instance with the correct "inputs" and "outputs" (≈1 line) inference_model = Model(inputs=[x0, a0, c0], outputs=outputs); ### END CODE HERE ### return inference_model Run the cell below to define your inference model. This model is hard coded to generate 50 values. 1inference_model = music_inference_model(LSTM_cell, densor, n_values = 78, n_a = 64, Ty = 50) Finally, this creates the zero-valued vectors you will use to initialize x and the LSTM state variables a and c. 123x_initializer = np.zeros((1, 1, 78))a_initializer = np.zeros((1, n_a))c_initializer = np.zeros((1, n_a)) Exercise: Implement predict_and_sample(). This function takes many arguments including the inputs [x_initializer, a_initializer, c_initializer]. In order to predict the output corresponding to this input, you will need to carry-out 3 steps: Use your inference model to predict an output given your set of inputs. The output pred should be a list of length $T_y$ where each element is a numpy-array of shape (1, n_values). Convert pred into a numpy array of $T_y$ indices. Each index corresponds is computed by taking the argmax of an element of the pred list. Hint. Convert the indices into their one-hot vector representations. Hint. 12345678910111213141516171819202122232425262728# GRADED FUNCTION: predict_and_sampledef predict_and_sample(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, c_initializer = c_initializer): """ Predicts the next value of values using the inference model. Arguments: inference_model -- Keras model instance for inference time x_initializer -- numpy array of shape (1, 1, 78), one-hot vector initializing the values generation a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel Returns: results -- numpy-array of shape (Ty, 78), matrix of one-hot vectors representing the values generated indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated """ ### START CODE HERE ### # Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer. pred = inference_model.predict([x_initializer, a_initializer, c_initializer]); # Step 2: Convert "pred" into an np.array() of indices with the maximum probabilities indices = np.argmax(np.array(pred), axis = -1); # Step 3: Convert indices to one-hot vectors, the shape of the results should be (1, ) results = to_categorical(indices, num_classes = x_initializer.shape[-1]); ### END CODE HERE ### return results, indices 1234results, indices = predict_and_sample(inference_model, x_initializer, a_initializer, c_initializer)print("np.argmax(results[12]) =", np.argmax(results[12]))print("np.argmax(results[17]) =", np.argmax(results[17]))print("list(indices[12:18]) =", list(indices[12:18])) np.argmax(results[12]) = 21 np.argmax(results[17]) = 7 list(indices[12:18]) = [array([21]), array([10]), array([57]), array([43]), array([12]), array([7])] Expected Output: Your results may differ because Keras’ results are not completely predictable. However, if you have trained your LSTM_cell with model.fit() for exactly 100 epochs as described above, you should very likely observe a sequence of indices that are not all identical. Moreover, you should observe that: np.argmax(results[12]) is the first element of list(indices[12:18]) and np.argmax(results[17]) is the last element of list(indices[12:18]). np.argmax(results[12]) = 1 np.argmax(results[12]) = 42 list(indices[12:18]) = [array([1]), array([42]), array([54]), array([17]), array([1]), array([42])] 3.3 - Generate musicFinally, you are ready to generate music. Your RNN generates a sequence of values. The following code generates music by first calling your predict_and_sample() function. These values are then post-processed into musical chords (meaning that multiple values or notes can be played at the same time). Most computational music algorithms use some post-processing because it is difficult to generate music that sounds good without such post-processing. The post-processing does things such as clean up the generated audio by making sure the same sound is not repeated too many times, that two successive notes are not too far from each other in pitch, and so on. One could argue that a lot of these post-processing steps are hacks; also, a lot the music generation literature has also focused on hand-crafting post-processors, and a lot of the output quality depends on the quality of the post-processing and not just the quality of the RNN. But this post-processing does make a huge difference, so lets use it in our implementation as well. Lets make some music! Run the following cell to generate music and record it into your out_stream. This can take a couple of minutes. 1out_stream = generate_music(inference_model) Predicting new values for different set of chords. Generated 51 sounds using the predicted values for the set of chords (&quot;1&quot;) and after pruning Generated 50 sounds using the predicted values for the set of chords (&quot;2&quot;) and after pruning Generated 50 sounds using the predicted values for the set of chords (&quot;3&quot;) and after pruning Generated 51 sounds using the predicted values for the set of chords (&quot;4&quot;) and after pruning Generated 51 sounds using the predicted values for the set of chords (&quot;5&quot;) and after pruning Your generated music is saved in output/my_music.midi To listen to your music, click File-&gt;Open… Then go to “output/“ and download “my_music.midi”. Either play it on your computer with an application that can read midi files if you have one, or use one of the free online “MIDI to mp3” conversion tools to convert this to mp3. As reference, here also is a 30sec audio clip we generated using this algorithm. 1IPython.display.Audio('./data/30s_trained_model.mp3') &lt;audio controls=&quot;controls&quot; &gt; y Your browser does not support the audio element. Congratulations!You have come to the end of the notebook. Here’s what you should remember: A sequence model can be used to generate musical values, which are then post-processed into midi music. Fairly similar models can be used to generate dinosaur names or to generate music, with the major difference being the input fed to the model. In Keras, sequence generation involves defining layers with shared weights, which are then repeated for the different time steps $1, \ldots, T_x$. Congratulations on completing this assignment and generating a jazz solo! References The ideas presented in this notebook came primarily from three computational music papers cited below. The implementation here also took significant inspiration and used many components from Ji-Sung Kim’s github repository. Ji-Sung Kim, 2016, deepjazz Jon Gillick, Kevin Tang and Robert Keller, 2009. Learning Jazz Grammars Robert Keller and David Morrison, 2007, A Grammatical Approach to Automatic Improvisation François Pachet, 1999, Surprising Harmonies We’re also grateful to François Germain for valuable feedback.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Building a Recurrent Neural Network Step by Step]]></title>
    <url>%2F2018%2F06%2F02%2FBuilding%2Ba%2BRecurrent%2BNeural%2BNetwork%2B-%2BStep%2Bby%2BStep%2B-%2Bv3%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 1st week and the copyright belongs to deeplearning.ai. Building your Recurrent Neural Network - Step by StepWelcome to Course 5’s first assignment! In this assignment, you will implement your first Recurrent Neural Network in numpy. Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have “memory”. They can read inputs $x^{\langle t \rangle}$ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a uni-directional RNN to take information from the past to process later inputs. A bidirection RNN can take context from both the past and the future. Notation: Superscript $[l]$ denotes an object associated with the $l^{th}$ layer. Example: $a^{[4]}$ is the $4^{th}$ layer activation. $W^{[5]}$ and $b^{[5]}$ are the $5^{th}$ layer parameters. Superscript $(i)$ denotes an object associated with the $i^{th}$ example. Example: $x^{(i)}$ is the $i^{th}$ training example input. Superscript $\langle t \rangle$ denotes an object at the $t^{th}$ time-step. Example: $x^{\langle t \rangle}$ is the input x at the $t^{th}$ time-step. $x^{(i)\langle t \rangle}$ is the input at the $t^{th}$ timestep of example $i$. Lowerscript $i$ denotes the $i^{th}$ entry of a vector. Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the activations in layer $l$. We assume that you are already familiar with numpy and/or have completed the previous courses of the specialization. Let’s get started! Let’s first import all the packages that you will need during this assignment. 12import numpy as npfrom rnn_utils import * 1 - Forward propagation for the basic Recurrent Neural NetworkLater this week, you will generate music using an RNN. The basic RNN that you will implement has the structure below. In this example, $T_x = T_y$. Figure 1: Basic RNN model Here’s how you can implement an RNN: Steps: Implement the calculations needed for one time-step of the RNN. Implement a loop over $T_x$ time-steps in order to process all the inputs, one at a time. Let’s go! 1.1 - RNN cellA Recurrent neural network can be seen as the repetition of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell. Figure 2: Basic RNN cell. Takes as input $x^{\langle t \rangle}$ (current input) and $a^{\langle t - 1\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\langle t \rangle}$ which is given to the next RNN cell and also used to predict $y^{\langle t \rangle}$ Exercise: Implement the RNN-cell described in Figure (2). Instructions: Compute the hidden state with tanh activation: $a^{\langle t \rangle} = \tanh(W_{aa} a^{\langle t-1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)$. Using your new hidden state $a^{\langle t \rangle}$, compute the prediction $\hat{y}^{\langle t \rangle} = softmax(W_{ya} a^{\langle t \rangle} + b_y)$. We provided you a function: softmax. Store $(a^{\langle t \rangle}, a^{\langle t-1 \rangle}, x^{\langle t \rangle}, parameters)$ in cache Return $a^{\langle t \rangle}$ , $y^{\langle t \rangle}$ and cache We will vectorize over $m$ examples. Thus, $x^{\langle t \rangle}$ will have dimension $(n_x,m)$, and $a^{\langle t \rangle}$ will have dimension $(n_a,m)$. 123456789101112131415161718192021222324252627282930313233343536373839# GRADED FUNCTION: rnn_cell_forwarddef rnn_cell_forward(xt, a_prev, parameters): """ Implements a single forward step of the RNN-cell as described in Figure (2) Arguments: xt -- your input data at timestep "t", numpy array of shape (n_x, m). a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m) parameters -- python dictionary containing: Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x) Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a) Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a) ba -- Bias, numpy array of shape (n_a, 1) by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1) Returns: a_next -- next hidden state, of shape (n_a, m) yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m) cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters) """ # Retrieve parameters from "parameters" Wax = parameters["Wax"] Waa = parameters["Waa"] Wya = parameters["Wya"] ba = parameters["ba"] by = parameters["by"] ### START CODE HERE ### (≈2 lines) # compute next activation state using the formula given above a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba); # compute output of the current cell using the formula given above yt_pred = softmax(np.dot(Wya, a_next) + by); ### END CODE HERE ### # store values you need for backward propagation in cache cache = (a_next, a_prev, xt, parameters) return a_next, yt_pred, cache 123456789101112131415np.random.seed(1)xt = np.random.randn(3,10)a_prev = np.random.randn(5,10)Waa = np.random.randn(5,5)Wax = np.random.randn(5,3)Wya = np.random.randn(2,5)ba = np.random.randn(5,1)by = np.random.randn(2,1)parameters = &#123;"Waa": Waa, "Wax": Wax, "Wya": Wya, "ba": ba, "by": by&#125;a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)print("a_next[4] = ", a_next[4])print("a_next.shape = ", a_next.shape)print("yt_pred[1] =", yt_pred[1])print("yt_pred.shape = ", yt_pred.shape) a_next[4] = [ 0.59584544 0.18141802 0.61311866 0.99808218 0.85016201 0.99980978 -0.18887155 0.99815551 0.6531151 0.82872037] a_next.shape = (5, 10) yt_pred[1] = [ 0.9888161 0.01682021 0.21140899 0.36817467 0.98988387 0.88945212 0.36920224 0.9966312 0.9982559 0.17746526] yt_pred.shape = (2, 10) Expected Output: a_next[4]: [ 0.59584544 0.18141802 0.61311866 0.99808218 0.85016201 0.99980978 -0.18887155 0.99815551 0.6531151 0.82872037] a_next.shape: (5, 10) yt[1]: [ 0.9888161 0.01682021 0.21140899 0.36817467 0.98988387 0.88945212 0.36920224 0.9966312 0.9982559 0.17746526] yt.shape: (2, 10) 1.2 - RNN forward passYou can see an RNN as the repetition of the cell you’ve just built. If your input sequence of data is carried over 10 time steps, then you will copy the RNN cell 10 times. Each cell takes as input the hidden state from the previous cell ($a^{\langle t-1 \rangle}$) and the current time-step’s input data ($x^{\langle t \rangle}$). It outputs a hidden state ($a^{\langle t \rangle}$) and a prediction ($y^{\langle t \rangle}$) for this time-step. Figure 3: Basic RNN. The input sequence $x = (x^{\langle 1 \rangle}, x^{\langle 2 \rangle}, …, x^{\langle T_x \rangle})$ is carried over $T_x$ time steps. The network outputs $y = (y^{\langle 1 \rangle}, y^{\langle 2 \rangle}, …, y^{\langle T_x \rangle})$. Exercise: Code the forward propagation of the RNN described in Figure (3). Instructions: Create a vector of zeros ($a$) that will store all the hidden states computed by the RNN. Initialize the “next” hidden state as $a_0$ (initial hidden state). Start looping over each time step, your incremental index is $t$ : Update the “next” hidden state and the cache by running rnn_cell_forward Store the “next” hidden state in $a$ ($t^{th}$ position) Store the prediction in y Add the cache to the list of caches Return $a$, $y$ and caches 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# GRADED FUNCTION: rnn_forwarddef rnn_forward(x, a0, parameters): """ Implement the forward propagation of the recurrent neural network described in Figure (3). Arguments: x -- Input data for every time-step, of shape (n_x, m, T_x). a0 -- Initial hidden state, of shape (n_a, m) parameters -- python dictionary containing: Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a) Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x) Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a) ba -- Bias numpy array of shape (n_a, 1) by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1) Returns: a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x) y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x) caches -- tuple of values needed for the backward pass, contains (list of caches, x) """ # Initialize "caches" which will contain the list of all caches caches = [] # Retrieve dimensions from shapes of x and Wy n_x, m, T_x = x.shape n_y, n_a = parameters["Wya"].shape ### START CODE HERE ### # initialize "a" and "y" with zeros (≈2 lines) a = np.zeros((n_a, m, T_x)); y_pred = np.zeros((n_y, m, T_x)); # Initialize a_next (≈1 line) a_next = a0; # loop over all time-steps for t in range(T_x): # Update next hidden state, compute the prediction, get the cache (≈1 line) a_next, yt_pred, cache = rnn_cell_forward(x[:, :, t], a_next, parameters); # Save the value of the new "next" hidden state in a (≈1 line) a[:, :, t] = a_next; # Save the value of the prediction in y (≈1 line) y_pred[:, :, t] = yt_pred; # Append "cache" to "caches" (≈1 line) caches.append(cache); ### END CODE HERE ### # store values needed for backward propagation in cache caches = (caches, x) return a, y_pred, caches 1234567891011121314151617np.random.seed(1)x = np.random.randn(3,10,4)a0 = np.random.randn(5,10)Waa = np.random.randn(5,5)Wax = np.random.randn(5,3)Wya = np.random.randn(2,5)ba = np.random.randn(5,1)by = np.random.randn(2,1)parameters = &#123;"Waa": Waa, "Wax": Wax, "Wya": Wya, "ba": ba, "by": by&#125;a, y_pred, caches = rnn_forward(x, a0, parameters)print("a[4][1] = ", a[4][1])print("a.shape = ", a.shape)print("y_pred[1][3] =", y_pred[1][3])print("y_pred.shape = ", y_pred.shape)print("caches[1][1][3] =", caches[1][1][3])print("len(caches) = ", len(caches)) a[4][1] = [-0.99999375 0.77911235 -0.99861469 -0.99833267] a.shape = (5, 10, 4) y_pred[1][3] = [ 0.79560373 0.86224861 0.11118257 0.81515947] y_pred.shape = (2, 10, 4) caches[1][1][3] = [-1.1425182 -0.34934272 -0.20889423 0.58662319] len(caches) = 2 Expected Output: a[4][1]: [-0.99999375 0.77911235 -0.99861469 -0.99833267] a.shape: (5, 10, 4) y[1][3]: [ 0.79560373 0.86224861 0.11118257 0.81515947] y.shape: (2, 10, 4) cache[1][1][3]: [-1.1425182 -0.34934272 -0.20889423 0.58662319] len(cache): 2 Congratulations! You’ve successfully built the forward propagation of a recurrent neural network from scratch. This will work well enough for some applications, but it suffers from vanishing gradient problems. So it works best when each output $y^{\langle t \rangle}$ can be estimated using mainly “local” context (meaning information from inputs $x^{\langle t’ \rangle}$ where $t’$ is not too far from $t$). In the next part, you will build a more complex LSTM model, which is better at addressing vanishing gradients. The LSTM will be better able to remember a piece of information and keep it saved for many timesteps. 2 - Long Short-Term Memory (LSTM) networkThis following figure shows the operations of an LSTM-cell. Figure 4: LSTM-cell. This tracks and updates a “cell state” or memory variable $c^{\langle t \rangle}$ at every time-step, which can be different from $a^{\langle t \rangle}$. Similar to the RNN example above, you will start by implementing the LSTM cell for a single time-step. Then you can iteratively call it from inside a for-loop to have it process an input with $T_x$ time-steps. About the gates- Forget gateFor the sake of this illustration, lets assume we are reading words in a piece of text, and want use an LSTM to keep track of grammatical structures, such as whether the subject is singular or plural. If the subject changes from a singular word to a plural word, we need to find a way to get rid of our previously stored memory value of the singular/plural state. In an LSTM, the forget gate lets us do this: $$\Gamma_f^{\langle t \rangle} = \sigma(W_f[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_f)\tag{1} $$ Here, $W_f$ are weights that govern the forget gate’s behavior. We concatenate $[a^{\langle t-1 \rangle}, x^{\langle t \rangle}]$ and multiply by $W_f$. The equation above results in a vector $\Gamma_f^{\langle t \rangle}$ with values between 0 and 1. This forget gate vector will be multiplied element-wise by the previous cell state $c^{\langle t-1 \rangle}$. So if one of the values of $\Gamma_f^{\langle t \rangle}$ is 0 (or close to 0) then it means that the LSTM should remove that piece of information (e.g. the singular subject) in the corresponding component of $c^{\langle t-1 \rangle}$. If one of the values is 1, then it will keep the information. - Update gateOnce we forget that the subject being discussed is singular, we need to find a way to update it to reflect that the new subject is now plural. Here is the formulat for the update gate: $$\Gamma_u^{\langle t \rangle} = \sigma(W_u[a^{\langle t-1 \rangle}, x^{\{t\}}] + b_u)\tag{2} $$ Similar to the forget gate, here $\Gamma_u^{\langle t \rangle}$ is again a vector of values between 0 and 1. This will be multiplied element-wise with $\tilde{c}^{\langle t \rangle}$, in order to compute $c^{\langle t \rangle}$. - Updating the cellTo update the new subject we need to create a new vector of numbers that we can add to our previous cell state. The equation we use is: $$ \tilde{c}^{\langle t \rangle} = \tanh(W_c[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_c)\tag{3} $$ Finally, the new cell state is: $$ c^{\langle t \rangle} = \Gamma_f^{\langle t \rangle}* c^{\langle t-1 \rangle} + \Gamma_u^{\langle t \rangle} *\tilde{c}^{\langle t \rangle} \tag{4} $$ - Output gateTo decide which outputs we will use, we will use the following two formulas: $$ \Gamma_o^{\langle t \rangle}= \sigma(W_o[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_o)\tag{5}$$$$ a^{\langle t \rangle} = \Gamma_o^{\langle t \rangle}* \tanh(c^{\langle t \rangle})\tag{6} $$ Where in equation 5 you decide what to output using a sigmoid function and in equation 6 you multiply that by the $\tanh$ of the previous state. 2.1 - LSTM cellExercise: Implement the LSTM cell described in the Figure (3). Instructions: Concatenate $a^{\langle t-1 \rangle}$ and $x^{\langle t \rangle}$ in a single matrix: $concat = \begin{bmatrix} a^{\langle t-1 \rangle} \\ x^{\langle t \rangle} \end{bmatrix}$ Compute all the formulas 1-6. You can use sigmoid() (provided) and np.tanh(). Compute the prediction $y^{\langle t \rangle}$. You can use softmax() (provided). 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# GRADED FUNCTION: lstm_cell_forwarddef lstm_cell_forward(xt, a_prev, c_prev, parameters): """ Implement a single forward step of the LSTM-cell as described in Figure (4) Arguments: xt -- your input data at timestep "t", numpy array of shape (n_x, m). a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m) c_prev -- Memory state at timestep "t-1", numpy array of shape (n_a, m) parameters -- python dictionary containing: Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x) bf -- Bias of the forget gate, numpy array of shape (n_a, 1) Wi -- Weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x) bi -- Bias of the save gate, numpy array of shape (n_a, 1) Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x) bc -- Bias of the first "tanh", numpy array of shape (n_a, 1) Wo -- Weight matrix of the focus gate, numpy array of shape (n_a, n_a + n_x) bo -- Bias of the focus gate, numpy array of shape (n_a, 1) Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a) by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1) Returns: a_next -- next hidden state, of shape (n_a, m) c_next -- next memory state, of shape (n_a, m) yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m) cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters) Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilda), c stands for the memory value """ # Retrieve parameters from "parameters" Wf = parameters["Wf"] bf = parameters["bf"] Wi = parameters["Wi"] bi = parameters["bi"] Wc = parameters["Wc"] bc = parameters["bc"] Wo = parameters["Wo"] bo = parameters["bo"] Wy = parameters["Wy"] by = parameters["by"] # Retrieve dimensions from shapes of xt and Wy n_x, m = xt.shape n_y, n_a = Wy.shape ### START CODE HERE ### # Concatenate a_prev and xt (≈3 lines) concat = np.zeros((n_a + n_x, m)); concat[: n_a, :] = a_prev; concat[n_a :, :] = xt; # Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines) ft = sigmoid(np.dot(Wf, concat) + bf); it = sigmoid(np.dot(Wi, concat) + bi); cct = np.tanh(np.dot(Wc, concat) + bc); c_next = ft * c_prev + it * cct; ot = sigmoid(np.dot(Wo, concat) + bo); a_next = ot * np.tanh(c_next); # Compute prediction of the LSTM cell (≈1 line) yt_pred = softmax(np.dot(Wy, a_next) + by); ### END CODE HERE ### # store values needed for backward propagation in cache cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) return a_next, c_next, yt_pred, cache 1234567891011121314151617181920212223242526np.random.seed(1)xt = np.random.randn(3,10)a_prev = np.random.randn(5,10)c_prev = np.random.randn(5,10)Wf = np.random.randn(5, 5+3)bf = np.random.randn(5,1)Wi = np.random.randn(5, 5+3)bi = np.random.randn(5,1)Wo = np.random.randn(5, 5+3)bo = np.random.randn(5,1)Wc = np.random.randn(5, 5+3)bc = np.random.randn(5,1)Wy = np.random.randn(2,5)by = np.random.randn(2,1)parameters = &#123;"Wf": Wf, "Wi": Wi, "Wo": Wo, "Wc": Wc, "Wy": Wy, "bf": bf, "bi": bi, "bo": bo, "bc": bc, "by": by&#125;a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)print("a_next[4] = ", a_next[4])print("a_next.shape = ", c_next.shape)print("c_next[2] = ", c_next[2])print("c_next.shape = ", c_next.shape)print("yt[1] =", yt[1])print("yt.shape = ", yt.shape)print("cache[1][3] =", cache[1][3])print("len(cache) = ", len(cache)) a_next[4] = [-0.66408471 0.0036921 0.02088357 0.22834167 -0.85575339 0.00138482 0.76566531 0.34631421 -0.00215674 0.43827275] a_next.shape = (5, 10) c_next[2] = [ 0.63267805 1.00570849 0.35504474 0.20690913 -1.64566718 0.11832942 0.76449811 -0.0981561 -0.74348425 -0.26810932] c_next.shape = (5, 10) yt[1] = [ 0.79913913 0.15986619 0.22412122 0.15606108 0.97057211 0.31146381 0.00943007 0.12666353 0.39380172 0.07828381] yt.shape = (2, 10) cache[1][3] = [-0.16263996 1.03729328 0.72938082 -0.54101719 0.02752074 -0.30821874 0.07651101 -1.03752894 1.41219977 -0.37647422] len(cache) = 10 Expected Output : a_next[4]: [-0.66408471 0.0036921 0.02088357 0.22834167 -0.85575339 0.00138482 0.76566531 0.34631421 -0.00215674 0.43827275] a_next.shape: (5, 10) c_next[2]: [ 0.63267805 1.00570849 0.35504474 0.20690913 -1.64566718 0.11832942 0.76449811 -0.0981561 -0.74348425 -0.26810932] c_next.shape: (5, 10) yt[1]: [ 0.79913913 0.15986619 0.22412122 0.15606108 0.97057211 0.31146381 0.00943007 0.12666353 0.39380172 0.07828381] yt.shape: (2, 10) cache[1][3]: [-0.16263996 1.03729328 0.72938082 -0.54101719 0.02752074 -0.30821874 0.07651101 -1.03752894 1.41219977 -0.37647422] len(cache): 10 2.2 - Forward pass for LSTMNow that you have implemented one step of an LSTM, you can now iterate this over this using a for-loop to process a sequence of $T_x$ inputs. Figure 4: LSTM over multiple time-steps. Exercise: Implement lstm_forward() to run an LSTM over $T_x$ time-steps. Note: $c^{\langle 0 \rangle}$ is initialized with zeros. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# GRADED FUNCTION: lstm_forwarddef lstm_forward(x, a0, parameters): """ Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3). Arguments: x -- Input data for every time-step, of shape (n_x, m, T_x). a0 -- Initial hidden state, of shape (n_a, m) parameters -- python dictionary containing: Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x) bf -- Bias of the forget gate, numpy array of shape (n_a, 1) Wi -- Weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x) bi -- Bias of the save gate, numpy array of shape (n_a, 1) Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x) bc -- Bias of the first "tanh", numpy array of shape (n_a, 1) Wo -- Weight matrix of the focus gate, numpy array of shape (n_a, n_a + n_x) bo -- Bias of the focus gate, numpy array of shape (n_a, 1) Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a) by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1) Returns: a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x) y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x) caches -- tuple of values needed for the backward pass, contains (list of all the caches, x) """ # Initialize "caches", which will track the list of all the caches caches = [] ### START CODE HERE ### # Retrieve dimensions from shapes of xt and Wy (≈2 lines) n_x, m, T_x = x.shape; n_y, n_a = parameters['Wy'].shape; # initialize "a", "c" and "y" with zeros (≈3 lines) a = np.zeros((n_a, m, T_x)); c = np.zeros((n_a, m, T_x)); y = np.zeros((n_y, m, T_x)); # Initialize a_next and c_next (≈2 lines) a_next = a0; c_next = np.zeros((n_a, m)); # loop over all time-steps for t in range(T_x): # Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line) a_next, c_next, yt_pred, cache = lstm_cell_forward(x[:, :, t], a_next, c_next, parameters); # Save the value of the new "next" hidden state in a (≈1 line) a[:, :, t] = a_next; # Save the value of the prediction in y (≈1 line) y[:, :, t] = yt_pred; # Save the value of the next cell state (≈1 line) c[:, :, t] = c_next; # Append the cache into caches (≈1 line) caches.append(cache); ### END CODE HERE ### # store values needed for backward propagation in cache caches = (caches, x) return a, y, c, caches 123456789101112131415161718192021222324np.random.seed(1)x = np.random.randn(3,10,7)a0 = np.random.randn(5,10)Wf = np.random.randn(5, 5+3)bf = np.random.randn(5,1)Wi = np.random.randn(5, 5+3)bi = np.random.randn(5,1)Wo = np.random.randn(5, 5+3)bo = np.random.randn(5,1)Wc = np.random.randn(5, 5+3)bc = np.random.randn(5,1)Wy = np.random.randn(2,5)by = np.random.randn(2,1)parameters = &#123;"Wf": Wf, "Wi": Wi, "Wo": Wo, "Wc": Wc, "Wy": Wy, "bf": bf, "bi": bi, "bo": bo, "bc": bc, "by": by&#125;a, y, c, caches = lstm_forward(x, a0, parameters)print("a[4][3][6] = ", a[4][3][6])print("a.shape = ", a.shape)print("y[1][4][3] =", y[1][4][3])print("y.shape = ", y.shape)print("caches[1][1[1]] =", caches[1][1][1])print("c[1][2][1]", c[1][2][1])print("len(caches) = ", len(caches)) a[4][3][6] = 0.172117767533 a.shape = (5, 10, 7) y[1][4][3] = 0.95087346185 y.shape = (2, 10, 7) caches[1][1[1]] = [ 0.82797464 0.23009474 0.76201118 -0.22232814 -0.20075807 0.18656139 0.41005165] c[1][2][1] -0.855544916718 len(caches) = 2 Expected Output: a[4][3][6] = 0.172117767533 a.shape = (5, 10, 7) y[1][4][3] = 0.95087346185 y.shape = (2, 10, 7) caches[1][1][1] = [ 0.82797464 0.23009474 0.76201118 -0.22232814 -0.20075807 0.18656139 0.41005165] c[1][2][1] = -0.855544916718 len(caches) = 2 Congratulations! You have now implemented the forward passes for the basic RNN and the LSTM. When using a deep learning framework, implementing the forward pass is sufficient to build systems that achieve great performance. The rest of this notebook is optional, and will not be graded. 3 - Backpropagation in recurrent neural networks (OPTIONAL / UNGRADED)In modern deep learning frameworks, you only have to implement the forward pass, and the framework takes care of the backward pass, so most deep learning engineers do not need to bother with the details of the backward pass. If however you are an expert in calculus and want to see the details of backprop in RNNs, you can work through this optional portion of the notebook. When in an earlier course you implemented a simple (fully connected) neural network, you used backpropagation to compute the derivatives with respect to the cost to update the parameters. Similarly, in recurrent neural networks you can to calculate the derivatives with respect to the cost in order to update the parameters. The backprop equations are quite complicated and we did not derive them in lecture. However, we will briefly present them below. 3.1 - Basic RNN backward passWe will start by computing the backward pass for the basic RNN-cell. Figure 5: RNN-cell’s backward pass. Just like in a fully-connected neural network, the derivative of the cost function $J$ backpropagates through the RNN by following the chain-rule from calculas. The chain-rule is also used to calculate $(\frac{\partial J}{\partial W_{ax}},\frac{\partial J}{\partial W_{aa}},\frac{\partial J}{\partial b})$ to update the parameters $(W_{ax}, W_{aa}, b_a)$. Deriving the one step backward functions:To compute the rnn_cell_backward you need to compute the following equations. It is a good exercise to derive them by hand. The derivative of $\tanh$ is $1-\tanh(x)^2$. You can find the complete proof here. Note that: $ \text{sech}(x)^2 = 1 - \tanh(x)^2$ Similarly for $\frac{ \partial a^{\langle t \rangle} } {\partial W_{ax}}, \frac{ \partial a^{\langle t \rangle} } {\partial W_{aa}}, \frac{ \partial a^{\langle t \rangle} } {\partial b}$, the derivative of $\tanh(u)$ is $(1-\tanh(u)^2)du$. The final two equations also follow same rule and are derived using the $\tanh$ derivative. Note that the arrangement is done in a way to get the same dimensions to match. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def rnn_cell_backward(da_next, cache): """ Implements the backward pass for the RNN-cell (single time-step). Arguments: da_next -- Gradient of loss with respect to next hidden state cache -- python dictionary containing useful values (output of rnn_step_forward()) Returns: gradients -- python dictionary containing: dx -- Gradients of input data, of shape (n_x, m) da_prev -- Gradients of previous hidden state, of shape (n_a, m) dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x) dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a) dba -- Gradients of bias vector, of shape (n_a, 1) """ # Retrieve values from cache (a_next, a_prev, xt, parameters) = cache # Retrieve values from parameters Wax = parameters["Wax"] Waa = parameters["Waa"] Wya = parameters["Wya"] ba = parameters["ba"] by = parameters["by"] ### START CODE HERE ### # compute the gradient of tanh with respect to a_next (≈1 line) dtanh = (1 - a_next * a_next) * da_next; # compute the gradient of the loss with respect to Wax (≈2 lines) dWax = np.dot(dtanh, xt.T); dxt = np.dot(Wax.T, dtanh); # compute the gradient with respect to Waa (≈2 lines) dWaa = np.dot(dtanh, a_prev.T); da_prev = np.dot(Waa.T, dtanh); # compute the gradient with respect to b (≈1 line) dba = np.sum(dtanh, keepdims = True, axis = -1); ### END CODE HERE ### # Store the gradients in a python dictionary gradients = &#123;"dxt": dxt, "da_prev": da_prev, "dWax": dWax, "dWaa": dWaa, "dba": dba&#125; return gradients 123456789101112131415161718192021222324np.random.seed(1)xt = np.random.randn(3,10)a_prev = np.random.randn(5,10)Wax = np.random.randn(5,3)Waa = np.random.randn(5,5)Wya = np.random.randn(2,5)b = np.random.randn(5,1)by = np.random.randn(2,1)parameters = &#123;"Wax": Wax, "Waa": Waa, "Wya": Wya, "ba": ba, "by": by&#125;a_next, yt, cache = rnn_cell_forward(xt, a_prev, parameters)da_next = np.random.randn(5,10)gradients = rnn_cell_backward(da_next, cache)print("gradients[\"dxt\"][1][2] =", gradients["dxt"][1][2])print("gradients[\"dxt\"].shape =", gradients["dxt"].shape)print("gradients[\"da_prev\"][2][3] =", gradients["da_prev"][2][3])print("gradients[\"da_prev\"].shape =", gradients["da_prev"].shape)print("gradients[\"dWax\"][3][1] =", gradients["dWax"][3][1])print("gradients[\"dWax\"].shape =", gradients["dWax"].shape)print("gradients[\"dWaa\"][1][2] =", gradients["dWaa"][1][2])print("gradients[\"dWaa\"].shape =", gradients["dWaa"].shape)print("gradients[\"dba\"][4] =", gradients["dba"][4])print("gradients[\"dba\"].shape =", gradients["dba"].shape) gradients[&quot;dxt&quot;][1][2] = -0.460564103059 gradients[&quot;dxt&quot;].shape = (3, 10) gradients[&quot;da_prev&quot;][2][3] = 0.0842968653807 gradients[&quot;da_prev&quot;].shape = (5, 10) gradients[&quot;dWax&quot;][3][1] = 0.393081873922 gradients[&quot;dWax&quot;].shape = (5, 3) gradients[&quot;dWaa&quot;][1][2] = -0.28483955787 gradients[&quot;dWaa&quot;].shape = (5, 5) gradients[&quot;dba&quot;][4] = [ 0.80517166] gradients[&quot;dba&quot;].shape = (5, 1) Expected Output: gradients[“dxt”][1][2] = -0.460564103059 gradients[“dxt”].shape = (3, 10) gradients[“da_prev”][2][3] = 0.0842968653807 gradients[“da_prev”].shape = (5, 10) gradients[“dWax”][3][1] = 0.393081873922 gradients[“dWax”].shape = (5, 3) gradients[“dWaa”][1][2] = -0.28483955787 gradients[“dWaa”].shape = (5, 5) gradients[“dba”][4] = [ 0.80517166] gradients[“dba”].shape = (5, 1) Backward pass through the RNNComputing the gradients of the cost with respect to $a^{\langle t \rangle}$ at every time-step $t$ is useful because it is what helps the gradient backpropagate to the previous RNN-cell. To do so, you need to iterate through all the time steps starting at the end, and at each step, you increment the overall $db_a$, $dW_{aa}$, $dW_{ax}$ and you store $dx$. Instructions: Implement the rnn_backward function. Initialize the return variables with zeros first and then loop through all the time steps while calling the rnn_cell_backward at each time timestep, update the other variables accordingly. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354def rnn_backward(da, caches): """ Implement the backward pass for a RNN over an entire sequence of input data. Arguments: da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x) caches -- tuple containing information from the forward pass (rnn_forward) Returns: gradients -- python dictionary containing: dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x) da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m) dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x) dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a) dba -- Gradient w.r.t the bias, of shape (n_a, 1) """ ### START CODE HERE ### # Retrieve values from the first cache (t=1) of caches (≈2 lines) caches, x = caches; a1, a0, x1, parameters = caches[0]; # Retrieve dimensions from da's and x1's shapes (≈2 lines) n_a, m, T_x = da.shape; n_x, m = x1.shape; # initialize the gradients with the right sizes (≈6 lines) dx = np.zeros((n_x, m, T_x)); dWax = np.zeros((parameters['Wax'].shape)); dWaa = np.zeros((parameters['Waa'].shape)); dba = np.zeros((parameters['ba'].shape)); da0 = np.zeros(a0.shape); da_prevt = np.zeros((n_a, m)); # Loop through all the time steps for t in reversed(range(T_x)): # Compute gradients at time step t. Choose wisely the "da_next" and the "cache" to use in the backward propagation step. (≈1 line) gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t]); # Retrieve derivatives from gradients (≈ 1 line) dxt, da_prevt, dWaxt, dWaat, dbat = gradients['dxt'], gradients['da_prev'], gradients['dWax'], gradients['dWaa'], gradients['dba']; # Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines) dWax += dWaxt; dWaa += dWaat; dba += dbat; dx[:, :, t] = dxt; # Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) da0 = da_prevt; ### END CODE HERE ### # Store the gradients in a python dictionary gradients = &#123;"dx": dx, "da0": da0, "dWax": dWax, "dWaa": dWaa,"dba": dba&#125; return gradients 1234567891011121314151617181920212223np.random.seed(1)x = np.random.randn(3,10,4)a0 = np.random.randn(5,10)Wax = np.random.randn(5,3)Waa = np.random.randn(5,5)Wya = np.random.randn(2,5)ba = np.random.randn(5,1)by = np.random.randn(2,1)parameters = &#123;"Wax": Wax, "Waa": Waa, "Wya": Wya, "ba": ba, "by": by&#125;a, y, caches = rnn_forward(x, a0, parameters)da = np.random.randn(5, 10, 4)gradients = rnn_backward(da, caches)print("gradients[\"dx\"][1][2] =", gradients["dx"][1][2])print("gradients[\"dx\"].shape =", gradients["dx"].shape)print("gradients[\"da0\"][2][3] =", gradients["da0"][2][3])print("gradients[\"da0\"].shape =", gradients["da0"].shape)print("gradients[\"dWax\"][3][1] =", gradients["dWax"][3][1])print("gradients[\"dWax\"].shape =", gradients["dWax"].shape)print("gradients[\"dWaa\"][1][2] =", gradients["dWaa"][1][2])print("gradients[\"dWaa\"].shape =", gradients["dWaa"].shape)print("gradients[\"dba\"][4] =", gradients["dba"][4])print("gradients[\"dba\"].shape =", gradients["dba"].shape) gradients[&quot;dx&quot;][1][2] = [-2.07101689 -0.59255627 0.02466855 0.01483317] gradients[&quot;dx&quot;].shape = (3, 10, 4) gradients[&quot;da0&quot;][2][3] = -0.314942375127 gradients[&quot;da0&quot;].shape = (5, 10) gradients[&quot;dWax&quot;][3][1] = 11.2641044965 gradients[&quot;dWax&quot;].shape = (5, 3) gradients[&quot;dWaa&quot;][1][2] = 2.30333312658 gradients[&quot;dWaa&quot;].shape = (5, 5) gradients[&quot;dba&quot;][4] = [-0.74747722] gradients[&quot;dba&quot;].shape = (5, 1) Expected Output: gradients[“dx”][1][2] = [-2.07101689 -0.59255627 0.02466855 0.01483317] gradients[“dx”].shape = (3, 10, 4) gradients[“da0”][2][3] = -0.314942375127 gradients[“da0”].shape = (5, 10) gradients[“dWax”][3][1] = 11.2641044965 gradients[“dWax”].shape = (5, 3) gradients[“dWaa”][1][2] = 2.30333312658 gradients[“dWaa”].shape = (5, 5) gradients[“dba”][4] = [-0.74747722] gradients[“dba”].shape = (5, 1) 3.2 - LSTM backward pass3.2.1 One Step backwardThe LSTM backward pass is slighltly more complicated than the forward one. We have provided you with all the equations for the LSTM backward pass below. (If you enjoy calculus exercises feel free to try deriving these from scratch yourself.) 3.2.2 gate derivatives $$d \Gamma_o^{\langle t \rangle} = da_{next}*\tanh(c_{next}) * \Gamma_o^{\langle t \rangle}*(1-\Gamma_o^{\langle t \rangle})\tag{7}$$ $$d\tilde c^{\langle t \rangle} = dc_{next}*\Gamma_u^{\langle t \rangle}+ \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * i_t * da_{next} * \tilde c^{\langle t \rangle} * (1-\tanh(\tilde c)^2) \tag{8}$$ $$d\Gamma_u^{\langle t \rangle} = dc_{next}*\tilde c^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * \tilde c^{\langle t \rangle} * da_{next}*\Gamma_u^{\langle t \rangle}*(1-\Gamma_u^{\langle t \rangle})\tag{9}$$ $$d\Gamma_f^{\langle t \rangle} = dc_{next}*\tilde c_{prev} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * c_{prev} * da_{next}*\Gamma_f^{\langle t \rangle}*(1-\Gamma_f^{\langle t \rangle})\tag{10}$$ 3.2.3 parameter derivatives $$ dW_f = d\Gamma_f^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{11} $$ $$ dW_u = d\Gamma_u^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{12} $$ $$ dW_c = d\tilde c^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{13} $$ $$ dW_o = d\Gamma_o^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{14}$$ To calculate $db_f, db_u, db_c, db_o$ you just need to sum across the horizontal (axis= 1) axis on $d\Gamma_f^{\langle t \rangle}, d\Gamma_u^{\langle t \rangle}, d\tilde c^{\langle t \rangle}, d\Gamma_o^{\langle t \rangle}$ respectively. Note that you should have the `keep_dims = True` option. Finally, you will compute the derivative with respect to the previous hidden state, previous memory state, and input. $$ da_{prev} = W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c^{\langle t \rangle} + W_o^T * d\Gamma_o^{\langle t \rangle} \tag{15}$$ Here, the weights for equations 13 are the first n_a, (i.e. $W_f = W_f[:n_a,:]$ etc...) $$ dc_{prev} = dc_{next}\Gamma_f^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} * (1- \tanh(c_{next})^2)*\Gamma_f^{\langle t \rangle}*da_{next} \tag{16}$$ $$ dx^{\langle t \rangle} = W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c_t + W_o^T * d\Gamma_o^{\langle t \rangle}\tag{17} $$ where the weights for equation 15 are from n_a to the end, (i.e. $W_f = W_f[n_a:,:]$ etc...) **Exercise:** Implement `lstm_cell_backward` by implementing equations $7-17$ below. Good luck! :) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364def lstm_cell_backward(da_next, dc_next, cache): """ Implement the backward pass for the LSTM-cell (single time-step). Arguments: da_next -- Gradients of next hidden state, of shape (n_a, m) dc_next -- Gradients of next cell state, of shape (n_a, m) cache -- cache storing information from the forward pass Returns: gradients -- python dictionary containing: dxt -- Gradient of input data at time-step t, of shape (n_x, m) da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m) dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x) dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x) dWi -- Gradient w.r.t. the weight matrix of the input gate, numpy array of shape (n_a, n_a + n_x) dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x) dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x) dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1) dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1) dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1) dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1) """ # Retrieve information from "cache" (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache ### START CODE HERE ### n_a, m = a_next.shape; n_x, m = xt.shape; # Compute gates related derivatives, you can find their values can be found by looking carefully at equations (7) to (10) (≈4 lines) dot = da_next * np.tanh(c_next) * ot * (1 - ot); dcct = (dc_next * it + ot * (1 - np.tanh(c_next) ** 2) * it * da_next) * (1 - cct ** 2); dit = (dc_next * cct + ot * (1 - np.tanh(c_next) ** 2) * cct * da_next) * it * (1 - it); dft = (dc_next * c_prev + ot * (1 - np.tanh(c_next) ** 2) * c_prev * da_next) * ft * (1 - ft); ## Code equations (7) to (10) (≈4 lines) ##dit = None ##dft = None ##dot = None ##dcct = None ## # Compute parameters related derivatives. Use equations (11)-(14) (≈8 lines) concat = np.concatenate((a_prev, xt), axis = 0).T; dWf = np.dot(dft, concat); dWi = np.dot(dit, concat); dWc = np.dot(dcct, concat); dWo = np.dot(dot, concat); dbf = np.sum(dft, keepdims = True, axis = -1); dbi = np.sum(dit, keepdims = True, axis = -1); dbc = np.sum(dcct, keepdims = True, axis = -1); dbo = np.sum(dot, keepdims = True, axis = -1); # Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (15)-(17). (≈3 lines) da_prev = np.dot(parameters['Wf'][:, : n_a].T, dft) + np.dot(parameters['Wi'][:, : n_a].T, dit) + np.dot(parameters['Wc'][:, : n_a].T, dcct) + np.dot(parameters['Wo'][:, : n_a].T, dot); dc_prev = dc_next * ft + ot * (1 - np.tanh(c_next) ** 2) * ft * da_next; dxt = np.dot(parameters['Wf'][:, n_a :].T, dft) + np.dot(parameters['Wi'][:, n_a :].T, dit) + np.dot(parameters['Wc'][:, n_a :].T, dcct) + np.dot(parameters['Wo'][:, n_a :].T, dot); ### END CODE HERE ### # Save gradients in dictionary gradients = &#123;"dxt": dxt, "da_prev": da_prev, "dc_prev": dc_prev, "dWf": dWf,"dbf": dbf, "dWi": dWi,"dbi": dbi, "dWc": dWc,"dbc": dbc, "dWo": dWo,"dbo": dbo&#125; return gradients 1234567891011121314151617181920212223242526272829303132333435363738394041424344np.random.seed(1)xt = np.random.randn(3,10)a_prev = np.random.randn(5,10)c_prev = np.random.randn(5,10)Wf = np.random.randn(5, 5+3)bf = np.random.randn(5,1)Wi = np.random.randn(5, 5+3)bi = np.random.randn(5,1)Wo = np.random.randn(5, 5+3)bo = np.random.randn(5,1)Wc = np.random.randn(5, 5+3)bc = np.random.randn(5,1)Wy = np.random.randn(2,5)by = np.random.randn(2,1)parameters = &#123;"Wf": Wf, "Wi": Wi, "Wo": Wo, "Wc": Wc, "Wy": Wy, "bf": bf, "bi": bi, "bo": bo, "bc": bc, "by": by&#125;a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)da_next = np.random.randn(5,10)dc_next = np.random.randn(5,10)gradients = lstm_cell_backward(da_next, dc_next, cache)print("gradients[\"dxt\"][1][2] =", gradients["dxt"][1][2])print("gradients[\"dxt\"].shape =", gradients["dxt"].shape)print("gradients[\"da_prev\"][2][3] =", gradients["da_prev"][2][3])print("gradients[\"da_prev\"].shape =", gradients["da_prev"].shape)print("gradients[\"dc_prev\"][2][3] =", gradients["dc_prev"][2][3])print("gradients[\"dc_prev\"].shape =", gradients["dc_prev"].shape)print("gradients[\"dWf\"][3][1] =", gradients["dWf"][3][1])print("gradients[\"dWf\"].shape =", gradients["dWf"].shape)print("gradients[\"dWi\"][1][2] =", gradients["dWi"][1][2])print("gradients[\"dWi\"].shape =", gradients["dWi"].shape)print("gradients[\"dWc\"][3][1] =", gradients["dWc"][3][1])print("gradients[\"dWc\"].shape =", gradients["dWc"].shape)print("gradients[\"dWo\"][1][2] =", gradients["dWo"][1][2])print("gradients[\"dWo\"].shape =", gradients["dWo"].shape)print("gradients[\"dbf\"][4] =", gradients["dbf"][4])print("gradients[\"dbf\"].shape =", gradients["dbf"].shape)print("gradients[\"dbi\"][4] =", gradients["dbi"][4])print("gradients[\"dbi\"].shape =", gradients["dbi"].shape)print("gradients[\"dbc\"][4] =", gradients["dbc"][4])print("gradients[\"dbc\"].shape =", gradients["dbc"].shape)print("gradients[\"dbo\"][4] =", gradients["dbo"][4])print("gradients[\"dbo\"].shape =", gradients["dbo"].shape) gradients[&quot;dxt&quot;][1][2] = 3.23055911511 gradients[&quot;dxt&quot;].shape = (3, 10) gradients[&quot;da_prev&quot;][2][3] = -0.0639621419711 gradients[&quot;da_prev&quot;].shape = (5, 10) gradients[&quot;dc_prev&quot;][2][3] = 0.797522038797 gradients[&quot;dc_prev&quot;].shape = (5, 10) gradients[&quot;dWf&quot;][3][1] = -0.147954838164 gradients[&quot;dWf&quot;].shape = (5, 8) gradients[&quot;dWi&quot;][1][2] = 1.05749805523 gradients[&quot;dWi&quot;].shape = (5, 8) gradients[&quot;dWc&quot;][3][1] = 2.30456216369 gradients[&quot;dWc&quot;].shape = (5, 8) gradients[&quot;dWo&quot;][1][2] = 0.331311595289 gradients[&quot;dWo&quot;].shape = (5, 8) gradients[&quot;dbf&quot;][4] = [ 0.18864637] gradients[&quot;dbf&quot;].shape = (5, 1) gradients[&quot;dbi&quot;][4] = [-0.40142491] gradients[&quot;dbi&quot;].shape = (5, 1) gradients[&quot;dbc&quot;][4] = [ 0.25587763] gradients[&quot;dbc&quot;].shape = (5, 1) gradients[&quot;dbo&quot;][4] = [ 0.13893342] gradients[&quot;dbo&quot;].shape = (5, 1) Expected Output: gradients[“dxt”][1][2] = 3.23055911511 gradients[“dxt”].shape = (3, 10) gradients[“da_prev”][2][3] = -0.0639621419711 gradients[“da_prev”].shape = (5, 10) gradients[“dc_prev”][2][3] = 0.797522038797 gradients[“dc_prev”].shape = (5, 10) gradients[“dWf”][3][1] = -0.147954838164 gradients[“dWf”].shape = (5, 8) gradients[“dWi”][1][2] = 1.05749805523 gradients[“dWi”].shape = (5, 8) gradients[“dWc”][3][1] = 2.30456216369 gradients[“dWc”].shape = (5, 8) gradients[“dWo”][1][2] = 0.331311595289 gradients[“dWo”].shape = (5, 8) gradients[“dbf”][4] = [ 0.18864637] gradients[“dbf”].shape = (5, 1) gradients[“dbi”][4] = [-0.40142491] gradients[“dbi”].shape = (5, 1) gradients[“dbc”][4] = [ 0.25587763] gradients[“dbc”].shape = (5, 1) gradients[“dbo”][4] = [ 0.13893342] gradients[“dbo”].shape = (5, 1) 3.3 Backward pass through the LSTM RNNThis part is very similar to the rnn_backward function you implemented above. You will first create variables of the same dimension as your return variables. You will then iterate over all the time steps starting from the end and call the one step function you implemented for LSTM at each iteration. You will then update the parameters by summing them individually. Finally return a dictionary with the new gradients. Instructions: Implement the lstm_backward function. Create a for loop starting from $T_x$ and going backward. For each step call lstm_cell_backward and update the your old gradients by adding the new gradients to them. Note that dxt is not updated but is stored. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172def lstm_backward(da, caches): """ Implement the backward pass for the RNN with LSTM-cell (over a whole sequence). Arguments: da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x) dc -- Gradients w.r.t the memory states, numpy-array of shape (n_a, m, T_x) caches -- cache storing information from the forward pass (lstm_forward) Returns: gradients -- python dictionary containing: dx -- Gradient of inputs, of shape (n_x, m, T_x) da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m) dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x) dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x) dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x) dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x) dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1) dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1) dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1) dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1) """ # Retrieve values from the first cache (t=1) of caches. (caches, x) = caches (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0] ### START CODE HERE ### # Retrieve dimensions from da's and x1's shapes (≈2 lines) n_a, m, T_x = da.shape n_x, m = x1.shape # initialize the gradients with the right sizes (≈12 lines) dx = np.zeros([n_x, m, T_x]) da0 = np.zeros([n_a, m]) da_prevt = np.zeros([n_a, m]) dc_prevt = np.zeros([n_a, m]) dWf = np.zeros([n_a, n_a + n_x]) dWi = np.zeros([n_a, n_a + n_x]) dWc = np.zeros([n_a, n_a + n_x]) dWo = np.zeros([n_a, n_a + n_x]) dbf = np.zeros([n_a, 1]) dbi = np.zeros([n_a, 1]) dbc = np.zeros([n_a, 1]) dbo = np.zeros([n_a, 1]) # loop back over the whole sequence for t in reversed(range(T_x)): # Compute all gradients using lstm_cell_backward gradients = lstm_cell_backward(da[:,:,t],dc_prevt,caches[t]) # da_prevt, dc_prevt = gradients['da_prev'], gradients["dc_prev"] # Store or add the gradient to the parameters' previous step's gradient dx[:,:,t] = gradients['dxt'] dWf = dWf+gradients['dWf'] dWi = dWi+gradients['dWi'] dWc = dWc+gradients['dWc'] dWo = dWo+gradients['dWo'] dbf = dbf+gradients['dbf'] dbi = dbi+gradients['dbi'] dbc = dbc+gradients['dbc'] dbo = dbo+gradients['dbo'] # Set the first activation's gradient to the backpropagated gradient da_prev. da0 = gradients['da_prev'] ### END CODE HERE ### # Store the gradients in a python dictionary gradients = &#123;"dx": dx, "da0": da0, "dWf": dWf,"dbf": dbf, "dWi": dWi,"dbi": dbi, "dWc": dWc,"dbc": dbc, "dWo": dWo,"dbo": dbo&#125; return gradients 123456789101112131415161718192021222324252627282930313233343536373839np.random.seed(1)x = np.random.randn(3,10,7)a0 = np.random.randn(5,10)Wf = np.random.randn(5, 5+3)bf = np.random.randn(5,1)Wi = np.random.randn(5, 5+3)bi = np.random.randn(5,1)Wo = np.random.randn(5, 5+3)bo = np.random.randn(5,1)Wc = np.random.randn(5, 5+3)bc = np.random.randn(5,1)parameters = &#123;"Wf": Wf, "Wi": Wi, "Wo": Wo, "Wc": Wc, "Wy": Wy, "bf": bf, "bi": bi, "bo": bo, "bc": bc, "by": by&#125;a, y, c, caches = lstm_forward(x, a0, parameters)da = np.random.randn(5, 10, 4)gradients = lstm_backward(da, caches)print("gradients[\"dx\"][1][2] =", gradients["dx"][1][2])print("gradients[\"dx\"].shape =", gradients["dx"].shape)print("gradients[\"da0\"][2][3] =", gradients["da0"][2][3])print("gradients[\"da0\"].shape =", gradients["da0"].shape)print("gradients[\"dWf\"][3][1] =", gradients["dWf"][3][1])print("gradients[\"dWf\"].shape =", gradients["dWf"].shape)print("gradients[\"dWi\"][1][2] =", gradients["dWi"][1][2])print("gradients[\"dWi\"].shape =", gradients["dWi"].shape)print("gradients[\"dWc\"][3][1] =", gradients["dWc"][3][1])print("gradients[\"dWc\"].shape =", gradients["dWc"].shape)print("gradients[\"dWo\"][1][2] =", gradients["dWo"][1][2])print("gradients[\"dWo\"].shape =", gradients["dWo"].shape)print("gradients[\"dbf\"][4] =", gradients["dbf"][4])print("gradients[\"dbf\"].shape =", gradients["dbf"].shape)print("gradients[\"dbi\"][4] =", gradients["dbi"][4])print("gradients[\"dbi\"].shape =", gradients["dbi"].shape)print("gradients[\"dbc\"][4] =", gradients["dbc"][4])print("gradients[\"dbc\"].shape =", gradients["dbc"].shape)print("gradients[\"dbo\"][4] =", gradients["dbo"][4])print("gradients[\"dbo\"].shape =", gradients["dbo"].shape) gradients[&quot;dx&quot;][1][2] = [-0.00173313 0.08287442 -0.30545663 -0.43281115] gradients[&quot;dx&quot;].shape = (3, 10, 4) gradients[&quot;da0&quot;][2][3] = -0.095911501954 gradients[&quot;da0&quot;].shape = (5, 10) gradients[&quot;dWf&quot;][3][1] = -0.0698198561274 gradients[&quot;dWf&quot;].shape = (5, 8) gradients[&quot;dWi&quot;][1][2] = 0.102371820249 gradients[&quot;dWi&quot;].shape = (5, 8) gradients[&quot;dWc&quot;][3][1] = -0.0624983794927 gradients[&quot;dWc&quot;].shape = (5, 8) gradients[&quot;dWo&quot;][1][2] = 0.0484389131444 gradients[&quot;dWo&quot;].shape = (5, 8) gradients[&quot;dbf&quot;][4] = [-0.0565788] gradients[&quot;dbf&quot;].shape = (5, 1) gradients[&quot;dbi&quot;][4] = [-0.15399065] gradients[&quot;dbi&quot;].shape = (5, 1) gradients[&quot;dbc&quot;][4] = [-0.29691142] gradients[&quot;dbc&quot;].shape = (5, 1) gradients[&quot;dbo&quot;][4] = [-0.29798344] gradients[&quot;dbo&quot;].shape = (5, 1) Expected Output: gradients[“dx”][1][2] = [-0.00173313 0.08287442 -0.30545663 -0.43281115] gradients[“dx”].shape = (3, 10, 4) gradients[“da0”][2][3] = -0.095911501954 gradients[“da0”].shape = (5, 10) gradients[“dWf”][3][1] = -0.0698198561274 gradients[“dWf”].shape = (5, 8) gradients[“dWi”][1][2] = 0.102371820249 gradients[“dWi”].shape = (5, 8) gradients[“dWc”][3][1] = -0.0624983794927 gradients[“dWc”].shape = (5, 8) gradients[“dWo”][1][2] = 0.0484389131444 gradients[“dWo”].shape = (5, 8) gradients[“dbf”][4] = [-0.0565788] gradients[“dbf”].shape = (5, 1) gradients[“dbi”][4] = [-0.06997391] gradients[“dbi”].shape = (5, 1) gradients[“dbc”][4] = [-0.27441821] gradients[“dbc”].shape = (5, 1) gradients[“dbo”][4] = [ 0.16532821] gradients[“dbo”].shape = (5, 1) Congratulations !Congratulations on completing this assignment. You now understand how recurrent neural networks work! Lets go on to the next exercise, where you’ll use an RNN to build a character-level language model.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dinosaurus Island Character level language model final]]></title>
    <url>%2F2018%2F06%2F02%2FDinosaurus%2BIsland%2B--%2BCharacter%2Blevel%2Blanguage%2Bmodel%2Bfinal%2B-%2Bv3%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 1st week and the copyright belongs to deeplearning.ai. Character level language model - Dinosaurus landWelcome to Dinosaurus Island! 65 million years ago, dinosaurs existed, and in this assignment they are back. You are in charge of a special task. Leading biology researchers are creating new breeds of dinosaurs and bringing them to life on earth, and your job is to give names to these dinosaurs. If a dinosaur does not like its name, it might go beserk, so choose wisely! Luckily you have learned some deep learning and you will use it to save the day. Your assistant has collected a list of all the dinosaur names they could find, and compiled them into this dataset. (Feel free to take a look by clicking the previous link.) To create new dinosaur names, you will build a character level language model to generate new names. Your algorithm will learn the different name patterns, and randomly generate new names. Hopefully this algorithm will keep you and your team safe from the dinosaurs’ wrath! By completing this assignment you will learn: How to store text data for processing using an RNN How to synthesize data, by sampling predictions at each time step and passing it to the next RNN-cell unit How to build a character-level text generation recurrent neural network Why clipping the gradients is important We will begin by loading in some functions that we have provided for you in rnn_utils. Specifically, you have access to functions such as rnn_forward and rnn_backward which are equivalent to those you’ve implemented in the previous assignment. 123import numpy as npfrom utils import *import random 1 - Problem Statement1.1 - Dataset and PreprocessingRun the following cell to read the dataset of dinosaur names, create a list of unique characters (such as a-z), and compute the dataset and vocabulary size. 12345data = open('dinos.txt', 'r').read()data= data.lower()chars = list(set(data))data_size, vocab_size = len(data), len(chars)print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size)) There are 19909 total characters and 27 unique characters in your data. The characters are a-z (26 characters) plus the “\n” (or newline character), which in this assignment plays a role similar to the &lt;EOS&gt; (or “End of sentence”) token we had discussed in lecture, only here it indicates the end of the dinosaur name rather than the end of a sentence. In the cell below, we create a python dictionary (i.e., a hash table) to map each character to an index from 0-26. We also create a second python dictionary that maps each index back to the corresponding character character. This will help you figure out what index corresponds to what character in the probability distribution output of the softmax layer. Below, char_to_ix and ix_to_char are the python dictionaries. 123char_to_ix = &#123; ch:i for i,ch in enumerate(sorted(chars)) &#125;ix_to_char = &#123; i:ch for i,ch in enumerate(sorted(chars)) &#125;print(ix_to_char) {0: &apos;\n&apos;, 1: &apos;a&apos;, 2: &apos;b&apos;, 3: &apos;c&apos;, 4: &apos;d&apos;, 5: &apos;e&apos;, 6: &apos;f&apos;, 7: &apos;g&apos;, 8: &apos;h&apos;, 9: &apos;i&apos;, 10: &apos;j&apos;, 11: &apos;k&apos;, 12: &apos;l&apos;, 13: &apos;m&apos;, 14: &apos;n&apos;, 15: &apos;o&apos;, 16: &apos;p&apos;, 17: &apos;q&apos;, 18: &apos;r&apos;, 19: &apos;s&apos;, 20: &apos;t&apos;, 21: &apos;u&apos;, 22: &apos;v&apos;, 23: &apos;w&apos;, 24: &apos;x&apos;, 25: &apos;y&apos;, 26: &apos;z&apos;} 1.2 - Overview of the modelYour model will have the following structure: Initialize parameters Run the optimization loop Forward propagation to compute the loss function Backward propagation to compute the gradients with respect to the loss function Clip the gradients to avoid exploding gradients Using the gradients, update your parameter with the gradient descent update rule. Return the learned parameters Figure 1: Recurrent Neural Network, similar to what you had built in the previous notebook “Building a RNN - Step by Step”. At each time-step, the RNN tries to predict what is the next character given the previous characters. The dataset $X = (x^{\langle 1 \rangle}, x^{\langle 2 \rangle}, …, x^{\langle T_x \rangle})$ is a list of characters in the training set, while $Y = (y^{\langle 1 \rangle}, y^{\langle 2 \rangle}, …, y^{\langle T_x \rangle})$ is such that at every time-step $t$, we have $y^{\langle t \rangle} = x^{\langle t+1 \rangle}$. 2 - Building blocks of the modelIn this part, you will build two important blocks of the overall model: Gradient clipping: to avoid exploding gradients Sampling: a technique used to generate characters You will then apply these two functions to build the model. 2.1 - Clipping the gradients in the optimization loopIn this section you will implement the clip function that you will call inside of your optimization loop. Recall that your overall loop structure usually consists of a forward pass, a cost computation, a backward pass, and a parameter update. Before updating the parameters, you will perform gradient clipping when needed to make sure that your gradients are not “exploding,” meaning taking on overly large values. In the exercise below, you will implement a function clip that takes in a dictionary of gradients and returns a clipped version of gradients if needed. There are different ways to clip gradients; we will use a simple element-wise clipping procedure, in which every element of the gradient vector is clipped to lie between some range [-N, N]. More generally, you will provide a maxValue (say 10). In this example, if any component of the gradient vector is greater than 10, it would be set to 10; and if any component of the gradient vector is less than -10, it would be set to -10. If it is between -10 and 10, it is left alone. Figure 2: Visualization of gradient descent with and without gradient clipping, in a case where the network is running into slight “exploding gradient” problems. Exercise: Implement the function below to return the clipped gradients of your dictionary gradients. Your function takes in a maximum threshold and returns the clipped versions of your gradients. You can check out this hint for examples of how to clip in numpy. You will need to use the argument out = .... 12345678910111213141516171819202122232425### GRADED FUNCTION: clipdef clip(gradients, maxValue): ''' Clips the gradients' values between minimum and maximum. Arguments: gradients -- a dictionary containing the gradients "dWaa", "dWax", "dWya", "db", "dby" maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue Returns: gradients -- a dictionary with the clipped gradients. ''' dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby'] ### START CODE HERE ### # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines) for gradient in [dWax, dWaa, dWya, db, dby]: np.clip(gradient, -maxValue, maxValue, out = gradient); ### END CODE HERE ### gradients = &#123;"dWaa": dWaa, "dWax": dWax, "dWya": dWya, "db": db, "dby": dby&#125; return gradients 12345678910111213np.random.seed(3)dWax = np.random.randn(5,3)*10dWaa = np.random.randn(5,5)*10dWya = np.random.randn(2,5)*10db = np.random.randn(5,1)*10dby = np.random.randn(2,1)*10gradients = &#123;"dWax": dWax, "dWaa": dWaa, "dWya": dWya, "db": db, "dby": dby&#125;gradients = clip(gradients, 10)print("gradients[\"dWaa\"][1][2] =", gradients["dWaa"][1][2])print("gradients[\"dWax\"][3][1] =", gradients["dWax"][3][1])print("gradients[\"dWya\"][1][2] =", gradients["dWya"][1][2])print("gradients[\"db\"][4] =", gradients["db"][4])print("gradients[\"dby\"][1] =", gradients["dby"][1]) gradients[&quot;dWaa&quot;][1][2] = 10.0 gradients[&quot;dWax&quot;][3][1] = -10.0 gradients[&quot;dWya&quot;][1][2] = 0.2971381536101662 gradients[&quot;db&quot;][4] = [10.] gradients[&quot;dby&quot;][1] = [8.45833407] Expected output: gradients[“dWaa”][1][2] 10.0 gradients[“dWax”][3][1] -10.0 gradients[“dWya”][1][2] 0.29713815361 gradients[“db”][4] [ 10.] gradients[“dby”][1] [ 8.45833407] 2.2 - SamplingNow assume that your model is trained. You would like to generate new text (characters). The process of generation is explained in the picture below: Figure 3: In this picture, we assume the model is already trained. We pass in $x^{\langle 1\rangle} = \vec{0}$ at the first time step, and have the network then sample one character at a time. Exercise: Implement the sample function below to sample characters. You need to carry out 4 steps: Step 1: Pass the network the first “dummy” input $x^{\langle 1 \rangle} = \vec{0}$ (the vector of zeros). This is the default input before we’ve generated any characters. We also set $a^{\langle 0 \rangle} = \vec{0}$ Step 2: Run one step of forward propagation to get $a^{\langle 1 \rangle}$ and $\hat{y}^{\langle 1 \rangle}$. Here are the equations: $$ a^{\langle t+1 \rangle} = \tanh(W_{ax} x^{\langle t \rangle } + W_{aa} a^{\langle t \rangle } + b)\tag{1}$$ $$ z^{\langle t + 1 \rangle } = W_{ya} a^{\langle t + 1 \rangle } + b_y \tag{2}$$ $$ \hat{y}^{\langle t+1 \rangle } = softmax(z^{\langle t + 1 \rangle })\tag{3}$$ Note that $\hat{y}^{\langle t+1 \rangle }$ is a (softmax) probability vector (its entries are between 0 and 1 and sum to 1). $\hat{y}^{\langle t+1 \rangle}_i$ represents the probability that the character indexed by “i” is the next character. We have provided a softmax() function that you can use. Step 3: Carry out sampling: Pick the next character’s index according to the probability distribution specified by $\hat{y}^{\langle t+1 \rangle }$. This means that if $\hat{y}^{\langle t+1 \rangle }_i = 0.16$, you will pick the index “i” with 16% probability. To implement it, you can use np.random.choice. Here is an example of how to use np.random.choice():123np.random.seed(0)p = np.array([0.1, 0.0, 0.7, 0.2])index = np.random.choice([0, 1, 2, 3], p = p.ravel()) This means that you will pick the index according to the distribution:$P(index = 0) = 0.1, P(index = 1) = 0.0, P(index = 2) = 0.7, P(index = 3) = 0.2$. Step 4: The last step to implement in sample() is to overwrite the variable x, which currently stores $x^{\langle t \rangle }$, with the value of $x^{\langle t + 1 \rangle }$. You will represent $x^{\langle t + 1 \rangle }$ by creating a one-hot vector corresponding to the character you’ve chosen as your prediction. You will then forward propagate $x^{\langle t + 1 \rangle }$ in Step 1 and keep repeating the process until you get a “\n” character, indicating you’ve reached the end of the dinosaur name. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# GRADED FUNCTION: sampledef sample(parameters, char_to_ix, seed): """ Sample a sequence of characters according to a sequence of probability distributions output of the RNN Arguments: parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. char_to_ix -- python dictionary mapping each character to an index. seed -- used for grading purposes. Do not worry about it. Returns: indices -- a list of length n containing the indices of the sampled characters. """ # Retrieve parameters and relevant shapes from "parameters" dictionary Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b'] vocab_size = by.shape[0] n_a = Waa.shape[1] ### START CODE HERE ### # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line) x = np.zeros((vocab_size, 1)); # Step 1': Initialize a_prev as zeros (≈1 line) a_prev = np.zeros((n_a, 1)); # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line) indices = [] # Idx is a flag to detect a newline character, we initialize it to -1 idx = -1 # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append # its index to "indices". We'll stop if we reach 50 characters (which should be very unlikely with a well # trained model), which helps debugging and prevents entering an infinite loop. counter = 0 newline_character = char_to_ix['\n'] while (idx != newline_character and counter != 50): # Step 2: Forward propagate x using the equations (1), (2) and (3) a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b); z = np.dot(Wya, a) + by; y = softmax(z); # for grading purposes np.random.seed(counter+seed) # Step 3: Sample the index of a character within the vocabulary from the probability distribution y idx = np.random.choice(range(len(y)), p = y.ravel()); # Append the index to "indices" indices.append(idx); # Step 4: Overwrite the input character as the one corresponding to the sampled index. x = np.zeros((vocab_size, 1)); x[idx] = 1; # Update "a_prev" to be "a" a_prev = a; # for grading purposes seed += 1 counter +=1 ### END CODE HERE ### if (counter == 50): indices.append(char_to_ix['\n']) return indices 1234567891011np.random.seed(2)_, n_a = 20, 100Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)parameters = &#123;"Wax": Wax, "Waa": Waa, "Wya": Wya, "b": b, "by": by&#125;indices = sample(parameters, char_to_ix, 0)print("Sampling:")print("list of sampled indices:", indices)print("list of sampled characters:", [ix_to_char[i] for i in indices]) Sampling: list of sampled indices: [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 3, 6, 23, 13, 1, 0] list of sampled characters: [&apos;l&apos;, &apos;q&apos;, &apos;x&apos;, &apos;n&apos;, &apos;m&apos;, &apos;i&apos;, &apos;j&apos;, &apos;v&apos;, &apos;x&apos;, &apos;f&apos;, &apos;m&apos;, &apos;k&apos;, &apos;l&apos;, &apos;f&apos;, &apos;u&apos;, &apos;o&apos;, &apos;u&apos;, &apos;n&apos;, &apos;c&apos;, &apos;b&apos;, &apos;a&apos;, &apos;u&apos;, &apos;r&apos;, &apos;x&apos;, &apos;g&apos;, &apos;y&apos;, &apos;f&apos;, &apos;y&apos;, &apos;r&apos;, &apos;j&apos;, &apos;p&apos;, &apos;b&apos;, &apos;c&apos;, &apos;h&apos;, &apos;o&apos;, &apos;l&apos;, &apos;k&apos;, &apos;g&apos;, &apos;a&apos;, &apos;l&apos;, &apos;j&apos;, &apos;b&apos;, &apos;g&apos;, &apos;g&apos;, &apos;k&apos;, &apos;c&apos;, &apos;f&apos;, &apos;w&apos;, &apos;m&apos;, &apos;a&apos;, &apos;\n&apos;] Expected output: list of sampled indices: [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 5, 6, 12, 25, 0, 0] list of sampled characters: [‘l’, ‘q’, ‘x’, ‘n’, ‘m’, ‘i’, ‘j’, ‘v’, ‘x’, ‘f’, ‘m’, ‘k’, ‘l’, ‘f’, ‘u’, ‘o’, ‘u’, ‘n’, ‘c’, ‘b’, ‘a’, ‘u’, ‘r’, ‘x’, ‘g’, ‘y’, ‘f’, ‘y’, ‘r’, ‘j’, ‘p’, ‘b’, ‘c’, ‘h’, ‘o’, ‘l’, ‘k’, ‘g’, ‘a’, ‘l’, ‘j’, ‘b’, ‘g’, ‘g’, ‘k’, ‘e’, ‘f’, ‘l’, ‘y’, ‘\n’, ‘\n’] 3 - Building the language modelIt is time to build the character-level language model for text generation. 3.1 - Gradient descentIn this section you will implement a function performing one step of stochastic gradient descent (with clipped gradients). You will go through the training examples one at a time, so the optimization algorithm will be stochastic gradient descent. As a reminder, here are the steps of a common optimization loop for an RNN: Forward propagate through the RNN to compute the loss Backward propagate through time to compute the gradients of the loss with respect to the parameters Clip the gradients if necessary Update your parameters using gradient descent Exercise: Implement this optimization process (one step of stochastic gradient descent). We provide you with the following functions: 12345678910111213141516def rnn_forward(X, Y, a_prev, parameters): """ Performs the forward propagation through the RNN and computes the cross-entropy loss. It returns the loss' value as well as a "cache" storing values to be used in the backpropagation.""" .... return loss, cache def rnn_backward(X, Y, parameters, cache): """ Performs the backward propagation through time to compute the gradients of the loss with respect to the parameters. It returns also all the hidden states.""" ... return gradients, adef update_parameters(parameters, gradients, learning_rate): """ Updates parameters using the Gradient Descent Update Rule.""" ... return parameters 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# GRADED FUNCTION: optimizedef optimize(X, Y, a_prev, parameters, learning_rate = 0.01): """ Execute one step of the optimization to train the model. Arguments: X -- list of integers, where each integer is a number that maps to a character in the vocabulary. Y -- list of integers, exactly the same as X but shifted one index to the left. a_prev -- previous hidden state. parameters -- python dictionary containing: Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x) Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a) Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a) b -- Bias, numpy array of shape (n_a, 1) by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1) learning_rate -- learning rate for the model. Returns: loss -- value of the loss function (cross-entropy) gradients -- python dictionary containing: dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x) dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a) dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a) db -- Gradients of bias vector, of shape (n_a, 1) dby -- Gradients of output bias vector, of shape (n_y, 1) a[len(X)-1] -- the last hidden state, of shape (n_a, 1) """ ### START CODE HERE ### # Forward propagate through time (≈1 line) loss, cache = rnn_forward(X, Y, a_prev, parameters); # Backpropagate through time (≈1 line) gradients, a = rnn_backward(X, Y, parameters, cache); # Clip your gradients between -5 (min) and 5 (max) (≈1 line) gradients = clip(gradients, maxValue = 5); # Update parameters (≈1 line) parameters = update_parameters(parameters, gradients, learning_rate); ### END CODE HERE ### return loss, gradients, a[len(X)-1] 1234567891011121314151617np.random.seed(1)vocab_size, n_a = 27, 100a_prev = np.random.randn(n_a, 1)Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)parameters = &#123;"Wax": Wax, "Waa": Waa, "Wya": Wya, "b": b, "by": by&#125;X = [12,3,5,11,22,3]Y = [4,14,11,22,25, 26]loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)print("Loss =", loss)print("gradients[\"dWaa\"][1][2] =", gradients["dWaa"][1][2])print("np.argmax(gradients[\"dWax\"]) =", np.argmax(gradients["dWax"]))print("gradients[\"dWya\"][1][2] =", gradients["dWya"][1][2])print("gradients[\"db\"][4] =", gradients["db"][4])print("gradients[\"dby\"][1] =", gradients["dby"][1])print("a_last[4] =", a_last[4]) Loss = 126.50397572165383 gradients[&quot;dWaa&quot;][1][2] = 0.1947093153471825 np.argmax(gradients[&quot;dWax&quot;]) = 93 gradients[&quot;dWya&quot;][1][2] = -0.007773876032003897 gradients[&quot;db&quot;][4] = [-0.06809825] gradients[&quot;dby&quot;][1] = [0.01538192] a_last[4] = [-1.] Expected output: Loss 126.503975722 gradients[“dWaa”][1][2] 0.194709315347 np.argmax(gradients[“dWax”]) 93 gradients[“dWya”][1][2] -0.007773876032 gradients[“db”][4] [-0.06809825] gradients[“dby”][1] [ 0.01538192] a_last[4] [-1.] 3.2 - Training the modelGiven the dataset of dinosaur names, we use each line of the dataset (one name) as one training example. Every 100 steps of stochastic gradient descent, you will sample 10 randomly chosen names to see how the algorithm is doing. Remember to shuffle the dataset, so that stochastic gradient descent visits the examples in random order. Exercise: Follow the instructions and implement model(). When examples[index] contains one dinosaur name (string), to create an example (X, Y), you can use this:123index = j % len(examples)X = [None] + [char_to_ix[ch] for ch in examples[index]] Y = X[1:] + [char_to_ix["\n"]] Note that we use: index= j % len(examples), where j = 1....num_iterations, to make sure that examples[index] is always a valid statement (index is smaller than len(examples)).The first entry of X being None will be interpreted by rnn_forward() as setting $x^{\langle 0 \rangle} = \vec{0}$. Further, this ensures that Y is equal to X but shifted one step to the left, and with an additional “\n” appended to signify the end of the dinosaur name. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# GRADED FUNCTION: modeldef model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27): """ Trains the model and generates dinosaur names. Arguments: data -- text corpus ix_to_char -- dictionary that maps the index to a character char_to_ix -- dictionary that maps a character to an index num_iterations -- number of iterations to train the model for n_a -- number of units of the RNN cell dino_names -- number of dinosaur names you want to sample at each iteration. vocab_size -- number of unique characters found in the text, size of the vocabulary Returns: parameters -- learned parameters """ # Retrieve n_x and n_y from vocab_size n_x, n_y = vocab_size, vocab_size # Initialize parameters parameters = initialize_parameters(n_a, n_x, n_y) # Initialize loss (this is required because we want to smooth our loss, don't worry about it) loss = get_initial_loss(vocab_size, dino_names) # Build list of all dinosaur names (training examples). with open("dinos.txt") as f: examples = f.readlines() examples = [x.lower().strip() for x in examples] # Shuffle list of all dinosaur names np.random.seed(0) np.random.shuffle(examples) # Initialize the hidden state of your LSTM a_prev = np.zeros((n_a, 1)) # Optimization loop for j in range(num_iterations): ### START CODE HERE ### # Use the hint above to define one training example (X,Y) (≈ 2 lines) index = j % len(examples); X = [None] + [char_to_ix[ch] for ch in examples[index]]; Y = X[1:] + [char_to_ix["\n"]]; learning_rate = 0.01; # num_partition = num_iterations / 10; # if j / num_partition &gt; 0 : # if j % num_partition == 0 : # learning_rate = 0.01 * (0.95 ** (j / num_partition)); # print("current learning rate: " + str(learning_rate)); # Perform one optimization step: Forward-prop -&gt; Backward-prop -&gt; Clip -&gt; Update parameters # Choose a learning rate of 0.01 curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate); ### END CODE HERE ### # Use a latency trick to keep the loss smooth. It happens here to accelerate the training. loss = smooth(loss, curr_loss) # Every 2000 Iteration, generate "n" characters thanks to sample() to check if the model is learning properly if j % 2000 == 0: print('Iteration: %d, Loss: %f' % (j, loss) + '\n') # The number of dinosaur names to print seed = 0 for name in range(dino_names): # Sample indices and print them sampled_indices = sample(parameters, char_to_ix, seed) print_sample(sampled_indices, ix_to_char) seed += 1 # To get the same result for grading purposed, increment the seed by one. print('\n') return parameters Run the following cell, you should observe your model outputting random-looking characters at the first iteration. After a few thousand iterations, your model should learn to generate reasonable-looking names. 1parameters = model(data, ix_to_char, char_to_ix) Iteration: 0, Loss: 23.087336 Nkzxwtdmfqoeyhsqwasjkjvu Kneb Kzxwtdmfqoeyhsqwasjkjvu Neb Zxwtdmfqoeyhsqwasjkjvu Eb Xwtdmfqoeyhsqwasjkjvu Iteration: 2000, Loss: 27.884160 Liusskeomnolxeros Hmdaairus Hytroligoraurus Lecalosapaus Xusicikoraurus Abalpsamantisaurus Tpraneronxeros Iteration: 4000, Loss: 25.901815 Mivrosaurus Inee Ivtroplisaurus Mbaaisaurus Wusichisaurus Cabaselachus Toraperlethosdarenitochusthiamamumamaon Iteration: 6000, Loss: 24.608779 Onwusceomosaurus Lieeaerosaurus Lxussaurus Oma Xusteonosaurus Eeahosaurus Toreonosaurus Iteration: 8000, Loss: 24.070350 Onxusichepriuon Kilabersaurus Lutrodon Omaaerosaurus Xutrcheps Edaksoje Trodiktonus Iteration: 10000, Loss: 23.844446 Onyusaurus Klecalosaurus Lustodon Ola Xusodonia Eeaeosaurus Troceosaurus Iteration: 12000, Loss: 23.291971 Onyxosaurus Kica Lustrepiosaurus Olaagrraiansaurus Yuspangosaurus Eealosaurus Trognesaurus Iteration: 14000, Loss: 23.382339 Meutromodromurus Inda Iutroinatorsaurus Maca Yusteratoptititan Ca Troclosaurus Iteration: 16000, Loss: 23.259291 Meustomia Indaadps Justolongchudosatrus Macabosaurus Yuspanhosaurus Caaerosaurus Trodon Iteration: 18000, Loss: 22.940799 Phusaurus Meicamitheastosaurus Mussteratops Peg Ytrong Egaltor Trolome Iteration: 20000, Loss: 22.894192 Meutrodon Lledansteh Lwuspconyxauosaurus Macalosaurus Yusocichugus Eiagosaurus Trrangosaurus Iteration: 22000, Loss: 22.851820 Onustolia Midcagosaurus Mwrrodonnonus Ola Yurodon Eiaeptia Trodoniohus Iteration: 24000, Loss: 22.700408 Meutosaurus Jmacagosaurus Kurrodon Macaistel Yuroeleton Eiaeror Trodonosaurus Iteration: 26000, Loss: 22.736918 Niutosaurus Liga Lustoingosaurus Necakroia Xrprinhtilus Eiaestehastes Trocilosaurus Iteration: 28000, Loss: 22.595568 Meutosaurus Kolaaeus Kystodonisaurus Macahtopadrus Xtrrararkaumurpasaurus Eiaeosaurus Trodmanolus Iteration: 30000, Loss: 22.609381 Meutosaurus Kracakosaurus Lustodon Macaisthachwisaurus Wusqandosaurus Eiacosaurus Trsatisaurus Iteration: 32000, Loss: 22.251308 Mausinasaurus Incaadropeglsaurus Itrosaurus Macamisaurus Wuroenatoraerax Ehanosaurus Trnanclodratosaurus Iteration: 34000, Loss: 22.477910 Mawspichaniaekorocimamroberax Inda Itrus Macaesis Wrosaurus Elaeosaurus Stegngosaurus ConclusionYou can see that your algorithm has started to generate plausible dinosaur names towards the end of the training. At first, it was generating random characters, but towards the end you could see dinosaur names with cool endings. Feel free to run the algorithm even longer and play with hyperparameters to see if you can get even better results. Our implemetation generated some really cool names like maconucon, marloralus and macingsersaurus. Your model hopefully also learned that dinosaur names tend to end in saurus, don, aura, tor, etc. If your model generates some non-cool names, don’t blame the model entirely–not all actual dinosaur names sound cool. (For example, dromaeosauroides is an actual dinosaur name and is in the training set.) But this model should give you a set of candidates from which you can pick the coolest! This assignment had used a relatively small dataset, so that you could train an RNN quickly on a CPU. Training a model of the english language requires a much bigger dataset, and usually needs much more computation, and could run for many hours on GPUs. We ran our dinosaur name for quite some time, and so far our favoriate name is the great, undefeatable, and fierce: Mangosaurus! 4 - Writing like ShakespeareThe rest of this notebook is optional and is not graded, but we hope you’ll do it anyway since it’s quite fun and informative. A similar (but more complicated) task is to generate Shakespeare poems. Instead of learning from a dataset of Dinosaur names you can use a collection of Shakespearian poems. Using LSTM cells, you can learn longer term dependencies that span many characters in the text–e.g., where a character appearing somewhere a sequence can influence what should be a different character much much later in ths sequence. These long term dependencies were less important with dinosaur names, since the names were quite short. Let’s become poets! We have implemented a Shakespeare poem generator with Keras. Run the following cell to load the required packages and models. This may take a few minutes. 12345678910from __future__ import print_functionfrom keras.callbacks import LambdaCallbackfrom keras.models import Model, load_model, Sequentialfrom keras.layers import Dense, Activation, Dropout, Input, Maskingfrom keras.layers import LSTMfrom keras.utils.data_utils import get_filefrom keras.preprocessing.sequence import pad_sequencesfrom shakespeare_utils import *import sysimport io C:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Using TensorFlow backend. Loading text data... Creating training set... number of training examples: 31412 Vectorizing training set... Loading model... To save you some time, we have already trained a model for ~1000 epochs on a collection of Shakespearian poems called “The Sonnets”. Let’s train the model for one more epoch. When it finishes training for an epoch—this will also take a few minutes—you can run generate_output, which will prompt asking you for an input (&lt;40 characters). The poem will start with your sentence, and our RNN-Shakespeare will complete the rest of the poem for you! For example, try “Forsooth this maketh no sense “ (don’t enter the quotation marks). Depending on whether you include the space at the end, your results might also differ–try it both ways, and try other inputs as well. 123print_callback = LambdaCallback(on_epoch_end=on_epoch_end)model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback]) Epoch 1/1 31412/31412 [==============================] - 244s 8ms/step - loss: 2.7302 &lt;keras.callbacks.History at 0x1c3ef0e8978&gt; 12# Run this cell to try with different inputs without having to re-train the model generate_output() Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: You are a flower Here is your poem: You are a flower, and tines wo why doaoty loving friel be lifles it, whene the ford, eoreing oned his byfor mine, the beauty astore, with the dune still weel, doth nof berioner others should best ay commors shall&apos;s feel how the, ti the vere datef me wenden conse, now this, and mateh and haris by deigh doy, how raccersake wiming to be the worlts in thine sho nuch their astaver beloned i ustind, that youn thou The RNN-Shakespeare model is very similar to the one you have built for dinosaur names. The only major differences are: LSTMs instead of the basic RNN to capture longer-range dependencies The model is a deeper, stacked LSTM model (2 layer) Using Keras instead of python to simplify the code If you want to learn more, you can also check out the Keras Team’s text generation implementation on GitHub: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py. Congratulations on finishing this notebook! References: This exercise took inspiration from Andrej Karpathy’s implementation: https://gist.github.com/karpathy/d4dee566867f8291f086. To learn more about text generation, also check out Karpathy’s blog post. For the Shakespearian poem generator, our implementation was based on the implementation of an LSTM text generator by the Keras team: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[recurrent neural networks]]></title>
    <url>%2F2018%2F06%2F01%2F01_recurrent-neural-networks%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal lecture note after studying the course nlp sequence models at the 1st week and the copyright belongs to deeplearning.ai. 01_why-sequence-modelsWelcome to this fifth course on deep learning. In this course, you learn about sequence models, one of the most exciting areas in deep learning. Models like recurrent neural networks or RNNs have transformed speech recognition, natural language processing and other areas. And in this course, you learn how to build these models for yourself. Let’s start by looking at a few examples of where sequence models can be useful. In speech recognition you are given an input audio clip X and asked to map it to a text transcript Y. Both the input and the output here are sequence data, because X is an audio clip and so that plays out over time and Y, the output, is a sequence of words. So sequence models such as a recurrent neural networks and other variations, you’ll learn about in a little bit have been very useful for speech recognition. Music generation is another example of a problem with sequence data. In this case, only the output Y is a sequence, the input can be the empty set, or it can be a single integer, maybe referring to the genre of music you want to generate or maybe the first few notes of the piece of music you want. But here X can be nothing or maybe just an integer and output Y is a sequence. In sentiment classification the input X is a sequence, so given the input phrase like, “There is nothing to like in this movie” how many stars do you think this review will be? Sequence models are also very useful for DNA sequence analysis. So your DNA is represented via the four alphabets A, C, G, and T. And so given a DNA sequence can you label which part of this DNA sequence say corresponds to a protein. In machine translation you are given an input sentence, voulez-vou chante avec moi? And you’re asked to output the translation in a different language. In video activity recognition you might be given a sequence of video frames and asked to recognize the activity. And in name entity recognition you might be given a sentence and asked to identify the people in that sentence. So all of these problems can be addressed as supervised learning with label data X, Y as the training set. But, as you can tell from this list of examples, there are a lot of different types of sequence problems. In some, both the input X and the output Y are sequences, and in that case (speech recognition), sometimes X and Y can have different lengths, or in this example (at DNA case) and this example(at Name entity recognition), X and Y have the same length. And in some of these examples only either X or only the opposite Y is a sequence. So in this course you learn about sequence models are applicable, so all of these different settings. So I hope this gives you a sense of the exciting set of problems that sequence models might be able to help you to address. With that let us go on to the next video where we start to define the notation we use to define these sequence-models. 02_notationIn the last video, you saw some of the wide range of applications through which you can apply sequence models. Let’s start by defining a notation that we’ll use to build up these sequence models. As a motivating example, let’s say you want to build a sequence model to input a sentence like this, Harry Potter and Hermione Granger invented a new spell. And these are characters by the way, from the Harry Potter sequence of novels by J. K. Rowling. And let say you want a sequence model to automatically tell you where are the peoples names in this sentence. So, this is a problem called Named-entity recognition and this is used by search engines for example, to index all of say the last 24 hours news of all the people mentioned in the news articles so that they can index them appropriately. And name into the recognition systems can be used to find people’s names, companies names, times, locations, countries names, currency names, and so on in different types of text. Now, given this input x let’s say that you want a model to operate y that has one outputs per input word and the target output the design y tells you for each of the input words is that part of a person’s name. And technically this maybe isn’t the best output representation, there are some more sophisticated output representations that tells you not just is a word part of a person’s name, but tells you where are the start and ends of people’s names their sentence, you want to know Harry Potter starts here, and ends here, starts here, and ends here. But for this motivating example, I’m just going to stick with this simpler output representation. Now, the input is the sequence of nine words. So, eventually we’re going to have nine sets of features to represent these nine words, and index into the positions and sequence, I’m going to use X and then superscript angle brackets 1, 2, 3 and so on up to X angle brackets nine to index into the different positions. I’m going to use $X^{}$ with the index t to index into positions, in the middle of the sequence. And t implies that these are temporal sequences although whether the sequences are temporal one or not, I’m going to use the index t to index into the positions in the sequence. And similarly for the outputs, we’re going to refer to these outputs as y and go back at 1, 2, 3 and so on up to y nine. Let’s also used T sub of x to denote the length of the input sequence, so in this case there are nine words. So $T_x$ is equal to 9 and we used $T_y$ to denote the length of the output sequence. In this example $T_x$ is equal to $T_y$ but you saw on the last video $T_x$ and $T_y$ can be different. So, you will remember that in the notation we’ve been using, we’ve been writing X round brackets i to denote the i training example. So, to refer to the TIF element or the TIF element in the sequence of training example i will use this notation and if $T_x$ is the length of a sequence then different examples in your training set can have different lengths. And so $T_x^i$ would be the input sequence length for training example i, and similarly $y^{(i)}$ means the TIF element in the output sequence of the i for an example and $T_y^i$ will be the length of the output sequence in the i training example. So into this example, $T_x^i$ is equal to 9 would be the highly different training example with a sentence of 15 words and $T_x^i$ will be close to 15 for that different training example. Now, that we’re starting to work in NLP or Natural Language Processing. Now, this is our first serious foray into NLP or Natural Language Processing. And one of the things we need to decide is, how to represent individual words in the sequence. So, how do you represent a word like Harry, and why should $x^{}$ really be? Let’s next talk about how we would represent individual words in a sentence. So, to represent a word in the sentence the first thing you do is come up with a Vocabulary. Sometimes also called a Dictionary and that means making a list of the words that you will use in your representations. So the first word in the vocabulary is a, that will be the first word in the dictionary. The second word is Aaron and then a little bit further down is the word and, and then eventually you get to the words Harry then eventually the word Potter, and then all the way down to maybe the last word in dictionary is Zulu. And so, a will be word one, Aaron is word two, and in my dictionary the word and appears in positional index 367. Harry appears in position 4075, Potter in position 6830, and Zulu is the last word to the dictionary is maybe word 10,000. So in this example, I’m going to use a dictionary with size 10,000 words. This is quite small by modern NLP applications. For commercial applications, for visual size commercial applications, dictionary sizes of 30 to 50,000 are more common and 100,000 is not uncommon. And then some of the large Internet companies will use dictionary sizes that are maybe a million words or even bigger than that. But you see a lot of commercial applications used dictionary sizes of maybe 30,000 or maybe 50,000 words. But I’m going to use 10,000 for illustration since it’s a nice round number. So, if you have chosen a dictionary of 10,000 words and one way to build this dictionary will be be to look through your training sets and find the top 10,000 occurring words, also look through some of the online dictionaries that tells you what are the most common 10,000 words in the English Language saved. What you can do is then use one hot representations to represent each of these words. For example, $x^{}$ which represents the word Harry would be a vector with all zeros except for a 1 in position 4075 because that was the position of Harry in the dictionary. And then $x^{}$ will be again similarly a vector of all zeros except for a 1 in position 6830 and then zeros everywhere else. The word and was represented as position 367 so $x^{}$ would be a vector with zeros of 1 in position 367 and then zeros everywhere else. And each of these would be a 10,000 dimensional vector if your vocabulary has 10,000 words. And this one A, I guess because A is the first whether the dictionary, then $x^{}$ which corresponds to word a, that would be the vector 1. This is the first element of the dictionary and then zero everywhere else. So in this representation, $x^{}$ for each of the values of t in a sentence will be a one-hot vector, one-hot because there’s exactly one one is on and zero everywhere else and you will have nine of them to represent the nine words in this sentence. And the goal is given this representation for X to learn a mapping using a sequence model to then target output y, I will do this as a supervised learning problem, I’m sure given the table data with both x and y. Then just one last detail, which we’ll talk more about in a later video is, what if you encounter a word that is not in your vocabulary? Well the answer is, you create a new token or a new fake word called Unknown Word which under note as follows and go back as UNK to represent words not in your vocabulary, we’ll come more to talk more about this later. So, to summarize in this video, we described a notation for describing your training set for both x and y for sequence data. In the next video let’s start to describe a Recurrent Neural Networks for learning the mapping from X to Y. 03_recurrent-neural-network-modelIn the last video, you saw the notation we used to define sequence learning problems. Now, let’s talk about how you can build a model, build a neural network to drawing the mapping from X to Y. Now, one thing you could do is try to use a standard neural network for this task. So in our previous example, we had nine input words. So you could imagine trying to take these nine input words, maybe the nine one hot vectors and feeding them into a standard neural network, maybe a few hidden layers and then eventually, have this output the nine values zero or one that tell you whether each word is part of a person’s name. But this turns out not to work well, and there are really two main problems with this. The first is that the inputs and outputs can be different lengths in different examples. So it’s not as if every single example has the same input length $T^{}$ or the same output length $T^{}$. And maybe if every sentence had a maximum length, maybe you could pad, or zero pad every input up to that maximum length, but this still doesn’t seem like a good representation. And in a second, it might be more serious problem is that a naive neural network architecture like this, it doesn’t share features learned across different positions of techs. In particular, if the neural network has learned that maybe the word heavy appearing in position one gives a sign that that is part of a person’s name, then one would be nice if it automatically figures out that heavy appearing in some other position, $X^{}$ also means that that might be a person’s name. And this is maybe similar to what you saw in convolutional neural networks where you want things learned for one part of the image to generalize quickly to other parts of the image, and we’d like similar effect for sequence data as well. And similar to what you saw with ConvNets using a better representation will also let you reduce the number of parameters in your model. So previously, we said that each of these is a 10,000 dimensional one vector. And so, this is just a very large input layer. If the total input size was maximum number of words times 10,000, and the weight matrix of this first layer would end up having an enormous number of parameters. So a recurrent neural network which will start to describe in the next slide, does not have either of these disadvantages. So what is a recurrent neural network? Let’s build one out. So if you are reading the sentence from left to right, the first word you read is the some first where say X1. What we’re going to do is take the first word and feed it into a neural network layer. I’m going to draw it like this. So that’s a hidden layer of the first neural network. And look at how the neural network maybe try to predict the output. So is this part of a person’s name or not? And what a recurrent neural network does is when it then goes on to read the second word in a sentence, say X2, instead of just predicting Y2 using only X2, it also gets to input some information from whether a computer that time-step ones. So in particular, the activation value from time-step one is passed on to time-step 2. And then, at the next time-step, a recurrent neural network inputs the third word X3, and it tries to predict, output some prediction y-hat 3, and so on, up until the last time-step where inputs $X^{&lt;T_x&gt;}$, and then it outputs Y hat TY. In this example, Tx=Ty, and the architecture will change a bit if Tx and Ty are not identical. And so, at each time-step, the recurrent neural network passes on this activation to the next time-step for it to use. And to kick off the whole thing, we’ll also have some made up activation at time zero. This is usually the vector of zeroes. Some researchers will initialize a zero randomly have other ways to initialize a zero but really having a vector zero is just a fake. Time Zero activation is the most common choice. And so that does input into the neural network. In some research papers or in some books, you see this type of neural network drawn with the following diagram in which every time-step, you input X and output Y hat, maybe sometimes there will be a T index there, and then to denote the recurrent connection, sometimes people will draw a loop like that, that the layer feeds back to itself. Sometimes they’ll draw a shaded box to denote that this is the shaded box here, denotes a time delay of one step. I personally find these recurrent diagrams much harder to interpret. And so throughout this course, I will tend to draw the on the road diagram like the one you have on the left. But if you see something like the diagram on the right in a textbook or in a research paper, what it really means, or the way I tend to think about it is the mentally unrolled into the diagram you have on the left hand side. The recurrent neural network scans through the data from left to right. And the parameters it uses for each time step are shared. So there will be a set of parameters which we’ll describe in greater detail on the next slide, but the parameters governing the connection from X1 to the hidden layer will be some set of the parameters we’re going to write as WAX, and it’s the same parameters $W_{ax}$ that it uses for every time-step I guess you could write $W_{ax}$ there as well. And the activations, the horizontal connections, will be governed by some set of parameters $W_{aa}$, and is the same parameters $W_{aa}$ use on every time-step, and similarly, the sum $W_{ya}$ that governs the output predictions. And I’ll describe in the next slide exactly how these parameters work. So in this recurrent neural network, what this means is that we’re making the prediction for Y3 against the information not only from X3, but also the information from X1 and X2, because the information of X1 can pass through this way to help the prediction with Y3. Now one weakness of this RNN is that it only uses the information that is earlier in the sequence to make a prediction, in particular, when predicting Y3, it doesn’t use information about the words X4, X5, X6 and so on. And so this is a problem because if you’re given a sentence, he said, “Teddy Roosevelt was a great president.” In order to decide whether or not the word Teddy is part of a person’s name, it be really useful to know not just information from the first two words but to know information from the later words in the sentence as well, because the sentence could also happen, he said, “Teddy bears are on sale!” And so, given just the first three words, it’s not possible to know for sure whether the word Teddy is part of a person’s name. In the first example, it is, in the second example, is not, but you can’t tell the difference if you look only at the first three words. So one limitation of this particular neural network structure is that the prediction at a certain time uses inputs or uses information from the inputs earlier in the sequence but not information later in the sequence. We will address this in a later video where we talk about a bidirectional recurrent neural networks or BRNNs. But for now, this simpler uni-directional neural network architecture will suffice for us to explain the key concepts. And we just have to make a quick modifications in these ideas later to enable say the prediction of Y-hat 3 to use both information earlier in the sequence as well as information later in the sequence, but we’ll get to that in a later video. So let’s not write to explicitly what are the calculations that this neural network does. Here’s a cleaned out version of the picture of the neural network. As I mentioned previously, typically, you started off with the input a0 equals the vector of all zeroes. Next. This is what a forward propagation looks like. To compute a1, you would compute that as an activation function g, applied to Waa times a0 plus W a x times x1 plus a bias was going to write it as ba, and then to compute y hat 1 the prediction of times that one, that will be some activation function, maybe a different activation function, than the one above. But apply to WYA times a1 plus b y. And the notation convention I’m going to use for the sub zero of these matrices like that example, W a x. The second index means that this W a x is going to be multiplied by some x like quantity, and this means that this is used to compute some a like quantity. Like like so. And similarly, you notice that here WYA is multiplied by a sum a like quantity to compute a y type quantity. The activation function used in-to compute the activations will often be a tonnage and the choice of an RNN and sometimes, values are also used although the tonnage is actually a pretty common choice. And we have other ways of preventing the vanishing gradient problem which we’ll talk about later this week. And depending on what your output y is, if it is a binary classification problem, then I guess you would use a sigmoid activation function or it could be a soft Max if you have a ky classification problem. But the choice of activation function here would depend on what type of output y you have. So, for the name entity recognition task, where Y was either zero or one. I guess the second g could be a signal and activation function. And I guess you could write g2 if you want to distinguish that this is these could be different activation functions but I usually won’t do that. And then, more generally at time t, a t will be g of W a a times a, from the previous time-step, plus W a x of x from the current time-step plus B a, and y hat t is equal to g, again, it could be different activation functions but g of WYA times a t plus B y. So, these equations define for propagation in the neural network. Where you would start off with a zeroes [inaudible] and then using a zero and X1, you will compute a1 and y hat one, and then you, take X2 and use X2 and A1 to compute A2 and Y hat two and so on, and you carry out for propagation going from the left to the right of this picture. Now, in order to help us develop the more complex neural networks, I’m actually going to take this notation and simplify it a little bit. So, let me copy these two equations in the next slide. Right. Here they are, and what I’m going to do is actually take- so to simplify the notation a bit, I’m actually going to take that and write in a slightly simpler way. And someone very does this a = g times just a matrix $W_a$ times a new quantity is going to be $a^{}$ comma $x^{}$ and then, plus B a. And so, that underlining quantity on the left and right are supposed to be equivalent. So, the way we define $W_{a}$ is we’ll take this matrix $W_{aa}$ and this matrix $W_{ax}$. And put them side by side and stack them horizontally as follows. And this will be the matrix $W_{a}$. So for example, if a was a hundred dimensional, and then another example, X was 10,000 dimensional, then $W_{aa}$ would have been a 100 by 100 dimensional matrix and $W_{ax}$ would have been a 100 by 10,000 dimensional matrix. And so stacking these two matrices together this would be 100 dimensional. This would be 100, and this would be I guess 10,000 elements. So $W_{a}$ will be a 100 by one zero one zero zero zero dimensional matrix. I guess this diagram on the left is not drawn to scale. Since $W_{ax}$ would be a very wide matrix. And what this notation means, is to just take the two vectors, and stack them together. So, let me use that notation to denote that we’re going to take the vector $a^{}$. So there’s a 100 dimensional and stack it on top of $a^{}$. So this ends up being a one zero one zero zero dimensional vector. And so hopefully, you check for yourself that this matrix times this vector, just gives you back to the original quantity. Right. Because now, this matrix $W_{aa}$ times $W_{ax}$ multiplied by this $a^{}$ $x^{}$ vector, this is just equal to $W_{aa}$ times $a^{}$ plus $W_{ax}$ times x t which is exactly what we had back over here. So, the advantages of this notation is that rather than carrying around two parameter matrices $W_{aa}$ and $W_{ax}$, we can compress them into just one parameter matrix $W_{a}$. And this will simplify a notation for when we develop more complex models. And then, for this, in a similar way I’m just going to rewrite this slightly with the ranges as $W_y$ $a^{}$ plus $b_y$. And now, we just have the substrates in the notation $W_y$ and $b_y$, it denotes what type of output quantity over computing. So $W_y$ indicates that there’s a weight matrix of computing a y like quantity and here a Wa and ba on top. In the case of those the parameters of computing that an a and activation output quantity. So, that’s it. You now know, what is a basic recurrent network. Next, let’s talk about back propagation and how you learn with these RNNs. 04_backpropagation-through-timeYou’ve already learned about the basic structure of an RNN. In this video, you’ll see how backpropagation in a recurrent neural network works. As usual, when you implement this in one of the programming frameworks, often, the programming framework will automatically take care of backpropagation. But I think it’s still useful to have a rough sense of how backprop works in RNNs. Let’s take a look. You’ve seen how, for forward prop, you would computes these activations from left to right as follows in the neural network, and so you’ve outputs all of the predictions. In backprop, as you might already have guessed, you end up carrying backpropagation calculations in basically the opposite direction of the forward prop arrows. So, let’s go through the forward propagation calculation. You’re given this input sequence $x^{}, x^{}, x^{}$, up to $x^{&lt;T_x&gt;}$. And then using $x^{}$ and say, $a^{}$, you’re going to compute the activation, times that one, and then together, $x^{}$ together with $a^{}$ are used to compute $a^{}$, and then $a^{}$, and so on, up to $a^{&lt;T_x&gt;}$. All right. And then to actually compute $a^{}$, you also need the parameters. We’ll just draw this in green, $W_a$ and $b_a$, those are the parameters that are used to compute $a^{}$. And then, these parameters are actually used for every single timestep so, these parameters are actually used to compute $a^{}$, $a^{}$, and so on, all the activations up to last timestep depend on the parameters $W_a$ and $b_a$. Let’s keep fleshing out this graph. Now, given $a^{}$, your neural network can then compute the first prediction, $\hat{y}^{}$, and then the second timestep, $\hat{y}^{}$, $\hat{y}^{}$, and so on, with $\hat{y}^{&lt;T_y&gt;}$. And let me again draw the parameters of a different color. So, to compute $\hat{y}$, you need the parameters, $W_y$ as well as $b_y$, and this goes into this node as well as all the others. So, I’ll draw this in green as well. Next, in order to compute backpropagation, you need a loss function. So let’s define an element-wise loss force, which is supposed for a certain word in the sequence. It is a person’s name, so $y^{}$ is one. And your neural network outputs some probability of maybe 0.1 of the particular word being a person’s name. So I’m going to define this as the standard logistic regression loss, also called the cross entropy loss. This may look familiar to you from where we were previously looking at binary classification problems. So this is the loss associated with a single prediction at a single position or at a single time set, t, for a single word. Let’s now define the overall loss of the entire sequence, so L will be defined as the sum overall t equals one to, i guess, $T_x$ or $T_y$. $T_x$ is equals to $T_y$ in this example of the losses for the individual timesteps, comma $y^{}$. And then, so, just have to L without this superscript T. This is the loss for the entire sequence. So, in a computation graph, to compute the loss given $\hat{y}^{}$, you can then compute the loss for the first timestep given that you compute the loss for the second timestep, the loss for the third timestep, and so on, the loss for the final timestep. And then lastly, to compute the overall loss, we will take these and sum them all up to compute the final L using that equation, which is the sum of the individual per timestep losses. So, this is the computation problem and from the earlier examples you’ve seen of backpropagation, it shouldn’t surprise you that backprop then just requires doing computations or parsing messages in the opposite directions. So, all of the four propagation steps arrows, so you end up doing that. And that then, allows you to compute all the appropriate quantities that lets you then, take the riveters, respected parameters, and update the parameters using gradient descent. Now, in this back propagation procedure, the most significant message or the most significant recursive calculation is this one, which goes from right to left, and that’s why it gives this algorithm as well, a pretty fast full name called backpropagation through time. And the motivation for this name is that for forward prop, you are scanning from left to right, increasing indices of the time, t, whereas, the backpropagation, you’re going from right to left, you’re kind of going backwards in time. So this gives this, I think a really cool name, backpropagation through time, where you’re going backwards in time, right? That phrase really makes it sound like you need a time machine to implement this output, but I just thought that backprop through time is just one of the coolest names for an algorithm. So, I hope that gives you a sense of how forward prop and backprop in RNN works. Now, so far, you’ve only seen this main motivating example in RNN, in which the length of the input sequence was equal to the length of the output sequence. In the next video, I want to show you a much wider range of RNN architecture, so I’ll let you tackle a much wider set of applications. Let’s go on to the next video. 05_different-types-of-rnnsSo far, you’ve seen an RNN architecture where the number of inputs, Tx, is equal to the number of outputs, Ty. It turns out that for other applications, Tx and Ty may not always be the same, and in this video, you’ll see a much richer family of RNN architectures. You might remember this slide from the first video of this week, where the input x and the output y can be many different types. And it’s not always the case that $T_x$ has to be equal to $T_y$. In particular, in this example, $T_x$ can be length one or even an empty set. And then, an example like movie sentiment classification, the output y could be just an integer from 1 to 5, whereas the input is a sequence. And in name entity recognition, in the example we’re using, the input length and the output length are identical, but there are also some problems were the input length and the output length can be different.They’re both our sequences but have different lengths, such as machine translation where a French sentence and English sentence can mean two different numbers of words to say the same thing. So it turns out that we could modify the basic RNN architecture to address all of these problems. And the presentation in this video was inspired by a blog post by Andrej Karpathy, titled, The Unreasonable Effectiveness of Recurrent Neural Networks. Let’s go through some examples. The example you’ve seen so far use $T_x$ equals $T_y$, where we had an input sequence x(1), x(2) up to x(Tx), and we had a recurrent neural network that works as follows when we would input x(1) to compute y hat (1), y hat (2), and so on up to y hat (Ty), as follows. And in early diagrams, I was drawing a bunch of circles here to denote neurons but I’m just going to make those little circles for most of this video, just to make the notation simpler. So, this is what you might call a many-to-many architecture because the input sequence has many inputs as a sequence and the outputs sequence is also has many outputs. Now, let’s look at a different example. Let’s say, you want to address sentiments classification. Here, x might be a piece of text, such as it might be a movie review that says, “There is nothing to like in this movie.” So x is going to be sequenced, and y might be a number from 1 to 5, or maybe 0 or 1. This is a positive review or a negative review, or it could be a number from 1 to 5. Do you think this is a one-star, two-star, three, four, or five-star review? So in this case, we can simplify the neural network architecture as follows. I will input x(1), x(2). So, input the words one at a time. So if the input text was, “There is nothing to like in this movie.” So “There is nothing to like in this movie,” would be the input. And then rather than having to use an output at every single time-step, we can then just have the RNN read into entire sentence and have it output y at the last time-step when it has already input the entire sentence. So, this neural network would be a many-to-one architecture. Because as many inputs, it inputs many words and then it just outputs one number. For the sake of completeness, there is also a one-to-one architecture. So this one is maybe less interesting. The smaller the standard neural network, we have some input x and we just had some output y. And so, this would be the type of neural network that we covered in the first two courses in this sequence. Now, in addition to many-to-one, you can also have a one-to-many architecture. So an example of a one-to-many neural network architecture will be music generation. And in fact, you get to implement this yourself in one of the primary exercises for this course where you go is have a neural network, output a set of notes corresponding to a piece of music. And the input x could be maybe just an integer, telling it what genre of music you want or what is the first note of the music you want, and if you don’t want to input anything, x could be a null input, could always be the vector zeroes as well. For that, the neural network architecture would be your input x. And then, have your RNN output. The first value, and then, have that, with no further inputs, output. The second value and then go on to output. The third value, and so on, until you synthesize the last notes of the musical piece. If you want, you can have this input a(0) as well. One technical now what you see in the later video is that, when you’re actually generating sequences, often you take these first synthesized output and feed it to the next layer as well. So the network architecture actually ends up looking like that. So, we’ve talked about many-to- many, many-to-one, one-to-many, as well as one-to-one. It turns out there’s one more interesting example of many-to-many which is worth describing. Which is when the input and the output length are different. So, in the many-to-many example, you saw just now, the input length and the output length have to be exactly the same. For an application like machine translation, the number of words in the input sentence, say a French sentence, and the number of words in the output sentence, say the translation into English, those sentences could be different lengths. So here’s an alternative new network architecture where you might have a neural network, first, reading the sentence. So first, reading the input, say French sentence that you want to translate to English. And having done that, you then, have the neural network output the translation. As all those y hat of (Ty). And so, with this architecture, Tx and Ty can be different lengths. And again, you could draw on the a(0) that you want. And so, this that neural network architecture has two distinct parts. There’s the encoder which takes as input, say a French sentence, and then, there’s is a decoder, which having read in the sentence, outputs the translation into a different language. So this would be an example of a many-to-many architecture. So by the end of this week, you have a good understanding of all the components needed to build these types of architectures. And then, technically, there’s one other architecture which we’ll talk about only in week four, which is attention based architectures. Which maybe isn’t cleanly captured by one of the diagrams we’ve drawn so far. So, to summarize the wide range of RNN architectures, there is one-to-one, although if it’s one-to-one, we could just give it this, and this is just a standard generic neural network. Well, you don’t need an RNN for this. But there is one-to-many. So, this was a music generation or sequenced generation as example. And then, there’s many-to-one, that would be an example of sentiment classification. Where you might want to read as input all the text with a movie review. And then, try to figure out that they liked the movie or not. There is many-to-many, so the name entity recognition, the example we’ve been using, was this where $T_x$ is equal to $T_y$. And then, finally, there’s this other version of many-to-many, where for applications like machine translation, $T_x$ and $T_y$ no longer have to be the same. So, now you know most of the building blocks, the building are pretty much all of these neural networks except that there are some subtleties with sequence generation, which is what we’ll discuss in the next video. So, I hope you saw from this video that using the basic building blocks of an RNN, there’s already a wide range of models that you might be able put together. But as I mentioned, there are some subtleties to sequence generation, which you’ll get to implement yourself as well in this week’s primary exercise where you implement a language model and hopefully, generate some fun sequences or some fun pieces of text. So, what I want to do in the next video, is go deeper into sequence generation. Let’s see the details in the next video. 06_language-model-and-sequence-generationLanguage modeling is one of the most basic and important tasks in natural language processing. There’s also one that RNNs do very well. In this video, you learn about how to build a language model using an RNN, and this will lead up to a fun programming exercise at the end of this week. Where you build a language model and use it to generate Shakespeare-like texting, other types of text. Let’s get started. So what is a language model? Let’s say you’re building this speech recognition system and you hear the sentence, the apple and pear salad was delicious. So what did you just hear me say? Did I say the apple and pair salad, or did I say the apple and pear salad? You probably think the second sentence is much more likely, and in fact, that’s what a good speech recognition system would help with even though these two sentences sound exactly the same. And the way a speech recognition system picks the second sentence is by using a language model which tells it what the probability is of either of these two sentences. For example, a language model might say that the chance for the first sentence is 3.2 by 10 to the -13. And the chance of the second sentence is say 5.7 by 10 to the -10. And so, with these probabilities, the second sentence is much more likely by over a factor of 10 to the 3 compared to the first sentence. And that’s why speech recognition system will pick the second choice. So what a language model does is given any sentence is job is to tell you what is the probability of a sentence, of that particular sentence. And by probability of sentence I mean, if you want to pick up a random newspaper, open a random email or pick a random webpage or listen to the next thing someone says, the friend of you says. What is the chance that the next sentence you use somewhere out there in the world will be a particular sentence like the apple and pear salad? [COUGH] And this is a fundamental component for both speech recognition systems as you’ve just seen, as well as for machine translation systems where translation systems wants output only sentences that are likely. And so the basic job of a language model is to input a sentence, which I’m going to write as a sequence $y^{}$, $y^{}$ up to $y^{&lt;T_y&gt;}$. And for language model will be useful to represent a sentences as outputs y rather than inputs x. But what the language model does is it estimates the probability of that particular sequence of words. So how do you build a language model? To build such a model using an RNN you would first need a training set comprising a large corpus of english text. Or text from whatever language you want to build a language model of. And the word corpus is an NLP terminology that just means a large body or a very large set of english text of english sentences. So let’s say you get a sentence in your training set as follows. Cats average 15 hours of sleep a day. The first thing you would do is tokenize this sentence. And that means you would form a vocabulary as we saw in an earlier video. And then map each of these words to, say, one hot vectors, alter indices in your vocabulary. One thing you might also want to do is model when sentences end. So another common thing to do is to add an extra token called a EOS. That stands for End Of Sentence that can help you figure out when a sentence ends. We’ll talk more about this later, but the EOS token can be appended to the end of every sentence in your training sets if you want your models explicitly capture when sentences end. We won’t use the end of sentence token for the programming exercise at the end of this week where for some applications, you might want to use this. And we’ll see later where this comes in handy. So in this example, we have y1, y2, y3, 4, 5, 6, 7, 8, 9. Nine inputs in this example if you append the end of sentence token to the end. And doing the tokenization step, you can decide whether or not the period should be a token as well. In this example, I’m just ignoring punctuation. So I’m just using day as another token. And omitting the period, if you want to treat the period or other punctuation as explicit token, then you can add the period to you vocabulary as well. Now, one other detail would be what if some of the words in your training set, are not in your vocabulary. So if your vocabulary uses 10,000 words, maybe the 10,000 most common words in English, then the term Mau as in the Egyptian Mau is a breed of cat, that might not be in one of your top 10,000 tokens. So in that case you could take the word Mau and replace it with a unique token called UNK or stands for unknown words and would just model, the chance of the unknown word instead of the specific word now. Having carried out the tokenization step which basically means taking the input sentence and mapping out to the individual tokens or the individual words in your vocabulary. Next let’s build an RNN to model the chance of these different sequences. And one of the things we’ll see on the next slide is that you end up setting the inputs x = y or you see that in a little bit. So let’s go on to built the RNN model and I’m going to continue to use this sentence as the running example. This will be an RNN architecture. At time 0 you’re going to end up computing some activation $a^{}$ as a function of some inputs $x^{}$, and $x^{}$ will just be set it to the set of all zeroes, to 0 vector. And the previous $a^{}$, by convention, also set that to vector zeroes. But what $a^{}$ does is it will make a soft max prediction to try to figure out what is the probability of the first words y. And so that’s going to be $y^{}$. So what this step does is really, it has a soft max it’s trying to predict. What is the probability of any word in the dictionary? That the first one is a, what’s the chance that the first word is Aaron? And then what’s the chance that the first word is cats? All the way to what’s the chance the first word is Zulu? Or what’s the first chance that the first word is an unknown word? Or what’s the first chance that the first word is the in the sentence they’ll have, shouldn’t have to read? Right, so $\hat{y}^{}$ is output to a softmax, it just predicts what’s the chance of the first word being, whatever it ends up being. And in our example, it wind up being the word cats, so this would be a 10,000 way soft max output, if you have a 10,000-word vocabulary. Or 10,002, I guess you could call unknown word and the sentence is two additional tokens. Then, the RNN steps forward to the next step and has some activation, $a^{}$ to the next step. And at this step, his job is try figure out, what is the second word? But now we will also give it the correct first word. So we’ll tell it that, gee, in reality, the first word was actually Cats so that’s $y^{}$. So tell it cats, and this is why $y^{} = x^{}$, and so at the second step the output is again predicted by a soft max. The RNN’s jobs to predict was the chance of a being whatever the word it is. Is it a or Aaron, or Cats or Zulu or unknown whether EOS or whatever given what had come previously. So in this case, I guess the right answer was average since the sentence starts with cats average. And then you go on to the next step of the RNN. Where you now compute $a^{}$. But to predict what is the third word, which is 15, we can now give it the first two words. So we’re going to tell it cats average are the first two words. So this next input here, $x^{} = y^{}$, so the word average is input, and this job is to figure out what is the next word in the sequence. So in other words trying to figure out what is the probability of anywhere than dictionary given that what just came before was cats. Average, right? And in this case, the right answer is 15 and so on. Until at the end, you end up at, I guess, time step 9, you end up feeding it $x^{}$, which is equal to $y^{}$, which is the word, day. And then this has $a^{}$, and its jpob iws to output $\hat{y}^{}$, and this happens to be the EOS token. So what’s the chance of whatever this given, everything that comes before, and hopefully it will predict that there’s a high chance of it, EOS and the sentence token. So each step in the RNN will look at some set of preceding words such as, given the first three words, what is the distribution over the next word? And so this RNN learns to predict one word at a time going from left to right. Next to train us to a network, we’re going to define the cos function. So, at a certain time, t, if the true word was yt and the new networks soft max predicted some $\hat{y}^{}$, then this is the soft max loss function that you should already be familiar with. And then the overall loss is just the sum overall time steps of the loss associated with the individual predictions. And if you train this RNN on the last training set, what you’ll be able to do is given any initial set of words, such as cats average 15 hours of, it can predict what is the chance of the next word. And given a new sentence say, $y^{}$, $y^{}$, $y^{}$with just a three words, for simplicity, the way you can figure out what is the chance of this entire sentence would be. Well, the first soft max tells you what’s the chance of $y^{}$. That would be this first output. And then the second one can tell you what’s the chance of p of $y^{}$ given $y^{}$. And then the third one tells you what’s the chance of $y^{}$ given $y^{}$ and $y^{}$. And so by multiplying out these three probabilities. And you’ll see much more details of this in the previous exercise. By multiply these three, you end up with the probability of the three sentence, of the three-word sentence. So that’s the basic structure of how you can train a language model using an RNN. If some of these ideas still seem a little bit abstract, don’t worry about it, you get to practice all of these ideas in their program exercise. But next it turns out one of the most fun things you could do with a language model is to sample sequences from the model. Let’s take a look at that in the next video. 07_sampling-novel-sequencesAfter you train a sequence model, one of the ways you can informally get a sense of what is learned is to have a sample novel sequences. Let’s take a look at how you could do that. So remember that a sequence model, models the chance of any particular sequence of words as follows, and so what we like to do is sample from this distribution to generate novel sequences of words. So the network was trained using this structure shown at the top. But to sample, you do something slightly different, so what you want to do is first sample what is the first word you want your model to generate. And so for that you input the usual $x^{}$ equals 0, $a^{}$ equals 0. And now your first time stamp will have some max probability over possible outputs. So what you do is you then randomly sample according to this softmax distribution. So what the soft max distribution gives you is it tells you what is the chance that it refers to this a, what is the chance that it refers to this Aaron? What’s the chance it refers to Zulu, what is the chance that the first word is the Unknown word token. Maybe it was a chance it was a end of sentence token. And then you take this vector and use, for example, the numpy command np.random.choice to sample according to distribution defined by this vector probabilities, and that lets you sample the first words. Next you then go on to the second time step, and now remember that the second time step is expecting this $\hat{y}^{}$ as input. But what you do is you then take the $\hat{y}^{}$ that you just sampled and pass that in here as the input to the next timestep. So whatever works, you just chose the first time step passes this input in the second position, and then this softmax will make a prediction for what is $\hat{y}^{}$. Example, let’s say that after you sample the first word, the first word happened to be “The”, which is very common choice of first word. Then you pass in “The” as $x^{}$, which is now equal to $\hat{y}^{}$. And now you’re trying to figure out what is the chance of what the second word is given that the first word is d. And this is going to be $\hat{y}^{}$. Then you again use this type of sampling function to sample $\hat{y}^{}$. And then at the next time stamp, you take whatever choice you had represented say as a one hard encoding. And pass that to next timestep and then you sample the third word to that whatever you chose, and you keep going until you get to the last time step. And so how do you know when the sequence ends? Well, one thing you could do is if the end of sentence token is part of your vocabulary, you could keep sampling until you generate an EOS token. And that tells you you’ve hit the end of a sentence and you can stop. Or alternatively, if you do not include this in your vocabulary then you can also just decide to sample 20 words or 100 words or something, and then keep going until you’ve reached that number of time steps. And this particular procedure will sometimes generate an unknown word token. If you want to make sure that your algorithm never generates this token, one thing you could do is just reject any sample that came out as unknown word token and just keep resampling from the rest of the vocabulary until you get a word that’s not an unknown word. Or you can just leave it in the output as well if you don’t mind having an unknown word output. So this is how you would generate a randomly chosen sentence from your RNN language model. Now, so far we’ve been building a words level RNN, by which I mean the vocabulary are words from English. Depending on your application, one thing you can do is also build a character level RNN. So in this case your vocabulary will just be the alphabets. Up to z, and as well as maybe space, punctuation if you wish, the digits 0 to 9. And if you want to distinguish the uppercase and lowercase, you can include the uppercase alphabets as well, and one thing you can do as you just look at your training set and look at the characters that appears there and use that to define the vocabulary. And if you build a character level language model rather than a word level language model, then your sequence $y^{}, y^{}, y^{}$, would be the individual characters in your training data, rather than the individual words in your training data. So for our previous example, the sentence cats average 15 hours of sleep a day. In this example, c would be $y^{}$, a would be $y^{}$, t will be $y^{}$, the space will be $y^{}$ and so on. Using a character level language model has some pros and cons. One is that you don’t ever have to worry about unknown word tokens. In particular, a character level language model is able to assign a sequence like mau, a non-zero probability. Whereas if mau was not in your vocabulary for the word level language model, you just have to assign it the unknown word token. But the main disadvantage of the character level language model is that you end up with much longer sequences. So many english sentences will have 10 to 20 words but may have many, many dozens of characters. And so character language models are not as good as word level language models at capturing long range dependencies between how the the earlier parts of the sentence also affect the later part of the sentence. And character level models are also just more computationally expensive to train. So the trend I’ve been seeing in natural language processing is that for the most part, word level language model are still used, but as computers gets faster there are more and more applications where people are, at least in some special cases, starting to look at more character level models. But they tend to be much hardware, much more computationally expensive to train, so they are not in widespread use today. Except for maybe specialized applications where you might need to deal with unknown words or other vocabulary words a lot. Or they are also used in more specialized applications where you have a more specialized vocabulary. So under these methods, what you can now do is build an RNN to look at the purpose of English text, build a word level, build a character language model, sample from the language model that you’ve trained. So here are some examples of text thatwere examples from a language model, actually from a culture level language model. And you get to implement something like this yourself in the programming exercise. If the model was trained on news articles, then it generates texts like that shown on the left. And this looks vaguely like news text, not quite grammatical, but maybe sounds a little bit like things that could be appearing news, concussion epidemic to be examined. And it was trained on Shakespearean text and then it generates stuff that sounds like Shakespeare could have written it. The mortal moon hath her eclipse in love. And subject of this thou art another this fold. When besser be my love to me see sabl’s. For whose are ruse of mine eyes heaves. So that’s it for the basic RNN, and how you can build a language model using it, as well as sample from the language model that you’ve trained. In the next few videos, I want to discuss further some of the challenges of training RNNs, as well as how to adjust some of these challenges, specifically vanishing gradients by building even more powerful models of the RNN. So in the next video let’s talk about the problem of vanishing the gradient and we will go on to talk about the GRU, Gate Recurring Unit as well as the LSTM models. 08_vanishing-gradients-with-rnnsYou’ve learned about how RNNs work and how they can be applied to problems like name entity recognition, as well as to language modeling, and you saw how backpropagation can be used to train in RNN. It turns out that one of the problems with a basic RNN algorithm is that it runs into vanishing gradient problems. Let’s discuss that, and then in the next few videos, we’ll talk about some solutions that will help to address this problem. So, you’ve seen pictures of RNNS that look like this. And let’s take a language modeling example. Let’s say you see this sentence, “The cat which already ate and maybe already ate a bunch of food that was delicious dot, dot, dot, dot, was full.” And so, to be consistent, just because cat is singular, it should be the cat was, were then was, “The cats which already ate a bunch of food was delicious, and apples, and pears, and so on, were full.” So to be consistent, it should be cat was or cats were. And this is one example of when language can have very long-term dependencies, where it worked at this much earlier can affect what needs to come much later in the sentence. But it turns out the basics RNN we’ve seen so far it’s not very good at capturing very long-term dependencies. To explain why, you might remember from our early discussions of training very deep neural networks, that we talked about the vanishing gradients problem. So this is a very, very deep neural network say, 100 layers or even much deeper than you would carry out forward prop, from left to right and then back prop. And we said that, if this is a very deep neural network, then the gradient from just output y, would have a very hard time propagating back to affect the weights of these earlier layers, to affect the computations in the earlier layers. And for an RNN with a similar problem, you have forward prop came from left to right, and then back prop, going from right to left. And it can be quite difficult, because of the same vanishing gradients problem, for the outputs of the errors associated with the later time steps to affect the computations that are earlier. And so in practice, what this means is, it might be difficult to get a neural network to realize that it needs to memorize the just see a singular noun or a plural noun, so that later on in the sequence that can generate either was or were, depending on whether it was singular or plural. And notice that in English, this stuff in the middle could be arbitrarily long, right? So you might need to memorize the singular/plural for a very long time before you get to use that bit of information. So because of this problem, the basic RNN model has many local influences, meaning that the output $y^{}$ is mainly influenced by values close to $y^{}$. And a value here is mainly influenced by inputs that are somewhere close. And it’s difficult for the output here to be strongly influenced by an input that was very early in the sequence. And this is because whatever the output is, whether this got it right, this got it wrong, it’s just very difficult for the area to backpropagate all the way to the beginning of the sequence, and therefore to modify how the neural network is doing computations earlier in the sequence. So this is a weakness of the basic RNN algorithm. One, which was not addressed in the next few videos. But if we don’t address it, then RNNs tend not to be very good at capturing long-range dependencies. And even though this discussion has focused on vanishing gradients, you will remember when we talked about very deep neural networks, that we also talked about exploding gradients. We’re doing back prop, the gradients should not just decrease exponentially, they may also increase exponentially with the number of layers you go through. It turns out that vanishing gradients tends to be the bigger problem with training RNNs, although when exploding gradients happens, it can be catastrophic because the exponentially large gradients can cause your parameters to become so large that your neural network parameters get really messed up. So it turns out that exploding gradients are easier to spot because the parameters just blow up and you might often see NaNs, or not a numbers, meaning results of a numerical overflow in your neural network computation. And if you do see exploding gradients, one solution to that is apply gradient clipping. And what that really means, all that means is look at your gradient vectors, and if it is bigger than some threshold, re-scale some of your gradient vector so that is not too big. So there are clips according to some maximum value. So if you see exploding gradients, if your derivatives do explode or you see NaNs, just apply gradient clipping, and that’s a relatively robust solution that will take care of exploding gradients. But vanishing gradients is much harder to solve and it will be the subject of the next few videos. So to summarize, in an earlier course, you saw how the training of very deep neural network, you can run into a vanishing gradient or exploding gradient problems with the derivative, either decreases exponentially or grows exponentially as a function of the number of layers. And in RNN, say in RNN processing data over a thousand times sets, over 10,000 times sets, that’s basically a 1,000 layer or they go 10,000 layer neural network, and so, it too runs into these types of problems. Exploding gradients, you could sort of address by just using gradient clipping, but vanishing gradients will take more work to address. So what we do in the next video is talk about GRU, the greater recurrent units, which is a very effective solution for addressing the vanishing gradient problem and will allow your neural network to capture much longer range dependencies. So, lets go on to the next video. 09_gated-recurrent-unit-gruYou’ve seen how a basic RNN works. In this video, you learn about the Gated Recurrent Unit which is a modification to the RNN hidden layer that makes it much better capturing long range connections and helps a lot with the vanishing gradient problems. Let’s take a look. You’ve already seen the formula for computing the activations at time t of RNN. It’s the activation function applied to the parameter Wa times the activations in the previous time set, the current input and then plus ba. So I’m going to draw this as a picture. So the RNN unit, I’m going to draw as a picture, drawn as a box which inputs a of t-1, the activation for the last time-step. And also inputs x and these two go together. And after some weights and after this type of linear calculation, if g is a tanh activation function, then after the tanh, it computes the output activation a. And the output activation a(t) might also be passed to say a softener unit or something that could then be used to output y. So this is maybe a visualization of the RNN unit of the hidden layer of the RNN in terms of a picture. And I want to show you this picture because we’re going to use a similar picture to explain the GRU or the Gated Recurrent Unit. Lots of the idea of GRU were due to these two papers respectively by Yu Young Chang, Kagawa, Gaza Hera, Chang Hung Chu and Jose Banjo. And I’m sometimes going to refer to this sentence which we’d seen in the last video to motivate that. Given a sentence like this, you might need to remember the cat was singular, to make sure you understand why that was rather than were. So the cat was for or the cats were for. So as we read in this sentence from left to right, the GRU unit is going to have a new variable called c, which stands for cell, for memory cell. And what the memory cell do is it will provide a bit of memory to remember, for example, whether cat was singular or plural, so that when it gets much further into the sentence it can still work under consideration whether the subject of the sentence was singular or plural. And so at time t the memory cell will have some value c of t. And what we see is that the GRU unit will actually output an activation value a of t that’s equal to c of t. And for now I wanted to use different symbol c and a to denote the memory cell value and the output activation value, even though they are the same. I’m using this notation because when we talk about LSTMs, a little bit later, these will be two different values. But for now, for the GRU, c of t is equal to the output activation a of t. So these are the equations that govern the computations of a GRU unit. And every time-step, we’re going to consider overwriting the memory cell with a value c tilde of t. So this is going to be a candidate for replacing c of t. And we’re going to compute this using an activation function tanh of Wc. And so that’s the parameter to make sure it’s Wc and we’ll plus this parameter matrix, the previous value of the memory cell, the activation value as well as the current input value $x^{}$, and then plus the bias. So c tilde of t is going to be a candidate for replacing $c^{}$. And then the key, really the important idea of the GRU it will be that we have a gate. So the gate, I’m going to call gamma u. This is the capital Greek alphabet gamma subscript u, and u stands for update gate, and this will be a value between zero and one. And to develop your intuition about how GRUs work, think of gamma u, this gate value, as being always zero or one. Although in practice, your compute it with a sigmoid function applied to this. So remember that the sigmoid function looks like this. And so it’s value is always between zero and one. And for most of the possible ranges of the input, the sigmoid function is either very, very close to zero or very, very close to one. So for intuition, think of gamma as being either zero or one most of the time. And this alphabet u stands for- I chose the alphabet gamma for this because if you look at a gate fence, looks a bit like this I guess, then there are a lot of gammas in this fence. So that’s why gamma u, we’re going to use to denote the gate. Also Greek alphabet G, right. G for gate. So G for gamma and G for gate. And then next, the key part of the GRU is this equation which is that we have come up with a candidate where we’re thinking of updating c using c tilde, and then the gate will decide whether or not we actually update it. And so the way to think about it is maybe this memory cell c is going to be set to either zero or one depending on whether the word you are considering, really the subject of the sentence is singular or plural. So because it’s singular, let’s say that we set this to one. And if it was plural, maybe we would set this to zero, and then the GRU unit would memorize the value of the $c^{}$ all the way until here, where this is still equal to one and so that tells it, oh, it’s singular so use the choice was. And the job of the gate, of gamma u, is to decide when do you update these values. In particular, when you see the phrase, the cat, you know they you’re talking about a new concept the especially subject of the sentence cat. So that would be a good time to update this bit and then maybe when you’re done using it, the cat blah blah blah was full, then you know, okay, I don’t need to memorize anymore, I can just forget that. So the specific equation we’ll use for the GRU is the following. Which is that the actual value of $c^{}$ will be equal to this gate times the candidate value plus one minus the gate times the old value, $c^{}$. So you notice that if the gate, if this update value, this equal to one, then it’s saying set the new value of $c^{}$ equal to this candidate value. So that’s like over here, set gate equal to one so go ahead and update that bit. And then for all of these values in the middle, you should have the gate equals zero. So this is saying don’t update it, don’t update it, don’t update it, just hang onto the old value. Because if gamma u is equal to zero, then this would be zero, and this would be one. And so it’s just setting $c^{}$ equal to the old value, even as you scan the sentence from left to right. So when the gate is equal to zero, we’re saying don’t update it, don’t update it, just hang on to the value and don’t forget what this value was. And so that way even when you get all the way down here, hopefully you’ve just been setting $c^{}$ equals $c^{}$ all along. And it still memorizes, the cat was singular. So let me also draw a picture to denote the GRU unit. And by the way, when you look in online blog posts and textbooks and tutorials these types of pictures are quite popular for explaining GRUs as well as we’ll see later, LSTM units. I personally find the equations easier to understand in a pictures. So if the picture doesn’t make sense. Don’t worry about it, but I’ll just draw in case helps some of you. So a GRU unit inputs $c^{}$, for the previous time-step and just happens to be equal to 80 minus one. So take that as input and then it also takes as input $x^{}$, then these two things get combined together. And with some appropriate weighting and some tanh, this gives you c tilde t which is a candidate for placing $c^{}$, and then with a different set of parameters and through a sigmoid activation function, this gives you gamma u, which is the update gate. And then finally, all of these things combine together through another operation. And I won’t write out the formula, but this box here which wish I shaded in purple represents this equation which we had down there. So that’s what this purple operation represents. And it takes as input the gate value, the candidate new value, or there is this gate value again and the old value for $c^{}$, right. So it takes as input this, this and this and together they generate the new value for the memory cell. And so that’s $c^{}$ equals a. And if you wish you could also use this process to soft max or something to make some prediction for $y^{}$. So that is the GRU unit or at least a slightly simplified version of it. And what is remarkably good at is through the gates deciding that when you’re scanning the sentence from left to right say, that’s a good time to update one particular memory cell and then to not change, not change it until you get to the point where you really need it to use this memory cell that is set even earlier in the sentence. And because the sigmoid value, now, because the gate is quite easy to set to zero right. So long as this quantity is a large negative value, then up to numerical around off the uptake gate will be essentially zero. Very, very, very close to zero. So when that’s the case, then this updated equation and subsetting $c^{}$ equals $c^{}$. And so this is very good at maintaining the value for the cell. And because gamma can be so close to zero, can be 0.000001 or even smaller than that, it doesn’t suffer from much of a vanishing gradient problem. Because when you say gamma so close to zero this becomes essentially $c^{}$ equals $c^{}$ and the value of $c^{}$ is maintained pretty much exactly even across many many many many time-steps. So this can help significantly with the vanishing gradient problem and therefore allow a neural network to go on even very long range dependencies, such as a cat and was related even if they’re separated by a lot of words in the middle. Now I just want to talk over some more details of how you implement this. In the equations I’ve written, $c^{}$ can be a vector. So if you have 100 dimensional or hidden activation value then $c^{}$ can be a 100 dimensional say. And so $\tilde{c}^{}$ would also be the same dimension, and gamma would also be the same dimension as the other things on drawing boxes. And in that case, these asterisks are actually element wise multiplication. So here if gamma u, if the gate is 100 dimensional vector, what it is really a 100 dimensional vector of bits, the value is mostly zero and one. That tells you of this 100 dimensional memory cell which are the bits you want to update. And, of course, in practice gamma won’t be exactly zero or one. Sometimes it takes values in the middle as well but it is convenient for intuition to think of it as mostly taking on values that are exactly zero, pretty much exactly zero or pretty much exactly one. And what these element wise multiplications do is it just element wise tells the GRU unit which other bits in your- It just tells your GRU which are the dimensions of your memory cell vector to update at every time-step. So you can choose to keep some bits constant while updating other bits. So, for example, maybe you use one bit to remember the singular or plural cat and maybe use some other bits to realize that you’re talking about food. And so because you’re talk about eating and talk about food, then you’d expect to talk about whether the cat is four letter, right. You can use different bits and change only a subset of the bits every point in time. You now understand the most important ideas of the GRU. What I’m presenting in this slide is actually a slightly simplified GRU unit. Let me describe the full GRU unit. So to do that, let me copy the three main equations. This one, this one and this one to the next slide. So here they are. And for the full GRU unit, I’m sure to make one change to this which is, for the first equation which was calculating the candidate new value for the memory cell, I’m going just to add one term. Let me pushed that a little bit to the right, and I’m going to add one more gate. So this is another gate $\Gamma_r$. You can think of r as standing for relevance. So this gate $\Gamma_r$ tells you how relevant is $c^{}$ to computing the next candidate for $c^{}$. And this gate $\Gamma_r$ is computed pretty much as you’d expect with a new parameter matrix $W_r$, and then the same things as input $x^{}$ plus $b_r$. So as you can imagine there are multiple ways to design these types of neural networks. And why do we have $\Gamma_r$ ? Why not use a simpler version from the previous slides? So it turns out that over many years researchers have experimented with many, many different possible versions of how to design these units, to try to have longer range connections, to try to have more the longer range effects and also address vanishing gradient problems. And the GRU is one of the most commonly used versions that researchers have converged to and found as robust and useful for many different problems. If you wish you could try to invent new versions of these units if you want, but the GRU is a standard one, that’s just common used. Although you can imagine that researchers have tried other versions that are similar but not exactly the same as what I’m writing down here as well. And the other common version is called an LSTM which stands for Long Short Term Memory which we’ll talk about in the next video. But GRUs and LSTMs are two specific instantiations of this set of ideas that are most commonly used. Just one note on notation. I tried to define a consistent notation to make these ideas easier to understand. If you look at the academic literature, you sometimes see people- If you look at the academic literature sometimes you see people using alternative notation to be $\tilde{x}$, u, r and h to refer to these quantities as well. But I try to use a more consistent notation between GRUs and LSTMs as well as using a more consistent notation gamma to refer to the gates, so hopefully make these ideas easier to understand. So that’s it for the GRU, for the Gate Recurrent Unit. This is one of the ideas in RNN that has enabled them to become much better at capturing very long range dependencies has made RNN much more effective. Next, as I briefly mentioned, the other most commonly used variation of this class of idea is something called the LSTM unit, Long Short Term Memory unit. Let’s take a look at that in the next video. 10_long-short-term-memory-lstmIn the last video, you learned about the GRU, the gated recurrent units, and how that can allow you to learn very long range connections in a sequence. The other type of unit that allows you to do this very well is the LSTM or the long short term memory units, and this is even more powerful than the GRU. Let’s take a look. Here are the equations from the previous video for the GRU. And for the GRU, we had $a^{}$ equals $c^{}$, and two gates, the optic gate and the relevance gate, $\tilde{c}^{}$, which is a candidate for replacing the memory cell, and then we use the update gate, $\Gamma_u$, to decide whether or not to update $c^{}$ using $\tilde{c}^{}$. The LSTM is an even slightly more powerful and more general version of the GRU, and is due to Sepp Hochreiter and Jurgen Schmidhuber. And this was a really seminal paper, a huge impact on sequence modelling. I think this paper is one of the more difficult to read. It goes quite along into theory of vanishing gradients. And so, I think more people have learned about the details of LSTM through maybe other places than from this particular paper even though I think this paper has had a wonderful impact on the Deep Learning community. But these are the equations that govern the LSTM. So, the book continued to the memory cell, c, and the candidate value for updating it, $\tilde{c}^{}$, will be this, and so on. Notice that for the LSTM, we will no longer have the case that $a^{}$ is equal to $c^{}$. So, this is what we use. And so, this is just like the equation on the left except that with now, more specially use $a^{}$ there or $a^{}$ instead of $c^{}$. And we’re not using this gamma or this relevance gate. Although you could have a variation of the LSTM where you put that back in, but with the more common version of the LSTM, doesn’t bother with that. And then we will have an update gate, same as before. So, W updates and we’re going to use $a^{}$ here, $x^{}$ plus $b_u$. And one new property of the LSTM is, instead of having one update gate control, both of these terms, we’re going to have two separate terms. So instead of $\Gamma_u$ and one minus $\Gamma_u$, we’re going have $\Gamma_u$ here. And forget gate, which we’re going to call $\Gamma_f$. So, this gate, $\Gamma_f$, is going to be sigmoid of pretty much what you’d expect, $x^{}$ plus $b_f$. And then, we’re going to have a new output gate which is sigma of $W_o$. And then again, pretty much what you’d expect, plus $b_o$. And then, the update value to the memory so will be $c^{}$ equals $\Gamma_u$. And this asterisk denotes element-wise multiplication. This is a vector-vector element-wise multiplication, plus, and instead of one minus $\Gamma_u$, we’re going to have a separate forget gate, $\Gamma_f$, times c of t minus one. So this gives the memory cell the option of keeping the old value $c^{}$ minus one and then just adding to it, this new value, $\tilde{c}^{}$. So, use a separate update and forget gates. So, this stands for update, forget, and output gate. And then finally, instead of $a^{}$ equals $c^{}$ $a^{}$ is $a^{}$ equal to the output gate element-wise multiplied by $c^{}$. So, these are the equations that govern the LSTM and you can tell it has three gates instead of two. So, it’s a bit more complicated and it places the gates into slightly different places. So, here again are the equations governing the behavior of the LSTM. Once again, it’s traditional to explain these things using pictures. So let me draw one here. And if these pictures are too complicated, don’t worry about it. I personally find the equations easier to understand than the picture. But I’ll just show the picture here for the intuitions it conveys. The bigger picture here was very much inspired by a blog post due to Chris Ola, title, Understanding LSTM Network, and the diagram drawing here is quite similar to one that he drew in his blog post. But the key thing is to take away from this picture are maybe that you use $a^{}$ and $x^{}$ to compute all the gate values. In this picture, you have $a^{}$, $x^{}$ coming together to compute the forget gate, to compute the update gates, and to compute output gate. And they also go through a tanh to compute $\tilde{c}^{}$. And then these values are combined in these complicated ways with element-wise multiplies and so on, to get $c^{}$ from the previous $c^{}$. Now, one element of this is interesting as you have a bunch of these in parallel. So, that’s one of them and you connect them. You then connect these temporally. So it does the input $x^{}$ then $x^{}$, $x^{}$. So, you can take these units and just hold them up as follows, where the output a at the previous timestep is the input a at the next timestep, the c. I’ve simplified to diagrams a little bit in the bottom. And one cool thing about this you’ll notice is that there’s this line at the top that shows how, so long as you set the forget and the update gate appropriately, it is relatively easy for the LSTM to have some value $c^{}$ and have that be passed all the way to the right to have your, maybe, $c^{}$ equals $c^{}$. And this is why the LSTM, as well as the GRU, is very good at memorizing certain values even for a long time, for certain real values stored in the memory cell even for many, many timesteps. So, that’s it for the LSTM. As you can imagine, there are also a few variations on this that people use. Perhaps, the most common one is that instead of just having the gate values be dependent only on $a^{}$, $x^{}$, sometimes, people also sneak in there the values $c^{}$ as well. This is called a peephole connection. Not a great name maybe but you’ll see, peephole connection. What that means is that the gate values may depend not just on $a^{}$ and on $x^{}$, but also on the previous memory cell value, and the peephole connection can go into all three of these gates’ computations. So that’s one common variation you see of LSTMs. One technical detail is that these are, say, 100-dimensional vectors. So if you have a 100-dimensional hidden memory cell unit, and so is this. And the, say, fifth element of $c^{}$ affects only the fifth element of the corresponding gates, so that relationship is one-to-one, where not every element of the 100-dimensional $c^{}$ can affect all elements of the case. But instead, the first element of $c^{}$ affects the first element of the case, second element affects the second element, and so on. But if you ever read the paper and see someone talk about the peephole connection, that’s when they mean that $c^{}$ is used to affect the gate value as well. So, that’s it for the LSTM. When should you use a GRU? And when should you use an LSTM? There isn’t widespread consensus in this. And even though I presented GRUs first, in the history of deep learning, LSTMs actually came much earlier, and then GRUs were relatively recent invention that were maybe derived as Pavia’s simplification of the more complicated LSTM model. Researchers have tried both of these models on many different problems, and on different problems, different algorithms will win out. So, there isn’t a universally-superior algorithm which is why I want to show you both of them. But I feel like when I am using these, the advantage of the GRU is that it’s a simpler model and so it is actually easier to build a much bigger network, it only has two gates, so computationally, it runs a bit faster. So, it scales the building somewhat bigger models but the LSTM is more powerful and more effective since it has three gates instead of two. If you want to pick one to use, I think LSTM has been the historically more proven choice. So, if you had to pick one, I think most people today will still use the LSTM as the default first thing to try. Although, I think in the last few years, GRUs had been gaining a lot of momentum and I feel like more and more teams are also using GRUs because they’re a bit simpler but often work just as well. It might be easier to scale them to even bigger problems. So, that’s it for LSTMs. Well, either GRUs or LSTMs, you’ll be able to build neural network that can capture a much longer range dependancy. 11_bidirectional-rnnBy now, you’ve seen most of the cheap building blocks of RNNs. But, there are just two more ideas that let you build much more powerful models. One is bidirectional RNNs, which lets you at a point in time to take information from both earlier and later in the sequence, so we’ll talk about that in this video. And second, is deep RNNs, which you’ll see in the next video. So let’s start with Bidirectional RNNs. So, to motivate bidirectional RNNs, let’s look at this network which you’ve seen a few times before in the context of named entity recognition. And one of the problems of this network is that, to figure out whether the third word Teddy is a part of the person’s name, it’s not enough to just look at the first part of the sentence. So to tell, if Y three should be zero or one, you need more information than just the first three words because the first three words doesn’t tell you if they’ll talking about Teddy bears or talk about the former US president, Teddy Roosevelt. So this is a unidirectional or forward directional only RNN. And, this comment I just made is true, whether these cells are standard RNN blocks or whether they’re GRU units or whether they’re LSTM blocks. But all of these blocks are in a forward only direction. So what a bidirectional RNN does or BRNN, is fix this issue. So, a bidirectional RNN works as follows. I’m going to use a simplified four inputs or maybe a four word sentence. So we have four inputs. X one through X four. So this networks heading there will have a forward recurrent components. So I’m going to call this, A one, A two, A three and A four, and I’m going to draw a right arrow over that to denote this is the forward recurrent component, and so they’ll be connected as follows. And so, each of these four recurrent units inputs the current X, and then feeds in to help predict Y-hat one, Y-hat two, Y-hat three, and Y-hat four. So, so far I haven’t done anything. Basically, we’ve drawn the RNN from the previous slide, but with the arrows placed in slightly funny positions. But I drew the arrows in this slightly funny positions because what we’re going to do is add a backward recurrent layer. So we’d have A one, left arrow to denote this is a backward connection, and then A two, backwards, A three, backwards and A four, backwards, so the left arrow denotes that it is a backward connection. And so, we’re then going to connect to network up as follows. And this A backward connections will be connected to each other going backward in time. So, notice that this network defines a Acyclic graph. And so, given an input sequence, X one through X four, the fourth sequence will first compute A forward one, then use that to compute A forward two, then A forward three, then A forward four. Whereas, the backward sequence would start by computing A backward four, and then go back and compute A backward three, and then as you are computing network activation, this is not backward this is forward prop. But the forward prop has part of the computation going from left to right and part of computation going from right to left in this diagram. But having computed A backward three, you can then use those activations to compute A backward two, and then A backward one, and then finally having computed all you had in the activations, you can then make your predictions. And so, for example, to make the predictions, your network will have something like Y-hat at time t is an activation function applied to WY with both the forward activation at time t, and the backward activation at time t being fed in to make that prediction at time t. So, if you look at the prediction at time set three for example, then information from X one can flow through here, forward one to forward two, they’re are all stated in the function here, to forward three to Y-hat three. So information from X one, X two, X three are all taken into account with information from X four can flow through a backward four to a backward three to Y three. So this allows the prediction at time three to take as input both information from the past, as well as information from the present which goes into both the forward and the backward things at this step, as well as information from the future. So, in particular, given a phrase like, “He said, Teddy Roosevelt…” To predict whether Teddy is a part of the person’s name, you take into account information from the past and from the future. So this is the bidirectional recurrent neural network and these blocks here can be not just the standard RNN block but they can also be GRU blocks or LSTM blocks. In fact, for a lots of NLP problems, for a lot of text with natural language processing problems, a bidirectional RNN with a LSTM appears to be commonly used. So, we have NLP problem and you have the complete sentence, you try to label things in the sentence, a bidirectional RNN with LSTM blocks both forward and backward would be a pretty views of first thing to try. So, that’s it for the bidirectional RNN and this is a modification they can make to the basic RNN architecture or the GRU or the LSTM, and by making this change you can have a model that uses RNN and or GRU or LSTM and is able to make predictions anywhere even in the middle of a sequence by taking into account information potentially from the entire sequence. The disadvantage of the bidirectional RNN is that you do need the entire sequence of data before you can make predictions anywhere. So, for example, if you’re building a speech recognition system, then the BRNN will let you take into account the entire speech utterance but if you use this straightforward implementation, you need to wait for the person to stop talking to get the entire utterance before you can actually process it and make a speech recognition prediction. So for a real type speech recognition applications, they’re somewhat more complex modules as well rather than just using the standard bidirectional RNN as you’ve seen here. But for a lot of natural language processing applications where you can get the entire sentence all the same time, the standard BRNN algorithm is actually very effective. So, that’s it for BRNNs and next and final video for this week, let’s talk about how to take all of these ideas RNNs, LSTMs and GRUs and the bidirectional versions and construct deep versions of them. 12_deep-rnnsThe different versions of RNNs you’ve seen so far will already work quite well by themselves. But for learning very complex functions sometimes is useful to stack multiple layers of RNNs together to build even deeper versions of these models. In this video, you’ll see how to build these deeper RNNs. Let’s take a look. So you remember for a standard neural network, you will have an input X. And then that’s stacked to some hidden layer and so that might have activations of say, $a^{}$ for the first hidden layer, and then that’s stacked to the next layer with activations $a^{}$, then maybe another layer, activations $a^{}$ and then you make a prediction $ŷ$. So a deep RNN is a bit like this, by taking this network that I just drew by hand and unrolling that in time. So let’s take a look. So here’s the standard RNN that you’ve seen so far. But I’ve changed the notation a little bit which is that, instead of writing this as $a^{}$ for the activation time zero, I’ve added this square bracket 1 to denote that this is for layer one. So the notation we’re going to use is $a^{[l]}$ to denote that it’s an activation associated with layer l and then to denote that that’s associated over time t. So this will have an activation on $a^{[1]}$, this would be $a^{[1]}$, $a^{[1]}$, $a^{[1]}$. And then we can just stack these things on top and so this will be a new network with three hidden layers. So let’s look at an example of how this value is computed. So $a^{[2]}$ has two inputs. It has the input coming from the bottom, and there’s the input coming from the left. So the computer has an activation function g applied to a weight matrix. This is going to be $W_a$ because computing an a quantity, an activation quantity. And for the second layer, and so I’m going to give this $a^{[2]}$, there’s that thing, comma $a^{[1]}$, there’s that thing, plus $b_a$ associated to the second layer. And that’s how you get that activation value. And so the same parameters $W_a^{[2]}$ and $b_a^{[2]}$ are used for every one of these computations at this layer. Whereas, in contrast, the first layer would have its own parameters $W_a^{[1]}$ and $b_a^{[1]}$. So whereas for standard RNNs like the one on the left, you know we’ve seen neural networks that are very, very deep, maybe over 100 layers. For RNNs, having three layers is already quite a lot. Because of the temporal dimension, these networks can already get quite big even if you have just a small handful of layers. And you don’t usually see these stacked up to be like 100 layers. One thing you do see sometimes is that you have recurrent layers that are stacked on top of each other. But then you might take the output here, let’s get rid of this, and then just have a bunch of deep layers that are not connected horizontally but have a deep network here that then finally predicts $y^{}$. And you can have the same deep network here that predicts $y^{}$. So this is a type of network architecture that we’re seeing a little bit more where you have three recurrent units that connected in time, followed by a network, followed by a network after that, as we seen for $y^{}$ and $y^{}$, of course. There’s a deep network, but that does not have the horizontal connections. So that’s one type of architecture we seem to be seeing more of. And quite often, these blocks don’t just have to be standard RNN, the simple RNN model. They can also be GRU blocks LSTM blocks. And finally, you can also build deep versions of the bidirectional RNN. Because deep RNNs are quite computationally expensive to train, there’s often a large temporal extent already, though you just don’t see as many deep recurrent layers. This has, I guess, three deep recurrent layers that are connected in time. You don’t see as many deep recurrent layers as you would see in a number of layers in a deep conventional neural network. So that’s it for deep RNNs. With what you’ve seen this week, ranging from the basic RNN, the basic recurrent unit, to the GRU, to the LSTM, to the bidirectional RNN, to the deep versions of this that you just saw, you now have a very rich toolbox for constructing very powerful models for learning sequence models. I hope you enjoyed this week’s videos. Best of luck with the problem exercises and I look forward to seeing you next week.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Face Recognition for the Happy House]]></title>
    <url>%2F2018%2F05%2F04%2FFace%2BRecognition%2Bfor%2Bthe%2BHappy%2BHouse%2B-%2Bv3%2F</url>
    <content type="text"><![CDATA[NoteThese are my personal programming assignments at the 3rd week after studying the course convolutional neural networks and the copyright belongs to deeplearning.ai. Face Recognition for the Happy HouseWelcome to the first assignment of week 4! Here you will build a face recognition system. Many of the ideas presented here are from FaceNet. In lecture, we also talked about DeepFace. Face recognition problems commonly fall into two categories: Face Verification - “is this the claimed person?”. For example, at some airports, you can pass through customs by letting a system scan your passport and then verifying that you (the person carrying the passport) are the correct person. A mobile phone that unlocks using your face is also using face verification. This is a 1:1 matching problem. Face Recognition - “who is this person?”. For example, the video lecture showed a face recognition video (https://www.youtube.com/watch?v=wr4rx0Spihs) of Baidu employees entering the office without needing to otherwise identify themselves. This is a 1:K matching problem. FaceNet learns a neural network that encodes a face image into a vector of 128 numbers. By comparing two such vectors, you can then determine if two pictures are of the same person. In this assignment, you will: Implement the triplet loss function Use a pretrained model to map face images into 128-dimensional encodings Use these encodings to perform face verification and face recognition In this exercise, we will be using a pre-trained model which represents ConvNet activations using a “channels first” convention, as opposed to the “channels last” convention used in lecture and previous programming assignments. In other words, a batch of images will be of shape $(m, n_C, n_H, n_W)$ instead of $(m, n_H, n_W, n_C)$. Both of these conventions have a reasonable amount of traction among open-source implementations; there isn’t a uniform standard yet within the deep learning community. Let’s load the required packages. 12345678910111213141516171819202122232425from keras.models import Sequentialfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenatefrom keras.models import Modelfrom keras.layers.normalization import BatchNormalizationfrom keras.layers.pooling import MaxPooling2D, AveragePooling2Dfrom keras.layers.merge import Concatenatefrom keras.layers.core import Lambda, Flatten, Densefrom keras.initializers import glorot_uniformfrom keras.engine.topology import Layerfrom keras import backend as KK.set_image_data_format('channels_first')import cv2import osimport numpy as npfrom numpy import genfromtxtimport pandas as pdimport tensorflow as tffrom fr_utils import *from inception_blocks_v2 import *%matplotlib inline%load_ext autoreload%autoreload 2np.set_printoptions(threshold=np.nan) Using TensorFlow backend. 0 - Naive Face VerificationIn Face Verification, you’re given two images and you have to tell if they are of the same person. The simplest way to do this is to compare the two images pixel-by-pixel. If the distance between the raw images are less than a chosen threshold, it may be the same person! Figure 1 Of course, this algorithm performs really poorly, since the pixel values change dramatically due to variations in lighting, orientation of the person’s face, even minor changes in head position, and so on. You’ll see that rather than using the raw image, you can learn an encoding $f(img)$ so that element-wise comparisons of this encoding gives more accurate judgements as to whether two pictures are of the same person. 1 - Encoding face images into a 128-dimensional vector1.1 - Using an ConvNet to compute encodingsThe FaceNet model takes a lot of data and a long time to train. So following common practice in applied deep learning settings, let’s just load weights that someone else has already trained. The network architecture follows the Inception model from Szegedy et al.. We have provided an inception network implementation. You can look in the file inception_blocks.py to see how it is implemented (do so by going to “File-&gt;Open…” at the top of the Jupyter notebook). The key things you need to know are: This network uses 96x96 dimensional RGB images as its input. Specifically, inputs a face image (or batch of $m$ face images) as a tensor of shape $(m, n_C, n_H, n_W) = (m, 3, 96, 96)$ It outputs a matrix of shape $(m, 128)$ that encodes each input face image into a 128-dimensional vector Run the cell below to create the model for face images. 1FRmodel = faceRecoModel(input_shape=(3, 96, 96)) 1print("Total Params:", FRmodel.count_params()) Total Params: 3743280 Expected Output Total Params: 3743280 By using a 128-neuron fully connected layer as its last layer, the model ensures that the output is an encoding vector of size 128. You then use the encodings the compare two face images as follows: Figure 2: By computing a distance between two encodings and thresholding, you can determine if the two pictures represent the same person So, an encoding is a good one if: The encodings of two images of the same person are quite similar to each other The encodings of two images of different persons are very different The triplet loss function formalizes this, and tries to “push” the encodings of two images of the same person (Anchor and Positive) closer together, while “pulling” the encodings of two images of different persons (Anchor, Negative) further apart. Figure 3: In the next part, we will call the pictures from left to right: Anchor (A), Positive (P), Negative (N) 1.2 - The Triplet LossFor an image $x$, we denote its encoding $f(x)$, where $f$ is the function computed by the neural network. Training will use triplets of images $(A, P, N)$: A is an “Anchor” image–a picture of a person. P is a “Positive” image–a picture of the same person as the Anchor image. N is a “Negative” image–a picture of a different person than the Anchor image. These triplets are picked from our training dataset. We will write $(A^{(i)}, P^{(i)}, N^{(i)})$ to denote the $i$-th training example. You’d like to make sure that an image $A^{(i)}$ of an individual is closer to the Positive $P^{(i)}$ than to the Negative image $N^{(i)}$) by at least a margin $\alpha$: $$\mid \mid f(A^{(i)}) - f(P^{(i)}) \mid \mid_2^2 + \alpha &lt; \mid \mid f(A^{(i)}) - f(N^{(i)}) \mid \mid_2^2$$ You would thus like to minimize the following “triplet cost”: $$\mathcal{J} = \sum^{m}_{i=1} \large[ \small \underbrace{\mid \mid f(A^{(i)}) - f(P^{(i)}) \mid \mid_2^2}_\text{(1)} - \underbrace{\mid \mid f(A^{(i)}) - f(N^{(i)}) \mid \mid_2^2}_\text{(2)} + \alpha \large ] \small_+ \tag{3}$$ Here, we are using the notation “$[z]_+$” to denote $max(z,0)$. Notes: The term (1) is the squared distance between the anchor “A” and the positive “P” for a given triplet; you want this to be small. The term (2) is the squared distance between the anchor “A” and the negative “N” for a given triplet, you want this to be relatively large, so it thus makes sense to have a minus sign preceding it. $\alpha$ is called the margin. It is a hyperparameter that you should pick manually. We will use $\alpha = 0.2$. Most implementations also normalize the encoding vectors to have norm equal one (i.e., $\mid \mid f(img)\mid \mid_2$=1); you won’t have to worry about that here. Exercise: Implement the triplet loss as defined by formula (3). Here are the 4 steps: Compute the distance between the encodings of “anchor” and “positive”: $\mid \mid f(A^{(i)}) - f(P^{(i)}) \mid \mid_2^2$ Compute the distance between the encodings of “anchor” and “negative”: $\mid \mid f(A^{(i)}) - f(N^{(i)}) \mid \mid_2^2$ Compute the formula per training example: $ \mid \mid f(A^{(i)}) - f(P^{(i)}) \mid - \mid \mid f(A^{(i)}) - f(N^{(i)}) \mid \mid_2^2 + \alpha$ Compute the full formula by taking the max with zero and summing over the training examples:$$\mathcal{J} = \sum^{m}_{i=1} \large[ \small \mid \mid f(A^{(i)}) - f(P^{(i)}) \mid \mid_2^2 - \mid \mid f(A^{(i)}) - f(N^{(i)}) \mid \mid_2^2+ \alpha \large ] \small_+ \tag{3}$$ Useful functions: tf.reduce_sum(), tf.square(), tf.subtract(), tf.add(), tf.maximum().For steps 1 and 2, you will need to sum over the entries of $\mid \mid f(A^{(i)}) - f(P^{(i)}) \mid \mid_2^2$ and $\mid \mid f(A^{(i)}) - f(N^{(i)}) \mid \mid_2^2$ while for step 4 you will need to sum over the training examples. 12345678910111213141516171819202122232425262728293031# GRADED FUNCTION: triplet_lossdef triplet_loss(y_true, y_pred, alpha = 0.2): """ Implementation of the triplet loss as defined by formula (3) Arguments: y_true -- true labels, required when you define a loss in Keras, you don't need it in this function. y_pred -- python list containing three objects: anchor -- the encodings for the anchor images, of shape (None, 128) positive -- the encodings for the positive images, of shape (None, 128) negative -- the encodings for the negative images, of shape (None, 128) Returns: loss -- real number, value of the loss """ anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2] ### START CODE HERE ### (≈ 4 lines) # Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1 pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis = -1); # Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1 neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis = -1); # Step 3: subtract the two previous distances and add alpha. basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha); # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples. loss = tf.reduce_sum(tf.maximum(basic_loss, 0)); ### END CODE HERE ### return loss 123456789with tf.Session() as test: tf.set_random_seed(1) y_true = (None, None, None) y_pred = (tf.random_normal([3, 128], mean=6, stddev=0.1, seed = 1), tf.random_normal([3, 128], mean=1, stddev=1, seed = 1), tf.random_normal([3, 128], mean=3, stddev=4, seed = 1)) loss = triplet_loss(y_true, y_pred) print("loss = " + str(loss.eval())) loss = 528.143 Expected Output: loss 528.143 2 - Loading the trained modelFaceNet is trained by minimizing the triplet loss. But since training requires a lot of data and a lot of computation, we won’t train it from scratch here. Instead, we load a previously trained model. Load a model using the following cell; this might take a couple of minutes to run. 12FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])load_weights_from_FaceNet(FRmodel) Here’re some examples of distances between the encodings between three individuals: Figure 4: Example of distance outputs between three individuals’ encodings Let’s now use this model to perform face verification and face recognition! 3 - Applying the modelBack to the Happy House! Residents are living blissfully since you implemented happiness recognition for the house in an earlier assignment. However, several issues keep coming up: The Happy House became so happy that every happy person in the neighborhood is coming to hang out in your living room. It is getting really crowded, which is having a negative impact on the residents of the house. All these random happy people are also eating all your food. So, you decide to change the door entry policy, and not just let random happy people enter anymore, even if they are happy! Instead, you’d like to build a Face verification system so as to only let people from a specified list come in. To get admitted, each person has to swipe an ID card (identification card) to identify themselves at the door. The face recognition system then checks that they are who they claim to be. 3.1 - Face VerificationLet’s build a database containing one encoding vector for each person allowed to enter the happy house. To generate the encoding we use img_to_encoding(image_path, model) which basically runs the forward propagation of the model on the specified image. Run the following code to build the database (represented as a python dictionary). This database maps each person’s name to a 128-dimensional encoding of their face. 12345678910111213database = &#123;&#125;database["danielle"] = img_to_encoding("images/danielle.png", FRmodel)database["younes"] = img_to_encoding("images/younes.jpg", FRmodel)database["tian"] = img_to_encoding("images/tian.jpg", FRmodel)database["andrew"] = img_to_encoding("images/andrew.jpg", FRmodel)database["kian"] = img_to_encoding("images/kian.jpg", FRmodel)database["dan"] = img_to_encoding("images/dan.jpg", FRmodel)database["sebastiano"] = img_to_encoding("images/sebastiano.jpg", FRmodel)database["bertrand"] = img_to_encoding("images/bertrand.jpg", FRmodel)database["kevin"] = img_to_encoding("images/kevin.jpg", FRmodel)database["felix"] = img_to_encoding("images/felix.jpg", FRmodel)database["benoit"] = img_to_encoding("images/benoit.jpg", FRmodel)database["arnaud"] = img_to_encoding("images/arnaud.jpg", FRmodel) Now, when someone shows up at your front door and swipes their ID card (thus giving you their name), you can look up their encoding in the database, and use it to check if the person standing at the front door matches the name on the ID. Exercise: Implement the verify() function which checks if the front-door camera picture (image_path) is actually the person called “identity”. You will have to go through the following steps: Compute the encoding of the image from image_path Compute the distance about this encoding and the encoding of the identity image stored in the database Open the door if the distance is less than 0.7, else do not open. As presented above, you should use the L2 distance (np.linalg.norm). (Note: In this implementation, compare the L2 distance, not the square of the L2 distance, to the threshold 0.7.) 123456789101112131415161718192021222324252627282930313233343536# GRADED FUNCTION: verifydef verify(image_path, identity, database, model): """ Function that verifies if the person on the "image_path" image is "identity". Arguments: image_path -- path to an image identity -- string, name of the person you'd like to verify the identity. Has to be a resident of the Happy house. database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors). model -- your Inception model instance in Keras Returns: dist -- distance between the image_path and the image of "identity" in the database. door_open -- True, if the door should open. False otherwise. """ ### START CODE HERE ### # Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (≈ 1 line) encoding = img_to_encoding(image_path, FRmodel); # Step 2: Compute distance with identity's image (≈ 1 line) dist = np.linalg.norm(encoding - database[identity]); # Step 3: Open the door if dist &lt; 0.7, else don't open (≈ 3 lines) if dist &lt; 0.7: print("It's " + str(identity) + ", welcome home!") door_open = True; else: print("It's not " + str(identity) + ", please go away") door_open = False; ### END CODE HERE ### return dist, door_open Younes is trying to enter the Happy House and the camera takes a picture of him (“http://pt8q6wt5q.bkt.clouddn.com/gitpage/deeplearning.ai/convolutional-neural-networks/jupter/week4/Face%2520Recognition%2520for%2520the%2520Happy%2520House/images/camera_0.jpg&quot;). Let’s run your verification algorithm on this picture: 1verify("images/camera_0.jpg", "younes", database, FRmodel) It&apos;s younes, welcome home! (0.65939283, True) Expected Output: It’s younes, welcome home! (0.65939283, True) Benoit, who broke the aquarium last weekend, has been banned from the house and removed from the database. He stole Kian’s ID card and came back to the house to try to present himself as Kian. The front-door camera took a picture of Benoit (“images/camera_2.jpg). Let’s run the verification algorithm to check if benoit can enter. 1verify("images/camera_2.jpg", "kian", database, FRmodel) It&apos;s not kian, please go away (0.86224014, False) Expected Output: It’s not kian, please go away (0.86224014, False) 3.2 - Face RecognitionYour face verification system is mostly working well. But since Kian got his ID card stolen, when he came back to the house that evening he couldn’t get in! To reduce such shenanigans, you’d like to change your face verification system to a face recognition system. This way, no one has to carry an ID card anymore. An authorized person can just walk up to the house, and the front door will unlock for them! You’ll implement a face recognition system that takes as input an image, and figures out if it is one of the authorized persons (and if so, who). Unlike the previous face verification system, we will no longer get a person’s name as another input. Exercise: Implement who_is_it(). You will have to go through the following steps: Compute the target encoding of the image from image_path Find the encoding from the database that has smallest distance with the target encoding. Initialize the min_dist variable to a large enough number (100). It will help you keep track of what is the closest encoding to the input’s encoding. Loop over the database dictionary’s names and encodings. To loop use for (name, db_enc) in database.items(). Compute L2 distance between the target “encoding” and the current “encoding” from the database. If this distance is less than the min_dist, then set min_dist to dist, and identity to name. 123456789101112131415161718192021222324252627282930313233343536373839404142434445# GRADED FUNCTION: who_is_itdef who_is_it(image_path, database, model): """ Implements face recognition for the happy house by finding who is the person on the image_path image. Arguments: image_path -- path to an image database -- database containing image encodings along with the name of the person on the image model -- your Inception model instance in Keras Returns: min_dist -- the minimum distance between image_path encoding and the encodings from the database identity -- string, the name prediction for the person on image_path """ ### START CODE HERE ### ## Step 1: Compute the target "encoding" for the image. Use img_to_encoding() see example above. ## (≈ 1 line) encoding = img_to_encoding(image_path, model); ## Step 2: Find the closest encoding ## # Initialize "min_dist" to a large value, say 100 (≈1 line) min_dist = 1000; # Loop over the database dictionary's names and encodings. for (name, db_enc) in database.items(): # Compute L2 distance between the target "encoding" and the current "emb" from the database. (≈ 1 line) dist = np.linalg.norm(encoding - db_enc); # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines) if dist &lt; min_dist: min_dist = dist; identity = name; ### END CODE HERE ### if min_dist &gt; 0.7: print("Not in the database.") else: print ("it's " + str(identity) + ", the distance is " + str(min_dist)) return min_dist, identity Younes is at the front-door and the camera takes a picture of him (“http://pt8q6wt5q.bkt.clouddn.com/gitpage/deeplearning.ai/convolutional-neural-networks/jupter/week4/Face%2520Recognition%2520for%2520the%2520Happy%2520House/images/camera_0.jpg&quot;). Let’s see if your who_it_is() algorithm identifies Younes. 1who_is_it("images/camera_0.jpg", database, FRmodel) it&apos;s younes, the distance is 0.659393 (0.65939283, &apos;younes&apos;) Expected Output: it’s younes, the distance is 0.659393 (0.65939283, ‘younes’) You can change “camera_0.jpg“ (picture of younes) to “camera_1.jpg“ (picture of bertrand) and see the result. Your Happy House is running well. It only lets in authorized persons, and people don’t need to carry an ID card around anymore! You’ve now seen how a state-of-the-art face recognition system works. Although we won’t implement it here, here’re some ways to further improve the algorithm: Put more images of each person (under different lighting conditions, taken on different days, etc.) into the database. Then given a new image, compare the new face to multiple pictures of the person. This would increae accuracy. Crop the images to just contain the face, and less of the “border” region around the face. This preprocessing removes some of the irrelevant pixels around the face, and also makes the algorithm more robust. What you should remember: Face verification solves an easier 1:1 matching problem; face recognition addresses a harder 1:K matching problem. The triplet loss is an effective loss function for training a neural network to learn an encoding of a face image. The same encoding can be used for verification and recognition. Measuring distances between two images’ encodings allows you to determine whether they are pictures of the same person. Congrats on finishing this assignment! References: Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). FaceNet: A Unified Embedding for Face Recognition and Clustering Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, Lior Wolf (2014). DeepFace: Closing the gap to human-level performance in face verification The pretrained model we use is inspired by Victor Sy Wang’s implementation and was loaded using his code: https://github.com/iwantooxxoox/Keras-OpenFace. Our implementation also took a lot of inspiration from the official FaceNet github repository: https://github.com/davidsandberg/facenet]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>convolutional-neural-networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[summary of convolutional neural networks]]></title>
    <url>%2F2018%2F05%2F04%2Fsummary_of_convolutional-neural-networks%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal summary after studying the course, convolutional neural networks, which belongs to Deep Learning Specialization. and the copyright belongs to deeplearning.ai. My personal notes${1_{st}}$ week: 01_foundations-of-convolutional-neural-networks 01_computer-vision 02_edge-detection-example 03_more-edge-detection 04_padding 05_strided-convolutions 06_convolutions-over-volume 07_one-layer-of-a-convolutional-network 08_simple-convolutional-network-example 09_pooling-layers 10_cnn-example 11_why-convolutions $2_{nd}$ week: 02_deep-convolutional-models-case-studies 01_case-studies 01_why-look-at-case-studies 02_classic-networks 03_resnets 04_why-resnets-work 05_networks-in-networks-and-1x1-convolutions 06_inception-network-motivation 07_inception-network 02_practical-advices-for-using-convnets 01_using-open-source-implementation 02_transfer-learning 03_data-augmentation 04_state-of-computer-vision $3_{rd}$ week : 03_object-detection 01_object-localization 02_landmark-detection 03_object-detection 04_convolutional-implementation-of-sliding-windows 05_bounding-box-predictions 06_intersection-over-union 07_non-max-suppression 08_anchor-boxes 09_yolo-algorithm 10_optional-region-proposals $4_{th}$ week : 04_special-applications-face-recognition-neural-style-transfer 01_face-recognition 01_what-is-face-recognition 02_one-shot-learning 03_siamese-network 04_triplet-loss 05_face-verification-and-binary-classification 02_neural-style-transfer 01_what-is-neural-style-transfer 02_what-are-deep-convnets-learning 03_cost-function 04_content-cost-function 05_style-cost-function 06_1d-and-3d-generalizations My personal programming assignments$1_{st}$ week : Convolution model Step by Step$2_{nd}$ week : Keras Tutorial Happy House, Residual Networks$3_{rd}$ week : Autonomous driving - Car detection$4_{th}$ week : Deep Learning &amp; Art Neural Style Transfer, Face Recognition for the Happy House]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>convolutional-neural-networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04_special-applications-face-recognition-neural-style-transfer]]></title>
    <url>%2F2018%2F05%2F04%2F04_special-applications-face-recognition-neural-style-transfer%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal note after studying the course of the 4th week convolutional neural networks and the copyright belongs to deeplearning.ai. 01_face-recognition01_what-is-face-recognitionHi, and welcome to this fourth and final week of this course on convolutional neural networks. By now, you’ve learned a lot about confidence. What I want to do this week is show you a couple important special applications of confidence. We’ll start the face recognition, and then go on later this week to neurosal transfer, which you get to implement in the problem exercise as well to create your own artwork. But first, let’s start the face recognition and just for fun, I want to show you a demo. When I was leading by those AI group, one of the teams I worked with led by Yuanqing Lin had built a face recognition system that I thought is really cool. Let’s take a look. So, I’m going to play this video here, but I can also get whoever is editing this raw video configure out to this better to splice in the raw video or take the one I’m playing here. I want to show you a face recognition demo. I’m in Baidu’s headquarters in China. Most companies require that to get inside, you swipe an ID card like this one but here we don’t need that. Using face recognition, check what I can do. When I walk up, it recognizes my face, it says, “Welcome Andrew,” and I just walk right through without ever having to use my ID card. Let me show you something else. I’m actually here with Lin Yuanqing, the director of IDL which developed all of this face recognition technology. I’m gonna hand him my ID card, which has my face printed on it, and he’s going to use it to try to sneak in using my picture instead of a live human. I’m gonna use Andrew’s card and try to sneak in and see what happens. So the system is not recognizing it, it refuses to recognize. Okay. Now, I’m going to use my own face. So face recognition technology like this is taking off very rapidly in China and I hope that this type of technology soon makes it way to other countries.. So, pretty cool, right? The video you just saw demoed both face recognition as well as liveness detection. The latter meaning making sure that you are a live human. It turns out liveness detection can be implemented using supervised learning as well to predict live human versus not live human but I want to spend less time on that. Instead, I want to focus our time on talking about how to build the face recognition portion of the system. First, let’s start by going over some of the terminology used in face recognition. In the face recognition literature, people often talk about face verification and face recognition. This is the face verification problem which is if you’re given an input image as well as a name or ID of a person and the job of the system is to verify whether or not the input image is that of the claimed person. So, sometimes this is also called a one to one problem where you just want to know if the person is the person they claim to be. So, the recognition problem is much harder than the verification problem. To see why, let’s say, you have a verification system that’s 99 percent accurate. So, 99 percent might not be too bad but now suppose that K is equal to 100 in a recognition system. If you apply this system to a recognition task with a 100 people in your database, you now have a hundred times of chance of making a mistake and if the chance of making mistakes on each person is just one percent. So, if you have a database of a 100 persons and if you want an acceptable recognition error, you might actually need a verification system with maybe 99.9 or even higher accuracy before you can run it on a database of 100 persons that have a high chance and still have a high chance of getting incorrect. In fact, if you have a database of 100 persons currently just be even quite a bit higher than 99 percent for that to work well. But what we do in the next few videos is focus on building a face verification system as a building block and then if the accuracy is high enough, then you probably use that in a recognition system as well. So in the next video, we’ll start describing how you can build a face verification system. It turns out one of the reasons that is a difficult problem is you need to solve a one shot learning problem. Let’s see in the next video what that means. 02_one-shot-learningOne of the challenges of face recognition is that you need to solve the one-shot learning problem. What that means is that for most face recognition applications you need to be able to recognize a person given just one single image, or given just one example of that person’s face. And, historically, deep learning algorithms don’t work well if you have only one training example. Let’s see an example of what this means and talk about how to address this problem. Let’s say you have a database of four pictures of employees in you’re organization. These are actually some of my colleagues at Deeplearning.AI, Khan, Danielle, Younes and Thian. Now let’s say someone shows up at the office and they want to be let through the turnstile. What the system has to do is, despite ever having seen only one image of Danielle, to recognize that this is actually the same person. And, in contrast, if it sees someone that’s not in this database, then it should recognize that this is not any of the four persons in the database. So in the one shot learning problem, you have to learn from just one example to recognize the person again. And you need this for most face recognition systems because you might have only one picture of each of your employees or of your team members in your employee database. So one approach you could try is to input the image of the person, feed it too a ConvNet. And have it output a label, y, using a softmax unit with four outputs or maybe five outputs corresponding to each of these four persons or none of the above. So that would be 5 outputs in the softmax. But this really doesn’t work well. Because if you have such a small training set it is really not enough to train a robust neural network for this task. And also what if a new person joins your team? So now you have 5 persons you need to recognize, so there should now be six outputs. Do you have to retrain the ConvNet every time? That just doesn’t seem like a good approach. So to carry out face recognition, to carry out one-shot learning. So instead, to make this work, what you’re going to do instead is learn a similarity function. In particular, you want a neural network to learn a function which going to denote d, which inputs two images and outputs the degree of difference between the two images. So if the two images are of the same person, you want this to output a small number. And if the two images are of two very different people you want it to output a large number. So during recognition time, if the degree of difference between them is less than some threshold called tau, which is a hyperparameter. Then you would predict that these two pictures are the same person. And if it is greater than tau, you would predict that these are different persons. And so this is how you address the face verification problem. To use this for a recognition task, what you do is, given this new picture, you will use this function d to compare these two images. And maybe I’ll output a very large number, let’s say 10, for this example. And then you compare this with the second image in your database. And because these two are the same person, hopefully you output a very small number. You do this for the other images in your database and so on. And based on this, you would figure out that this is actually that person, which is Danielle. And in contrast, if someone not in your database shows up, as you use the function d to make all of these pairwise comparisons, hopefully d will output have a very large number for all four pairwise comparisons. And then you say that this is not any one of the four persons in the database. Notice how this allows you to solve the one-shot learning problem. So long as you can learn this function d, which inputs a pair of images and tells you, basically, if they’re the same person or different persons. Then if you have someone new join your team, you can add a fifth person to your database, and it just works fine. So you’ve seen how learning this function d, which inputs two images, allows you to address the one-shot learning problem. In the next video, let’s take a look at how you can actually train the neural network to learn dysfunction d. 03_siamese-networkThe job of the function d, which you learned about in the last video, is to input two faces and tell you how similar or how different they are. A good way to do this is to use a Siamese network. Let’s take a look. You’re used to seeing pictures of confidence like these where you input an image, let’s say x1. And through a sequence of convolutional and pulling and fully connected layers, end up with a feature vector like that. And sometimes this is fed to a softmax unit to make a classification. We’re not going to use that in this video. Instead, we’re going to focus on this vector of let’s say 128 numbers computed by some fully connected layer that is deeper in the network. And I’m going to give this list of 128 numbers a name. I’m going to call this f of x1, and you should think of f of x1 as an encoding of the input image x1. So it’s taken the input image, here this picture of Kian, and is re-representing it as a vector of 128 numbers. The way you can build a face recognition system is then that if you want to compare two pictures, let’s say this first picture with this second picture here. What you can do is feed this second picture to the same neural network with the same parameters and get a different vector of 128 numbers, which encodes this second picture. So I’m going to call this second picture. So I’m going to call this encoding of this second picture f of x2, and here I’m using x1 and x2 just to denote two input images. They don’t necessarily have to be the first and second examples in your training sets. It can be any two pictures. Finally, if you believe that these encodings are a good representation of these two images, what you can do is then define the image d of distance between x1 and x2 as the norm of the difference between the encodings of these two images. So this idea of running two identical, convolutional neural networks on two different inputs and then comparing them, sometimes that’s called a Siamese neural network architecture. And a lot of the ideas I’m presenting here came from this paper due to Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf in the research system that they developed called DeepFace. And many of the ideas I’m presenting here came from a paper due to Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf in a system that they developed called DeepFace. So how do you train this Siamese neural network? Remember that these two neural networks have the same parameters. So what you want to do is really train the neural network so that the encoding that it computes results in a function d that tells you when two pictures are of the same person. So more formally, the parameters of the neural network define an encoding f of xi. So given any input image xi, the neural network outputs this 128 dimensional encoding f of xi. So more formally, what you want to do is learn parameters so that if two pictures, xi and xj, are of the same person, then you want that distance between their encodings to be small. And in the previous slide, l was using x1 and x2, but it’s really any pair xi and xj from your training set. And in contrast, if xi and xj are of different persons, then you want that distance between their encodings to be large. So as you vary the parameters in all of these layers of the neural network, you end up with different encodings. And what you can do is use back propagation and vary all those parameters in order to make sure these conditions are satisfied. So you’ve learned about the Siamese network architecture and have a sense of what you want the neural network to output for you in terms of what would make a good encoding. But how do you actually define an objective function to make a neural network learn to do what we just discussed here? Let’s see how you can do that in the next video using the triplet loss function. 04_triplet-lossOne way to learn the parameters of the neural network so that it gives you a good encoding for your pictures of faces is to define an applied gradient descent on the triplet loss function. Let’s see what that means. To apply the triplet loss, you need to compare pairs of images. For example, given this picture, to learn the parameters of the neural network, you have to look at several pictures at the same time. For example, given this pair of images, you want their encodings to be similar because these are the same person. Whereas, given this pair of images, you want their encodings to be quite different because these are different persons. In the terminology of the triplet loss, what you’re going do is always look at one anchor image and then you want to distance between the anchor and the positive image, really a positive example, meaning as the same person to be similar. Whereas, you want the anchor when pairs are compared to the negative example for their distances to be much further apart. So, this is what gives rise to the term triplet loss, which is that you’ll always be looking at three images at a time. You’ll be looking at an anchor image, a positive image, as well as a negative image. And I’m going to abbreviate anchor positive and negative as A, P, and N. So to formalize this, what you want is for the parameters of your neural network of your encodings to have the following property, which is that you want the encoding between the anchor minus the encoding of the positive example, you want this to be small and in particular, you want this to be less than or equal to the distance of the squared norm between the encoding of the anchor and the encoding of the negative, where of course, this is d of A, P and this is d of A, N. And you can think of d as a distance function, which is why we named it with the alphabet d. Now, if we move to term from the right side of this equation to the left side, what you end up with is f of A minus f of P squared minus, let’s take the right-hand side now, minus F of N squared, you want this to be less than or equal to zero. But now, we’re going to make a slight change to this expression, which is one trivial way to make sure this is satisfied, is to just learn everything equals zero. If f always equals zero, then this is zero minus zero, which is zero, this is zero minus zero which is zero. And so, well, by saying f of any image equals a vector of all zeroes, you can almost trivially satisfy this equation. So, to make sure that the neural network doesn’t just output zero for all the encoding, so to make sure that it doesn’t set all the encodings equal to each other. Another way for the neural network to give a trivial output is if the encoding for every image was identical to the encoding to every other image, in which case, you again get zero minus zero. So to prevent a neural network from doing that, what we’re going to do is modify this objective to say that, this doesn’t need to be just less than or equal to zero, it needs to be quite a bit smaller than zero. So, in particular, if we say this needs to be less than negative alpha, where alpha is another hyperparameter, then this prevents a neural network from outputting the trivial solutions. And by convention, usually, we write plus alpha instead of negative alpha there. And this is also called, a margin, which is terminology that you’d be familiar with if you’ve also seen the literature on support vector machines, but don’t worry about it if you haven’t. And we can also modify this equation on top by adding this margin parameter. So to give an example, let’s say the margin is set to 0.2. If in this example, d of the anchor and the positive is equal to 0.5, then you won’t be satisfied if d between the anchor and the negative was just a little bit bigger, say 0.51. Even though 0.51 is bigger than 0.5, you’re saying, that’s not good enough, we want a dfA, N to be much bigger than dfA, P and in particular, you want this to be at least 0.7 or higher. Alternatively, to achieve this margin or this gap of at least 0.2, you could either push this up or push this down so that there is at least this gap of this alpha, hyperparameter alpha 0.2 between the distance between the anchor and the positive versus the anchor and the negative. So that’s what having a margin parameter here does, which is it pushes the anchor positive pair and the anchor negative pair further away from each other. So, let’s take this equation we have here at the bottom, and on the next slide, formalize it, and define the triplet loss function. So, the triplet loss function is defined on triples of images. So, given three images, A, P, and N, the anchor positive and negative examples. So the positive examples is of the same person as the anchor, but the negative is of a different person than the anchor. We’re going to define the loss as follows. The loss on this example, which is really defined on a triplet of images is, let me first copy over what we had on the previous slide. So, that was fA minus fP squared minus fA minus fN squared, and then plus alpha, the margin parameter. And what you want is for this to be less than or equal to zero. So, to define the loss function, let’s take the max between this and zero. So, the effect of taking the max here is that, so long as this is less than zero, then the loss is zero, because the max is something less than equal to zero, when zero is going to be zero. So, so long as you achieve the goal of making this thing I’ve underlined in green, so long as you’ve achieved the objective of making that less than or equal to zero, then the loss on this example is equals to zero. But if on the other hand, if this is greater than zero, then if you take the max, the max we end up selecting, this thing I’ve underlined in green, and so you would have a positive loss. So by trying to minimize this, this has the effect of trying to send this thing to be zero, less than or equal to zero. And then, so long as there’s zero or less than or equal to zero, the neural network doesn’t care how much further negative it is. So, this is how you define the loss on a single triplet and the overall cost function for your neural network can be sum over a training set of these individual losses on different triplets. So, if you have a training set of say 10,000 pictures with 1,000 different persons, what you’d have to do is take your 10,000 pictures and use it to generate, to select triplets like this and then train your learning algorithm using gradient descent on this type of cost function, which is really defined on triplets of images drawn from your training set. Notice that in order to define this dataset of triplets, you do need some pairs of A and P. Pairs of pictures of the same person. So the purpose of training your system, you do need a dataset where you have multiple pictures of the same person. That’s why in this example, I said if you have 10,000 pictures of 1,000 different person, so maybe have 10 pictures on average of each of your 1,000 persons to make up your entire dataset. If you had just one picture of each person, then you can’t actually train this system. But of course after training, if you’re applying this, but of course after having trained the system, you can then apply it to your one shot learning problem where for your face recognition system, maybe you have only a single picture of someone you might be trying to recognize. But for your training set, you do need to make sure you have multiple images of the same person at least for some people in your training set so that you can have pairs of anchor and positive images. Now, how do you actually choose these triplets to form your training set? One of the problems if you choose A, P, and N randomly from your training set subject to A and P being from the same person, and A and N being different persons, one of the problems is that if you choose them so that they’re at random, then this constraint is very easy to satisfy. Because given two randomly chosen pictures of people, chances are A and N are much different than A and P. I hope you still recognize this notation, this d(A, P) was what we had written on the last few slides as this encoding. So this is just equal to this squared known distance between the encodings that we have on the previous slide. But if A and N are two randomly chosen different persons, then there is a very high chance that this will be much bigger more than the margin alpha that that term on the left. And so, the neural network won’t learn much from it. So to construct a training set, what you want to do is to choose triplets A, P, and N that are hard to train on. So in particular, what you want is for all triplets that this constraint be satisfied. So, a triplet that is hard will be if you choose values for A, P, and N so that maybe d(A, P) is actually quite close to d(A,N). So in that case, the learning algorithm has to try extra hard to take this thing on the right and try to push it up or take this thing on the left and try to push it down so that there is at least a margin of alpha between the left side and the right side. And the effect of choosing these triplets is that it increases the computational efficiency of your learning algorithm. If you choose your triplets randomly, then too many triplets would be really easy, and so, gradient descent won’t do anything because your neural network will just get them right, pretty much all the time. And it’s only by using hard triplets that the gradient descent procedure has to do some work to try to push these quantities further away from those quantities. And if you’re interested, the details are presented in this paper by Florian Schroff, Dmitry Kalinichenko, and James Philbin, where they have a system called FaceNet, which is where a lot of the ideas I’m presenting in this video come from. By the way, this is also a fun fact about how algorithms are often named in the deep learning world, which is if you work in a certain domain, then we call that blank. You often have a system called blank net or deep blank. So, we’ve been talking about face recognition. So this paper is called FaceNet, and in the last video, you just saw deep face. But this idea of a blank net or deep blank is a very popular way of naming algorithms in the deep learning world. And you should feel free to take a look at that paper if you want to learn some of these other details for speeding up your algorithm by choosing the most useful triplets to train on, it is a nice paper. So, just to wrap up, to train on triplet loss, you need to take your training set and map it to a lot of triples. So, here is our triple with an anchor and a positive, both for the same person and the negative of a different person. Here’s another one where the anchor and positive are of the same person but the anchor and negative are of different persons and so on. And what you do having defined this training sets of anchor positive and negative triples is use gradient descent to try to minimize the cost function J we defined on an earlier slide, and that will have the effect of that propagating to all of the parameters of the neural network in order to learn an encoding so that d of two images will be small when these two images are of the same person, and they’ll be large when these are two images of different persons. . Now, it turns out that today’s face recognition systems especially the large scale commercial face recognition systems are trained on very large datasets. Datasets north of a million images is not uncommon, some companies are using north of 10 million images and some companies have north of 100 million images with which to try to train these systems. So these are very large datasets even by modern standards, these dataset assets are not easy to acquire. Fortunately, some of these companies have trained these large networks and posted parameters online. So, rather than trying to train one of these networks from scratch, this is one domain where because of the share data volume sizes, this is one domain where often it might be useful for you to download someone else’s pre-train model, rather than do everything from scratch yourself. But even if you do download someone else’s pre-train model, I think it’s still useful to know how these algorithms were trained or in case you need to apply these ideas from scratch yourself for some application. So that’s it for the triplet loss. In the next video, I want to show you also some other variations on siamese networks and how to train these systems. Let’s go onto the next video. 05_face-verification-and-binary-classificationThe Triplet Loss is one good way to learn the parameters of a continent for face recognition. There’s another way to learn these parameters. Let me show you how face recognition can also be posed as a straight binary classification problem. Another way to train a neural network, is to take this pair of neural networks to take this Siamese Network and have them both compute these embeddings, maybe 128 dimensional embeddings, maybe even higher dimensional, and then have these be input to a logistic regression unit to then just make a prediction. Where the target output will be one if both of these are the same persons, and zero if both of these are of different persons. So, this is a way to treat face recognition just as a binary classification problem. And this is an alternative to the triplet loss for training a system like this. Now, what does this final logistic regression unit actually do? The output y hat will be a sigmoid function, applied to some set of features but rather than just feeding in, these encodings, what you can do is take the differences between the encodings. So, let me show you what I mean. Let’s say, I write a sum over K equals 1 to 128 of the absolute value, taken element wise between the two different encodings. Let me just finish writing this out and then we’ll see what this means. In this notation, f of x i is the encoding of the image $x_i$ and the substitute k means to just select out the kth components of this vector. This is taking the element Y’s difference in absolute values between these two encodings. And what you might do is think of these 128 numbers as features that you then feed into logistic regression. And, you’ll find that logistic regression can add additional parameters $w_i$, and $b$ similar to a normal logistic regression unit. And you would train appropriate weighting on these 128 features in order to predict whether or not these two images are of the same person or of different persons. So, this will be one pretty useful way to learn to predict zero or one whether these are the same person or different persons. And there are a few other variations on how you can compute this formula that I had underlined in green. For example, another formula could be this k minus f of $x_j$, k squared divided by f of x i on plus f of x j k. This is sometimes called the chi square form. This is the Greek alphabet chi. But this is sometimes called a $\chi$ square similarity. And this and other variations are explored in this deep face paper, which I referenced earlier as well. So in this learning formulation, the input is a pair of images, so this is really your training input x and the output y is either zero or one depending on whether you’re inputting a pair of similar or dissimilar images. And same as before, you’re training is Siamese Network so that means that, this neural network up here has parameters that are what they’re really tied to the parameters in this lower neural network. And this system can work pretty well as well. Lastly, just to mention, one computational trick that can help neural deployment significantly, which is that, if this is the new image, so this is an employee walking in hoping that the turnstile the doorway will open for them and that this is from your database image. Then instead of having to compute, this embedding every single time, where you can do is actually pre-compute that, so, when the new employee walks in, what you can do is use this upper components to compute that encoding and use it, then compare it to your pre-computed encoding and then use that to make a prediction y hat. Because you don’t need to store the raw images and also because if you have a very large database of employees, you don’t need to compute these encodings every single time for every employee database. This idea of free computing, some of these encodings can save a significant computation. And this type of pre-computation works both for this type of Siamese Central architecture where you treat face recognition as a binary classification problem, as well as, when you were learning encodings maybe using the Triplet Loss function as described in the last couple of videos. And so just to wrap up, to treat face verification supervised learning, you create a training set of just pairs of images now is of triplets of pairs of images where the target label is one. When these are a pair of pictures of the same person and where the tag label is zero, when these are pictures of different persons and you use different pairs to train the neural network to train the scientists that were using back propagation. So, this version that you just saw of treating face verification and by extension face recognition as a binary classification problem, this works quite well as well. As sort of that, I hope that you now know, whether it would take to train your own face verification or your own face recognition system one that can do one. 02_neural-style-transfer01_what-is-neural-style-transferOne of the most fun and exciting applications of ConvNet recently has been Neural Style Transfer. You get to implement this yourself and generate your own artwork in the problem exercise. But what is Neural Style Transfer? Let me show you a few examples. Let’s say you take this image, this is actually taken from the Stanford University not far from my Stanford office and you want this picture recreated in the style of this image on the right. This is actually Van Gogh’s, Starry Night painting. What Neural Style Transfer allows you to do is generated new image like the one below which is a picture of the Stanford University Campus that painted but drawn in the style of the image on the right. In order to describe how you can implement this yourself, I’m going to use C to denote the content image, S to denote the style image, and G to denote the image you will generate. Here’s another example, let’s say you have this content image so let’s see this is of the Golden Gate Bridge in San Francisco and you have this style image, this is actually Pablo Picasso image. You can then combine these to generate this image G which is the Golden Gate painted in the style of that Picasso shown on the right. The examples shown on this slide were generated by Justin Johnson. What you’ll learn in the next few videos is how you can generate these images yourself. In order to implement Neural Style Transfer, you need to look at the features extracted by ConvNet at various layers, the shallow and the deeper layers of a ConvNet. Before diving into how you can implement a Neural Style Transfer, what I want to do in the next video is try to give you better intuition about whether all these layers of a ConvNet really computing. Let’s take a look at that in the next video. 02_what-are-deep-convnets-learningWhat are deep ConvNets really learning? In this video, I want to share with you some visualizations that will help you hone your intuition about what the deeper layers of a ConvNet really are doing. And this will help us think through how you can implement neural style transfer as well. Let’s start with an example. Lets say you’ve trained a ConvNet, this is an alex net like network, and you want to visualize what the hidden units in different layers are computing. Here’s what you can do. Let’s start with a hidden unit in layer 1. And suppose you scan through your training sets and find out what are the images or what are the image patches that maximize that unit’s activation. So in other words pause your training set through your neural network, and figure out what is the image that maximizes that particular unit’s activation. Now, notice that a hidden unit in layer 1, will see only a relatively small portion of the neural network. And so if you visualize, if you plot what activated unit’s activation, it makes makes sense to plot just a small image patches, because all of the image that that particular unit sees. So if you pick one hidden unit and find the nine input images that maximizes that unit’s activation, you might find nine image patches like this.So looks like that in the lower region of an image that this particular hidden unit sees, it’s looking for an egde or a line that looks like that. So those are the nine image patches that maximally activate one hidden unit’s activation. Now, you can then pick a different hidden unit in layer 1 and do the same thing.So that’s a different hidden unit, and looks like this second one, represented by these 9 image patches here. Looks like this hidden unit is looking for a line sort of in that portion of its input region, we’ll also call this receptive field. And if you do this for other hidden units, you’ll find other hidden units, tend to activate in image patches that look like that.This one seems to have a preference for a vertical light edge, but with a preference that the left side of it be green.This one really prefers orange colors, and this is an interesting image patch. This red and green together will make a brownish or a brownish-orangish color, but the neuron is still happy to activate with that, and so on.So this is nine different representative neurons and for each of them the nine image patches that they maximally activate on. So this gives you a sense that, units, train hidden units in layer 1, they’re often looking for relatively simple features such as edge or a particular shade of color. And all of the examples I’m using in this video come from this paper by Mathew Zeiler and Rob Fergus, titled visualizing and understanding convolutional networks. And I’m just going to use one of the simpler ways to visualize what a hidden unit in a neural network is computing. If you read their paper, they have some other more sophisticated ways of visualizing when the ConvNet is running as well. But now you have repeated this procedure several times for nine hidden units in layer 1. What if you do this for some of the hidden units in the deeper layers of the neuron network. And what does the neural network then learning at a deeper layers. So in the deeper layers, a hidden unit will see a larger region of the image. Where at the extreme end each pixel could hypothetically affect the output of these later layers of the neural network. So later units are actually seen larger image patches, I’m still going to plot the image patches as the same size on these slides. But if we repeat this procedure, this is what you had previously for layer 1, and this is a visualization of what maximally activates nine different hidden units in layer 2. So I want to be clear about what this visualization is. These are the nine patches that cause one hidden unit to be highly activated. And then each grouping, this is a different set of nine image patches that cause one hidden unit to be activated. So this visualization shows nine hidden units in layer 2, and for each of them shows nine image patches that causes that hidden unit to have a very large output, a very large activation. And you can repeat these for deeper layers as well. Now, on this slide, I know it’s kind of hard to see these tiny little image patches, so let me zoom in for some of them. For layer 1, this is what you saw. So for example, this is that first unit we saw which was highly activated, if in the region of the input image, you can see there’s an edge maybe at that angle. Now let’s zoom in for layer 2 as well, to that visualization. So this is interesting, layer 2 looks it’s detecting more complex shapes and patterns. So for example, this hidden unit looks like it’s looking for a vertical texture with lots of vertical lines. This hidden unit looks like its highly activated when there’s a rounder shape to the left part of the image. Here’s one that is looking for very thin vertical lines and so on. And so the features the second layer is detecting are getting more complicated. How about layer 3? Let’s zoom into that, in fact let me zoom in even bigger, so you can see this better, these are the things that maximally activate layer 3. But let’s zoom in even bigger, and so this is pretty interesting again. It looks like there is a hidden unit that seems to respond highly to a rounder shape in the lower left hand portion of the image, maybe. So that ends up detecting a lot of cars, dogs and wonders is even starting to detect people. And this one look like it is detecting certain textures like honeycomb shapes, or square shapes, this irregular texture. And some of these it’s difficult to look at and manually figure out what is it detecting, but it is clearly starting to detect more complex patterns. How about the next layer? Well, here is layer 4, and you’ll see that the features or the patterns is detecting or even more complex. It looks like this has learned almost a dog detector, but all these dogs likewise similar, right? Is this, I don’t know what dog species or dog breed this is. But now all those are dogs, but they look relatively similar as dogs go. Looks like this hidden unit and therefore it is detecting water. This looks like it is actually detecting the legs of a bird and so on. And then layer 5 is detecting even more sophisticated things. So you’ll notice there’s also a neuron that seems to be a dog detector, but set of dogs detecting here seems to be more varied. And then this seems to be detecting keyboards and things with a keyboard like texture, although maybe lots of dots against background. I think this neuron here may be detecting text, it’s always hard to be sure. And then this one here is detecting flowers. So we’ve gone a long way from detecting relatively simple things such as edges in layer 1 to textures in layer 2, up to detecting very complex objects in the deeper layers. So I hope this gives you some better intuition about what the shallow and deeper layers of a neural network are computing. Next, let’s use this intuition to start building a neural-style transfer algorithm. 03_cost-functionTo build a Neural Style Transfer system, let’s define a cost function for the generated image. What you see later is that by minimizing this cost function, you can generate the image that you want. Remember what the problem formulation is. You’re given a content image C, given a style image S and you goal is to generate a new image G. In order to implement neural style transfer, what you’re going to do is define a cost function J of G that measures how good is a particular generated image and we’ll use gradient to descent to minimize J of G in order to generate this image. How good is a particular image? Well, we’re going to define two parts to this cost function. The first part is called the content cost. This is a function of the content image and of the generated image and what it does is it measures how similar is the contents of the generated image to the content of the content image C. And then going to add that to a style cost function which is now a function of S,G and what this does is it measures how similar is the style of the image G to the style of the image S. Finally, we’ll weight these with two hyper parameters alpha and beta to specify the relative weighting between the content costs and the style cost. It seems redundant to use two different hyper parameters to specify the relative cost of the weighting. One hyper parameter seems like it would be enough but the original authors of the Neural Style Transfer Algorithm, use two different hyper parameters. I’m just going to follow their convention here. The Neural Style Transfer Algorithm I’m going to present in the next few videos is due to Leon Gatys, Alexander Ecker and Matthias. Their papers is not too hard to read so after watching these few videos if you wish, I certainly encourage you to take a look at their paper as well if you want. The way the algorithm would run is as follows, having to find the cost function J of G in order to actually generate a new image what you do is the following. You would initialize the generated image G randomly so it might be 100 by 100 by 3 or 500 by 500 by 3 or whatever dimension you want it to be. Then we’ll define the cost function J of G on the previous slide. What you can do is use gradient descent to minimize this so you can update G as G minus the derivative respect to the cost function of J of G. In this process, you’re actually updating the pixel values of this image G which is a 100 by 100 by 3 maybe rgb channel image. Here’s an example, let’s say you start with this content image and this style image. This is a another probably Picasso image. Then when you initialize G randomly, you’re initial randomly generated image is just this white noise image with each pixel value chosen at random. As you run gradient descent, you minimize the cost function J of G slowly through the pixel value so then you get slowly an image that looks more and more like your content image rendered in the style of your style image. In this video, you saw the overall outline of the Neural Style Transfer Algorithm where you define a cost function for the generated image G and minimize it. Next, we need to see how to define the content cost function as well as the style cost function. Let’s take a look at that starting in the next video. 04_content-cost-functionThe cost function of the neural style transfer algorithm had a content cost component and a style cost component. Let’s start by defining the content cost component. Remember that this is the overall cost function of the neural style transfer algorithm. So, let’s figure out what should the content cost function be. Let’s say that you use hidden layer l to compute the content cost. If l is a very small number, if you use hidden layer one, then it will really force your generated image to pixel values very similar to your content image. Whereas, if you use a very deep layer, then it’s just asking, “Well, if there is a dog in your content image, then make sure there is a dog somewhere in your generated image. “ So in practice, layer l chosen somewhere in between. It’s neither too shallow nor too deep in the neural network. And because you program this yourself, in the problem exercise that you did at the end of this week, I’ll leave you to gain some intuitions with the concrete examples in the problem exercise as well. But usually, I was chosen to be somewhere in the middle of the layers of the neural network, neither too shallow nor too deep. What you can do is then use a pre-trained ConvNet, maybe a VGG network, or could be some other neural network as well. And now, you want to measure, given a content image and given a generated image, how similar are they in content. So let’s let this a_superscript_l and this be the activations of layer l on these two images, on the images C and G. So, if these two activations are similar, then that would seem to imply that both images have similar content. So, what we’ll do is define J_content(C,G) as just how soon or how different are these two activations. So, we’ll take the element-wise difference between these hidden unit activations in layer l, between when you pass in the content image compared to when you pass in the generated image, and take that squared. And you could have a normalization constant in front or not, so it’s just one of the two or something else. It doesn’t really matter since this can be adjusted as well by this hyperparameter alpha. So, just be clear on using this notation as if both of these have been unrolled into vectors, so then, this becomes the square root of the l_2 norm between this and this, after you’ve unrolled them both into vectors. There’s really just the element-wise sum of squared differences between these two activation. But it’s really just the element-wise sum of squares of differences between the activations in layer l, between the images in C and G. And so, when later you perform gradient descent on J_of_G to try to find a value of G, so that the overall cost is low, this will incentivize the algorithm to find an image G, so that these hidden layer activations are similar to what you got for the content image. So, that’s how you define the content cost function for the neural style transfer. Next, let’s move on to the style cost function. 05_style-cost-functionIn the last video, you saw how to define the content cost function for the neural style transfer. Next, let’s take a look at the style cost function. So, what is the style of an image mean? Let’s say you have an input image like this, they used to seeing a convnet like that, compute features that there’s different layers. And let’s say you’ve chosen some layer L, maybe that layer to define the measure of the style of an image. What we need to do is define the style as the correlation between activations across different channels in this layer L activation. So here’s what I mean by that. Let’s say you take that layer L activation. So this is going to be nh by nw by nc block of activations, and we’re going to ask how correlated are the activations across different channels. So to explain what I mean by this may be slightly cryptic phrase, let’s take this block of activations and let me shade the different channels by a different colors. So in this below example, we have say five channels and which is why I have five shades of color here. In practice, of course, in neural network we usually have a lot more channels than five, but using just five makes it drawing easier. But to capture the style of an image, what you’re going to do is the following. Let’s look at the first two channels. Let’s see for the red channel and the yellow channel and say how correlated are activations in these first two channels. So, for example, in the lower right hand corner, you have some activation in the first channel and some activation in the second channel. So that gives you a pair of numbers. And what you do is look at different positions across this block of activations and just look at those two pairs of numbers, one in the first channel, the red channel, one in the yellow channel, the second channel. And you just look at these two pairs of numbers and see when you look across all of these positions, all of these nh by nw positions, how correlated are these two numbers. So, why does this capture style? Let’s look another example. Here’s one of the visualizations from the earlier video. This comes from again the paper by Matthew Zeiler and Rob Fergus that I have reference earlier. And let’s say for the sake of arguments, that the red neuron corresponds to, and let’s say for the sake of arguments, that the red channel corresponds to this neurons (at the second grid cell which is circled in red color), so we’re trying to figure out if there’s this little vertical texture in a particular position in the nh and let’s say that this second channel, this yellow second channel corresponds to this neuron (at the 4th grid cell which is circled in yellow color), which is vaguely looking for orange colored patches. What does it mean for these two channels to be highly correlated? Well, if they’re highly correlated what that means is whatever part of the image has this type of subtle vertical texture, that part of the image will probably have these orange-ish tint. And what does it mean for them to be uncorrelated? Well, it means that whenever there is this vertical texture, it’s probably won’t have that orange-ish tint. And so the correlation tells you which of these high level texture components tend to occur or not occur together in part of an image and that’s the degree of correlation that gives you one way of measuring how often these different high level features, such as vertical texture or this orange tint or other things as well, how often they occur and how often they occur together and don’t occur together in different parts of an image. And so, if we use the degree of correlation between channels as a measure of the style, then what you can do is measure the degree to which in your generated image, this first channel is correlated or uncorrelated with the second channel and that will tell you in the generated image how often this type of vertical texture occurs or doesn’t occur with this orange-ish tint and this gives you a measure of how similar is the style of the generated image to the style of the input style image. So let’s now formalize this intuition. So what you can to do is given an image computes something called a style matrix, which will measure all those correlations we talks about on the last slide. So, more formally, let’s let a superscript l, subscript i, j,k denote the activation at position i,j,k in hidden layer l. So i indexes into the height, j indexes into the width, and k indexes across the different channels. So, in the previous slide, we had five channels that k will index across those five channels. So what the style matrix will do is you’re going to compute a matrix clauses G superscript square bracketed l. This is going to be an nc by nc dimensional matrix, so it’d be a square matrix. Remember you have nc channels and so you have an nc by nc dimensional matrix in order to measure how correlated each pair of them is. So particular G, l, k, k prime will measure how correlated are the activations in channel k compared to the activations in channel k prime. Well here, k and k prime will range from 1 through nc, the number of channels they’re all up in that layer. So more formally, the way you compute G, l and I’m just going to write down the formula for computing one elements. So the k, k prime elements of this. This is going to be sum of a i, sum of a j, of deactivation and that layer i, j, k times the activation at i, j, k prime. So, here, remember i and j index across to a different positions in the block, indexes over the height and width. So i is the sum from one to nh and j is a sum from one to nw and k here and k prime index over the channel so k and k prime range from one to the total number of channels in that layer of the neural network. So all this is doing is summing over the different positions that the image over the height and width and just multiplying the activations together of the channels k and k prime and that’s the definition of G,k,k prime. And you do this for every value of k and k prime to compute this matrix G, also called the style matrix. And so notice that if both of these activations tend to be large together, then G, k, k prime will be large, whereas if they are uncorrelated then g,k, k prime might be small. And technically, I’ve been using the term correlation to convey intuition but this is actually the unnormalized cross-variance of the areas because we’re not subtracting out the mean and this is just multiplied by these elements directly. So this is how you compute the style of an image. And you’d actually do this for both the style image s,n for the generated image G. So just to distinguish that this is the style image, maybe let me add a round bracket S there, just to denote that this is the style image for the image S and those are the activations on the image S. And what you do is then compute the same thing for the generated image. So it’s really the same thing summarized sum of a j, a, i, j, k, l, a, i, j,k,l and the summation indices are the same. Let’s follow this and you want to just denote this is for the generated image, I’ll just put the round brackets G there. So, now, you have two matrices they capture what is the style with the image s and what is the style of the image G. And, by the way, we’ve been using the alphabet capital G to denote these matrices. In linear algebra, these are also called the Gram matrix of these in called grand matrices but in this video, I’m just going to use the term style matrix because this term Gram matrix that most of these using capital G to denote these matrices. Finally, the cost function, the style cost function. If you’re doing this on layer l between s and G, you can now define that to be just the difference between these two matrices, G l, G square and these are matrices. So just take it from the previous one. This is just the sum of squares of the element wise differences between these two matrices and just divides this out this is going to be sum over k, sum over k prime of these differences of s, k, k prime minus G l, G, k, k prime and then the sum of square of the elements. The authors actually used this for the normalization constants two times of nh, nw, in that layer, nc in that layer and I’ll square this and you can put this up here as well. But a normalization constant doesn’t matter that much because this causes multiplied by some hyperparameter b anyway. So just to finish up, this is the style cost function defined using layer l and as you saw on the previous slide, this is basically the Frobenius norm between the two star matrices computed on the image s and on the image G Frobenius on squared and never by the just low normalization constants, which isn’t that important. And, finally, it turns out that you get more visually pleasing results if you use the style cost function from multiple different layers. So, the overall style cost function, you can define as sum over all the different layers of the style cost function for that layer. We should define them all weighted by some set of parameters, by some set of additional hyperparameters, which we’ll denote as lambda l here. So what it does is allows you to use different layers in a neural network. Well of the early ones, which measure relatively simpler low level features like edges as well as some later layers, which measure high level features and cause a neural network to take both low level and high level correlations into account when computing style. And, in the following exercise, you gain more intuition about what might be reasonable choices for this type of parameter lambda as well. And so just to wrap this up, you can now define the overall cost function as alpha times the content cost between c and G plus beta times the style cost between s and G and then just create in the sense or a more sophisticated optimization algorithm if you want in order to try to find an image G that normalize, that tries to minimize this cost function j of G. And if you do that, you can generate pretty good looking neural artistic and if you do that you’ll be able to generate some pretty nice novel artwork. So that’s it for neural style transfer and I hope you have fun implementing it in this week’s printing exercise. Before wrapping up this week, there’s just one last thing I want to share of you, which is how to do convolutions over 1D or 3D data rather than over only 2D images. Let’s go into the last video. 06_1d-and-3d-generalizationsYou have learned a lot about ConvNets, everything ranging from the architecture of the ConvNet to how to use it for image recognition, to object detection, to face recognition and neural-style transfer. And even though most of the discussion has focused on images, on sort of 2D data, because images are so pervasive. It turns out that many of the ideas you’ve learned about also apply, not just to 2D images but also to 1D data as well as to 3D data. Let’s take a look. In the first week of this course, you learned about the 2D convolution, where you might input a 14 x 14 image and convolve that with a 5 x 5 filter. And you saw how 14 x 14 convolved with 5 x 5, this gives you a 10 x 10 output. And if you have multiple channels, maybe those 14 x 14 x 3, then it would be 5 x 5 that matches the same 3. And then if you have multiple filters, say 16 filters, you end up with 10 x 10 x 16. It turns out that a similar idea can be applied to 1D data as well. For example, on the left is an EKG signal, also called an electrocardioagram. Basically if you place an electrode over your chest, this measures the little voltages that vary across your chest as your heart beats. Because the little electric waves generated by your heart’s beating can be measured with a pair of electrodes. And so this is an EKG of someone’s heart beating. And so each of these peaks corresponds to one heartbeat. So if you want to use EKG signals to make medical diagnoses, for example, then you would have 1D data because what EKG data is, is it’s a time series showing the voltage at each instant in time. So rather than a 14 x 14 dimensional input, maybe you just have a 14 dimensional input. And in that case, you might want to convolve this with a 1 dimensional filter. So rather than the 5 by 5, you just have 5 dimensional filter. So with 2D data what a convolution will allow you to do was to take the same 5 x 5 feature detector and apply it across at different positions throughout the image. And that’s how you wound up with your 10 x 10 output. What a 1D filter allows you to do is take your 5 dimensional filter and similarly apply that in lots of different positions throughout this 1D signal. And so if you apply this convolution, what you find is that a 14 dimensional thing convolved with this 5 dimensional thing, this would give you a 10 dimensional output. And again, if you have multiple channels, you might have in this case you can use just 1 channel, if you have 1 lead or 1 electrode for EKG, so times 5 x 1. And if you have 16 filters, maybe end up with 10 x 16 over there, and this could be one layer of your ConvNet. And then for the next layer of your ConvNet, if you input a 10 x 16 dimensional input and you might convolve that with a 5 dimensional filter again. Then these have 16 channels, so that has a match. And we have 32 filters, then the output of another layer would be 6 x 32, if you have 32 filters, right? And the analogy to the the 2D data, this is similar to all of the 10 x 10 x 16 data and convolve it with a 5 x 5 x 16, and that has to match. That will give you a 6 by 6 dimensional output, and you have 32 filters, that’s where the 32 comes from. So all of these ideas apply also to 1D data, where you can have the same feature detector, such as this, apply to a variety of positions. For example, to detect the different heartbeats in an EKG signal. But to use the same set of features to detect the heartbeats even at different positions along these time series, and so ConvNet can be used even on 1D data. For along with 1D data applications, you actually use a recurrent neural network, which you learn about in the next course. But some people can also try using ConvNets in these problems. And in the next course on sequence models, which we will talk about recurring neural networks and LCM and other models like that. We’ll talk about the pros and cons of using 1D ConvNets versus some of those other models that are explicitly designed to sequenced data. So that’s the generalization from 2D to 1D. How about 3D data? Well, what is three dimensional data? It is that, instead of having a 1D list of numbers or a 2D matrix of numbers, you now have a 3D block, a three dimensional input volume of numbers. So here’s the example of that which is if you take a CT scan, this is a type of X-ray scan that gives a three dimensional model of your body. But what a CT scan does is it takes different slices through your body. So as you scan through a CT scan which I’m doing here, you can look at different slices of the human torso to see how they look and so this data is fundamentally three dimensional. And one way to think of this data is if your data now has some height, some width, and then also some depth. Where this is the different slices through this volume, are the different slices through the torso. So if you want to apply a ConvNet to detect features in this three dimensional CAT scan or CT scan, then you can generalize the ideas from the first slide to three dimensional convolutions as well. So if you have a 3D volume, and for the sake of simplicity let’s say is 14 x 14 x 14 and so this is the height, width, and depth of the input CT scan. And again, just like images they’ll all have to be square, a 3D volume doesn’t have to be a perfect cube as well. So the height and width of a image can be different, and in the same way the height and width and the depth of a CT scan can be different. But I’m just using 14 x 14 x 14 here to simplify the discussion. And if you convolve this with a now a 5 x 5 x 5 filter, so you’re filters now are also three dimensional then this would give you a 10 x 10 x 10 volume. And technically, you could also have by 1, if this is the number of channels. So this is just a 3D volume, but your data can also have different numbers of channels, then this would be times 1 as well. Because the number of channels here and the number of channels here has to match. And then if you have 16 filters did a 5 x 5 x 5 x 1 then the next output will be a 10 x 10 x 10 x 16. So this could be one layer of your ConvNet over 3D data, and if the next layer of the ConvNet convolves this again with a 5 x 5 x 5 x 16 dimensional filter. So this number of channels has to match data as usual, and if you have 32 filters then similar to what you saw was ConvNet of the images. Now you’ll end up with a 6 x 6 x 6 volume across 32 channels. So 3D data can also be learned on, sort of directly using a three dimensional ConvNet. And what these filters do is really detect features across your 3D data, CAT scans, medical scans as one example of 3D volumes. But another example of data, you could treat as a 3D volume would be movie data, where the different slices could be different slices in time through a movie. And you could use this to detect motion or people taking actions in movies. So that’s it on generalization of ConvNets from 2D data to also 1D as well as 3D data. Image data is so pervasive that the vast majority of ConvNets are on 2D data, on image data, but I hope that these other models will be helpful to you as well. So this is it, this is the last video of this week and the last video of this course on ConvNets. You’ve learned a lot about ConvNets and I hope you find many of these ideas useful for your future work. So congratulations on finishing these videos. I hope you enjoyed this week’s exercise and I look forward also to seeing you in the next course on sequence models.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>convolutional-neural-networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning & Art Neural Style Transfer]]></title>
    <url>%2F2018%2F05%2F04%2FArt%2BGeneration%2Bwith%2BNeural%2BStyle%2BTransfer%2B-%2Bv3%2F</url>
    <content type="text"><![CDATA[NoteThese are my personal programming assignments at the 4th week after studying the course convolutional neural networks and the copyright belongs to deeplearning.ai. Deep Learning &amp; Art: Neural Style TransferWelcome to the second assignment of this week. In this assignment, you will learn about Neural Style Transfer. This algorithm was created by Gatys et al. (2015) (https://arxiv.org/abs/1508.06576). In this assignment, you will: Implement the neural style transfer algorithm Generate novel artistic images using your algorithm Most of the algorithms you’ve studied optimize a cost function to get a set of parameter values. In Neural Style Transfer, you’ll optimize a cost function to get pixel values! 123456789101112import osimport sysimport scipy.ioimport scipy.miscimport matplotlib.pyplot as pltfrom matplotlib.pyplot import imshowfrom PIL import Imagefrom nst_utils import *import numpy as npimport tensorflow as tf%matplotlib inline C:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters 1 - Problem StatementNeural Style Transfer (NST) is one of the most fun techniques in deep learning. As seen below, it merges two images, namely, a “content” image (C) and a “style” image (S), to create a “generated” image (G). The generated image G combines the “content” of the image C with the “style” of image S. In this example, you are going to generate an image of the Louvre museum in Paris (content image C), mixed with a painting by Claude Monet, a leader of the impressionist movement (style image S). Let’s see how you can do this. 2 - Transfer LearningNeural Style Transfer (NST) uses a previously trained convolutional network, and builds on top of that. The idea of using a network trained on a different task and applying it to a new task is called transfer learning. Following the original NST paper (https://arxiv.org/abs/1508.06576), we will use the VGG network. Specifically, we’ll use VGG-19, a 19-layer version of the VGG network. This model has already been trained on the very large ImageNet database, and thus has learned to recognize a variety of low level features (at the earlier layers) and high level features (at the deeper layers). Run the following code to load parameters from the VGG model. This may take a few seconds. 12model = load_vgg_model("pretrained-model/imagenet-vgg-verydeep-19.mat")print(model) {&apos;input&apos;: &lt;tf.Variable &apos;Variable:0&apos; shape=(1, 300, 400, 3) dtype=float32_ref&gt;, &apos;conv1_1&apos;: &lt;tf.Tensor &apos;Relu:0&apos; shape=(1, 300, 400, 64) dtype=float32&gt;, &apos;conv1_2&apos;: &lt;tf.Tensor &apos;Relu_1:0&apos; shape=(1, 300, 400, 64) dtype=float32&gt;, &apos;avgpool1&apos;: &lt;tf.Tensor &apos;AvgPool:0&apos; shape=(1, 150, 200, 64) dtype=float32&gt;, &apos;conv2_1&apos;: &lt;tf.Tensor &apos;Relu_2:0&apos; shape=(1, 150, 200, 128) dtype=float32&gt;, &apos;conv2_2&apos;: &lt;tf.Tensor &apos;Relu_3:0&apos; shape=(1, 150, 200, 128) dtype=float32&gt;, &apos;avgpool2&apos;: &lt;tf.Tensor &apos;AvgPool_1:0&apos; shape=(1, 75, 100, 128) dtype=float32&gt;, &apos;conv3_1&apos;: &lt;tf.Tensor &apos;Relu_4:0&apos; shape=(1, 75, 100, 256) dtype=float32&gt;, &apos;conv3_2&apos;: &lt;tf.Tensor &apos;Relu_5:0&apos; shape=(1, 75, 100, 256) dtype=float32&gt;, &apos;conv3_3&apos;: &lt;tf.Tensor &apos;Relu_6:0&apos; shape=(1, 75, 100, 256) dtype=float32&gt;, &apos;conv3_4&apos;: &lt;tf.Tensor &apos;Relu_7:0&apos; shape=(1, 75, 100, 256) dtype=float32&gt;, &apos;avgpool3&apos;: &lt;tf.Tensor &apos;AvgPool_2:0&apos; shape=(1, 38, 50, 256) dtype=float32&gt;, &apos;conv4_1&apos;: &lt;tf.Tensor &apos;Relu_8:0&apos; shape=(1, 38, 50, 512) dtype=float32&gt;, &apos;conv4_2&apos;: &lt;tf.Tensor &apos;Relu_9:0&apos; shape=(1, 38, 50, 512) dtype=float32&gt;, &apos;conv4_3&apos;: &lt;tf.Tensor &apos;Relu_10:0&apos; shape=(1, 38, 50, 512) dtype=float32&gt;, &apos;conv4_4&apos;: &lt;tf.Tensor &apos;Relu_11:0&apos; shape=(1, 38, 50, 512) dtype=float32&gt;, &apos;avgpool4&apos;: &lt;tf.Tensor &apos;AvgPool_3:0&apos; shape=(1, 19, 25, 512) dtype=float32&gt;, &apos;conv5_1&apos;: &lt;tf.Tensor &apos;Relu_12:0&apos; shape=(1, 19, 25, 512) dtype=float32&gt;, &apos;conv5_2&apos;: &lt;tf.Tensor &apos;Relu_13:0&apos; shape=(1, 19, 25, 512) dtype=float32&gt;, &apos;conv5_3&apos;: &lt;tf.Tensor &apos;Relu_14:0&apos; shape=(1, 19, 25, 512) dtype=float32&gt;, &apos;conv5_4&apos;: &lt;tf.Tensor &apos;Relu_15:0&apos; shape=(1, 19, 25, 512) dtype=float32&gt;, &apos;avgpool5&apos;: &lt;tf.Tensor &apos;AvgPool_4:0&apos; shape=(1, 10, 13, 512) dtype=float32&gt;} The model is stored in a python dictionary where each variable name is the key and the corresponding value is a tensor containing that variable’s value. To run an image through this network, you just have to feed the image to the model. In TensorFlow, you can do so using the tf.assign function. In particular, you will use the assign function like this:1model["input"].assign(image) This assigns the image as an input to the model. After this, if you want to access the activations of a particular layer, say layer 4_2 when the network is run on this image, you would run a TensorFlow session on the correct tensor conv4_2, as follows:1sess.run(model["conv4_2"]) 3 - Neural Style TransferWe will build the NST algorithm in three steps: Build the content cost function $J_{content}(C,G)$ Build the style cost function $J_{style}(S,G)$ Put it together to get $J(G) = \alpha J_{content}(C,G) + \beta J_{style}(S,G)$. 3.1 - Computing the content costIn our running example, the content image C will be the picture of the Louvre Museum in Paris. Run the code below to see a picture of the Louvre. 12content_image = scipy.misc.imread("images/louvre.jpg")imshow(content_image) C:\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: DeprecationWarning: `imread` is deprecated! `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0. Use ``imageio.imread`` instead. if __name__ == &apos;__main__&apos;: &lt;matplotlib.image.AxesImage at 0x23c512646a0&gt; The content image (C) shows the Louvre museum’s pyramid surrounded by old Paris buildings, against a sunny sky with a few clouds. 3.1.1 - How do you ensure the generated image G matches the content of the image C? As we saw in lecture, the earlier (shallower) layers of a ConvNet tend to detect lower-level features such as edges and simple textures, and the later (deeper) layers tend to detect higher-level features such as more complex textures as well as object classes. We would like the “generated” image G to have similar content as the input image C. Suppose you have chosen some layer’s activations to represent the content of an image. In practice, you’ll get the most visually pleasing results if you choose a layer in the middle of the network–neither too shallow nor too deep. (After you have finished this exercise, feel free to come back and experiment with using different layers, to see how the results vary.) So, suppose you have picked one particular hidden layer to use. Now, set the image C as the input to the pretrained VGG network, and run forward propagation. Let $a^{(C)}$ be the hidden layer activations in the layer you had chosen. (In lecture, we had written this as $a^{l}$, but here we’ll drop the superscript $[l]$ to simplify the notation.) This will be a $n_H \times n_W \times n_C$ tensor. Repeat this process with the image G: Set G as the input, and run forward progation. Let $a^{(G)}$ be the corresponding hidden layer activation. We will define as the content cost function as: $$J_{content}(C,G) = \frac{1}{4 \times n_H \times n_W \times n_C}\sum _{ \text{all entries}} (a^{(C)} - a^{(G)})^2\tag{1} $$ Here, $n_H, n_W$ and $n_C$ are the height, width and number of channels of the hidden layer you have chosen, and appear in a normalization term in the cost. For clarity, note that $a^{(C)}$ and $a^{(G)}$ are the volumes corresponding to a hidden layer’s activations. In order to compute the cost $J_{content}(C,G)$, it might also be convenient to unroll these 3D volumes into a 2D matrix, as shown below. (Technically this unrolling step isn’t needed to compute $J_{content}$, but it will be good practice for when you do need to carry out a similar operation later for computing the style const $J_{style}$.) Exercise: Compute the “content cost” using TensorFlow. Instructions: The 3 steps to implement this function are: Retrieve dimensions from a_G: To retrieve dimensions from a tensor X, use: X.get_shape().as_list() Unroll a_C and a_G as explained in the picture above If you are stuck, take a look at Hint1 and Hint2. Compute the content cost: If you are stuck, take a look at Hint3, Hint4 and Hint5. 123456789101112131415161718192021222324252627# GRADED FUNCTION: compute_content_costdef compute_content_cost(a_C, a_G): """ Computes the content cost Arguments: a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G Returns: J_content -- scalar that you compute using equation 1 above. """ ### START CODE HERE ### # Retrieve dimensions from a_G (≈1 line) m, n_H, n_W, n_C = a_G.get_shape().as_list(); # Reshape a_C and a_G (≈2 lines) a_C_unrolled = tf.reshape(a_C, [n_H * n_W, n_C]); a_G_unrolled = tf.reshape(a_G, [n_H * n_W, n_C]); # compute the cost with tensorflow (≈1 line) J_content = 1./(4 * n_H * n_W * n_C)*tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled, a_G_unrolled))); ### END CODE HERE ### return J_content 12345678tf.reset_default_graph()with tf.Session() as test: tf.set_random_seed(1) a_C = tf.random_normal([1, 4, 4, 3], mean=1, stddev=4) a_G = tf.random_normal([1, 4, 4, 3], mean=1, stddev=4) J_content = compute_content_cost(a_C, a_G) print("J_content = " + str(J_content.eval())) J_content = 6.7655926 Expected Output: J_content 6.76559 What you should remember: The content cost takes a hidden layer activation of the neural network, and measures how different $a^{(C)}$ and $a^{(G)}$ are. When we minimize the content cost later, this will help make sure $G$ has similar content as $C$. 3.2 - Computing the style costFor our running example, we will use the following style image: 12style_image = scipy.misc.imread("images/monet_800600.jpg")imshow(style_image) C:\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: DeprecationWarning: `imread` is deprecated! `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0. Use ``imageio.imread`` instead. if __name__ == &apos;__main__&apos;: &lt;matplotlib.image.AxesImage at 0x23c57b880f0&gt; This painting was painted in the style of impressionism. Lets see how you can now define a “style” const function $J_{style}(S,G)$. 3.2.1 - Style matrixThe style matrix is also called a “Gram matrix.” In linear algebra, the Gram matrix G of a set of vectors $(v_{1},\dots ,v_{n})$ is the matrix of dot products, whose entries are ${\displaystyle G_{ij} = v_{i}^T v_{j} = np.dot(v_{i}, v_{j}) }$. In other words, $G_{ij}$ compares how similar $v_i$ is to $v_j$: If they are highly similar, you would expect them to have a large dot product, and thus for $G_{ij}$ to be large. Note that there is an unfortunate collision in the variable names used here. We are following common terminology used in the literature, but $G$ is used to denote the Style matrix (or Gram matrix) as well as to denote the generated image $G$. We will try to make sure which $G$ we are referring to is always clear from the context. In NST, you can compute the Style matrix by multiplying the “unrolled” filter matrix with their transpose: The result is a matrix of dimension $(n_C,n_C)$ where $n_C$ is the number of filters. The value $G_{ij}$ measures how similar the activations of filter $i$ are to the activations of filter $j$. One important part of the gram matrix is that the diagonal elements such as $G_{ii}$ also measures how active filter $i$ is. For example, suppose filter $i$ is detecting vertical textures in the image. Then $G_{ii}$ measures how common vertical textures are in the image as a whole: If $G_{ii}$ is large, this means that the image has a lot of vertical texture. By capturing the prevalence of different types of features ($G_{ii}$), as well as how much different features occur together ($G_{ij}$), the Style matrix $G$ measures the style of an image. Exercise:Using TensorFlow, implement a function that computes the Gram matrix of a matrix A. The formula is: The gram matrix of A is $G_A = AA^T$. If you are stuck, take a look at Hint 1 and Hint 2. 12345678910111213141516# GRADED FUNCTION: gram_matrixdef gram_matrix(A): """ Argument: A -- matrix of shape (n_C, n_H*n_W) Returns: GA -- Gram matrix of A, of shape (n_C, n_C) """ ### START CODE HERE ### (≈1 line) GA = tf.matmul(A, tf.matrix_transpose(A)); ### END CODE HERE ### return GA 12345678tf.reset_default_graph()with tf.Session() as test: tf.set_random_seed(1) A = tf.random_normal([3, 2*1], mean=1, stddev=4) GA = gram_matrix(A) print("GA = " + str(GA.eval())) GA = [[ 6.422305 -4.429122 -2.096682] [-4.429122 19.465837 19.563871] [-2.096682 19.563871 20.686462]] Expected Output: GA [[ 6.42230511 -4.42912197 -2.09668207] [ -4.42912197 19.46583748 19.56387138] [ -2.09668207 19.56387138 20.6864624 ]] 3.2.2 - Style costAfter generating the Style matrix (Gram matrix), your goal will be to minimize the distance between the Gram matrix of the “style” image S and that of the “generated” image G. For now, we are using only a single hidden layer $a^{[l]}$, and the corresponding style cost for this layer is defined as: $$J_{style}^{[l]}(S,G) = \frac{1}{4 \times {n_C}^2 \times (n_H \times n_W)^2} \sum _{i=1}^{n_C}\sum_{j=1}^{n_C}(G^{(S)}_{ij} - G^{(G)}_{ij})^2\tag{2} $$ where $G^{(S)}$ and $G^{(G)}$ are respectively the Gram matrices of the “style” image and the “generated” image, computed using the hidden layer activations for a particular hidden layer in the network. Exercise: Compute the style cost for a single layer. Instructions: The 3 steps to implement this function are: Retrieve dimensions from the hidden layer activations a_G: To retrieve dimensions from a tensor X, use: X.get_shape().as_list() Unroll the hidden layer activations a_S and a_G into 2D matrices, as explained in the picture above. You may find Hint1 and Hint2 useful. Compute the Style matrix of the images S and G. (Use the function you had previously written.) Compute the Style cost: You may find Hint3, Hint4 and Hint5 useful. 123456789101112131415161718192021222324252627282930# GRADED FUNCTION: compute_layer_style_costdef compute_layer_style_cost(a_S, a_G): """ Arguments: a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G Returns: J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2) """ ### START CODE HERE ### # Retrieve dimensions from a_G (≈1 line) m, n_H, n_W, n_C = a_G.get_shape().as_list(); # Reshape the images to have them of shape (n_C, n_H*n_W) (≈2 lines) a_S = tf.reshape(tf.transpose(a_S, perm=[3, 2, 1, 0]), [n_C, n_H * n_W]); a_G = tf.reshape(tf.transpose(a_G, perm=[3, 2, 1, 0]), [n_C, n_H * n_W]); # Computing gram_matrices for both images S and G (≈2 lines) GS = gram_matrix(a_S); GG = gram_matrix(a_G); # Computing the loss (≈1 line) J_style_layer = tf.reduce_sum(tf.square(tf.subtract(GS, GG))) / (4 * n_C ** 2 * (n_H * n_W) ** 2); ### END CODE HERE ### return J_style_layer 123456789tf.reset_default_graph()with tf.Session() as test: tf.set_random_seed(1) a_S = tf.random_normal([1, 4, 4, 3], mean=1, stddev=4) a_G = tf.random_normal([1, 4, 4, 3], mean=1, stddev=4) J_style_layer = compute_layer_style_cost(a_S, a_G) print("J_style_layer = " + str(J_style_layer.eval())) J_style_layer = 9.190277 Expected Output: J_style_layer 9.19028 3.2.3 Style WeightsSo far you have captured the style from only one layer. We’ll get better results if we “merge” style costs from several different layers. After completing this exercise, feel free to come back and experiment with different weights to see how it changes the generated image $G$. But for now, this is a pretty reasonable default: 123456STYLE_LAYERS = [ ('conv1_1', 0.2), ('conv2_1', 0.2), ('conv3_1', 0.2), ('conv4_1', 0.2), ('conv5_1', 0.2)] You can combine the style costs for different layers as follows: $$J_{style}(S,G) = \sum_{l} \lambda^{[l]} J^{[l]}_{style}(S,G)$$ where the values for $\lambda^{[l]}$ are given in STYLE_LAYERS. We’ve implemented a compute_style_cost(…) function. It simply calls your compute_layer_style_cost(...) several times, and weights their results using the values in STYLE_LAYERS. Read over it to make sure you understand what it’s doing. 12345678910111213141516171819202122232425262728293031323334353637def compute_style_cost(model, STYLE_LAYERS): """ Computes the overall style cost from several chosen layers Arguments: model -- our tensorflow model STYLE_LAYERS -- A python list containing: - the names of the layers we would like to extract style from - a coefficient for each of them Returns: J_style -- tensor representing a scalar value, style cost defined above by equation (2) """ # initialize the overall style cost J_style = 0 for layer_name, coeff in STYLE_LAYERS: # Select the output tensor of the currently selected layer out = model[layer_name] # Set a_S to be the hidden layer activation from the layer we have selected, by running the session on out a_S = sess.run(out) # Set a_G to be the hidden layer activation from same layer. Here, a_G references model[layer_name] # and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that # when we run the session, this will be the activations drawn from the appropriate layer, with G as input. a_G = out # Compute style_cost for the current layer J_style_layer = compute_layer_style_cost(a_S, a_G) # Add coeff * J_style_layer of this layer to overall style cost J_style += coeff * J_style_layer return J_style Note: In the inner-loop of the for-loop above, a_G is a tensor and hasn’t been evaluated yet. It will be evaluated and updated at each iteration when we run the TensorFlow graph in model_nn() below. What you should remember: The style of an image can be represented using the Gram matrix of a hidden layer’s activations. However, we get even better results combining this representation from multiple different layers. This is in contrast to the content representation, where usually using just a single hidden layer is sufficient. Minimizing the style cost will cause the image $G$ to follow the style of the image $S$. 3.3 - Defining the total cost to optimizeFinally, let’s create a cost function that minimizes both the style and the content cost. The formula is: $$J(G) = \alpha J_{content}(C,G) + \beta J_{style}(S,G)$$ Exercise: Implement the total cost function which includes both the content cost and the style cost. 123456789101112131415161718192021# GRADED FUNCTION: total_costdef total_cost(J_content, J_style, alpha = 10, beta = 40): """ Computes the total cost function Arguments: J_content -- content cost coded above J_style -- style cost coded above alpha -- hyperparameter weighting the importance of the content cost beta -- hyperparameter weighting the importance of the style cost Returns: J -- total cost as defined by the formula above. """ ### START CODE HERE ### (≈1 line) J = alpha * J_content + beta * J_style; ### END CODE HERE ### return J 12345678tf.reset_default_graph()with tf.Session() as test: np.random.seed(3) J_content = np.random.randn() J_style = np.random.randn() J = total_cost(J_content, J_style) print("J = " + str(J)) J = 35.34667875478276 Expected Output: J 35.34667875478276 What you should remember: The total cost is a linear combination of the content cost $J_{content}(C,G)$ and the style cost $J_{style}(S,G)$ $\alpha$ and $\beta$ are hyperparameters that control the relative weighting between content and style 4 - Solving the optimization problemFinally, let’s put everything together to implement Neural Style Transfer! Here’s what the program will have to do: Create an Interactive Session Load the content image Load the style image Randomly initialize the image to be generated Load the VGG16 model Build the TensorFlow graph: Run the content image through the VGG16 model and compute the content cost Run the style image through the VGG16 model and compute the style cost Compute the total cost Define the optimizer and the learning rate Initialize the TensorFlow graph and run it for a large number of iterations, updating the generated image at every step. Lets go through the individual steps in detail. You’ve previously implemented the overall cost $J(G)$. We’ll now set up TensorFlow to optimize this with respect to $G$. To do so, your program has to reset the graph and use an “Interactive Session“. Unlike a regular session, the “Interactive Session” installs itself as the default session to build a graph. This allows you to run variables without constantly needing to refer to the session object, which simplifies the code. Lets start the interactive session. 12345# Reset the graphtf.reset_default_graph()# Start interactive sessionsess = tf.InteractiveSession() Let’s load, reshape, and normalize our “content” image (the Louvre museum picture): 12content_image = scipy.misc.imread("images/louvre_small.jpg")content_image = reshape_and_normalize_image(content_image) C:\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: DeprecationWarning: `imread` is deprecated! `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0. Use ``imageio.imread`` instead. if __name__ == &apos;__main__&apos;: Let’s load, reshape and normalize our “style” image (Claude Monet’s painting): 12style_image = scipy.misc.imread("images/monet.jpg")style_image = reshape_and_normalize_image(style_image) C:\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: DeprecationWarning: `imread` is deprecated! `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0. Use ``imageio.imread`` instead. if __name__ == &apos;__main__&apos;: Now, we initialize the “generated” image as a noisy image created from the content_image. By initializing the pixels of the generated image to be mostly noise but still slightly correlated with the content image, this will help the content of the “generated” image more rapidly match the content of the “content” image. (Feel free to look in nst_utils.py to see the details of generate_noise_image(...); to do so, click “File–&gt;Open…” at the upper-left corner of this Jupyter notebook.) 12generated_image = generate_noise_image(content_image)imshow(generated_image[0]) &lt;matplotlib.image.AxesImage at 0x23c62573828&gt; Error in callback &lt;function install_repl_displayhook.&lt;locals&gt;.post_execute at 0x0000023C51DE00D0&gt; (for post_execute): --------------------------------------------------------------------------- ValueError Traceback (most recent call last) C:\Anaconda3\lib\site-packages\matplotlib\pyplot.py in post_execute() 148 def post_execute(): 149 if matplotlib.is_interactive(): --&gt; 150 draw_all() 151 152 # IPython &gt;= 2 C:\Anaconda3\lib\site-packages\matplotlib\_pylab_helpers.py in draw_all(cls, force) 148 for f_mgr in cls.get_all_fig_managers(): 149 if force or f_mgr.canvas.figure.stale: --&gt; 150 f_mgr.canvas.draw_idle() 151 152 atexit.register(Gcf.destroy_all) C:\Anaconda3\lib\site-packages\matplotlib\backend_bases.py in draw_idle(self, *args, **kwargs) 2059 if not self._is_idle_drawing: 2060 with self._idle_draw_cntx(): -&gt; 2061 self.draw(*args, **kwargs) 2062 2063 def draw_cursor(self, event): C:\Anaconda3\lib\site-packages\matplotlib\backends\backend_agg.py in draw(self) 428 # if toolbar: 429 # toolbar.set_cursor(cursors.WAIT) --&gt; 430 self.figure.draw(self.renderer) 431 finally: 432 # if toolbar: C:\Anaconda3\lib\site-packages\matplotlib\artist.py in draw_wrapper(artist, renderer, *args, **kwargs) 53 renderer.start_filter() 54 ---&gt; 55 return draw(artist, renderer, *args, **kwargs) 56 finally: 57 if artist.get_agg_filter() is not None: C:\Anaconda3\lib\site-packages\matplotlib\figure.py in draw(self, renderer) 1297 1298 mimage._draw_list_compositing_images( -&gt; 1299 renderer, self, artists, self.suppressComposite) 1300 1301 renderer.close_group(&apos;figure&apos;) C:\Anaconda3\lib\site-packages\matplotlib\image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite) 136 if not_composite or not has_images: 137 for a in artists: --&gt; 138 a.draw(renderer) 139 else: 140 # Composite any adjacent images together C:\Anaconda3\lib\site-packages\matplotlib\artist.py in draw_wrapper(artist, renderer, *args, **kwargs) 53 renderer.start_filter() 54 ---&gt; 55 return draw(artist, renderer, *args, **kwargs) 56 finally: 57 if artist.get_agg_filter() is not None: C:\Anaconda3\lib\site-packages\matplotlib\axes\_base.py in draw(self, renderer, inframe) 2435 renderer.stop_rasterizing() 2436 -&gt; 2437 mimage._draw_list_compositing_images(renderer, self, artists) 2438 2439 renderer.close_group(&apos;axes&apos;) C:\Anaconda3\lib\site-packages\matplotlib\image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite) 136 if not_composite or not has_images: 137 for a in artists: --&gt; 138 a.draw(renderer) 139 else: 140 # Composite any adjacent images together C:\Anaconda3\lib\site-packages\matplotlib\artist.py in draw_wrapper(artist, renderer, *args, **kwargs) 53 renderer.start_filter() 54 ---&gt; 55 return draw(artist, renderer, *args, **kwargs) 56 finally: 57 if artist.get_agg_filter() is not None: C:\Anaconda3\lib\site-packages\matplotlib\image.py in draw(self, renderer, *args, **kwargs) 564 else: 565 im, l, b, trans = self.make_image( --&gt; 566 renderer, renderer.get_image_magnification()) 567 if im is not None: 568 renderer.draw_image(gc, l, b, im) C:\Anaconda3\lib\site-packages\matplotlib\image.py in make_image(self, renderer, magnification, unsampled) 791 return self._make_image( 792 self._A, bbox, transformed_bbox, self.axes.bbox, magnification, --&gt; 793 unsampled=unsampled) 794 795 def _check_unsampled_image(self, renderer): C:\Anaconda3\lib\site-packages\matplotlib\image.py in _make_image(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border) 482 # (of int or float) 483 # or an RGBA array of re-sampled input --&gt; 484 output = self.to_rgba(output, bytes=True, norm=False) 485 # output is now a correctly sized RGBA array of uint8 486 C:\Anaconda3\lib\site-packages\matplotlib\cm.py in to_rgba(self, x, alpha, bytes, norm) 255 if xx.dtype.kind == &apos;f&apos;: 256 if norm and xx.max() &gt; 1 or xx.min() &lt; 0: --&gt; 257 raise ValueError(&quot;Floating point image RGB values &quot; 258 &quot;must be in the 0..1 range.&quot;) 259 if bytes: ValueError: Floating point image RGB values must be in the 0..1 range. --------------------------------------------------------------------------- ValueError Traceback (most recent call last) C:\Anaconda3\lib\site-packages\IPython\core\formatters.py in __call__(self, obj) 339 pass 340 else: --&gt; 341 return printer(obj) 342 # Finally look for special method names 343 method = get_real_method(obj, self.print_method) C:\Anaconda3\lib\site-packages\IPython\core\pylabtools.py in &lt;lambda&gt;(fig) 236 237 if &apos;png&apos; in formats: --&gt; 238 png_formatter.for_type(Figure, lambda fig: print_figure(fig, &apos;png&apos;, **kwargs)) 239 if &apos;retina&apos; in formats or &apos;png2x&apos; in formats: 240 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)) C:\Anaconda3\lib\site-packages\IPython\core\pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs) 120 121 bytes_io = BytesIO() --&gt; 122 fig.canvas.print_figure(bytes_io, **kw) 123 data = bytes_io.getvalue() 124 if fmt == &apos;svg&apos;: C:\Anaconda3\lib\site-packages\matplotlib\backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs) 2214 orientation=orientation, 2215 dryrun=True, -&gt; 2216 **kwargs) 2217 renderer = self.figure._cachedRenderer 2218 bbox_inches = self.figure.get_tightbbox(renderer) C:\Anaconda3\lib\site-packages\matplotlib\backends\backend_agg.py in print_png(self, filename_or_obj, *args, **kwargs) 505 506 def print_png(self, filename_or_obj, *args, **kwargs): --&gt; 507 FigureCanvasAgg.draw(self) 508 renderer = self.get_renderer() 509 original_dpi = renderer.dpi C:\Anaconda3\lib\site-packages\matplotlib\backends\backend_agg.py in draw(self) 428 # if toolbar: 429 # toolbar.set_cursor(cursors.WAIT) --&gt; 430 self.figure.draw(self.renderer) 431 finally: 432 # if toolbar: C:\Anaconda3\lib\site-packages\matplotlib\artist.py in draw_wrapper(artist, renderer, *args, **kwargs) 53 renderer.start_filter() 54 ---&gt; 55 return draw(artist, renderer, *args, **kwargs) 56 finally: 57 if artist.get_agg_filter() is not None: C:\Anaconda3\lib\site-packages\matplotlib\figure.py in draw(self, renderer) 1297 1298 mimage._draw_list_compositing_images( -&gt; 1299 renderer, self, artists, self.suppressComposite) 1300 1301 renderer.close_group(&apos;figure&apos;) C:\Anaconda3\lib\site-packages\matplotlib\image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite) 136 if not_composite or not has_images: 137 for a in artists: --&gt; 138 a.draw(renderer) 139 else: 140 # Composite any adjacent images together C:\Anaconda3\lib\site-packages\matplotlib\artist.py in draw_wrapper(artist, renderer, *args, **kwargs) 53 renderer.start_filter() 54 ---&gt; 55 return draw(artist, renderer, *args, **kwargs) 56 finally: 57 if artist.get_agg_filter() is not None: C:\Anaconda3\lib\site-packages\matplotlib\axes\_base.py in draw(self, renderer, inframe) 2435 renderer.stop_rasterizing() 2436 -&gt; 2437 mimage._draw_list_compositing_images(renderer, self, artists) 2438 2439 renderer.close_group(&apos;axes&apos;) C:\Anaconda3\lib\site-packages\matplotlib\image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite) 136 if not_composite or not has_images: 137 for a in artists: --&gt; 138 a.draw(renderer) 139 else: 140 # Composite any adjacent images together C:\Anaconda3\lib\site-packages\matplotlib\artist.py in draw_wrapper(artist, renderer, *args, **kwargs) 53 renderer.start_filter() 54 ---&gt; 55 return draw(artist, renderer, *args, **kwargs) 56 finally: 57 if artist.get_agg_filter() is not None: C:\Anaconda3\lib\site-packages\matplotlib\image.py in draw(self, renderer, *args, **kwargs) 564 else: 565 im, l, b, trans = self.make_image( --&gt; 566 renderer, renderer.get_image_magnification()) 567 if im is not None: 568 renderer.draw_image(gc, l, b, im) C:\Anaconda3\lib\site-packages\matplotlib\image.py in make_image(self, renderer, magnification, unsampled) 791 return self._make_image( 792 self._A, bbox, transformed_bbox, self.axes.bbox, magnification, --&gt; 793 unsampled=unsampled) 794 795 def _check_unsampled_image(self, renderer): C:\Anaconda3\lib\site-packages\matplotlib\image.py in _make_image(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border) 482 # (of int or float) 483 # or an RGBA array of re-sampled input --&gt; 484 output = self.to_rgba(output, bytes=True, norm=False) 485 # output is now a correctly sized RGBA array of uint8 486 C:\Anaconda3\lib\site-packages\matplotlib\cm.py in to_rgba(self, x, alpha, bytes, norm) 255 if xx.dtype.kind == &apos;f&apos;: 256 if norm and xx.max() &gt; 1 or xx.min() &lt; 0: --&gt; 257 raise ValueError(&quot;Floating point image RGB values &quot; 258 &quot;must be in the 0..1 range.&quot;) 259 if bytes: ValueError: Floating point image RGB values must be in the 0..1 range. &lt;matplotlib.figure.Figure at 0x23c6251cda0&gt; Next, as explained in part (2), let’s load the VGG16 model. 1model = load_vgg_model("pretrained-model/imagenet-vgg-verydeep-19.mat") To get the program to compute the content cost, we will now assign a_C and a_G to be the appropriate hidden layer activations. We will use layer conv4_2 to compute the content cost. The code below does the following: Assign the content image to be the input to the VGG model. Set a_C to be the tensor giving the hidden layer activation for layer “conv4_2”. Set a_G to be the tensor giving the hidden layer activation for the same layer. Compute the content cost using a_C and a_G. 12345678910111213141516# Assign the content image to be the input of the VGG model. sess.run(model['input'].assign(content_image))# Select the output tensor of layer conv4_2out = model['conv4_2']# Set a_C to be the hidden layer activation from the layer we have selecteda_C = sess.run(out)# Set a_G to be the hidden layer activation from same layer. Here, a_G references model['conv4_2'] # and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that# when we run the session, this will be the activations drawn from the appropriate layer, with G as input.a_G = out# Compute the content costJ_content = compute_content_cost(a_C, a_G) Note: At this point, a_G is a tensor and hasn’t been evaluated. It will be evaluated and updated at each iteration when we run the Tensorflow graph in model_nn() below. 12345# Assign the input of the model to be the "style" image sess.run(model['input'].assign(style_image))# Compute the style costJ_style = compute_style_cost(model, STYLE_LAYERS) Exercise: Now that you have J_content and J_style, compute the total cost J by calling total_cost(). Use alpha = 10 and beta = 40. 123### START CODE HERE ### (1 line)J = total_cost(J_content, J_style);### END CODE HERE ### You’d previously learned how to set up the Adam optimizer in TensorFlow. Lets do that here, using a learning rate of 2.0. See reference 12345# define optimizer (1 line)optimizer = tf.train.AdamOptimizer(2.0)# define train_step (1 line)train_step = optimizer.minimize(J) Exercise: Implement the model_nn() function which initializes the variables of the tensorflow graph, assigns the input image (initial generated image) as the input of the VGG16 model and runs the train_step for a large number of steps. 123456789101112131415161718192021222324252627282930313233343536373839def model_nn(sess, input_image, num_iterations = 200): # Initialize global variables (you need to run the session on the initializer) ### START CODE HERE ### (1 line) sess.run(tf.global_variables_initializer()); ### END CODE HERE ### # Run the noisy input image (initial generated image) through the model. Use assign(). ### START CODE HERE ### (1 line) sess.run(model['input'].assign(input_image)); ### END CODE HERE ### for i in range(num_iterations): # Run the session on the train_step to minimize the total cost ### START CODE HERE ### (1 line) sess.run(train_step); ### END CODE HERE ### # Compute the generated image by running the session on the current model['input'] ### START CODE HERE ### (1 line) generated_image = sess.run(model['input']); ### END CODE HERE ### # Print every 20 iteration. if i%20 == 0: Jt, Jc, Js = sess.run([J, J_content, J_style]) print("Iteration " + str(i) + " :") print("total cost = " + str(Jt)) print("content cost = " + str(Jc)) print("style cost = " + str(Js)) # save current generated image in the "/output" directory save_image("output/" + str(i) + ".png", generated_image) # save last generated image save_image('output/generated_image.jpg', generated_image) return generated_image Run the following cell to generate an artistic image. It should take about 3min on CPU for every 20 iterations but you start observing attractive results after ≈140 iterations. Neural Style Transfer is generally trained using GPUs. 1model_nn(sess, generated_image) Expected Output: Iteration 0 : total cost = 5.05035e+09 content cost = 7877.67 style cost = 1.26257e+08 You’re done! After running this, in the upper bar of the notebook click on “File” and then “Open”. Go to the “/output” directory to see all the saved images. Open “generated_image” to see the generated image! :) You should see something the image presented below on the right: We didn’t want you to wait too long to see an initial result, and so had set the hyperparameters accordingly. To get the best looking results, running the optimization algorithm longer (and perhaps with a smaller learning rate) might work better. After completing and submitting this assignment, we encourage you to come back and play more with this notebook, and see if you can generate even better looking images. Here are few other examples: The beautiful ruins of the ancient city of Persepolis (Iran) with the style of Van Gogh (The Starry Night) The tomb of Cyrus the great in Pasargadae with the style of a Ceramic Kashi from Ispahan. A scientific study of a turbulent fluid with the style of a abstract blue fluid painting. 5 - Test with your own image (Optional/Ungraded)Finally, you can also rerun the algorithm on your own images! To do so, go back to part 4 and change the content image and style image with your own pictures. In detail, here’s what you should do: Click on “File -&gt; Open” in the upper tab of the notebook Go to “/images” and upload your images (requirement: (WIDTH = 300, HEIGHT = 225)), rename them “my_content.png” and “my_style.png” for example. Change the code in part (3.4) from :12content_image = scipy.misc.imread("images/louvre.jpg")style_image = scipy.misc.imread("images/claude-monet.jpg") to:12content_image = scipy.misc.imread("images/my_content.jpg")style_image = scipy.misc.imread("images/my_style.jpg") Rerun the cells (you may need to restart the Kernel in the upper tab of the notebook). You can share your generated images with us on social media with the hashtag #deeplearniNgAI or by direct tagging! You can also tune your hyperparameters: Which layers are responsible for representing the style? STYLE_LAYERS How many iterations do you want to run the algorithm? num_iterations What is the relative weighting between content and style? alpha/beta 6 - ConclusionGreat job on completing this assignment! You are now able to use Neural Style Transfer to generate artistic images. This is also your first time building a model in which the optimization algorithm updates the pixel values rather than the neural network’s parameters. Deep learning has many different types of models and this is only one of them! What you should remember: Neural Style Transfer is an algorithm that given a content image C and a style image S can generate an artistic image It uses representations (hidden layer activations) based on a pretrained ConvNet. The content cost function is computed using one hidden layer’s activations. The style cost function for one layer is computed using the Gram matrix of that layer’s activations. The overall style cost function is obtained using several hidden layers. Optimizing the total cost function results in synthesizing new images. This was the final programming exercise of this course. Congratulations–you’ve finished all the programming exercises of this course on Convolutional Networks! We hope to also see you in Course 5, on Sequence models! References:The Neural Style Transfer algorithm was due to Gatys et al. (2015). Harish Narayanan and Github user “log0” also have highly readable write-ups from which we drew inspiration. The pre-trained network used in this implementation is a VGG network, which is due to Simonyan and Zisserman (2015). Pre-trained weights were from the work of the MathConvNet team. Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, (2015). A Neural Algorithm of Artistic Style (https://arxiv.org/abs/1508.06576) Harish Narayanan, Convolutional neural networks for artistic style transfer. https://harishnarayanan.org/writing/artistic-style-transfer/ Log0, TensorFlow Implementation of “A Neural Algorithm of Artistic Style”. http://www.chioka.in/tensorflow-implementation-neural-algorithm-of-artistic-style Karen Simonyan and Andrew Zisserman (2015). Very deep convolutional networks for large-scale image recognition (https://arxiv.org/pdf/1409.1556.pdf) MatConvNet. http://www.vlfeat.org/matconvnet/pretrained/]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>convolutional-neural-networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Autonomous driving - Car detection]]></title>
    <url>%2F2018%2F05%2F03%2FAutonomous%2Bdriving%2Bapplication%2B-%2BCar%2Bdetection%2B-%2Bv3%2F</url>
    <content type="text"><![CDATA[NoteThese are my personal programming assignments at the 3rd week after studying the course convolutional neural networks and the copyright belongs to deeplearning.ai. Welcome to your week 3 programming assignment. You will learn about object detection using the very powerful YOLO model. Many of the ideas in this notebook are described in the two YOLO papers: Redmon et al., 2016 (https://arxiv.org/abs/1506.02640) and Redmon and Farhadi, 2016 (https://arxiv.org/abs/1612.08242). You will learn to: Use object detection on a car detection dataset Deal with bounding boxes Run the following cell to load the packages and dependencies that are going to be useful for your journey! 1234567891011121314151617import argparseimport osimport matplotlib.pyplot as pltfrom matplotlib.pyplot import imshowimport scipy.ioimport scipy.miscimport numpy as npimport pandas as pdimport PILimport tensorflow as tffrom keras import backend as Kfrom keras.layers import Input, Lambda, Conv2Dfrom keras.models import load_model, Modelfrom yolo_utils import read_classes, read_anchors, generate_colors, preprocess_image, draw_boxes, scale_boxesfrom yad2k.models.keras_yolo import yolo_head, yolo_boxes_to_corners, preprocess_true_boxes, yolo_loss, yolo_body%matplotlib inline C:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Using TensorFlow backend. Important Note: As you can see, we import Keras’s backend as K. This means that to use a Keras function in this notebook, you will need to write: K.function(...). 1 - Problem StatementYou are working on a self-driving car. As a critical component of this project, you’d like to first build a car detection system. To collect data, you’ve mounted a camera to the hood (meaning the front) of the car, which takes pictures of the road ahead every few seconds while you drive around. Pictures taken from a car-mounted camera while driving around Silicon Valley. We would like to especially thank drive.ai for providing this dataset! Drive.ai is a company building the brains of self-driving vehicles. You’ve gathered all these images into a folder and have labelled them by drawing bounding boxes around every car you found. Here’s an example of what your bounding boxes look like. Figure 1 : Definition of a box If you have 80 classes that you want YOLO to recognize, you can represent the class label $c$ either as an integer from 1 to 80, or as an 80-dimensional vector (with 80 numbers) one component of which is 1 and the rest of which are 0. The video lectures had used the latter representation; in this notebook, we will use both representations, depending on which is more convenient for a particular step. In this exercise, you will learn how YOLO works, then apply it to car detection. Because the YOLO model is very computationally expensive to train, we will load pre-trained weights for you to use. 2 - YOLOYOLO (“you only look once”) is a popular algoritm because it achieves high accuracy while also being able to run in real-time. This algorithm “only looks once” at the image in the sense that it requires only one forward propagation pass through the network to make predictions. After non-max suppression, it then outputs recognized objects together with the bounding boxes. 2.1 - Model detailsFirst things to know: The input is a batch of images of shape (m, 608, 608, 3) The output is a list of bounding boxes along with the recognized classes. Each bounding box is represented by 6 numbers $(p_c, b_x, b_y, b_h, b_w, c)$ as explained above. If you expand $c$ into an 80-dimensional vector, each bounding box is then represented by 85 numbers. We will use 5 anchor boxes. So you can think of the YOLO architecture as the following: IMAGE (m, 608, 608, 3) -&gt; DEEP CNN -&gt; ENCODING (m, 19, 19, 5, 85). Lets look in greater detail at what this encoding represents. Figure 2 : Encoding architecture for YOLO If the center/midpoint of an object falls into a grid cell, that grid cell is responsible for detecting that object. Since we are using 5 anchor boxes, each of the 19 x19 cells thus encodes information about 5 boxes. Anchor boxes are defined only by their width and height. For simplicity, we will flatten the last two last dimensions of the shape (19, 19, 5, 85) encoding. So the output of the Deep CNN is (19, 19, 425). Figure 3 : Flattening the last two last dimensions Now, for each box (of each cell) we will compute the following elementwise product and extract a probability that the box contains a certain class. Figure 4 : Find the class detected by each box Here’s one way to visualize what YOLO is predicting on an image: For each of the 19x19 grid cells, find the maximum of the probability scores (taking a max across both the 5 anchor boxes and across different classes). Color that grid cell according to what object that grid cell considers the most likely. Doing this results in this picture: Figure 5 : Each of the 19x19 grid cells colored according to which class has the largest predicted probability in that cell. Note that this visualization isn’t a core part of the YOLO algorithm itself for making predictions; it’s just a nice way of visualizing an intermediate result of the algorithm. Another way to visualize YOLO’s output is to plot the bounding boxes that it outputs. Doing that results in a visualization like this: Figure 6 : Each cell gives you 5 boxes. In total, the model predicts: 19x19x5 = 1805 boxes just by looking once at the image (one forward pass through the network)! Different colors denote different classes. In the figure above, we plotted only boxes that the model had assigned a high probability to, but this is still too many boxes. You’d like to filter the algorithm’s output down to a much smaller number of detected objects. To do so, you’ll use non-max suppression. Specifically, you’ll carry out these steps: Get rid of boxes with a low score (meaning, the box is not very confident about detecting a class) Select only one box when several boxes overlap with each other and detect the same object. 2.2 - Filtering with a threshold on class scoresYou are going to apply a first filter by thresholding. You would like to get rid of any box for which the class “score” is less than a chosen threshold. The model gives you a total of 19x19x5x85 numbers, with each box described by 85 numbers. It’ll be convenient to rearrange the (19,19,5,85) (or (19,19,425)) dimensional tensor into the following variables: box_confidence: tensor of shape $(19 \times 19, 5, 1)$ containing $p_c$ (confidence probability that there’s some object) for each of the 5 boxes predicted in each of the 19x19 cells. boxes: tensor of shape $(19 \times 19, 5, 4)$ containing $(b_x, b_y, b_h, b_w)$ for each of the 5 boxes per cell. box_class_probs: tensor of shape $(19 \times 19, 5, 80)$ containing the detection probabilities $(c_1, c_2, … c_{80})$ for each of the 80 classes for each of the 5 boxes per cell. Exercise: Implement yolo_filter_boxes(). Compute box scores by doing the elementwise product as described in Figure 4. The following code may help you choose the right operator: 123a = np.random.randn(19 * 19, 5, 1)b = np.random.randn(19 * 19, 5, 80)c = a * b # shape of c will be (19*19, 5, 80) For each box, find: the index of the class with the maximum box score (Hint) (Be careful with what axis you choose; consider using axis=-1) the corresponding box score (Hint) (Be careful with what axis you choose; consider using axis=-1) Create a mask by using a threshold. As a reminder: ([0.9, 0.3, 0.4, 0.5, 0.1] &lt; 0.4) returns: [False, True, False, False, True]. The mask should be True for the boxes you want to keep. Use TensorFlow to apply the mask to box_class_scores, boxes and box_classes to filter out the boxes we don’t want. You should be left with just the subset of boxes you want to keep. (Hint) Reminder: to call a Keras function, you should use K.function(...). 123456789101112131415161718192021222324252627282930313233343536373839404142434445# GRADED FUNCTION: yolo_filter_boxesdef yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = .6): """Filters YOLO boxes by thresholding on object and class confidence. Arguments: box_confidence -- tensor of shape (19, 19, 5, 1) boxes -- tensor of shape (19, 19, 5, 4) box_class_probs -- tensor of shape (19, 19, 5, 80) threshold -- real value, if [ highest class probability score &lt; threshold], then get rid of the corresponding box Returns: scores -- tensor of shape (None,), containing the class probability score for selected boxes boxes -- tensor of shape (None, 4), containing (b_x, b_y, b_h, b_w) coordinates of selected boxes classes -- tensor of shape (None,), containing the index of the class detected by the selected boxes Note: "None" is here because you don't know the exact number of selected boxes, as it depends on the threshold. For example, the actual output size of scores would be (10,) if there are 10 boxes. """ # Step 1: Compute box scores ### START CODE HERE ### (≈ 1 line) box_scores = box_confidence * box_class_probs; ### END CODE HERE ### # Step 2: Find the box_classes thanks to the max box_scores, keep track of the corresponding score ### START CODE HERE ### (≈ 2 lines) box_classes = K.argmax(box_scores, -1); box_class_scores = K.max(box_scores, -1, keepdims = False); ### END CODE HERE ### # Step 3: Create a filtering mask based on "box_class_scores" by using "threshold". The mask should have the # same dimension as box_class_scores, and be True for the boxes you want to keep (with probability &gt;= threshold) ### START CODE HERE ### (≈ 1 line) filtering_mask = box_class_scores &gt;= threshold; ### END CODE HERE ### # Step 4: Apply the mask to scores, boxes and classes ### START CODE HERE ### (≈ 3 lines) scores = tf.boolean_mask(box_class_scores, filtering_mask); boxes = tf.boolean_mask(boxes, filtering_mask); classes = tf.boolean_mask(box_classes, filtering_mask); ### END CODE HERE ### return scores, boxes, classes 1234567891011with tf.Session() as test_a: box_confidence = tf.random_normal([19, 19, 5, 1], mean=1, stddev=4, seed = 1) boxes = tf.random_normal([19, 19, 5, 4], mean=1, stddev=4, seed = 1) box_class_probs = tf.random_normal([19, 19, 5, 80], mean=1, stddev=4, seed = 1) scores, boxes, classes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = 0.5) print("scores[2] = " + str(scores[2].eval())); print("boxes[2] = " + str(boxes[2].eval())); print("classes[2] = " + str(classes[2].eval())); print("scores.shape = " + str(scores.shape)); print("boxes.shape = " + str(boxes.shape)); print("classes.shape = " + str(classes.shape)); scores[2] = 10.750582 boxes[2] = [ 8.426533 3.2713668 -0.5313436 -4.9413733] classes[2] = 7 scores.shape = (?,) boxes.shape = (?, 4) classes.shape = (?,) Expected Output: variable value scores[2] 10.7506 boxes[2] [ 8.42653275 3.27136683 -0.5313437 -4.94137383] classes[2] 7 scores.shape (?,) boxes.shape (?, 4) classes.shape (?,) 2.3 - Non-max suppressionEven after filtering by thresholding over the classes scores, you still end up a lot of overlapping boxes. A second filter for selecting the right boxes is called non-maximum suppression (NMS). Figure 7 : In this example, the model has predicted 3 cars, but it’s actually 3 predictions of the same car. Running non-max suppression (NMS) will select only the most accurate (highest probabiliy) one of the 3 boxes. Non-max suppression uses the very important function called “Intersection over Union”, or IoU. Figure 8 : Definition of “Intersection over Union”. Exercise: Implement iou(). Some hints: In this exercise only, we define a box using its two corners (upper left and lower right): (x1, y1, x2, y2) rather than the midpoint and height/width. To calculate the area of a rectangle you need to multiply its height (y2 - y1) by its width (x2 - x1). You’ll also need to find the coordinates (xi1, yi1, xi2, yi2) of the intersection of two boxes. Remember that: xi1 = maximum of the x1 coordinates of the two boxes yi1 = maximum of the y1 coordinates of the two boxes xi2 = minimum of the x2 coordinates of the two boxes yi2 = minimum of the y2 coordinates of the two boxes In order to compute the intersection area, you need to make sure the height and width of the intersection are positive, otherwise the intersection area should be zero. Use max(height, 0) and max(width, 0). In this code, we use the convention that (0,0) is the top-left corner of an image, (1,0) is the upper-right corner, and (1,1) the lower-right corner. 1234567891011121314151617181920212223242526272829303132# GRADED FUNCTION: ioudef iou(box1, box2): """Implement the intersection over union (IoU) between box1 and box2 Arguments: box1 -- first box, list object with coordinates (x1, y1, x2, y2) box2 -- second box, list object with coordinates (x1, y1, x2, y2) """ # Calculate the (y1, x1, y2, x2) coordinates of the intersection of box1 and box2. Calculate its Area. ### START CODE HERE ### (≈ 5 lines) xi1 = max(box1[0], box2[0]); yi1 = max(box1[1], box2[1]); xi2 = min(box1[2], box2[2]); yi2 = min(box1[3], box2[3]); inter_area = max((xi2 - xi1), 0) * max((yi2 - yi1), 0); ### END CODE HERE ### # Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B) ### START CODE HERE ### (≈ 3 lines) box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1]); box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1]); union_area = box1_area + box2_area - inter_area; ### END CODE HERE ### # compute the IoU ### START CODE HERE ### (≈ 1 line) iou = inter_area / union_area; ### END CODE HERE ### return iou 123box1 = (2, 1, 4, 3)box2 = (1, 2, 3, 4) print("iou = " + str(iou(box1, box2))) iou = 0.14285714285714285 Expected Output: variable value iou = 0.14285714285714285 You are now ready to implement non-max suppression. The key steps are: Select the box that has the highest score. Compute its overlap with all other boxes, and remove boxes that overlap it more than iou_threshold. Go back to step 1 and iterate until there’s no more boxes with a lower score than the current selected box. This will remove all boxes that have a large overlap with the selected boxes. Only the “best” boxes remain. Exercise: Implement yolo_non_max_suppression() using TensorFlow. TensorFlow has two built-in functions that are used to implement non-max suppression (so you don’t actually need to use your iou() implementation): tf.image.non_max_suppression() K.gather() 1234567891011121314151617181920212223242526272829303132333435363738# GRADED FUNCTION: yolo_non_max_suppressiondef yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5): """ Applies Non-max suppression (NMS) to set of boxes Arguments: scores -- tensor of shape (None,), output of yolo_filter_boxes() boxes -- tensor of shape (None, 4), output of yolo_filter_boxes() that have been scaled to the image size (see later) classes -- tensor of shape (None,), output of yolo_filter_boxes() max_boxes -- integer, maximum number of predicted boxes you'd like iou_threshold -- real value, "intersection over union" threshold used for NMS filtering Returns: scores -- tensor of shape (, None), predicted score for each box boxes -- tensor of shape (4, None), predicted box coordinates classes -- tensor of shape (, None), predicted class for each box Note: The "None" dimension of the output tensors has obviously to be less than max_boxes. Note also that this function will transpose the shapes of scores, boxes, classes. This is made for convenience. """ max_boxes_tensor = K.variable(max_boxes, dtype='int32') # tensor to be used in tf.image.non_max_suppression() K.get_session().run(tf.variables_initializer([max_boxes_tensor])) # initialize variable max_boxes_tensor # Use tf.image.non_max_suppression() to get the list of indices corresponding to boxes you keep ### START CODE HERE ### (≈ 1 line) nms_indices = tf.image.non_max_suppression(boxes, scores, max_boxes, iou_threshold); ### END CODE HERE ### # Use K.gather() to select only nms_indices from scores, boxes and classes ### START CODE HERE ### (≈ 3 lines) scores = K.gather(scores, nms_indices); boxes = K.gather(boxes, nms_indices); classes = K.gather(classes, nms_indices); ### END CODE HERE ### return scores, boxes, classes 1234567891011with tf.Session() as test_b: scores = tf.random_normal([54,], mean=1, stddev=4, seed = 1) boxes = tf.random_normal([54, 4], mean=1, stddev=4, seed = 1) classes = tf.random_normal([54,], mean=1, stddev=4, seed = 1) scores, boxes, classes = yolo_non_max_suppression(scores, boxes, classes) print("scores[2] = " + str(scores[2].eval())) print("boxes[2] = " + str(boxes[2].eval())) print("classes[2] = " + str(classes[2].eval())) print("scores.shape = " + str(scores.eval().shape)) print("boxes.shape = " + str(boxes.eval().shape)) print("classes.shape = " + str(classes.eval().shape)) scores[2] = 6.938395 boxes[2] = [-5.299932 3.1379814 4.450367 0.95942086] classes[2] = -2.2452729 scores.shape = (10,) boxes.shape = (10, 4) classes.shape = (10,) Expected Output: variable value scores[2] 6.9384 boxes[2] [-5.299932 3.13798141 4.45036697 0.95942086] classes[2] -2.24527 scores.shape (10,) boxes.shape (10, 4) classes.shape (10,) 2.4 Wrapping up the filteringIt’s time to implement a function taking the output of the deep CNN (the 19x19x5x85 dimensional encoding) and filtering through all the boxes using the functions you’ve just implemented. Exercise: Implement yolo_eval() which takes the output of the YOLO encoding and filters the boxes using score threshold and NMS. There’s just one last implementational detail you have to know. There’re a few ways of representing boxes, such as via their corners or via their midpoint and height/width. YOLO converts between a few such formats at different times, using the following functions (which we have provided): 1boxes = yolo_boxes_to_corners(box_xy, box_wh) which converts the yolo box coordinates (x,y,w,h) to box corners’ coordinates (x1, y1, x2, y2) to fit the input of yolo_filter_boxes1boxes = scale_boxes(boxes, image_shape) YOLO’s network was trained to run on 608x608 images. If you are testing this data on a different size image–for example, the car detection dataset had 720x1280 images–this step rescales the boxes so that they can be plotted on top of the original 720x1280 image. Don’t worry about these two functions; we’ll show you where they need to be called. 12345678910111213141516171819202122232425262728293031323334353637383940414243# GRADED FUNCTION: yolo_evaldef yolo_eval(yolo_outputs, image_shape = (720., 1280.), max_boxes=10, score_threshold=.6, iou_threshold=.5): """ Converts the output of YOLO encoding (a lot of boxes) to your predicted boxes along with their scores, box coordinates and classes. Arguments: yolo_outputs -- output of the encoding model (for image_shape of (608, 608, 3)), contains 4 tensors: box_confidence: tensor of shape (None, 19, 19, 5, 1) box_xy: tensor of shape (None, 19, 19, 5, 2) box_wh: tensor of shape (None, 19, 19, 5, 2) box_class_probs: tensor of shape (None, 19, 19, 5, 80) image_shape -- tensor of shape (2,) containing the input shape, in this notebook we use (608., 608.) (has to be float32 dtype) max_boxes -- integer, maximum number of predicted boxes you'd like score_threshold -- real value, if [ highest class probability score &lt; threshold], then get rid of the corresponding box iou_threshold -- real value, "intersection over union" threshold used for NMS filtering Returns: scores -- tensor of shape (None, ), predicted score for each box boxes -- tensor of shape (None, 4), predicted box coordinates classes -- tensor of shape (None,), predicted class for each box """ ### START CODE HERE ### # Retrieve outputs of the YOLO model (≈1 line) box_confidence, box_xy, box_wh, box_class_probs = yolo_outputs; # Convert boxes to be ready for filtering functions boxes = yolo_boxes_to_corners(box_xy, box_wh) # Use one of the functions you've implemented to perform Score-filtering with a threshold of score_threshold (≈1 line) scores, boxes, classes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, score_threshold); # Scale boxes back to original image shape. boxes = scale_boxes(boxes, image_shape) # Use one of the functions you've implemented to perform Non-max suppression with a threshold of iou_threshold (≈1 line) scores, boxes, classes = yolo_non_max_suppression(scores, boxes, classes, max_boxes, iou_threshold); ### END CODE HERE ### return scores, boxes, classes 123456789101112with tf.Session() as test_b: yolo_outputs = (tf.random_normal([19, 19, 5, 1], mean=1, stddev=4, seed = 1), tf.random_normal([19, 19, 5, 2], mean=1, stddev=4, seed = 1), tf.random_normal([19, 19, 5, 2], mean=1, stddev=4, seed = 1), tf.random_normal([19, 19, 5, 80], mean=1, stddev=4, seed = 1)) scores, boxes, classes = yolo_eval(yolo_outputs) print("scores[2] = " + str(scores[2].eval())) print("boxes[2] = " + str(boxes[2].eval())) print("classes[2] = " + str(classes[2].eval())) print("scores.shape = " + str(scores.eval().shape)) print("boxes.shape = " + str(boxes.eval().shape)) print("classes.shape = " + str(classes.eval().shape)) scores[2] = 138.79124 boxes[2] = [1292.3297 -278.52167 3876.9893 -835.56494] classes[2] = 54 scores.shape = (10,) boxes.shape = (10, 4) classes.shape = (10,) Expected Output: variable value scores[2] 138.791 boxes[2] [ 1292.32971191 -278.52166748 3876.98925781 -835.56494141] classes[2] 54 scores.shape (10,) boxes.shape (10, 4) classes.shape (10,) Summary for YOLO:- Input image (608, 608, 3)- The input image goes through a CNN, resulting in a (19,19,5,85) dimensional output.- After flattening the last two dimensions, the output is a volume of shape (19, 19, 425): - Each cell in a 19x19 grid over the input image gives 425 numbers. - 425 = 5 x 85 because each cell contains predictions for 5 boxes, corresponding to 5 anchor boxes, as seen in lecture. - 85 = 5 + 80 where 5 is because $(p_c, b_x, b_y, b_h, b_w)$ has 5 numbers, and and 80 is the number of classes we’d like to detect- You then select only few boxes based on: - Score-thresholding: throw away boxes that have detected a class with a score less than the threshold - Non-max suppression: Compute the Intersection over Union and avoid selecting overlapping boxes- This gives you YOLO’s final output. 3 - Test YOLO pretrained model on imagesIn this part, you are going to use a pretrained model and test it on the car detection dataset. As usual, you start by creating a session to start your graph. Run the following cell. 1sess = K.get_session() 3.1 - Defining classes, anchors and image shape.Recall that we are trying to detect 80 classes, and are using 5 anchor boxes. We have gathered the information about the 80 classes and 5 boxes in two files “coco_classes.txt” and “yolo_anchors.txt”. Let’s load these quantities into the model by running the next cell. The car detection dataset has 720x1280 images, which we’ve pre-processed into 608x608 images. 123class_names = read_classes("model_data/coco_classes.txt")anchors = read_anchors("model_data/yolo_anchors.txt")image_shape = (720., 1280.) 3.2 - Loading a pretrained modelTraining a YOLO model takes a very long time and requires a fairly large dataset of labelled bounding boxes for a large range of target classes. You are going to load an existing pretrained Keras YOLO model stored in “yolo.h5”. (These weights come from the official YOLO website, and were converted using a function written by Allan Zelener. References are at the end of this notebook. Technically, these are the parameters from the “YOLOv2” model, but we will more simply refer to it as “YOLO” in this notebook.) Run the cell below to load the model from this file. 1yolo_model = load_model("model_data/yolo.h5") C:\Anaconda3\lib\site-packages\keras\models.py:282: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually. warnings.warn(&apos;No training configuration found in save file: &apos; This loads the weights of a trained YOLO model. Here’s a summary of the layers your model contains. 1yolo_model.summary() __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) (None, 608, 608, 3) 0 __________________________________________________________________________________________________ conv2d_1 (Conv2D) (None, 608, 608, 32) 864 input_1[0][0] __________________________________________________________________________________________________ batch_normalization_1 (BatchNor (None, 608, 608, 32) 128 conv2d_1[0][0] __________________________________________________________________________________________________ leaky_re_lu_1 (LeakyReLU) (None, 608, 608, 32) 0 batch_normalization_1[0][0] __________________________________________________________________________________________________ max_pooling2d_1 (MaxPooling2D) (None, 304, 304, 32) 0 leaky_re_lu_1[0][0] __________________________________________________________________________________________________ conv2d_2 (Conv2D) (None, 304, 304, 64) 18432 max_pooling2d_1[0][0] __________________________________________________________________________________________________ batch_normalization_2 (BatchNor (None, 304, 304, 64) 256 conv2d_2[0][0] __________________________________________________________________________________________________ leaky_re_lu_2 (LeakyReLU) (None, 304, 304, 64) 0 batch_normalization_2[0][0] __________________________________________________________________________________________________ max_pooling2d_2 (MaxPooling2D) (None, 152, 152, 64) 0 leaky_re_lu_2[0][0] __________________________________________________________________________________________________ conv2d_3 (Conv2D) (None, 152, 152, 128 73728 max_pooling2d_2[0][0] __________________________________________________________________________________________________ batch_normalization_3 (BatchNor (None, 152, 152, 128 512 conv2d_3[0][0] __________________________________________________________________________________________________ leaky_re_lu_3 (LeakyReLU) (None, 152, 152, 128 0 batch_normalization_3[0][0] __________________________________________________________________________________________________ conv2d_4 (Conv2D) (None, 152, 152, 64) 8192 leaky_re_lu_3[0][0] __________________________________________________________________________________________________ batch_normalization_4 (BatchNor (None, 152, 152, 64) 256 conv2d_4[0][0] __________________________________________________________________________________________________ leaky_re_lu_4 (LeakyReLU) (None, 152, 152, 64) 0 batch_normalization_4[0][0] __________________________________________________________________________________________________ conv2d_5 (Conv2D) (None, 152, 152, 128 73728 leaky_re_lu_4[0][0] __________________________________________________________________________________________________ batch_normalization_5 (BatchNor (None, 152, 152, 128 512 conv2d_5[0][0] __________________________________________________________________________________________________ leaky_re_lu_5 (LeakyReLU) (None, 152, 152, 128 0 batch_normalization_5[0][0] __________________________________________________________________________________________________ max_pooling2d_3 (MaxPooling2D) (None, 76, 76, 128) 0 leaky_re_lu_5[0][0] __________________________________________________________________________________________________ conv2d_6 (Conv2D) (None, 76, 76, 256) 294912 max_pooling2d_3[0][0] __________________________________________________________________________________________________ batch_normalization_6 (BatchNor (None, 76, 76, 256) 1024 conv2d_6[0][0] __________________________________________________________________________________________________ leaky_re_lu_6 (LeakyReLU) (None, 76, 76, 256) 0 batch_normalization_6[0][0] __________________________________________________________________________________________________ conv2d_7 (Conv2D) (None, 76, 76, 128) 32768 leaky_re_lu_6[0][0] __________________________________________________________________________________________________ batch_normalization_7 (BatchNor (None, 76, 76, 128) 512 conv2d_7[0][0] __________________________________________________________________________________________________ leaky_re_lu_7 (LeakyReLU) (None, 76, 76, 128) 0 batch_normalization_7[0][0] __________________________________________________________________________________________________ conv2d_8 (Conv2D) (None, 76, 76, 256) 294912 leaky_re_lu_7[0][0] __________________________________________________________________________________________________ batch_normalization_8 (BatchNor (None, 76, 76, 256) 1024 conv2d_8[0][0] __________________________________________________________________________________________________ leaky_re_lu_8 (LeakyReLU) (None, 76, 76, 256) 0 batch_normalization_8[0][0] __________________________________________________________________________________________________ max_pooling2d_4 (MaxPooling2D) (None, 38, 38, 256) 0 leaky_re_lu_8[0][0] __________________________________________________________________________________________________ conv2d_9 (Conv2D) (None, 38, 38, 512) 1179648 max_pooling2d_4[0][0] __________________________________________________________________________________________________ batch_normalization_9 (BatchNor (None, 38, 38, 512) 2048 conv2d_9[0][0] __________________________________________________________________________________________________ leaky_re_lu_9 (LeakyReLU) (None, 38, 38, 512) 0 batch_normalization_9[0][0] __________________________________________________________________________________________________ conv2d_10 (Conv2D) (None, 38, 38, 256) 131072 leaky_re_lu_9[0][0] __________________________________________________________________________________________________ batch_normalization_10 (BatchNo (None, 38, 38, 256) 1024 conv2d_10[0][0] __________________________________________________________________________________________________ leaky_re_lu_10 (LeakyReLU) (None, 38, 38, 256) 0 batch_normalization_10[0][0] __________________________________________________________________________________________________ conv2d_11 (Conv2D) (None, 38, 38, 512) 1179648 leaky_re_lu_10[0][0] __________________________________________________________________________________________________ batch_normalization_11 (BatchNo (None, 38, 38, 512) 2048 conv2d_11[0][0] __________________________________________________________________________________________________ leaky_re_lu_11 (LeakyReLU) (None, 38, 38, 512) 0 batch_normalization_11[0][0] __________________________________________________________________________________________________ conv2d_12 (Conv2D) (None, 38, 38, 256) 131072 leaky_re_lu_11[0][0] __________________________________________________________________________________________________ batch_normalization_12 (BatchNo (None, 38, 38, 256) 1024 conv2d_12[0][0] __________________________________________________________________________________________________ leaky_re_lu_12 (LeakyReLU) (None, 38, 38, 256) 0 batch_normalization_12[0][0] __________________________________________________________________________________________________ conv2d_13 (Conv2D) (None, 38, 38, 512) 1179648 leaky_re_lu_12[0][0] __________________________________________________________________________________________________ batch_normalization_13 (BatchNo (None, 38, 38, 512) 2048 conv2d_13[0][0] __________________________________________________________________________________________________ leaky_re_lu_13 (LeakyReLU) (None, 38, 38, 512) 0 batch_normalization_13[0][0] __________________________________________________________________________________________________ max_pooling2d_5 (MaxPooling2D) (None, 19, 19, 512) 0 leaky_re_lu_13[0][0] __________________________________________________________________________________________________ conv2d_14 (Conv2D) (None, 19, 19, 1024) 4718592 max_pooling2d_5[0][0] __________________________________________________________________________________________________ batch_normalization_14 (BatchNo (None, 19, 19, 1024) 4096 conv2d_14[0][0] __________________________________________________________________________________________________ leaky_re_lu_14 (LeakyReLU) (None, 19, 19, 1024) 0 batch_normalization_14[0][0] __________________________________________________________________________________________________ conv2d_15 (Conv2D) (None, 19, 19, 512) 524288 leaky_re_lu_14[0][0] __________________________________________________________________________________________________ batch_normalization_15 (BatchNo (None, 19, 19, 512) 2048 conv2d_15[0][0] __________________________________________________________________________________________________ leaky_re_lu_15 (LeakyReLU) (None, 19, 19, 512) 0 batch_normalization_15[0][0] __________________________________________________________________________________________________ conv2d_16 (Conv2D) (None, 19, 19, 1024) 4718592 leaky_re_lu_15[0][0] __________________________________________________________________________________________________ batch_normalization_16 (BatchNo (None, 19, 19, 1024) 4096 conv2d_16[0][0] __________________________________________________________________________________________________ leaky_re_lu_16 (LeakyReLU) (None, 19, 19, 1024) 0 batch_normalization_16[0][0] __________________________________________________________________________________________________ conv2d_17 (Conv2D) (None, 19, 19, 512) 524288 leaky_re_lu_16[0][0] __________________________________________________________________________________________________ batch_normalization_17 (BatchNo (None, 19, 19, 512) 2048 conv2d_17[0][0] __________________________________________________________________________________________________ leaky_re_lu_17 (LeakyReLU) (None, 19, 19, 512) 0 batch_normalization_17[0][0] __________________________________________________________________________________________________ conv2d_18 (Conv2D) (None, 19, 19, 1024) 4718592 leaky_re_lu_17[0][0] __________________________________________________________________________________________________ batch_normalization_18 (BatchNo (None, 19, 19, 1024) 4096 conv2d_18[0][0] __________________________________________________________________________________________________ leaky_re_lu_18 (LeakyReLU) (None, 19, 19, 1024) 0 batch_normalization_18[0][0] __________________________________________________________________________________________________ conv2d_19 (Conv2D) (None, 19, 19, 1024) 9437184 leaky_re_lu_18[0][0] __________________________________________________________________________________________________ batch_normalization_19 (BatchNo (None, 19, 19, 1024) 4096 conv2d_19[0][0] __________________________________________________________________________________________________ conv2d_21 (Conv2D) (None, 38, 38, 64) 32768 leaky_re_lu_13[0][0] __________________________________________________________________________________________________ leaky_re_lu_19 (LeakyReLU) (None, 19, 19, 1024) 0 batch_normalization_19[0][0] __________________________________________________________________________________________________ batch_normalization_21 (BatchNo (None, 38, 38, 64) 256 conv2d_21[0][0] __________________________________________________________________________________________________ conv2d_20 (Conv2D) (None, 19, 19, 1024) 9437184 leaky_re_lu_19[0][0] __________________________________________________________________________________________________ leaky_re_lu_21 (LeakyReLU) (None, 38, 38, 64) 0 batch_normalization_21[0][0] __________________________________________________________________________________________________ batch_normalization_20 (BatchNo (None, 19, 19, 1024) 4096 conv2d_20[0][0] __________________________________________________________________________________________________ space_to_depth_x2 (Lambda) (None, 19, 19, 256) 0 leaky_re_lu_21[0][0] __________________________________________________________________________________________________ leaky_re_lu_20 (LeakyReLU) (None, 19, 19, 1024) 0 batch_normalization_20[0][0] __________________________________________________________________________________________________ concatenate_1 (Concatenate) (None, 19, 19, 1280) 0 space_to_depth_x2[0][0] leaky_re_lu_20[0][0] __________________________________________________________________________________________________ conv2d_22 (Conv2D) (None, 19, 19, 1024) 11796480 concatenate_1[0][0] __________________________________________________________________________________________________ batch_normalization_22 (BatchNo (None, 19, 19, 1024) 4096 conv2d_22[0][0] __________________________________________________________________________________________________ leaky_re_lu_22 (LeakyReLU) (None, 19, 19, 1024) 0 batch_normalization_22[0][0] __________________________________________________________________________________________________ conv2d_23 (Conv2D) (None, 19, 19, 425) 435625 leaky_re_lu_22[0][0] ================================================================================================== Total params: 50,983,561 Trainable params: 50,962,889 Non-trainable params: 20,672 __________________________________________________________________________________________________ Note: On some computers, you may see a warning message from Keras. Don’t worry about it if you do–it is fine. Reminder: this model converts a preprocessed batch of input images (shape: (m, 608, 608, 3)) into a tensor of shape (m, 19, 19, 5, 85) as explained in Figure (2). 3.3 - Convert output of the model to usable bounding box tensorsThe output of yolo_model is a (m, 19, 19, 5, 85) tensor that needs to pass through non-trivial processing and conversion. The following cell does that for you. 1yolo_outputs = yolo_head(yolo_model.output, anchors, len(class_names)) You added yolo_outputs to your graph. This set of 4 tensors is ready to be used as input by your yolo_eval function. 3.4 - Filtering boxesyolo_outputs gave you all the predicted boxes of yolo_model in the correct format. You’re now ready to perform filtering and select only the best boxes. Lets now call yolo_eval, which you had previously implemented, to do this. 1scores, boxes, classes = yolo_eval(yolo_outputs, image_shape); 3.5 - Run the graph on an imageLet the fun begin. You have created a (sess) graph that can be summarized as follows: yolo_model.input is given to yolo_model. The model is used to compute the output yolo_model.output yolo_model.output is processed by yolo_head. It gives you yolo_outputs yolo_outputs goes through a filtering function, yolo_eval. It outputs your predictions: scores, boxes, classes Exercise: Implement predict() which runs the graph to test YOLO on an image.You will need to run a TensorFlow session, to have it compute scores, boxes, classes. The code below also uses the following function:1image, image_data = preprocess_image("images/" + image_file, model_image_size = (608, 608)) which outputs: image: a python (PIL) representation of your image used for drawing boxes. You won’t need to use it. image_data: a numpy-array representing the image. This will be the input to the CNN. Important note: when a model uses BatchNorm (as is the case in YOLO), you will need to pass an additional placeholder in the feed_dict {K.learning_phase(): 0}. 1234567891011121314151617181920212223242526272829303132333435363738def predict(sess, image_file): """ Runs the graph stored in "sess" to predict boxes for "image_file". Prints and plots the preditions. Arguments: sess -- your tensorflow/Keras session containing the YOLO graph image_file -- name of an image stored in the "images" folder. Returns: out_scores -- tensor of shape (None, ), scores of the predicted boxes out_boxes -- tensor of shape (None, 4), coordinates of the predicted boxes out_classes -- tensor of shape (None, ), class index of the predicted boxes Note: "None" actually represents the number of predicted boxes, it varies between 0 and max_boxes. """ # Preprocess your image image, image_data = preprocess_image("images/" + image_file, model_image_size = (608, 608)) # Run the session with the correct tensors and choose the correct placeholders in the feed_dict. # You'll need to use feed_dict=&#123;yolo_model.input: ... , K.learning_phase(): 0&#125;) ### START CODE HERE ### (≈ 1 line) out_scores, out_boxes, out_classes = sess.run([scores, boxes, classes], feed_dict = &#123;yolo_model.input:image_data, K.learning_phase(): 0&#125;); ### END CODE HERE ### # Print predictions info print('Found &#123;&#125; boxes for &#123;&#125;'.format(len(out_boxes), image_file)) # Generate colors for drawing bounding boxes. colors = generate_colors(class_names) # Draw bounding boxes on the image file draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors) # Save the predicted bounding box on the image image.save(os.path.join("out", image_file), quality=90) # Display the results in the notebook output_image = scipy.misc.imread(os.path.join("out", image_file)) imshow(output_image) return out_scores, out_boxes, out_classes Run the following cell on the “test.jpg” image to verify that your function is correct. 1out_scores, out_boxes, out_classes = predict(sess, "test.jpg") Found 7 boxes for test.jpg car 0.60 (925, 285) (1045, 374) car 0.66 (706, 279) (786, 350) bus 0.67 (5, 266) (220, 407) car 0.70 (947, 324) (1280, 705) car 0.74 (159, 303) (346, 440) car 0.80 (761, 282) (942, 412) car 0.89 (367, 300) (745, 648) C:\Anaconda3\lib\site-packages\ipykernel\__main__.py:35: DeprecationWarning: `imread` is deprecated! `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0. Use ``imageio.imread`` instead. Expected Output: Found 7 boxes for test.jpg|variable|value|| :—————– | :———————————————– || car | 0.60 (925, 285) (1045, 374) || car | 0.66 (706, 279) (786, 350) || bus | 0.67 (5, 266) (220, 407) || car | 0.70 (947, 324) (1280, 705) || car | 0.74 (159, 303) (346, 440) || car | 0.80 (761, 282) (942, 412) || car | 0.89 (367, 300) (745, 648) | The model you’ve just run is actually able to detect 80 different classes listed in “coco_classes.txt”. To test the model on your own images: 1. Click on &quot;File&quot; in the upper bar of this notebook, then click &quot;Open&quot; to go on your Coursera Hub. 2. Add your image to this Jupyter Notebook&apos;s directory, in the &quot;images&quot; folder 3. Write your image&apos;s name in the cell above code 4. Run the code and see the output of the algorithm! If you were to run your session in a for loop over all your images. Here’s what you would get: Predictions of the YOLO model on pictures taken from a camera while driving around the Silicon Valley Thanks drive.ai for providing this dataset! What you should remember: YOLO is a state-of-the-art object detection model that is fast and accurate It runs an input image through a CNN which outputs a 19x19x5x85 dimensional volume. The encoding can be seen as a grid where each of the 19x19 cells contains information about 5 boxes. You filter through all the boxes using non-max suppression. Specifically: Score thresholding on the probability of detecting a class to keep only accurate (high probability) boxes Intersection over Union (IoU) thresholding to eliminate overlapping boxes Because training a YOLO model from randomly initialized weights is non-trivial and requires a large dataset as well as lot of computation, we used previously trained model parameters in this exercise. If you wish, you can also try fine-tuning the YOLO model with your own dataset, though this would be a fairly non-trivial exercise. References: The ideas presented in this notebook came primarily from the two YOLO papers. The implementation here also took significant inspiration and used many components from Allan Zelener’s github repository. The pretrained weights used in this exercise came from the official YOLO website. Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi - You Only Look Once: Unified, Real-Time Object Detection (2015) Joseph Redmon, Ali Farhadi - YOLO9000: Better, Faster, Stronger (2016) Allan Zelener - YAD2K: Yet Another Darknet 2 Keras The official YOLO website (https://pjreddie.com/darknet/yolo/) Car detection dataset:The Drive.ai Sample Dataset (provided by drive.ai) is licensed under a Creative Commons Attribution 4.0 International License. We are especially grateful to Brody Huval, Chih Hu and Rahul Patel for collecting and providing this dataset. 12### TEST YOUR IMAGESout_scores, out_boxes, out_classes = predict(sess, "test_3.jpg"); Found 6 boxes for test_3.jpg bus 0.62 (374, 321) (795, 426) person 0.65 (801, 382) (898, 504) person 0.66 (979, 378) (1024, 543) person 0.67 (527, 370) (631, 521) car 0.75 (2, 449) (175, 642) motorbike 0.75 (799, 453) (884, 549) C:\Anaconda3\lib\site-packages\ipykernel\__main__.py:35: DeprecationWarning: `imread` is deprecated! `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0. Use ``imageio.imread`` instead.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>convolutional-neural-networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03_object-detection]]></title>
    <url>%2F2018%2F05%2F03%2F03_object-detection%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal note after studying the course of the 3rd week convolutional neural networks and the copyright belongs to deeplearning.ai. 01_object-localizationHello and welcome back. This week you learn about object detection. This is one of the areas of computer vision that’s just exploding and is working so much better than just a couple of years ago. In order to build up to object detection, you first learn about object localization. Let’s start by defining what that means. You’re already familiar with the image classification task where an algorithm looks at this picture and might be responsible for saying this is a car. So that was classification. The problem you learn to build in your network to address later on this video is classification with localization. Which means not only do you have to label this as say a car but the algorithm also is responsible for putting a bounding box, or drawing a red rectangle around the position of the car in the image. So that’s called the classification with localization problem. Where the term localization refers to figuring out where in the picture is the car you’ve detective. Later this week, you then learn about the detection problem where now there might be multiple objects in the picture and you have to detect them all and and localized them all. And if you’re doing this for an autonomous driving application, then you might need to detect not just other cars, but maybe other pedestrians and motorcycles and maybe even other objects. So you’ll see that later this week. So in the terminology we’ll use this week, the classification and the classification of localization problems usually have one object. Usually one big object in the middle of the image that you’re trying to recognize or recognize and localize. In contrast, in the detection problem there can be multiple objects. And in fact, maybe even multiple objects of different categories within a single image. So the ideas you’ve learned about for image classification will be useful for classification with localization. And that the ideas you learn for localization will then turn out to be useful for detection. So let’s start by talking about classification with localization. You’re already familiar with the image classification problem, in which you might input a picture into a ConvNet with multiple layers so that’s our ConvNet. And this results in a vector features that is fed to maybe a softmax unit that outputs the predicted clause. So if you are building a self driving car, maybe your object categories are the following. Where you might have a pedestrian, or a car, or a motorcycle, or a background. This means none of the above. So if there’s no pedestrian, no car, no motorcycle, then you might have an output background. So these are your classes, they have a softmax with four possible outputs. So this is the standard classification pipeline. How about if you want to localize the car in the image as well. To do that, you can change your neural network to have a few more output units that output a bounding box. So, in particular, you can have the neural network output four more numbers, and I’m going to call them bx, by, bh, and bw. And these four numbers parameterized the bounding box of the detected object. So in these videos, I am going to use the notational convention that the upper left of the image, I’m going to denote as the coordinate (0,0), and at the lower right is (1,1). So, specifying the bounding box, the red rectangle requires specifying the midpoint. So that’s the point bx, by as well as the height, that would be bh, as well as the width, bw of this bounding box. So now if your training set contains not just the object cross label, which a neural network is trying to predict up here, but it also contains four additional numbers. Giving the bounding box then you can use supervised learning to make your algorithm outputs not just a class label but also the four parameters to tell you where is the bounding box of the object you detected. So in this example the ideal bx might be about 0.5 because this is about halfway to the right to the image. by might be about 0.7 since it’s about maybe 70% to the way down to the image. bh might be about 0.3 because the height of this red square is about 30% of the overall height of the image. And bw might be about 0.4 let’s say because the width of the red box is about 0.4 of the overall width of the entire image. So let’s formalize this a bit more in terms of how we define the target label y for this as a supervised learning task. So just as a reminder these are our four classes, and the neural network now outputs those four numbers $b_x, b_y, b_h, b_w$ as well as a class label, or maybe probabilities of the class labels. So, let’s define the target label y as follows. Is going to be a vector where the first component $p_c$ is going to be, is there an object? So, if the object is, classes 1, 2 or 3, $p_c$ will be equal to 1. And if it’s the background class, so if it’s none of the objects you’re trying to detect, then $p_c$ will be 0. And $p_c$ you can think of that as standing for the probability that there’s an object. Probability that one of the classes you’re trying to detect is there. So something other than the background class. Next if there is an object, then you wanted to output $b_x$, $b_y$, $b_h$ and $b_w$, the bounding box for the object you detected. And finally if there is an object, so if $p_c$ is equal to 1, you wanted to also output $c_1$, $c_2$ and $c_3$ which tells us is it the class 1, class 2 or class 3. So is it a pedestrian, a car or a motorcycle. And remember in the problem we’re addressing we assume that your image has only one object. So at most, one of these objects appears in the picture, in this classification with localization problem. So let’s go through a couple of examples. If this is a training set image, so if that is x, then y will be the first component pc will be equal to 1 because there is an object, then bx, by, by, bh and bw will specify the bounding box. So your labeled training set will need bounding boxes in the labels. And then finally this is a car, so it’s class 2. So c1 will be 0 because it’s not a pedestrian, c2 will be 1 because it is car, c3 will be 0 since it is not a motorcycle. So among c1, c2 and c3 at most one of them should be equal to 1. So that’s if there’s an object in the image. What if there’s no object in the image? What if we have a training example where x is equal to that? In this case, $p_c$ would be equal to 0, and the rest of the elements of this, will be don’t cares, so I’m going to write question marks in all of them. So this is a don’t care, because if there is no object in this image, then you don’t care what bounding box the neural network outputs as well as which of the three objects, c1, c2, c3 it thinks it is. So given a set of label training examples, this is how you will construct x, the input image as well as y, the cost label both for images where there is an object and for images where there is no object. And the set of this will then define your training set. Finally, next let’s describe the loss function you use to train the neural network. So the ground true label was y and the neural network outputs some yhat. What should be the loss be? Well if you’re using squared error then the loss can be (y1 hat- y1) squared + (y2 hat- y2) squared + …+( y8 hat- y8) squared. Notice that y here has eight components. So that goes from sum of the squares of the difference of the elements. And that’s the loss if y1=1. So that’s the case where there is an object. So y1= pc. So, pc = 1, that if there is an object in the image then the loss can be the sum of squares of all the different elements. The other case is if y1=0, so that’s if this pc = 0. In that case the loss can be just (y1 hat-y1) squared, because in that second case, all of the rest of the components are don’t care us. And so all you care about is how accurately is the neural network ourputting pc in that case. So just a recap, if y1 = 1, that’s this case, then you can use squared error to penalize square deviation from the predicted, and the actual output of all eight components. Whereas if y1 = 0, then the second to the eighth components I don’t care. So all you care about is how accurately is your neural network estimating y1, which is equal to pc. Just as a side comment for those of you that want to know all the details, I’ve used the squared error just to simplify the description here. In practice you could improbably use a log likelihood loss for the c1, c2, c3 to the softmax output. One of those elements usually you can use squared error or something like squared error for the bounding box coordinates and if a $p_c$ you could use something like the logistics regression loss. Although even if you use squared error it’ll probably work okay. So that’s how you get a neural network to not just classify an object but also to localize it. The idea of having a neural network output a bunch of real numbers to tell you where things are in a picture turns out to be a very powerful idea. In the next video I want to share with you some other places where this idea of having a neural network output a set of real numbers, almost as a regression task, can be very powerful to use elsewhere in computer vision as well. So let’s go on to the next video. 02_landmark-detectionIn the previous video, you saw how you can get a neural network to output four numbers of bx, by, bh, and bw to specify the bounding box of an object you want a neural network to localize. In more general cases, you can have a neural network just output X and Y coordinates of important points and image, sometimes called landmarks, that you want the neural networks to recognize. Let me show you a few examples. Let’s say you’re building a face recognition application and for some reason, you want the algorithm to tell you where is the corner of someone’s eye. So that point has an X and Y coordinate, so you can just have a neural network have its final layer and have it just output two more numbers which I’m going to call our lx and ly to just tell you the coordinates of that corner of the person’s eye. Now, what if you want it to tell you all four corners of the eye, really of both eyes. So, if we call the points, the first, second, third and fourth points going from left to right, then you could modify the neural network now to output l1x, l1y for the first point and l2x, l2y for the second point and so on, so that the neural network can output the estimated position of all those four points of the person’s face. But what if you don’t want just those four points? What do you want to output this point, and this point and this point and this point along the eye? Maybe I’ll put some key points along the mouth, so you can extract the mouth shape and tell if the person is smiling or frowning, maybe extract a few key points along the edges of the nose but you could define some number, for the sake of argument, let’s say 64 points or 64 landmarks on the face. Maybe even some points that help you define the edge of the face, defines the jaw line but by selecting a number of landmarks and generating a label training sets that contains all of these landmarks, you can then have the neural network to tell you where are all the key positions or the key landmarks on a face. So what you do is you have this image, a person’s face as input, have it go through a convnet and have a convnet, then have some set of features, maybe have it output 0 or 1, like zero face changes or not and then have it also output l1x, l1y and so on down to l64x, l64y. And here I’m using l to stand for a landmark. So this example would have 129 output units, one for is your face or not? And then if you have 64 landmarks, that’s sixty-four times two, so 128 plus one output units and this can tell you if there’s a face as well as where all the key landmarks on the face. So, this is a basic building block for recognizing emotions from faces and if you played with the Snapchat and the other entertainment, also AR augmented reality filters like the Snapchat photos can draw a crown on the face and have other special effects. Being able to detect these landmarks on the face, there’s also a key building block for the computer graphics effects that warp the face or drawing various special effects like putting a crown or a hat on the person. Of course, in order to treat a network like this, you will need a label training set. We have a set of images as well as labels Y where people, where someone will have had to go through and laboriously annotate all of these landmarks. One last example, if you are interested in people pose detection, you could also define a few key positions like the midpoint of the chest, the left shoulder, left elbow, the wrist, and so on, and just have a neural network to annotate key positions in the person’s pose as well and by having a neural network output, all of those points I’m annotating, you could also have the neural network output the pose of the person. And of course, to do that you also need to specify on these key landmarks like maybe l1x and l1y is the midpoint of the chest down to maybe l32x, l32y, if you use 32 coordinates to specify the pose of the person. So, this idea might seem quite simple of just adding a bunch of output units to output the X,Y coordinates of different landmarks you want to recognize. To be clear, the identity of landmark one must be consistent across different images like maybe landmark one is always this corner of the eye, landmark two is always this corner of the eye, landmark three, landmark four, and so on. So, the labels have to be consistent across different images. But if you can hire labelers or label yourself a big enough data set to do this, then a neural network can output all of these landmarks which is going to used to carry out other interesting effect such as with the pose of the person, maybe try to recognize someone’s emotion from a picture, and so on. So that’s it for landmark detection. Next, let’s take these building blocks and use it to start building up towards object detection. 03_object-detectionYou’ve learned about Object Localization as well as Landmark Detection. Now, let’s build up to other object detection algorithm. In this video, you’ll learn how to use a ConvNet to perform object detection using something called the Sliding Windows Detection Algorithm. Let’s say you want to build a car detection algorithm. Here’s what you can do. You can first create a label training set, so x and y with closely cropped examples of cars. So, this is image x has a positive example, there’s a car, here’s a car, here’s a car, and then there’s not a car, there’s not a car. And for our purposes in this training set, you can start off with the one with the car closely cropped images. Meaning that x is pretty much only the car. So, you can take a picture and crop out and just cut out anything else that’s not part of a car. So you end up with the car centered in pretty much the entire image. Given this label training set, you can then train a ConvNet that inputs an image, like one of these closely cropped images. And then the job of the cofinite is to output y, zero or one, is there a car or not. Once you’ve trained up this ConvNet, you can then use it in Sliding Windows Detection. So the way you do that is, if you have a test image like this what you do is you start by picking a certain window size, shown down there. And then you would input into this ConvNet a small rectangular region. So, take just this below red square, input that into the ConvNet, and have a ConvNet make a prediction. And presumably for that little region in the red square, it’ll say, no that little red square does not contain a car. In the Sliding Windows Detection Algorithm, what you do is you then pass as input a second image now bounded by this red square shifted a little bit over and feed that to the ConvNet. So, you’re feeding just the region of the image in the red squares of the ConvNet and run the ConvNet again. And then you do that with a third image and so on. And you keep going until you’ve slid the window across every position in the image. And I’m using a pretty large stride in this example just to make the animation go faster. But the idea is you basically go through every region of this size, and pass lots of little cropped images into the ConvNet and have it classified zero or one for each position as some stride. jjNow, having done this once with running this was called the sliding window through the image. You then repeat it, but now use a larger window. So, now you take a slightly larger region and run that region. So, resize this region into whatever input size the ConvNet is expecting, and feed that to the ConvNet and have it output zero or one. And then slide the window over again using some stride and so on. And you run that throughout your entire image until you get to the end. And then you might do the third time using even larger windows and so on. Right. And the hope is that if you do this, then so long as there’s a car somewhere in the image that there will be a window where, for example if you are passing in this window into the cofinite, hopefully the cofinite will have outputs one for that input region. So then you detect that there is a car there. So this algorithm is called Sliding Windows Detection because you take these windows, these square boxes, and slide them across the entire image and classify every square region with some stride as containing a car or not. Now there’s a huge disadvantage of Sliding Windows Detection, which is the computational cost. Because you’re cropping out so many different square regions in the image and running each of them independently through a ConvNet. And if you use a very coarse stride, a very big stride, a very big step size, then that will reduce the number of windows you need to pass through the ConvNet, but that courser granularity may hurt performance. Whereas if you use a very fine granularity or a very small stride, then the huge number of all these little regions you’re passing through the ConvNet means that means there is a very high computational cost. So, before the rise of Neural Networks people used to use much simpler classifiers like a simple linear classifier over hand engineer features in order to perform object detection. And in that era because each classifier was relatively cheap to compute, it was just a linear function, Sliding Windows Detection ran okay. It was not a bad method, but with ConvNet now running a single classification task is much more expensive and sliding windows this way is infeasibily slow. And unless you use a very fine granularity or a very small stride, you end up not able to localize the objects that accurately within the image as well. Fortunately however, this problem of computational cost has a pretty good solution. In particular, the Sliding Windows Object Detector can be implemented convolutionally or much more efficiently. Let’s see in the next video how you can do that. 04_convolutional-implementation-of-sliding-windowsIn the last video, you learned about the sliding windows object detection algorithm using a convnet but we saw that it was too slow. In this video, you’ll learn how to implement that algorithm convolutionally. Let’s see what this means. To build up towards the convolutional implementation of sliding windows let’s first see how you can turn fully connected layers in neural network into convolutional layers. We’ll do that first on this slide and then the next slide, we’ll use the ideas from this slide to show you the convolutional implementation. So let’s say that your object detection algorithm inputs 14 by 14 by 3 images. This is quite small but just for illustrative purposes, and let’s say it then uses 5 by 5 filters, and let’s say it uses 16 of them to map it from 14 by 14 by 3 to 10 by 10 by 16. And then does a 2 by 2 max pooling to reduce it to 5 by 5 by 16. Then has a fully connected layer to connect to 400 units. Then now they’re fully connected layer and then finally outputs a Y using a softmax unit. In order to make the change we’ll need to in a second, I’m going to change this picture a little bit and instead I’m going to view Y as four numbers, corresponding to the cause probabilities of the four causes that softmax units is classified amongst. And the full causes could be pedestrian, car, motorcycle, and background or something else. Now, what I’d like to do is show how these layers can be turned into convolutional layers. So, the convnet will draw same as before for the first few layers. And now, one way of implementing this next layer, this fully connected layer is to implement this as a 5 by 5 filter and let’s use 400 5 by 5 filters. So if you take a 5 by 5 by 16 image and convolve it with a 5 by 5 filter, remember, a 5 by 5 filter is implemented as 5 by 5 by 16 because our convention is that the filter looks across all 16 channels. So this 16 and this 16 must match and so the outputs will be 1 by 1. And if you have 400 of these 5 by 5 by 16 filters, then the output dimension is going to be 1 by 1 by 400. So rather than viewing these 400 as just a set of nodes, we’re going to view this as a 1 by 1 by 400 volume. Mathematically, this is the same as a fully connected layer because each of these 400 nodes has a filter of dimension 5 by 5 by 16. So each of those 400 values is some arbitrary linear function of these 5 by 5 by 16 activations from the previous layer. Next, to implement the next convolutional layer, we’re going to implement a 1 by 1 convolution. If you have 400 1 by 1 filters then, with 400 filters the next layer will again be 1 by 1 by 400. So that gives you this next fully connected layer. And then finally, we’re going to have another 1 by 1 filter, followed by a softmax activation. So as to give a 1 by 1 by 4 volume to take the place of these four numbers that the network was operating. So this shows how you can take these fully connected layers and implement them using convolutional layers so that these sets of units instead are not implemented as 1 by 1 by 400 and 1 by 1 by 4 volumes. Armed of this conversion, let’s see how you can have a convolutional implementation of sliding windows object detection. The presentation on this slide is based on the OverFeat paper, referenced at the bottom, by Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Robert Fergus and Yann Lecun. Let’s say that your sliding windows convnet inputs 14 by 14 by 3 images and again, I’m just using small numbers like the 14 by 14 image in this slide mainly to make the numbers and illustrations simpler. So as before, you have a neural network as follows that eventually outputs a 1 by 1 by 4 volume, which is the output of your softmax. Again, to simplify the drawing here, 14 by 14 by 3 is technically a volume 5 by 5 or 10 by 10 by 16, the second clear volume. But to simplify the drawing for this slide, I’m just going to draw the front face of this volume. So instead of drawing 1 by 1 by 400 volume, I’m just going to draw the 1 by 1 cause of all of these. So just dropped the three components of these drawings, just for this slide. So let’s say that your convnet inputs 14 by 14 images or 14 by 14 by 3 images and your tested image is 16 by 16 by 3. So now added that yellow stripe to the border of this image. In the original sliding windows algorithm, you might want to input the blue region into a convnet and run that once to generate a consecration 01 and then slightly down a bit, least he uses a stride of two pixels and then you might slide that to the right by two pixels to input this green rectangle into the convnet and we run the whole convnet and get another label, 01. Then you might input this orange region into the convnet and run it one more time to get another label. And then do it the fourth and final time with this lower right purple square. To run sliding windows on this 16 by 16 by 3 image is pretty small image. You run this convnet four times in order to get four labels. But it turns out a lot of this computation done by these four convnets is highly duplicative. So what the convolutional implementation of sliding windows does is it allows these four forward passes in the convnet to share a lot of computation. Specifically, here’s what you can do. You can take the convnet and just run it same parameters, the same 5 by 5 filters, also 16 5 by 5 filters and run it. Now, you can have a 12 by 12 by 16 output volume. Then do the max pool, same as before. Now you have a 6 by 6 by 16, runs through your same 400 5 by 5 filters to get now your 2 by 2 by 40 volume. So now instead of a 1 by 1 by 400 volume, we have instead a 2 by 2 by 400 volume. Run it through a 1 by 1 filter gives you another 2 by 2 by 400 instead of 1 by 1 like 400. Do that one more time and now you’re left with a 2 by 2 by 4 output volume instead of 1 by 1 by 4. It turns out that this blue 1 by 1 by 4 subset gives you the result of running in the upper left hand corner 14 by 14 image. This upper right 1 by 1 by 4 volume gives you the upper right result. The lower left gives you the results of implementing the convnet on the lower left 14 by 14 region. And the lower right 1 by 1 by 4 volume gives you the same result as running the convnet on the lower right 14 by 14 medium. And if you step through all the steps of the calculation, let’s look at the green example, if you had cropped out just this region and passed it through the convnet through the convnet on top, then the first layer’s activations would have been exactly this region. The next layer’s activation after max pooling would have been exactly this region and then the next layer, the next layer would have been as follows. So what this process does, what this convolution implementation does is, instead of forcing you to run four propagation on four subsets of the input image independently, Instead, it combines all four into one form of computation and shares a lot of the computation in the regions of image that are common. So all four of the 14 by 14 patches we saw here. Now let’s just go through a bigger example. Let’s say you now want to run sliding windows on a 28 by 28 by 3 image. It turns out If you run four from the same way then you end up with an 8 by 8 by 4 output. And just go small and surviving sliding windows with that 14 by 14 region. And that corresponds to running a sliding windows first on that region thus, giving you the output corresponding the upper left hand corner. Then using a slider too to shift one window over, one window over, one window over and so on and the eight positions. So that gives you this first row and then as you go down the image as well, that gives you all of these 8 by 8 by 4 outputs. Because of the max pooling up too that this corresponds to running your neural network with a stride of two on the original image. So just to recap, to implement sliding windows, previously, what you do is you crop out a region. Let’s say this is 14 by 14 and run that through your convnet and do that for the next region over, then do that for the next 14 by 14 region, then the next one, then the next one, then the next one, then the next one and so on, until hopefully that one recognizes the car. But now, instead of doing it sequentially, with this convolutional implementation that you saw in the previous slide, you can implement the entire image, all maybe 28 by 28 and convolutionally make all the predictions at the same time by one forward pass through this big convnet and hopefully have it recognize the position of the car. So that’s how you implement sliding windows convolutionally and it makes the whole thing much more efficient. Now, this algorithm still has one weakness, which is the position of the bounding boxes is not going to be too accurate. In the next video, let’s see how you can fix that problem. 05_bounding-box-predictionsIn the last video, you learned how to use a convolutional implementation of sliding windows. That’s more computationally efficient, but it still has a problem of not quite outputting the most accurate bounding boxes. In this video, let’s see how you can get your bounding box predictions to be more accurate. With sliding windows, you take this three sets of locations and run the crossfire through it. And in this case, none of the boxes really match up perfectly with the position of the car. So, maybe that box is the best match. And also, it looks like in drawn through, the perfect bounding box isn’t even quite square, it’s actually has a slightly wider rectangle or slightly horizontal aspect ratio. So, is there a way to get this algorithm to outputs more accurate bounding boxes? A good way to get this output more accurate bounding boxes is with the YOLO algorithm. YOLO stands for, You Only Look Once. And is an algorithm due to Joseph Redmon, Santosh Divvala, Ross Girshick and Ali Farhadi. Here’s what you do. Let’s say you have an input image at 100 by 100, you’re going to place down a grid on this image. And for the purposes of illustration, I’m going to use a 3 by 3 grid. Although in an actual implementation, you use a finer one, like maybe a 19 by 19 grid. And the basic idea is you’re going to take the image classification and localization algorithm that you saw in the first video of this week and apply that to each of the nine grid cells of this image. So the more concrete, here’s how you define the labels you use for training. So for each of the nine grid cells, you specify a label Y, where the label Y is this eight dimensional vector, same as you saw previously. Your first output $p_c$ 01 depending on whether or not there’s an image in that grid cell and then $b_x, b_y, b_h, b_w$ to specify the bounding box if there is an image, if there is an object associated with that grid cell. And then say, $c_1, c_2, c_3$, if you try and recognize three classes not counting the background class. So you try to recognize pedestrian’s class, motorcycles and the background class. Then $c_1, c_2, c_3$ be the pedestrian, car and motorcycle classes. So in this image, we have nine grid cells, so you have a vector like this for each of the grid cells. So let’s start with the upper left grid cell, this one up here. For that one, there is no object. So, the label vector Y for the upper left grid cell would be zero, and then don’t cares for the rest of these. The output label Y would be the same for this grid cell, and this grid cell, and all the grid cells with nothing, with no interesting object in them. Now, how about this grid cell(the 5th grid cell)? To give a bit more detail, this image has two objects. And what the YOLO algorithm does is it takes the midpoint of each of the two objects and then assigns the object to the grid cell containing the midpoint. So the left car is assigned to this grid cell(the 4th grid cell), and the car on the right, which is this midpoint, is assigned to this grid cell(the 6th grid cell). And so even though the central grid cell(the 5th grid cell) has some parts of both cars, we’ll pretend the central grid cell has no interesting object so that the central grid cell the class label Y also looks like this vector with no object, and so the first component $p_c$, and then the rest are don’t cares. Whereas for this cell, this cell that I have circled in green on the left, the target label Y would be as follows. There is an object, and then you write $b_x, b_y, b_h, b_w$, to specify the position of this bounding box. And then you have, let’s see, if class one was a pedestrian, then that was zero. Class two is a car, that’s one. Class three was a motorcycle, that’s zero. And then similarly, for the grid cell on their right because that does have an object in it, it will also have some vector like this as the target label corresponding to the grid cell on the right. So, for each of these nine grid cells, you end up with a eight dimensional output vector. And because you have 3 by 3 grid cells, you have nine grid cells, the total volume of the output is going to be 3 by 3 by 8. So the target output is going to be 3 by 3 by 8 because you have 3 by 3 grid cells. And for each of the 3 by 3 grid cells, you have a eight dimensional Y vector. So the target output volume is 3 by 3 by 8. Where for example, this 1 by 1 by 8 volume in the upper left corresponds to the target output vector for the upper left of the nine grid cells. And so for each of the 3 by 3 positions, for each of these nine grid cells, does it correspond in eight dimensional target vector Y that you want to the output. Some of which could be don’t cares, if there’s no object there. And that’s why the total target outputs, the output label for this image is now itself a 3 by 3 by 8 volume. So now, to train your neural network, the input is 100 by 100 by 3, that’s the input image. And then you have a usual convnet with conv, layers of max pool layers, and so on. So that in the end, you have this, should choose the conv layers and the max pool layers, and so on, so that this eventually maps to a 3 by 3 by 8 output volume. And so what you do is you have an input X which is the input image like that, and you have these target labels Y which are 3 by 3 by 8, and you use map propagation to train the neural network to map from any input X to this type of output volume Y. So the advantage of this algorithm is that the neural network outputs precise bounding boxes as follows. So at test time, what you do is you feed an input image X and run forward prop until you get this output Y. And then for each of the nine outputs of each of the 3 by 3 positions in which of the output, you can then just read off 1 or 0. Is there an object associated with that one of the nine positions? And that there is an object, what object it is, and where is the bounding box for the object in that grid cell? And so long as you don’t have more than one object in each grid cell, this algorithm should work okay. And the problem of having multiple objects within the grid cell is something we’ll address later. Of use a relatively small 3 by 3 grid, in practice, you might use a much finer, grid maybe 19 by 19. So you end up with 19 by 19 by 8, and that also makes your grid much finer. It reduces the chance that there are multiple objects assigned to the same grid cell. And just as a reminder, the way you assign an object to grid cell as you look at the midpoint of an object and then you assign that object to whichever one grid cell contains the midpoint of the object. So each object, even if the objects spends multiple grid cells, that object is assigned only to one of the nine grid cells, or one of the 3 by 3, or one of the 19 by 19 grid cells. Algorithm of a 19 by 19 grid, the chance of an object of two midpoints of objects appearing in the same grid cell is just a bit smaller. So notice two things, first, this is a lot like the image classification and localization algorithm that we talked about in the first video of this week. And that it outputs the bounding boxs coordinates explicitly. And so this allows in your network to output bounding boxes of any aspect ratio, as well as, output much more precise coordinates that aren’t just dictated by the stripe size of your sliding windows classifier. And second, this is a convolutional implementation and you’re not implementing this algorithm nine times on the 3 by 3 grid or if you’re using a 19 by 19 grid.19 squared is 361. So, you’re not running the same algorithm 361 times or 19 squared times. Instead, this is one single convolutional implantation, where you use one consonant with a lot of shared computation between all the computations needed for all of your 3 by 3 or all of your 19 by 19 grid cells. So, this is a pretty efficient algorithm. And in fact, one nice thing about the YOLO algorithm, which is constant popularity is because this is a convolutional implementation, it actually runs very fast. So this works even for real time object detection. Now, before wrapping up, there’s one more detail I want to share with you, which is, how do you encode these bounding boxes $b_x, b_y, b_h, b_w$? Let’s discuss that on the next slide. So, given these two cars, remember, we have the 3 by 3 grid. Let’s take the example of the car on the right. So, in this grid cell there is an object and so the target label y will be one, that was $p_c$ is equal to one. And then $b_x, b_y, b_h, b_w$, and then 0 1 0. So, how do you specify the bounding box? In the YOLO algorithm, relative to this square, when I take the convention that the upper left point here is 0 0 and this lower right point is 1 1. So to specify the position of that midpoint, that orange dot, bx might be, let’s say x looks like is about 0.4. Maybe its about 0.4 of the way to their right. And then y, looks I guess maybe 0.3. And then the height of the bounding box is specified as a fraction of the overall width of this box. So, the width of this red box is maybe 90% of that blue line. And so BH is 0.9 and the height of this is maybe one half of the overall height of the grid cell. So in that case, BW would be, let’s say 0.5. So, in other words, this $b_x, b_y, b_h, b_w$ as specified relative to the grid cell. And so bx and by, this has to be between 0 and 1, right? Because pretty much by definition that orange dot is within the bounds of that grid cell is assigned to. If it wasn’t between 0 and 1 it was outside the square, then we’ll have been assigned to a different grid cell. But these could be greater than one. In particular if you have a car where the bounding box was that, then the height and width of the bounding box, this could be greater than one. So, there are multiple ways of specifying the bounding boxes, but this would be one convention that’s quite reasonable. Although, if you read the YOLO research papers, the YOLO research line there were other parameterizations that work even a little bit better, but I hope this gives one reasonable condition that should work okay. Although, there are some more complicated parameterizations involving sigmoid functions to make sure this is between 0 and 1. And using an explanation parameterization to make sure that these are non-negative, since 0.9, 0.5, this has to be greater or equal to zero. There are some other more advanced parameterizations that work things a little bit better, but the one you saw here should work okay. So, that’s it for the YOLO or the You Only Look Once algorithm. And in the next few videos I’ll show you a few other ideas that will help make this algorithm even better. In the meantime, if you want, you can take a look at YOLO paper reference at the bottom of these past couple slides I use. Although, just one warning, if you take a look at these papers which is the YOLO paper is one of the harder papers to read. I remember, when I was reading this paper for the first time, I had a really hard time figuring out what was going on. And I wound up asking a couple of my friends, very good researchers to help me figure it out, and even they had a hard time understanding some of the details of the paper. So, if you look at the paper, it’s okay if you have a hard time figuring it out. I wish it was more uncommon, but it’s not that uncommon, sadly, for even senior researchers, that review research papers and have a hard time figuring out the details. And have to look at open source code, or contact the authors, or something else to figure out the details of these outcomes. But don’t let me stop you from taking a look at the paper yourself though if you wish, but this is one of the harder ones. So, that though, you now understand the basics of the YOLO algorithm. Let’s go on to some additional pieces that will make this algorithm work even better. 06_intersection-over-unionSo how do you tell if your object detection algorithm is working well? In this video, you’ll learn about a function called, “Intersection Over Union”. And as we use both for evaluating your object detection algorithm, as well as in the next video, using it to add another component to your object detection algorithm, to make it work even better. Let’s get started. In the object detection task, you expected to localize the object as well. So if that’s the ground-truth bounding box, and if your algorithm outputs this bounding box in purple, is this a good outcome or a bad one? So what the intersection over union function does, or IoU does, is it computes the intersection over union of these two bounding boxes. So, the union of these two bounding boxes is this area, is really the area that is contained in either bounding boxes, whereas the intersection is this smaller region here. So what the intersection of a union does is it computes the size of the intersection. So that orange shaded area, and divided by the size of the union, which is that green shaded area. And by convention, the low compute division task will judge that your answer is correct if the IoU is greater than 0.5. And if the predicted and the ground-truth bounding boxes overlapped perfectly, the IoU would be one, because the intersection would equal to the union. But in general, so long as the IoU is greater than or equal to 0.5, then the answer will look okay, look pretty decent. And by convention, very often 0.5 is used as a threshold to judge as whether the predicted bounding box is correct or not. This is just a convention. If you want to be more stringent, you can judge an answer as correct, only if the IoU is greater than equal to 0.6 or some other number. But the higher the IoUs, the more accurate the bounding the box. And so, this is one way to map localization, to accuracy where you just count up the number of times an algorithm correctly detects and localizes an object where you could use a definition like this, of whether or not the object is correctly localized. And again 0.5 is just a human chosen convention. There’s no particularly deep theoretical reason for it. You can also choose some other threshold like 0.6 if you want to be more stringent. I sometimes see people use more stringent criteria like 0.6 or maybe 0.7. I rarely see people drop the threshold below 0.5. Now, what motivates the definition of IoU, as a way to evaluate whether or not your object localization algorithm is accurate or not. But more generally, IoU is a measure of the overlap between two bounding boxes. Where if you have two boxes, you can compute the intersection, compute the union, and take the ratio of the two areas. And so this is also a way of measuring how similar two boxes are to each other. And we’ll see this use again this way in the next video when we talk about non-max suppression. So that’s it for IoU or Intersection over Union. Not to be confused with the promissory note concept in IoU, where if you lend someone money they write you a note that says, “ Oh I owe you this much money,” so that’s also called an IoU. It’s totally a different concept, that maybe it’s cool that these two things have a similar name. So now, onto this definition of IoU, Intersection of Union. In the next video, I want to discuss with you non-max suppression, which is a tool you can use to make the outputs of YOLO work even better. So let’s go on to the next video. 07_non-max-suppressionOne of the problems of Object Detection as you’ve learned about this so far, is that your algorithm may find multiple detections of the same objects. Rather than detecting an object just once, it might detect it multiple times. Non-max suppression is a way for you to make sure that your algorithm detects each object only once. Let’s go through an example. Let’s say you want to detect pedestrians, cars, and motorcycles in this image. You might place a grid over this, and this is a 19 by 19 grid. Now, while technically this car has just one midpoint, so it should be assigned just one grid cell. And the car on the left also has just one midpoint, so technically only one of those grid cells should predict that there is a car. In practice, you’re running an object classification and localization algorithm for every one of these split cells. So it’s quite possible that this split cell might think that the center of a car is in it, and so might this, and so might this, and for the car on the left as well. Maybe not only this box, if this is a test image you’ve seen before, not only that box might decide things that’s on the car, maybe this box, and this box and maybe others as well will also think that they’ve found the car. Let’s step through an example of how non-max suppression will work. So, because you’re running the image classification and localization algorithm on every grid cell, on 361 grid cells, it’s possible that many of them will raise their hand and say, “My $p_c$, my chance of thinking I have an object in it is large.” Rather than just having two of the grid cells out of the 19 squared or 361 think they have detected an object. So, when you run your algorithm, you might end up with multiple detections of each object. So, what non-max suppression does, is it cleans up these detections. So they end up with just one detection per car, rather than multiple detections per car. So concretely, what it does, is it first looks at the probabilities associated with each of these detections count on $p_c$s, although there are some details you’ll learn about in this week’s problem exercises, is actually $p_c$ times C1, or C2, or C3. But for now, let’s just say is $p_c$ with the probability of a detection. And it first takes the largest one, which in this case is 0.9 and says, “That’s my most confident detection, so let’s highlight that and just say I found the car there.” Having done that the non-max suppression part then looks at all of the remaining rectangles and all the ones with a high overlap, with a high IOU, with this one that you’ve just output will get suppressed. So those two rectangles with the 0.6 and the 0.7. Both of those overlap a lot with the light blue rectangle. So those, you are going to suppress and darken them to show that they are being suppressed. Next, you then go through the remaining rectangles and find the one with the highest probability, the highest $p_c$, which in this case is this one with 0.8. So let’s commit to that and just say, “Oh, I’ve detected a car there.” And then, the non-max suppression part is to then get rid of any other ones with a high IOU. So now, every rectangle has been either highlighted or darkened. And if you just get rid of the darkened rectangles, you are left with just the highlighted ones, and these are your two final predictions. So, this is non-max suppression. And non-max means that you’re going to output your maximal probabilities classifications but suppress the close-by ones that are non-maximal. Hence the name, non-max suppression. Let’s go through the details of the algorithm. First, on this 19 by 19 grid, you’re going to get a 19 by 19 by eight output volume. Although, for this example, I’m going to simplify it to say that you only doing car detection. So, let me get rid of the C1, C2, C3, and pretend for this line, that each output for each of the 19 by 19, so for each of the 361, which is 19 squared, for each of the 361 positions, you get an output prediction of the following. Which is the chance there’s an object, and then the bounding box. And if you have only one object, there’s no C1, C2, C3 prediction. The details of what happens, you have multiple objects, I’ll leave to the programming exercise, which you’ll work on towards the end of this week. Now, to intimate non-max suppression, the first thing you can do is discard all the boxes, discard all the predictions of the bounding boxes with $p_c$ less than or equal to some threshold, let’s say 0.6. So we’re going to say that unless you think there’s at least a 0.6 chance it is an object there, let’s just get rid of it. This has caused all of the low probability output boxes. The way to think about this is for each of the 361 positions, you output a bounding box together with a probability of that bounding box being a good one. So we’re just going to discard all the bounding boxes that were assigned a low probability. Next, while there are any remaining bounding boxes that you’ve not yet discarded or processed, you’re going to repeatedly pick the box with the highest probability, with the highest $p_c$, and then output that as a prediction. So this is a process on a previous slide of taking one of the bounding boxes, and making it lighter in color. So you commit to outputting that as a prediction for that there is a car there. Next, you then discard any remaining box. Any box that you have not output as a prediction, and that was not previously discarded. So discard any remaining box with a high overlap, with a high IOU, with the box that you just output in the previous step. This second step in the while loop was when on the previous slide you would darken any remaining bounding box that had a high overlap with the bounding box that we just made lighter, that we just highlighted. And so, you keep doing this while there’s still any remaining boxes that you’ve not yet processed, until you’ve taken each of the boxes and either output it as a prediction, or discarded it as having too high an overlap, or too high an IOU, with one of the boxes that you have just output as your predicted position for one of the detected objects. I’ve described the algorithm using just a single object on this slide. If you actually tried to detect three objects say pedestrians, cars, and motorcycles, then the output vector will have three additional components. And it turns out, the right thing to do is to independently carry out non-max suppression three times, one on each of the outputs classes. But the details of that, I’ll leave to this week’s program exercise where you get to implement that yourself, where you get to implement non-max suppression yourself on multiple object classes. So that’s it for non-max suppression, and if you implement the Object Detection algorithm we’ve described, you actually get pretty decent results. But before wrapping up our discussion of the YOLO algorithm, there’s just one last idea I want to share with you, which makes the algorithm work much better, which is the idea of using anchor boxes. Let’s go on to the next video. 08_anchor-boxesOne of the problems with object detection as you have seen it so far is that each of the grid cells can detect only one object. What if a grid cell wants to detect multiple objects? Here is what you can do. You can use the idea of anchor boxes. Let’s start with an example. Let’s say you have an image like this. And for this example, I am going to continue to use a 3 by 3 grid. Notice that the midpoint of the pedestrian and the midpoint of the car are in almost the same place and both of them fall into the same grid cell. So, for that grid cell, if Y outputs this vector where you are detecting three causes, pedestrians, cars and motorcycles, it won’t be able to output two detections. So I have to pick one of the two detections to output. With the idea of anchor boxes, what you are going to do, is pre-define two different shapes called, anchor boxes or anchor box shapes. And what you are going to do is now, be able to associate two predictions with the two anchor boxes. And in general, you might use more anchor boxes, maybe five or even more. But for this video, I am just going to use two anchor boxes just to make the description easier. So what you do is you define the cross label to be, instead of this vector on the left, you basically repeat this twice. S, you will have PC, PX, PY, PH, PW, C1, C2, C3, and these are the eight outputs associated with anchor box 1. And then you repeat that PC, PX and so on down to C1, C2, C3, and other eight outputs associated with anchor box 2. So, because the shape of the pedestrian is more similar to the shape of anchor box 1 than anchor box 2, you can use these eight numbers to encode that $p_c$ as one, yes there is a pedestrian. Use this to encode the bounding box around the pedestrian, and then use this to encode that that object is a pedestrian. And then because the box around the car is more similar to the shape of anchor box 2 than anchor box 1, you can then use this to encode that the second object here is the car, and have the bounding box and so on be all the parameters associated with the detected car. So to summarize, previously, before you are using anchor boxes, you did the following, which is for each object in the training set and the training set image, it was assigned to the grid cell that corresponds to that object’s midpoint. And so the output Y was 3 by 3 by 8 because you have a 3 by 3 grid. And for each grid position, we had that output vector which is PC, then the bounding box, and C1, C2, C3. With the anchor box, you now do that following. Now, each object is assigned to the same grid cell as before, assigned to the grid cell that contains the object’s midpoint, but it is assigned to a grid cell and anchor box with the highest IoU with the object’s shape. So, you have two anchor boxes, you will take an object and see. So if you have an object with this shape, what you do is take your two anchor boxes. Maybe one anchor box is this this shape that’s anchor box 1, maybe anchor box 2 is this shape, and then you see which of the two anchor boxes has a higher IoU, will be drawn through bounding box. And whichever it is, that object then gets assigned not just to a grid cell but to a pair. It gets assigned to grid cell comma anchor box pair. And that’s how that object gets encoded in the target label. And so now, the output Y is going to be 3 by 3 by 16. Because as you saw on the previous slide, Y is now 16 dimensional. Or if you want, you can also view this as 3 by 3 by 2 by 8 because there are now two anchor boxes and Y is eight dimensional. And dimension of Y being eight was because we have three objects causes if you have more objects than the dimension of Y would be even higher. So let’s go through a conrete example. For this grid cell, let’s specify what is Y. So the pedestrian is more similar to the shape of anchor box 1. So for the pedestrian, we’re going to assign it to the top half of this vector. So yes, there is an object, there will be some bounding box associated at the pedestrian. And I guess if a pedestrian is cos one, then we see one as one, and then zero, zero. And then the shape of the car is more similar to anchor box 2. And so the rest of this vector will be one and then the bounding box associated with the car, and then the car is C2, so there’s zero, one, zero. And so that’s the label Y for that lower middle grid cell that this arrow was pointing to. Now, what if this grid cell only had a car and had no pedestrian? If it only had a car, then assuming that the shape of the bounding box around the car is still more similar to anchor box 2, then the target label Y, if there was just a car there and the pedestrian had gone away, it will still be the same for the anchor box 2 component. Remember that this is a part of the vector corresponding to anchor box 2. And for the part of the vector corresponding to anchor box 1, what you do is you just say there is no object there. So $p_c$ is zero, and then the rest of these will be don’t cares. Now, just some additional details. What if you have two anchor boxes but three objects in the same grid cell? That’s one case that this algorithm doesn’t handle well. Hopefully, it won’t happen. But if it does, this algorithm doesn’t have a great way of handling it. I will just influence some default tiebreaker for that case. Or what if you have two objects associated with the same grid cell, but both of them have the same anchor box shape? Again, that’s another case that this algorithm doesn’t handle well. If you influence some default way of tiebreaking if that happens, hopefully this won’t happen with your data set, it won’t happen much at all. And so, it shouldn’t affect performance as much. So, that’s it for anchor boxes. And even though I’d motivated anchor boxes as a way to deal with what happens if two objects appear in the same grid cell, in practice, that happens quite rarely, especially if you use a 19 by 19 rather than a 3 by 3 grid. The chance of two objects having the same midpoint rather these 361 cells, it does happen, but it doesn’t happen that often. Maybe even better motivation or even better results that anchor boxes gives you is it allows your learning algorithm to specialize better. In particular, if your data set has some tall, skinny objects like pedestrians, and some white objects like cars, then this allows your learning algorithm to specialize so that some of the outputs can specialize in detecting white, fat objects like cars, and some of the output units can specialize in detecting tall, skinny objects like pedestrians. So finally, how do you choose the anchor boxes? And people used to just choose them by hand or choose maybe five or 10 anchor box shapes that spans a variety of shapes that seems to cover the types of objects you seem to detect. As a much more advanced version, just in the advance common for those of who have other knowledge in machine learning, and even better way to do this in one of the later YOLO research papers, is to use a K-means algorithm, to group together two types of objects shapes you tend to get. And then to use that to select a set of anchor boxes that this most stereotypically representative of the maybe multiple, of the maybe dozens of object causes you’re trying to detect. But that’s a more advanced way to automatically choose the anchor boxes. And if you just choose by hand a variety of shapes that reasonably expands the set of object shapes, you expect to detect some tall, skinny ones, some fat, white ones. That should work with these as well. So that’s it for anchor boxes. In the next video, let’s take everything we’ve seen and tie it back together into the YOLO algorithm. 09_yolo-algorithmYou’ve already seen most of the components of object detection. In this video, let’s put all the components together to form the YOLO object detection algorithm. First, let’s see how you construct your training set. Suppose you’re trying to train an algorithm to detect three objects: pedestrians, cars, and motorcycles. And you will need to explicitly have the full background class, so just the class labels here. If you’re using two anchor boxes, then the outputs y will be three by three because you are using three by three grid cell, by two, this is the number of anchors, by eight because that’s the dimension of this. Eight is actually five which is plus the number of classes. So five because you have $p_c$ and then the bounding boxes, that’s five, and then c1, c2, c3. That dimension is equal to the number of classes. And you can either view this as three by three by two by eight, or by three by three by sixteen. So to construct the training set, you go through each of these nine grid cells and form the appropriate target vector y. So take this first grid cell, there’s nothing worth detecting in that grid cell. None of the three classes pedestrian, car and motocycle, appear in the upper left grid cell and so, the target y corresponding to that grid cell would be equal to this. Where Pc for the first anchor box is zero because there’s nothing associated for the first anchor box, and is also zero for the second anchor box and so on all of these other values are don’t cares. Now, most of the grid cells have nothing in them, but for that box over there, you would have this target vector y. So assuming that your training set has a bounding box like this for the car, it’s just a little bit wider than it is tall. And so if your anchor boxes are that, this is a anchor box one, this is anchor box two, then the red box has just slightly higher IoU with anchor box two. And so the car gets associated with this lower portion of the vector. So notice then that Pc associate anchor box one is zero. So you have don’t cares all these components. Then you have this Pc is equal to one, then you should use these to specify the position of the red bounding box, and then specify that the correct object is class two. Right that it is a car. So you go through this and for each of your nine grid positions each of your three by three grid positions, you would come up with a vector like this. Come up with a 16 dimensional vector. And so that’s why the final output volume is going to be 3 by 3 by 16. Oh and as usual for simplicity on the slide I’ve used a 3 by 3 the grid. In practice it might be more like a 19 by 19 by 16. Or in fact if you use more anchor boxes, maybe 19 by 19 by 5 x 8 because five times eight is 40. So it will be 19 by 19 by 40. That’s if you use five anchor boxes. So that’s training and you train ConvNet that inputs an image, maybe 100 by 100 by 3, and your ConvNet would then finally output this output volume in our example, 3 by 3 by 16 or 3 by 3 by 2 by 8. Next, let’s look at how your algorithm can make predictions. Given an image, your neural network will output this by 3 by 3 by 2 by 8 volume, where for each of the nine grid cells you get a vector like that. So for the grid cell here on the upper left, if there’s no object there, hopefully, your neural network will output zero here, and zero here, and it will output some other values. Your neural network can’t output a question mark, can’t output a don’t care. So I’ll put some numbers for the rest. But these numbers will basically be ignored because the neural network is telling you that there’s no object there. So it doesn’t really matter whether the output is a bounding box or there’s is a car. So basically just be some set of numbers, more or less noise. In contrast, for this box over here hopefully, the value of y to the output for that box at the bottom left, hopefully would be something like zero for bounding box one. And then just open a bunch of numbers, just noise. Hopefully, you’ll also output a set of numbers that corresponds to specifying a pretty accurate bounding box for the car. So that’s how the neural network will make predictions. Finally, you run this through non-max suppression. So just to make it interesting. Let’s look at the new test set image. Here’s how you would run non-max suppression. If you’re using two anchor boxes, then for each of the non-grid cells, you get two predicted bounding boxes. Some of them will have very low probability, very low Pc, but you still get two predicted bounding boxes for each of the nine grid cells. So let’s say, those are the bounding boxes you get. And notice that some of the bounding boxes can go outside the height and width of the grid cell that they came from. Next, you then get rid of the low probability predictions. So get rid of the ones that even the neural network says, gee this object probably isn’t there. So get rid of those. And then finally if you have three classes you’re trying to detect, you’re trying to detect pedestrians, cars and motorcycles. What you do is, for each of the three classes, independently run non-max suppression for the objects that were predicted to come from that class. But use non-max suppression for the predictions of the pedestrians class, run non-max suppression for the car class, and non-max suppression for the motorcycle class. But run that basically three times to generate the final predictions. And so the output of this is hopefully that you will have detected all the cars and all the pedestrians in this image. So that’s it for the YOLO object detection algorithm. Which is really one of the most effective object detection algorithms, that also encompasses many of the best ideas across the entire computer vision literature that relate to object detection. And you get a chance to practice implementing many components of this yourself, in this week’s problem exercise. So I hope you enjoy this week’s problem exercise. There’s also an optional video that follows this one which you can either watch or not watch as you please. But either way I also look forward to seeing you next week. 10_optional-region-proposalsIf you look at the object detection literature, there’s a set of ideas called region proposals that’s been very influential in computer vision as well. I wanted to make this video optional because I tend to use the region proposal instead of algorithm a bit less often but nonetheless, it has been an influential body of work and an idea that you might come across in your own work. Let’s take a look. So if you recall the sliding windows idea, you would take a train classifier and run it across all of these different windows and run the detector to see if there’s a car, pedestrian, or maybe a motorcycle. Now, you could run the algorithm convolutionally, but one downside that the algorithm is it just classifiers a lot of the regions where there’s clearly no object. So this rectangle down here is pretty much blank. It’s clearly nothing interesting there to classify, and maybe it was also running it on this rectangle, which look likes there’s nothing that interesting there. So what Russ Girshik, Jeff Donahue, Trevor Darrell, and Jitendra Malik proposed in the paper, as cited to the bottom of the slide, is an algorithm called R-CNN, which stands for Regions with convolutional networks or regions with CNNs. And what that does is it tries to pick just a few regions that makes sense to run your continent classifier. So rather than running your sliding windows on every single window, you instead select just a few windows and run your continent classifier on just a few windows. The way that they perform the region proposals is to run an algorithm called a segmentation algorithm, that results in this output on the right, in order to figure out what could be objects. So, for example, the segmentation algorithm finds a block over here. And so you might pick that pounding balls and say, “Let’s run a classifier on that blob.” It looks like this little green thing finds a block there, as you might also run the classifier on that rectangle to see if there’s some interesting there. And in this case, this blue block, if you run a classifier on that, hope you find the pedestrian, and if you run it on this light cyan block, maybe you’ll find a car, maybe not,. I’m not sure. So the details of this, this is called a segmentation algorithm, and what you do is you find maybe 2000 blobs and place bounding boxes around about 2000 blobs and value classifier on just those 2000 blobs, and this can be a much smaller number of positions on which to run your continent classifier, then if you have to run it at every single position throughout the image. And this is a special case if you are running your continent not just on square-shaped regions but running them on tall skinny regions to try to find pedestrians or running them on your white fat regions try to find cars and running them at multiple scales as well. So that’s the R-CNN or the region with CNN, a region of CNN features idea. Now, it turns out the R-CNN algorithm is still quite slow. So there’s been a line of work to explore how to speed up this algorithm. So the basic R-CNN algorithm with proposed regions using some algorithm and then classifier the proposed regions one at a time. And for each of the regions, they will output the label. So is there a car? Is there a pedestrian? Is there a motorcycle there? And then also outputs a bounding box, so you can get an accurate bounding box if indeed there is a object in that region. So just to be clear, the R-CNN algorithm doesn’t just trust the bounding box it was given. It also outputs a bounding box, B X B Y B H B W, in order to get a more accurate bounding box and whatever happened to surround the blob that the image segmentation algorithm gave it. So it can get pretty accurate bounding boxes. Now, one downside of the R-CNN algorithm was that it is actually quite slow. So over the years, there been a few improvements to the R-CNN algorithm. Russ Girshik proposed the fast R-CNN algorithm, and it’s basically the R-CNN algorithm but with a convolutional implementation of sliding windows. So the original implementation would actually classify the regions one at a time. So far, R-CNN use a convolutional implementation of sliding windows, and this is roughly similar to the idea you saw in the fourth video of this week. And that speeds up R-CNN quite a bit. It turns out that one of the problems of fast R-CNN algorithm is that the clustering step to propose the regions is still quite slow and so a different group, Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Son, proposed the faster R-CNN algorithm, which uses a convolutional neural network instead of one of the more traditional segmentation algorithms to propose a blob on those regions, and that wound up running quite a bit faster than the fast R-CNN algorithm. Although, I think the faster R-CNN algorithm, most implementations are usually still quit a bit slower than the YOLO algorithm. So the idea of region proposals has been quite influential in computer vision, and I wanted you to know about these ideas because you see others still used these ideas, for myself, and this is my personal opinion, not the opinion of the computer vision research committee as a whole. I think that we can propose an interesting idea but that not having two steps, first, proposed region and then crossfire, being able to do everything more or at the same time, similar to the YOLO or the You Only Look Once algorithm that seems to me like a more promising direction for the long term. But that’s my personal opinion and not necessary the opinion of the whole computer vision research committee. So feel free to take that with a grain of salt, but I think that the R-CNN idea, you might come across others using it. So it was worth learning as well so you can understand others algorithms better. So we’re now finished up our material for this week on object detection. I hope you enjoy working on this week’s problem exercise, and I look forward to seeing you this week.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>convolutional-neural-networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Residual Networks]]></title>
    <url>%2F2018%2F05%2F02%2FResidual%2BNetworks%2B-%2Bv2%2F</url>
    <content type="text"><![CDATA[NoteThese are my personal programming assignments at the 2nd week after studying the course convolutional neural networks and the copyright belongs to deeplearning.ai. Residual NetworksWelcome to the second assignment of this week! You will learn how to build very deep convolutional networks, using Residual Networks (ResNets). In theory, very deep networks can represent very complex functions; but in practice, they are hard to train. Residual Networks, introduced by He et al., allow you to train much deeper networks than were previously practically feasible. In this assignment, you will: Implement the basic building blocks of ResNets. Put together these building blocks to implement and train a state-of-the-art neural network for image classification. This assignment will be done in Keras. Before jumping into the problem, let’s run the cell below to load the required packages. 123456789101112131415161718192021import numpy as npfrom keras import layersfrom keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2Dfrom keras.models import Model, load_modelfrom keras.preprocessing import imagefrom keras.utils import layer_utilsfrom keras.utils.data_utils import get_filefrom keras.applications.imagenet_utils import preprocess_inputimport pydotfrom IPython.display import SVGfrom keras.utils.vis_utils import model_to_dotfrom keras.utils import plot_modelfrom resnets_utils import *from keras.initializers import glorot_uniformimport scipy.miscfrom matplotlib.pyplot import imshow%matplotlib inlineimport keras.backend as KK.set_image_data_format('channels_last')K.set_learning_phase(1) Using TensorFlow backend. 1 - The problem of very deep neural networksLast week, you built your first convolutional neural network. In recent years, neural networks have become deeper, with state-of-the-art networks going from just a few layers (e.g., AlexNet) to over a hundred layers. The main benefit of a very deep network is that it can represent very complex functions. It can also learn features at many different levels of abstraction, from edges (at the lower layers) to very complex features (at the deeper layers). However, using a deeper network doesn’t always help. A huge barrier to training them is vanishing gradients: very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent unbearably slow. More specifically, during gradient descent, as you backprop from the final layer back to the first layer, you are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero (or, in rare cases, grow exponentially quickly and “explode” to take very large values). During training, you might therefore see the magnitude (or norm) of the gradient for the earlier layers descrease to zero very rapidly as training proceeds: Figure 1 : Vanishing gradient The speed of learning decreases very rapidly for the early layers as the network trains You are now going to solve this problem by building a Residual Network! 2 - Building a Residual NetworkIn ResNets, a “shortcut” or a “skip connection” allows the gradient to be directly backpropagated to earlier layers: Figure 2 : A ResNet block showing a skip-connection The image on the left shows the “main path” through the network. The image on the right adds a shortcut to the main path. By stacking these ResNet blocks on top of each other, you can form a very deep network. We also saw in lecture that having ResNet blocks with the shortcut also makes it very easy for one of the blocks to learn an identity function. This means that you can stack on additional ResNet blocks with little risk of harming training set performance. (There is also some evidence that the ease of learning an identity function–even more than skip connections helping with vanishing gradients–accounts for ResNets’ remarkable performance.) Two main types of blocks are used in a ResNet, depending mainly on whether the input/output dimensions are same or different. You are going to implement both of them. 2.1 - The identity blockThe identity block is the standard block used in ResNets, and corresponds to the case where the input activation (say $a^{[l]}$) has the same dimension as the output activation (say $a^{[l+2]}$). To flesh out the different steps of what happens in a ResNet’s identity block, here is an alternative diagram showing the individual steps: Figure 3 : Identity block. Skip connection “skips over” 2 layers. The upper path is the “shortcut path.” The lower path is the “main path.” In this diagram, we have also made explicit the CONV2D and ReLU steps in each layer. To speed up training we have also added a BatchNorm step. Don’t worry about this being complicated to implement–you’ll see that BatchNorm is just one line of code in Keras! In this exercise, you’ll actually implement a slightly more powerful version of this identity block, in which the skip connection “skips over” 3 hidden layers rather than 2 layers. It looks like this: Figure 4 : Identity block. Skip connection “skips over” 3 layers. Here’re the individual steps. First component of main path: The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be conv_name_base + &#39;2a&#39;. Use 0 as the seed for the random initialization. The first BatchNorm is normalizing the channels axis. Its name should be bn_name_base + &#39;2a&#39;. Then apply the ReLU activation function. This has no name and no hyperparameters. Second component of main path: The second CONV2D has $F_2$ filters of shape $(f,f)$ and a stride of (1,1). Its padding is “same” and its name should be conv_name_base + &#39;2b&#39;. Use 0 as the seed for the random initialization. The second BatchNorm is normalizing the channels axis. Its name should be bn_name_base + &#39;2b&#39;. Then apply the ReLU activation function. This has no name and no hyperparameters. Third component of main path: The third CONV2D has $F_3$ filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be conv_name_base + &#39;2c&#39;. Use 0 as the seed for the random initialization. The third BatchNorm is normalizing the channels axis. Its name should be bn_name_base + &#39;2c&#39;. Note that there is no ReLU activation function in this component. Final step: The shortcut and the input are added together. Then apply the ReLU activation function. This has no name and no hyperparameters. Exercise: Implement the ResNet identity block. We have implemented the first component of the main path. Please read over this carefully to make sure you understand what it is doing. You should implement the rest. To implement the Conv2D step: See reference To implement BatchNorm: See reference (axis: Integer, the axis that should be normalized (typically the channels axis)) For the activation, use: Activation(&#39;relu&#39;)(X) To add the value passed forward by the shortcut: See reference 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# GRADED FUNCTION: identity_blockdef identity_block(X, f, filters, stage, block): """ Implementation of the identity block as defined in Figure 3 Arguments: X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev) f -- integer, specifying the shape of the middle CONV's window for the main path filters -- python list of integers, defining the number of filters in the CONV layers of the main path stage -- integer, used to name the layers, depending on their position in the network block -- string/character, used to name the layers, depending on their position in the network Returns: X -- output of the identity block, tensor of shape (n_H, n_W, n_C) """ # defining name basis conv_name_base = 'res' + str(stage) + block + '_branch' bn_name_base = 'bn' + str(stage) + block + '_branch' # Retrieve Filters F1, F2, F3 = filters # Save the input value. You'll need this later to add back to the main path. X_shortcut = X # First component of main path X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X); X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X); X = Activation('relu')(X); ### START CODE HERE ### # Second component of main path (≈3 lines) X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X); X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X); X = Activation('relu')(X); # Third component of main path (≈2 lines) X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X); X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X); # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines) X = layers.add([X, X_shortcut]); X = Activation('relu')(X); ### END CODE HERE ### return X 12345678910tf.reset_default_graph()with tf.Session() as test: np.random.seed(1) A_prev = tf.placeholder("float", [3, 4, 4, 6]) X = np.random.randn(3, 4, 4, 6) A = identity_block(A_prev, f = 2, filters = [2, 4, 6], stage = 1, block = 'a') test.run(tf.global_variables_initializer()) out = test.run([A], feed_dict=&#123;A_prev: X, K.learning_phase(): 0&#125;) print("out = " + str(out[0][1][1][0])) out = [ 0.94822985 0. 1.16101444 2.747859 0. 1.36677003] Expected Output: out [ 0.94822985 0. 1.16101444 2.747859 0. 1.36677003] 2.2 - The convolutional blockYou’ve implemented the ResNet identity block. Next, the ResNet “convolutional block” is the other type of block. You can use this type of block when the input and output dimensions don’t match up. The difference with the identity block is that there is a CONV2D layer in the shortcut path: Figure 4 : Convolutional block The CONV2D layer in the shortcut path is used to resize the input $x$ to a different dimension, so that the dimensions match up in the final addition needed to add the shortcut value back to the main path. (This plays a similar role as the matrix $W_s$ discussed in lecture.) For example, to reduce the activation dimensions’s height and width by a factor of 2, you can use a 1x1 convolution with a stride of 2. The CONV2D layer on the shortcut path does not use any non-linear activation function. Its main role is to just apply a (learned) linear function that reduces the dimension of the input, so that the dimensions match up for the later addition step. The details of the convolutional block are as follows. First component of main path: The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be conv_name_base + &#39;2a&#39;. The first BatchNorm is normalizing the channels axis. Its name should be bn_name_base + &#39;2a&#39;. Then apply the ReLU activation function. This has no name and no hyperparameters. Second component of main path: The second CONV2D has $F_2$ filters of (f,f) and a stride of (1,1). Its padding is “same” and it’s name should be conv_name_base + &#39;2b&#39;. The second BatchNorm is normalizing the channels axis. Its name should be bn_name_base + &#39;2b&#39;. Then apply the ReLU activation function. This has no name and no hyperparameters. Third component of main path: The third CONV2D has $F_3$ filters of (1,1) and a stride of (1,1). Its padding is “valid” and it’s name should be conv_name_base + &#39;2c&#39;. The third BatchNorm is normalizing the channels axis. Its name should be bn_name_base + &#39;2c&#39;. Note that there is no ReLU activation function in this component. Shortcut path: The CONV2D has $F_3$ filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be conv_name_base + &#39;1&#39;. The BatchNorm is normalizing the channels axis. Its name should be bn_name_base + &#39;1&#39;. Final step: The shortcut and the main path values are added together. Then apply the ReLU activation function. This has no name and no hyperparameters. Exercise: Implement the convolutional block. We have implemented the first component of the main path; you should implement the rest. As before, always use 0 as the seed for the random initialization, to ensure consistency with our grader. Conv Hint BatchNorm Hint (axis: Integer, the axis that should be normalized (typically the features axis)) For the activation, use: Activation(&#39;relu&#39;)(X) Addition Hint 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# GRADED FUNCTION: convolutional_blockdef convolutional_block(X, f, filters, stage, block, s = 2): """ Implementation of the convolutional block as defined in Figure 4 Arguments: X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev) f -- integer, specifying the shape of the middle CONV's window for the main path filters -- python list of integers, defining the number of filters in the CONV layers of the main path stage -- integer, used to name the layers, depending on their position in the network block -- string/character, used to name the layers, depending on their position in the network s -- Integer, specifying the stride to be used Returns: X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C) """ # defining name basis conv_name_base = 'res' + str(stage) + block + '_branch' bn_name_base = 'bn' + str(stage) + block + '_branch' # Retrieve Filters F1, F2, F3 = filters # Save the input value X_shortcut = X ##### MAIN PATH ##### # First component of main path X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X) X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X) X = Activation('relu')(X) ### START CODE HERE ### # Second component of main path (≈3 lines) X = Conv2D(F2, (f, f), strides = (1,1), name = conv_name_base + '2b', padding = 'same', kernel_initializer = glorot_uniform(seed=0))(X) X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X); X = Activation('relu')(X); # Third component of main path (≈2 lines) X = Conv2D(F3, (1, 1), strides = (1,1), name = conv_name_base + '2c', padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X); X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X); ##### SHORTCUT PATH #### (≈2 lines) X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), name = conv_name_base + '1', padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X_shortcut); X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut); # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines) X = layers.add([X, X_shortcut]); X = Activation('relu')(X); ### END CODE HERE ### return X 12345678910tf.reset_default_graph()with tf.Session() as test: np.random.seed(1) A_prev = tf.placeholder("float", [3, 4, 4, 6]) X = np.random.randn(3, 4, 4, 6) A = convolutional_block(A_prev, f = 2, filters = [2, 4, 6], stage = 1, block = 'a') test.run(tf.global_variables_initializer()) out = test.run([A], feed_dict=&#123;A_prev: X, K.learning_phase(): 0&#125;) print("out = " + str(out[0][1][1][0])) out = [ 0.09018463 1.23489773 0.46822017 0.0367176 0. 0.65516603] Expected Output: out [ 0.09018463 1.23489773 0.46822017 0.0367176 0. 0.65516603] 3 - Building your first ResNet model (50 layers)You now have the necessary blocks to build a very deep ResNet. The following figure describes in detail the architecture of this neural network. “ID BLOCK” in the diagram stands for “Identity block,” and “ID BLOCK x3” means you should stack 3 identity blocks together. Figure 5 : ResNet-50 model The details of this ResNet-50 model are: Zero-padding pads the input with a pad of (3,3) Stage 1: The 2D Convolution has 64 filters of shape (7,7) and uses a stride of (2,2). Its name is “conv1”. BatchNorm is applied to the channels axis of the input. MaxPooling uses a (3,3) window and a (2,2) stride. Stage 2: The convolutional block uses three set of filters of size [64,64,256], “f” is 3, “s” is 1 and the block is “a”. The 2 identity blocks use three set of filters of size [64,64,256], “f” is 3 and the blocks are “b” and “c”. Stage 3: The convolutional block uses three set of filters of size [128,128,512], “f” is 3, “s” is 2 and the block is “a”. The 3 identity blocks use three set of filters of size [128,128,512], “f” is 3 and the blocks are “b”, “c” and “d”. Stage 4: The convolutional block uses three set of filters of size [256, 256, 1024], “f” is 3, “s” is 2 and the block is “a”. The 5 identity blocks use three set of filters of size [256, 256, 1024], “f” is 3 and the blocks are “b”, “c”, “d”, “e” and “f”. Stage 5: The convolutional block uses three set of filters of size [512, 512, 2048], “f” is 3, “s” is 2 and the block is “a”. The 2 identity blocks use three set of filters of size [512, 512, 2048], “f” is 3 and the blocks are “b” and “c”. The 2D Average Pooling uses a window of shape (2,2) and its name is “avg_pool”. The flatten doesn’t have any hyperparameters or name. The Fully Connected (Dense) layer reduces its input to the number of classes using a softmax activation. Its name should be &#39;fc&#39; + str(classes). Exercise: Implement the ResNet with 50 layers described in the figure above. We have implemented Stages 1 and 2. Please implement the rest. (The syntax for implementing Stages 3-5 should be quite similar to that of Stage 2.) Make sure you follow the naming convention in the text above. You’ll need to use this function: Average pooling see reference Here’re some other functions we used in the code below: Conv2D: See reference BatchNorm: See reference (axis: Integer, the axis that should be normalized (typically the features axis)) Zero padding: See reference Max pooling: See reference Fully conected layer: See reference Addition: See reference 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# GRADED FUNCTION: ResNet50def ResNet50(input_shape = (64, 64, 3), classes = 6): """ Implementation of the popular ResNet50 the following architecture: CONV2D -&gt; BATCHNORM -&gt; RELU -&gt; MAXPOOL -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; CONVBLOCK -&gt; IDBLOCK*3 -&gt; CONVBLOCK -&gt; IDBLOCK*5 -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; AVGPOOL -&gt; TOPLAYER Arguments: input_shape -- shape of the images of the dataset classes -- integer, number of classes Returns: model -- a Model() instance in Keras """ # Define the input as a tensor with shape input_shape X_input = Input(input_shape) # Zero-Padding X = ZeroPadding2D((3, 3))(X_input) # Stage 1 X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X) X = BatchNormalization(axis = 3, name = 'bn_conv1')(X) X = Activation('relu')(X) X = MaxPooling2D((3, 3), strides=(2, 2))(X) # Stage 2 X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1) X = identity_block(X, 3, [64, 64, 256], stage=2, block='b') X = identity_block(X, 3, [64, 64, 256], stage=2, block='c') ### START CODE HERE ### # Stage 3 (≈4 lines) X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2); X = identity_block(X, 3, [128, 128, 512], stage=3, block='b'); X = identity_block(X, 3, [128, 128, 512], stage=3, block='c'); X = identity_block(X, 3, [128, 128, 512], stage=3, block='d'); # Stage 4 (≈6 lines) X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2); X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b'); X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c'); X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d'); X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e'); X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f'); # Stage 5 (≈3 lines) X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2); X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b'); X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c'); # AVGPOOL (≈1 line). Use "X = AveragePooling2D(...)(X)" X = AveragePooling2D(pool_size = (2, 2), name = 'avg_pool')(X); ### END CODE HERE ### # output layer X = Flatten()(X) X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X) # Create model model = Model(inputs = X_input, outputs = X, name='ResNet50') return model Run the following code to build the model’s graph. If your implementation is not correct you will know it by checking your accuracy when running model.fit(...) below. 1model = ResNet50(input_shape = (64, 64, 3), classes = 6) As seen in the Keras Tutorial Notebook, prior training a model, you need to configure the learning process by compiling the model. 1model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) The model is now ready to be trained. The only thing you need is a dataset. Let’s load the SIGNS Dataset. Figure 6 : SIGNS dataset 12345678910111213141516X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()# Normalize image vectorsX_train = X_train_orig/255.X_test = X_test_orig/255.# Convert training and test labels to one hot matricesY_train = convert_to_one_hot(Y_train_orig, 6).TY_test = convert_to_one_hot(Y_test_orig, 6).Tprint ("number of training examples = " + str(X_train.shape[0]))print ("number of test examples = " + str(X_test.shape[0]))print ("X_train shape: " + str(X_train.shape))print ("Y_train shape: " + str(Y_train.shape))print ("X_test shape: " + str(X_test.shape))print ("Y_test shape: " + str(Y_test.shape)) number of training examples = 1080 number of test examples = 120 X_train shape: (1080, 64, 64, 3) Y_train shape: (1080, 6) X_test shape: (120, 64, 64, 3) Y_test shape: (120, 6) Run the following cell to train your model on 2 epochs with a batch size of 32. On a CPU it should take you around 5min per epoch. 1model.fit(X_train, Y_train, epochs = 2, batch_size = 32) Epoch 1/2 1080/1080 [==============================] - 244s - loss: 2.9276 - acc: 0.2657 Epoch 2/2 1080/1080 [==============================] - 245s - loss: 1.9963 - acc: 0.3833 &lt;keras.callbacks.History at 0x7f2b2976ae10&gt; Expected Output: Epoch 1/2 loss: between 1 and 5, acc: between 0.2 and 0.5, although your results can be different from ours. Epoch 2/2 loss: between 1 and 5, acc: between 0.2 and 0.5, you should see your loss decreasing and the accuracy increasing. Let’s see how this model (trained on only two epochs) performs on the test set. 123preds = model.evaluate(X_test, Y_test)print ("Loss = " + str(preds[0]))print ("Test Accuracy = " + str(preds[1])) 120/120 [==============================] - 9s Loss = 2.66479465167 Test Accuracy = 0.166666666667 Expected Output: Test Accuracy between 0.16 and 0.25 For the purpose of this assignment, we’ve asked you to train the model only for two epochs. You can see that it achieves poor performances. Please go ahead and submit your assignment; to check correctness, the online grader will run your code only for a small number of epochs as well. After you have finished this official (graded) part of this assignment, you can also optionally train the ResNet for more iterations, if you want. We get a lot better performance when we train for ~20 epochs, but this will take more than an hour when training on a CPU. Using a GPU, we’ve trained our own ResNet50 model’s weights on the SIGNS dataset. You can load and run our trained model on the test set in the cells below. It may take ≈1min to load the model. 1model = load_model('ResNet50.h5') 123preds = model.evaluate(X_test, Y_test)print ("Loss = " + str(preds[0]))print ("Test Accuracy = " + str(preds[1])) 120/120 [==============================] - 9s Loss = 0.530178320408 Test Accuracy = 0.866666662693 ResNet50 is a powerful model for image classification when it is trained for an adequate number of iterations. We hope you can use what you’ve learnt and apply it to your own classification problem to perform state-of-the-art accuracy. Congratulations on finishing this assignment! You’ve now implemented a state-of-the-art image classification system! 4 - Test on your own image (Optional/Ungraded)If you wish, you can also take a picture of your own hand and see the output of the model. To do this: 1. Click on &quot;File&quot; in the upper bar of this notebook, then click &quot;Open&quot; to go on your Coursera Hub. 2. Add your image to this Jupyter Notebook&apos;s directory, in the &quot;images&quot; folder 3. Write your image&apos;s name in the following code 4. Run the code and check if the algorithm is right! 12345678910img_path = 'images/my_image.jpg'img = image.load_img(img_path, target_size=(64, 64))x = image.img_to_array(img)x = np.expand_dims(x, axis=0)x = preprocess_input(x)print('Input image shape:', x.shape)my_image = scipy.misc.imread(img_path)imshow(my_image)print("class prediction vector [p(0), p(1), p(2), p(3), p(4), p(5)] = ")print(model.predict(x)) Input image shape: (1, 64, 64, 3) class prediction vector [p(0), p(1), p(2), p(3), p(4), p(5)] = [[ 1. 0. 0. 0. 0. 0.]] You can also print a summary of your model by running the following code. 1model.summary() ____________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ==================================================================================================== input_1 (InputLayer) (None, 64, 64, 3) 0 ____________________________________________________________________________________________________ zero_padding2d_1 (ZeroPadding2D) (None, 70, 70, 3) 0 input_1[0][0] ____________________________________________________________________________________________________ conv1 (Conv2D) (None, 32, 32, 64) 9472 zero_padding2d_1[0][0] ____________________________________________________________________________________________________ bn_conv1 (BatchNormalization) (None, 32, 32, 64) 256 conv1[0][0] ____________________________________________________________________________________________________ activation_4 (Activation) (None, 32, 32, 64) 0 bn_conv1[0][0] ____________________________________________________________________________________________________ max_pooling2d_1 (MaxPooling2D) (None, 15, 15, 64) 0 activation_4[0][0] ____________________________________________________________________________________________________ res2a_branch2a (Conv2D) (None, 15, 15, 64) 4160 max_pooling2d_1[0][0] ____________________________________________________________________________________________________ bn2a_branch2a (BatchNormalizatio (None, 15, 15, 64) 256 res2a_branch2a[0][0] ____________________________________________________________________________________________________ activation_5 (Activation) (None, 15, 15, 64) 0 bn2a_branch2a[0][0] ____________________________________________________________________________________________________ res2a_branch2b (Conv2D) (None, 15, 15, 64) 36928 activation_5[0][0] ____________________________________________________________________________________________________ bn2a_branch2b (BatchNormalizatio (None, 15, 15, 64) 256 res2a_branch2b[0][0] ____________________________________________________________________________________________________ activation_6 (Activation) (None, 15, 15, 64) 0 bn2a_branch2b[0][0] ____________________________________________________________________________________________________ res2a_branch2c (Conv2D) (None, 15, 15, 256) 16640 activation_6[0][0] ____________________________________________________________________________________________________ res2a_branch1 (Conv2D) (None, 15, 15, 256) 16640 max_pooling2d_1[0][0] ____________________________________________________________________________________________________ bn2a_branch2c (BatchNormalizatio (None, 15, 15, 256) 1024 res2a_branch2c[0][0] ____________________________________________________________________________________________________ bn2a_branch1 (BatchNormalization (None, 15, 15, 256) 1024 res2a_branch1[0][0] ____________________________________________________________________________________________________ add_2 (Add) (None, 15, 15, 256) 0 bn2a_branch2c[0][0] bn2a_branch1[0][0] ____________________________________________________________________________________________________ activation_7 (Activation) (None, 15, 15, 256) 0 add_2[0][0] ____________________________________________________________________________________________________ res2b_branch2a (Conv2D) (None, 15, 15, 64) 16448 activation_7[0][0] ____________________________________________________________________________________________________ bn2b_branch2a (BatchNormalizatio (None, 15, 15, 64) 256 res2b_branch2a[0][0] ____________________________________________________________________________________________________ activation_8 (Activation) (None, 15, 15, 64) 0 bn2b_branch2a[0][0] ____________________________________________________________________________________________________ res2b_branch2b (Conv2D) (None, 15, 15, 64) 36928 activation_8[0][0] ____________________________________________________________________________________________________ bn2b_branch2b (BatchNormalizatio (None, 15, 15, 64) 256 res2b_branch2b[0][0] ____________________________________________________________________________________________________ activation_9 (Activation) (None, 15, 15, 64) 0 bn2b_branch2b[0][0] ____________________________________________________________________________________________________ res2b_branch2c (Conv2D) (None, 15, 15, 256) 16640 activation_9[0][0] ____________________________________________________________________________________________________ bn2b_branch2c (BatchNormalizatio (None, 15, 15, 256) 1024 res2b_branch2c[0][0] ____________________________________________________________________________________________________ add_3 (Add) (None, 15, 15, 256) 0 bn2b_branch2c[0][0] activation_7[0][0] ____________________________________________________________________________________________________ activation_10 (Activation) (None, 15, 15, 256) 0 add_3[0][0] ____________________________________________________________________________________________________ res2c_branch2a (Conv2D) (None, 15, 15, 64) 16448 activation_10[0][0] ____________________________________________________________________________________________________ bn2c_branch2a (BatchNormalizatio (None, 15, 15, 64) 256 res2c_branch2a[0][0] ____________________________________________________________________________________________________ activation_11 (Activation) (None, 15, 15, 64) 0 bn2c_branch2a[0][0] ____________________________________________________________________________________________________ res2c_branch2b (Conv2D) (None, 15, 15, 64) 36928 activation_11[0][0] ____________________________________________________________________________________________________ bn2c_branch2b (BatchNormalizatio (None, 15, 15, 64) 256 res2c_branch2b[0][0] ____________________________________________________________________________________________________ activation_12 (Activation) (None, 15, 15, 64) 0 bn2c_branch2b[0][0] ____________________________________________________________________________________________________ res2c_branch2c (Conv2D) (None, 15, 15, 256) 16640 activation_12[0][0] ____________________________________________________________________________________________________ bn2c_branch2c (BatchNormalizatio (None, 15, 15, 256) 1024 res2c_branch2c[0][0] ____________________________________________________________________________________________________ add_4 (Add) (None, 15, 15, 256) 0 bn2c_branch2c[0][0] activation_10[0][0] ____________________________________________________________________________________________________ activation_13 (Activation) (None, 15, 15, 256) 0 add_4[0][0] ____________________________________________________________________________________________________ res3a_branch2a (Conv2D) (None, 8, 8, 128) 32896 activation_13[0][0] ____________________________________________________________________________________________________ bn3a_branch2a (BatchNormalizatio (None, 8, 8, 128) 512 res3a_branch2a[0][0] ____________________________________________________________________________________________________ activation_14 (Activation) (None, 8, 8, 128) 0 bn3a_branch2a[0][0] ____________________________________________________________________________________________________ res3a_branch2b (Conv2D) (None, 8, 8, 128) 147584 activation_14[0][0] ____________________________________________________________________________________________________ bn3a_branch2b (BatchNormalizatio (None, 8, 8, 128) 512 res3a_branch2b[0][0] ____________________________________________________________________________________________________ activation_15 (Activation) (None, 8, 8, 128) 0 bn3a_branch2b[0][0] ____________________________________________________________________________________________________ res3a_branch2c (Conv2D) (None, 8, 8, 512) 66048 activation_15[0][0] ____________________________________________________________________________________________________ res3a_branch1 (Conv2D) (None, 8, 8, 512) 131584 activation_13[0][0] ____________________________________________________________________________________________________ bn3a_branch2c (BatchNormalizatio (None, 8, 8, 512) 2048 res3a_branch2c[0][0] ____________________________________________________________________________________________________ bn3a_branch1 (BatchNormalization (None, 8, 8, 512) 2048 res3a_branch1[0][0] ____________________________________________________________________________________________________ add_5 (Add) (None, 8, 8, 512) 0 bn3a_branch2c[0][0] bn3a_branch1[0][0] ____________________________________________________________________________________________________ activation_16 (Activation) (None, 8, 8, 512) 0 add_5[0][0] ____________________________________________________________________________________________________ res3b_branch2a (Conv2D) (None, 8, 8, 128) 65664 activation_16[0][0] ____________________________________________________________________________________________________ bn3b_branch2a (BatchNormalizatio (None, 8, 8, 128) 512 res3b_branch2a[0][0] ____________________________________________________________________________________________________ activation_17 (Activation) (None, 8, 8, 128) 0 bn3b_branch2a[0][0] ____________________________________________________________________________________________________ res3b_branch2b (Conv2D) (None, 8, 8, 128) 147584 activation_17[0][0] ____________________________________________________________________________________________________ bn3b_branch2b (BatchNormalizatio (None, 8, 8, 128) 512 res3b_branch2b[0][0] ____________________________________________________________________________________________________ activation_18 (Activation) (None, 8, 8, 128) 0 bn3b_branch2b[0][0] ____________________________________________________________________________________________________ res3b_branch2c (Conv2D) (None, 8, 8, 512) 66048 activation_18[0][0] ____________________________________________________________________________________________________ bn3b_branch2c (BatchNormalizatio (None, 8, 8, 512) 2048 res3b_branch2c[0][0] ____________________________________________________________________________________________________ add_6 (Add) (None, 8, 8, 512) 0 bn3b_branch2c[0][0] activation_16[0][0] ____________________________________________________________________________________________________ activation_19 (Activation) (None, 8, 8, 512) 0 add_6[0][0] ____________________________________________________________________________________________________ res3c_branch2a (Conv2D) (None, 8, 8, 128) 65664 activation_19[0][0] ____________________________________________________________________________________________________ bn3c_branch2a (BatchNormalizatio (None, 8, 8, 128) 512 res3c_branch2a[0][0] ____________________________________________________________________________________________________ activation_20 (Activation) (None, 8, 8, 128) 0 bn3c_branch2a[0][0] ____________________________________________________________________________________________________ res3c_branch2b (Conv2D) (None, 8, 8, 128) 147584 activation_20[0][0] ____________________________________________________________________________________________________ bn3c_branch2b (BatchNormalizatio (None, 8, 8, 128) 512 res3c_branch2b[0][0] ____________________________________________________________________________________________________ activation_21 (Activation) (None, 8, 8, 128) 0 bn3c_branch2b[0][0] ____________________________________________________________________________________________________ res3c_branch2c (Conv2D) (None, 8, 8, 512) 66048 activation_21[0][0] ____________________________________________________________________________________________________ bn3c_branch2c (BatchNormalizatio (None, 8, 8, 512) 2048 res3c_branch2c[0][0] ____________________________________________________________________________________________________ add_7 (Add) (None, 8, 8, 512) 0 bn3c_branch2c[0][0] activation_19[0][0] ____________________________________________________________________________________________________ activation_22 (Activation) (None, 8, 8, 512) 0 add_7[0][0] ____________________________________________________________________________________________________ res3d_branch2a (Conv2D) (None, 8, 8, 128) 65664 activation_22[0][0] ____________________________________________________________________________________________________ bn3d_branch2a (BatchNormalizatio (None, 8, 8, 128) 512 res3d_branch2a[0][0] ____________________________________________________________________________________________________ activation_23 (Activation) (None, 8, 8, 128) 0 bn3d_branch2a[0][0] ____________________________________________________________________________________________________ res3d_branch2b (Conv2D) (None, 8, 8, 128) 147584 activation_23[0][0] ____________________________________________________________________________________________________ bn3d_branch2b (BatchNormalizatio (None, 8, 8, 128) 512 res3d_branch2b[0][0] ____________________________________________________________________________________________________ activation_24 (Activation) (None, 8, 8, 128) 0 bn3d_branch2b[0][0] ____________________________________________________________________________________________________ res3d_branch2c (Conv2D) (None, 8, 8, 512) 66048 activation_24[0][0] ____________________________________________________________________________________________________ bn3d_branch2c (BatchNormalizatio (None, 8, 8, 512) 2048 res3d_branch2c[0][0] ____________________________________________________________________________________________________ add_8 (Add) (None, 8, 8, 512) 0 bn3d_branch2c[0][0] activation_22[0][0] ____________________________________________________________________________________________________ activation_25 (Activation) (None, 8, 8, 512) 0 add_8[0][0] ____________________________________________________________________________________________________ res4a_branch2a (Conv2D) (None, 4, 4, 256) 131328 activation_25[0][0] ____________________________________________________________________________________________________ bn4a_branch2a (BatchNormalizatio (None, 4, 4, 256) 1024 res4a_branch2a[0][0] ____________________________________________________________________________________________________ activation_26 (Activation) (None, 4, 4, 256) 0 bn4a_branch2a[0][0] ____________________________________________________________________________________________________ res4a_branch2b (Conv2D) (None, 4, 4, 256) 590080 activation_26[0][0] ____________________________________________________________________________________________________ bn4a_branch2b (BatchNormalizatio (None, 4, 4, 256) 1024 res4a_branch2b[0][0] ____________________________________________________________________________________________________ activation_27 (Activation) (None, 4, 4, 256) 0 bn4a_branch2b[0][0] ____________________________________________________________________________________________________ res4a_branch2c (Conv2D) (None, 4, 4, 1024) 263168 activation_27[0][0] ____________________________________________________________________________________________________ res4a_branch1 (Conv2D) (None, 4, 4, 1024) 525312 activation_25[0][0] ____________________________________________________________________________________________________ bn4a_branch2c (BatchNormalizatio (None, 4, 4, 1024) 4096 res4a_branch2c[0][0] ____________________________________________________________________________________________________ bn4a_branch1 (BatchNormalization (None, 4, 4, 1024) 4096 res4a_branch1[0][0] ____________________________________________________________________________________________________ add_9 (Add) (None, 4, 4, 1024) 0 bn4a_branch2c[0][0] bn4a_branch1[0][0] ____________________________________________________________________________________________________ activation_28 (Activation) (None, 4, 4, 1024) 0 add_9[0][0] ____________________________________________________________________________________________________ res4b_branch2a (Conv2D) (None, 4, 4, 256) 262400 activation_28[0][0] ____________________________________________________________________________________________________ bn4b_branch2a (BatchNormalizatio (None, 4, 4, 256) 1024 res4b_branch2a[0][0] ____________________________________________________________________________________________________ activation_29 (Activation) (None, 4, 4, 256) 0 bn4b_branch2a[0][0] ____________________________________________________________________________________________________ res4b_branch2b (Conv2D) (None, 4, 4, 256) 590080 activation_29[0][0] ____________________________________________________________________________________________________ bn4b_branch2b (BatchNormalizatio (None, 4, 4, 256) 1024 res4b_branch2b[0][0] ____________________________________________________________________________________________________ activation_30 (Activation) (None, 4, 4, 256) 0 bn4b_branch2b[0][0] ____________________________________________________________________________________________________ res4b_branch2c (Conv2D) (None, 4, 4, 1024) 263168 activation_30[0][0] ____________________________________________________________________________________________________ bn4b_branch2c (BatchNormalizatio (None, 4, 4, 1024) 4096 res4b_branch2c[0][0] ____________________________________________________________________________________________________ add_10 (Add) (None, 4, 4, 1024) 0 bn4b_branch2c[0][0] activation_28[0][0] ____________________________________________________________________________________________________ activation_31 (Activation) (None, 4, 4, 1024) 0 add_10[0][0] ____________________________________________________________________________________________________ res4c_branch2a (Conv2D) (None, 4, 4, 256) 262400 activation_31[0][0] ____________________________________________________________________________________________________ bn4c_branch2a (BatchNormalizatio (None, 4, 4, 256) 1024 res4c_branch2a[0][0] ____________________________________________________________________________________________________ activation_32 (Activation) (None, 4, 4, 256) 0 bn4c_branch2a[0][0] ____________________________________________________________________________________________________ res4c_branch2b (Conv2D) (None, 4, 4, 256) 590080 activation_32[0][0] ____________________________________________________________________________________________________ bn4c_branch2b (BatchNormalizatio (None, 4, 4, 256) 1024 res4c_branch2b[0][0] ____________________________________________________________________________________________________ activation_33 (Activation) (None, 4, 4, 256) 0 bn4c_branch2b[0][0] ____________________________________________________________________________________________________ res4c_branch2c (Conv2D) (None, 4, 4, 1024) 263168 activation_33[0][0] ____________________________________________________________________________________________________ bn4c_branch2c (BatchNormalizatio (None, 4, 4, 1024) 4096 res4c_branch2c[0][0] ____________________________________________________________________________________________________ add_11 (Add) (None, 4, 4, 1024) 0 bn4c_branch2c[0][0] activation_31[0][0] ____________________________________________________________________________________________________ activation_34 (Activation) (None, 4, 4, 1024) 0 add_11[0][0] ____________________________________________________________________________________________________ res4d_branch2a (Conv2D) (None, 4, 4, 256) 262400 activation_34[0][0] ____________________________________________________________________________________________________ bn4d_branch2a (BatchNormalizatio (None, 4, 4, 256) 1024 res4d_branch2a[0][0] ____________________________________________________________________________________________________ activation_35 (Activation) (None, 4, 4, 256) 0 bn4d_branch2a[0][0] ____________________________________________________________________________________________________ res4d_branch2b (Conv2D) (None, 4, 4, 256) 590080 activation_35[0][0] ____________________________________________________________________________________________________ bn4d_branch2b (BatchNormalizatio (None, 4, 4, 256) 1024 res4d_branch2b[0][0] ____________________________________________________________________________________________________ activation_36 (Activation) (None, 4, 4, 256) 0 bn4d_branch2b[0][0] ____________________________________________________________________________________________________ res4d_branch2c (Conv2D) (None, 4, 4, 1024) 263168 activation_36[0][0] ____________________________________________________________________________________________________ bn4d_branch2c (BatchNormalizatio (None, 4, 4, 1024) 4096 res4d_branch2c[0][0] ____________________________________________________________________________________________________ add_12 (Add) (None, 4, 4, 1024) 0 bn4d_branch2c[0][0] activation_34[0][0] ____________________________________________________________________________________________________ activation_37 (Activation) (None, 4, 4, 1024) 0 add_12[0][0] ____________________________________________________________________________________________________ res4e_branch2a (Conv2D) (None, 4, 4, 256) 262400 activation_37[0][0] ____________________________________________________________________________________________________ bn4e_branch2a (BatchNormalizatio (None, 4, 4, 256) 1024 res4e_branch2a[0][0] ____________________________________________________________________________________________________ activation_38 (Activation) (None, 4, 4, 256) 0 bn4e_branch2a[0][0] ____________________________________________________________________________________________________ res4e_branch2b (Conv2D) (None, 4, 4, 256) 590080 activation_38[0][0] ____________________________________________________________________________________________________ bn4e_branch2b (BatchNormalizatio (None, 4, 4, 256) 1024 res4e_branch2b[0][0] ____________________________________________________________________________________________________ activation_39 (Activation) (None, 4, 4, 256) 0 bn4e_branch2b[0][0] ____________________________________________________________________________________________________ res4e_branch2c (Conv2D) (None, 4, 4, 1024) 263168 activation_39[0][0] ____________________________________________________________________________________________________ bn4e_branch2c (BatchNormalizatio (None, 4, 4, 1024) 4096 res4e_branch2c[0][0] ____________________________________________________________________________________________________ add_13 (Add) (None, 4, 4, 1024) 0 bn4e_branch2c[0][0] activation_37[0][0] ____________________________________________________________________________________________________ activation_40 (Activation) (None, 4, 4, 1024) 0 add_13[0][0] ____________________________________________________________________________________________________ res4f_branch2a (Conv2D) (None, 4, 4, 256) 262400 activation_40[0][0] ____________________________________________________________________________________________________ bn4f_branch2a (BatchNormalizatio (None, 4, 4, 256) 1024 res4f_branch2a[0][0] ____________________________________________________________________________________________________ activation_41 (Activation) (None, 4, 4, 256) 0 bn4f_branch2a[0][0] ____________________________________________________________________________________________________ res4f_branch2b (Conv2D) (None, 4, 4, 256) 590080 activation_41[0][0] ____________________________________________________________________________________________________ bn4f_branch2b (BatchNormalizatio (None, 4, 4, 256) 1024 res4f_branch2b[0][0] ____________________________________________________________________________________________________ activation_42 (Activation) (None, 4, 4, 256) 0 bn4f_branch2b[0][0] ____________________________________________________________________________________________________ res4f_branch2c (Conv2D) (None, 4, 4, 1024) 263168 activation_42[0][0] ____________________________________________________________________________________________________ bn4f_branch2c (BatchNormalizatio (None, 4, 4, 1024) 4096 res4f_branch2c[0][0] ____________________________________________________________________________________________________ add_14 (Add) (None, 4, 4, 1024) 0 bn4f_branch2c[0][0] activation_40[0][0] ____________________________________________________________________________________________________ activation_43 (Activation) (None, 4, 4, 1024) 0 add_14[0][0] ____________________________________________________________________________________________________ res5a_branch2a (Conv2D) (None, 2, 2, 512) 524800 activation_43[0][0] ____________________________________________________________________________________________________ bn5a_branch2a (BatchNormalizatio (None, 2, 2, 512) 2048 res5a_branch2a[0][0] ____________________________________________________________________________________________________ activation_44 (Activation) (None, 2, 2, 512) 0 bn5a_branch2a[0][0] ____________________________________________________________________________________________________ res5a_branch2b (Conv2D) (None, 2, 2, 512) 2359808 activation_44[0][0] ____________________________________________________________________________________________________ bn5a_branch2b (BatchNormalizatio (None, 2, 2, 512) 2048 res5a_branch2b[0][0] ____________________________________________________________________________________________________ activation_45 (Activation) (None, 2, 2, 512) 0 bn5a_branch2b[0][0] ____________________________________________________________________________________________________ res5a_branch2c (Conv2D) (None, 2, 2, 2048) 1050624 activation_45[0][0] ____________________________________________________________________________________________________ res5a_branch1 (Conv2D) (None, 2, 2, 2048) 2099200 activation_43[0][0] ____________________________________________________________________________________________________ bn5a_branch2c (BatchNormalizatio (None, 2, 2, 2048) 8192 res5a_branch2c[0][0] ____________________________________________________________________________________________________ bn5a_branch1 (BatchNormalization (None, 2, 2, 2048) 8192 res5a_branch1[0][0] ____________________________________________________________________________________________________ add_15 (Add) (None, 2, 2, 2048) 0 bn5a_branch2c[0][0] bn5a_branch1[0][0] ____________________________________________________________________________________________________ activation_46 (Activation) (None, 2, 2, 2048) 0 add_15[0][0] ____________________________________________________________________________________________________ res5b_branch2a (Conv2D) (None, 2, 2, 512) 1049088 activation_46[0][0] ____________________________________________________________________________________________________ bn5b_branch2a (BatchNormalizatio (None, 2, 2, 512) 2048 res5b_branch2a[0][0] ____________________________________________________________________________________________________ activation_47 (Activation) (None, 2, 2, 512) 0 bn5b_branch2a[0][0] ____________________________________________________________________________________________________ res5b_branch2b (Conv2D) (None, 2, 2, 512) 2359808 activation_47[0][0] ____________________________________________________________________________________________________ bn5b_branch2b (BatchNormalizatio (None, 2, 2, 512) 2048 res5b_branch2b[0][0] ____________________________________________________________________________________________________ activation_48 (Activation) (None, 2, 2, 512) 0 bn5b_branch2b[0][0] ____________________________________________________________________________________________________ res5b_branch2c (Conv2D) (None, 2, 2, 2048) 1050624 activation_48[0][0] ____________________________________________________________________________________________________ bn5b_branch2c (BatchNormalizatio (None, 2, 2, 2048) 8192 res5b_branch2c[0][0] ____________________________________________________________________________________________________ add_16 (Add) (None, 2, 2, 2048) 0 bn5b_branch2c[0][0] activation_46[0][0] ____________________________________________________________________________________________________ activation_49 (Activation) (None, 2, 2, 2048) 0 add_16[0][0] ____________________________________________________________________________________________________ res5c_branch2a (Conv2D) (None, 2, 2, 512) 1049088 activation_49[0][0] ____________________________________________________________________________________________________ bn5c_branch2a (BatchNormalizatio (None, 2, 2, 512) 2048 res5c_branch2a[0][0] ____________________________________________________________________________________________________ activation_50 (Activation) (None, 2, 2, 512) 0 bn5c_branch2a[0][0] ____________________________________________________________________________________________________ res5c_branch2b (Conv2D) (None, 2, 2, 512) 2359808 activation_50[0][0] ____________________________________________________________________________________________________ bn5c_branch2b (BatchNormalizatio (None, 2, 2, 512) 2048 res5c_branch2b[0][0] ____________________________________________________________________________________________________ activation_51 (Activation) (None, 2, 2, 512) 0 bn5c_branch2b[0][0] ____________________________________________________________________________________________________ res5c_branch2c (Conv2D) (None, 2, 2, 2048) 1050624 activation_51[0][0] ____________________________________________________________________________________________________ bn5c_branch2c (BatchNormalizatio (None, 2, 2, 2048) 8192 res5c_branch2c[0][0] ____________________________________________________________________________________________________ add_17 (Add) (None, 2, 2, 2048) 0 bn5c_branch2c[0][0] activation_49[0][0] ____________________________________________________________________________________________________ activation_52 (Activation) (None, 2, 2, 2048) 0 add_17[0][0] ____________________________________________________________________________________________________ avg_pool (AveragePooling2D) (None, 1, 1, 2048) 0 activation_52[0][0] ____________________________________________________________________________________________________ flatten_1 (Flatten) (None, 2048) 0 avg_pool[0][0] ____________________________________________________________________________________________________ fc6 (Dense) (None, 6) 12294 flatten_1[0][0] ==================================================================================================== Total params: 23,600,006 Trainable params: 23,546,886 Non-trainable params: 53,120 ____________________________________________________________________________________________________ Finally, run the code below to visualize your ResNet50. You can also download a .png picture of your model by going to “File -&gt; Open…-&gt; model.png”. 12plot_model(model, to_file='model.png')SVG(model_to_dot(model).create(prog='dot', format='svg')) What you should remember: Very deep “plain” networks don’t work in practice because they are hard to train due to vanishing gradients. The skip-connections help to address the Vanishing Gradient problem. They also make it easy for a ResNet block to learn an identity function. There are two main type of blocks: The identity block and the convolutional block. Very deep Residual Networks are built by stacking these blocks together. ReferencesThis notebook presents the ResNet algorithm due to He et al. (2015). The implementation here also took significant inspiration and follows the structure given in the github repository of Francois Chollet: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - Deep Residual Learning for Image Recognition (2015) Francois Chollet’s github repository: https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>convolutional-neural-networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras Tutorial Happy House]]></title>
    <url>%2F2018%2F05%2F02%2FKeras%2B-%2BTutorial%2B-%2BHappy%2BHouse%2Bv2%2F</url>
    <content type="text"><![CDATA[NoteThese are my personal programming assignments at the 2nd week after studying the course convolutional neural networks and the copyright belongs to deeplearning.ai. Keras tutorial - the Happy HouseWelcome to the first assignment of week 2. In this assignment, you will: Learn to use Keras, a high-level neural networks API (programming framework), written in Python and capable of running on top of several lower-level frameworks including TensorFlow and CNTK. See how you can in a couple of hours build a deep learning algorithm. Why are we using Keras? Keras was developed to enable deep learning engineers to build and experiment with different models very quickly. Just as TensorFlow is a higher-level framework than Python, Keras is an even higher-level framework and provides additional abstractions. Being able to go from idea to result with the least possible delay is key to finding good models. However, Keras is more restrictive than the lower-level frameworks, so there are some very complex models that you can implement in TensorFlow but not (without more difficulty) in Keras. That being said, Keras will work fine for many common models. In this exercise, you’ll work on the “Happy House” problem, which we’ll explain below. Let’s load the required packages and solve the problem of the Happy House! 123456789101112131415161718192021import numpy as npfrom keras import layersfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2Dfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2Dfrom keras.models import Modelfrom keras.preprocessing import imagefrom keras.utils import layer_utilsfrom keras.utils.data_utils import get_filefrom keras.applications.imagenet_utils import preprocess_inputimport pydotfrom IPython.display import SVGfrom keras.utils.vis_utils import model_to_dotfrom keras.utils import plot_modelfrom kt_utils import *import keras.backend as KK.set_image_data_format('channels_last')import matplotlib.pyplot as pltfrom matplotlib.pyplot import imshow%matplotlib inline Using TensorFlow backend. Note: As you can see, we’ve imported a lot of functions from Keras. You can use them easily just by calling them directly in the notebook. Ex: X = Input(...) or X = ZeroPadding2D(...). 1 - The Happy HouseFor your next vacation, you decided to spend a week with five of your friends from school. It is a very convenient house with many things to do nearby. But the most important benefit is that everybody has commited to be happy when they are in the house. So anyone wanting to enter the house must prove their current state of happiness. Figure 1 : the Happy House As a deep learning expert, to make sure the “Happy” rule is strictly applied, you are going to build an algorithm which that uses pictures from the front door camera to check if the person is happy or not. The door should open only if the person is happy. You have gathered pictures of your friends and yourself, taken by the front-door camera. The dataset is labbeled. Run the following code to normalize the dataset and learn about its shapes. 12345678910111213141516X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()# Normalize image vectorsX_train = X_train_orig/255.X_test = X_test_orig/255.# ReshapeY_train = Y_train_orig.TY_test = Y_test_orig.Tprint ("number of training examples = " + str(X_train.shape[0]))print ("number of test examples = " + str(X_test.shape[0]))print ("X_train shape: " + str(X_train.shape))print ("Y_train shape: " + str(Y_train.shape))print ("X_test shape: " + str(X_test.shape))print ("Y_test shape: " + str(Y_test.shape)) number of training examples = 600 number of test examples = 150 X_train shape: (600, 64, 64, 3) Y_train shape: (600, 1) X_test shape: (150, 64, 64, 3) Y_test shape: (150, 1) Details of the “Happy” dataset: Images are of shape (64,64,3) Training: 600 pictures Test: 150 pictures It is now time to solve the “Happy” Challenge. 2 - Building a model in KerasKeras is very good for rapid prototyping. In just a short time you will be able to build a model that achieves outstanding results. Here is an example of a model in Keras: 1234567891011121314151617181920212223def model(input_shape): # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image! X_input = Input(input_shape) # Zero-Padding: pads the border of X_input with zeroes X = ZeroPadding2D((3, 3))(X_input) # CONV -&gt; BN -&gt; RELU Block applied to X X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0')(X) X = BatchNormalization(axis = 3, name = 'bn0')(X) X = Activation('relu')(X) # MAXPOOL X = MaxPooling2D((2, 2), name='max_pool')(X) # FLATTEN X (means convert it to a vector) + FULLYCONNECTED X = Flatten()(X) X = Dense(1, activation='sigmoid', name='fc')(X) # Create model. This creates your Keras model instance, you'll use this instance to train/test the model. model = Model(inputs = X_input, outputs = X, name='HappyModel') return model Note that Keras uses a different convention with variable names than we’ve previously used with numpy and TensorFlow. In particular, rather than creating and assigning a new variable on each step of forward propagation such as X, Z1, A1, Z2, A2, etc. for the computations for the different layers, in Keras code each line above just reassigns X to a new value using X = .... In other words, during each step of forward propagation, we are just writing the latest value in the commputation into the same variable X. The only exception was X_input, which we kept separate and did not overwrite, since we needed it at the end to create the Keras model instance (model = Model(inputs = X_input, ...) above). Exercise: Implement a HappyModel(). This assignment is more open-ended than most. We suggest that you start by implementing a model using the architecture we suggest, and run through the rest of this assignment using that as your initial model. But after that, come back and take initiative to try out other model architectures. For example, you might take inspiration from the model above, but then vary the network architecture and hyperparameters however you wish. You can also use other functions such as AveragePooling2D(), GlobalMaxPooling2D(), Dropout(). Note: You have to be careful with your data’s shapes. Use what you’ve learned in the videos to make sure your convolutional, pooling and fully-connected layers are adapted to the volumes you’re applying it to. 1234567891011121314151617181920212223242526272829303132333435# GRADED FUNCTION: HappyModeldef HappyModel(input_shape): """ Implementation of the HappyModel. Arguments: input_shape -- shape of the images of the dataset Returns: model -- a Model() instance in Keras """ ### START CODE HERE ### # Feel free to use the suggested outline in the text above to get started, and run through the whole # exercise (including the later portions of this notebook) once. The come back also try out other # network architectures as well. X_input = Input(input_shape); X = ZeroPadding2D((3,3))(X_input); X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0')(X) X = BatchNormalization(axis = 3, name = 'bn0')(X); X = Activation('relu')(X); X = MaxPooling2D((2, 2), name = 'max_pool')(X); X = Flatten()(X); X = Dense(1, activation = 'sigmoid', name = 'fc0')(X); model = Model(X_input, X, 'HappyModel'); ### END CODE HERE ### return model You have now built a function to describe your model. To train and test this model, there are four steps in Keras: Create the model by calling the function above Compile the model by calling model.compile(optimizer = &quot;...&quot;, loss = &quot;...&quot;, metrics = [&quot;accuracy&quot;]) Train the model on train data by calling model.fit(x = ..., y = ..., epochs = ..., batch_size = ...) Test the model on test data by calling model.evaluate(x = ..., y = ...) If you want to know more about model.compile(), model.fit(), model.evaluate() and their arguments, refer to the official Keras documentation. Exercise: Implement step 1, i.e. create the model. 123### START CODE HERE ### (1 line)happyModel = HappyModel(X_train[1, :, :, :].shape);### END CODE HERE ### Exercise: Implement step 2, i.e. compile the model to configure the learning process. Choose the 3 arguments of compile() wisely. Hint: the Happy Challenge is a binary classification problem. 123### START CODE HERE ### (1 line)happyModel.compile(optimizer = 'Adam', loss='binary_crossentropy', metrics=['accuracy'] )### END CODE HERE ### Exercise: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size. 123### START CODE HERE ### (1 line)happyModel.fit(x = X_train, y = Y_train, batch_size = 32, epochs = 20);### END CODE HERE ### Epoch 1/20 600/600 [==============================] - 13s - loss: 1.6428 - acc: 0.6250 Epoch 2/20 600/600 [==============================] - 14s - loss: 0.3686 - acc: 0.8517 Epoch 3/20 600/600 [==============================] - 14s - loss: 0.1996 - acc: 0.9183 Epoch 4/20 600/600 [==============================] - 13s - loss: 0.1802 - acc: 0.9267 Epoch 5/20 600/600 [==============================] - 13s - loss: 0.1505 - acc: 0.9367 Epoch 6/20 600/600 [==============================] - 14s - loss: 0.2538 - acc: 0.8900 Epoch 7/20 600/600 [==============================] - 14s - loss: 0.1022 - acc: 0.9683 Epoch 8/20 600/600 [==============================] - 14s - loss: 0.0881 - acc: 0.9667 Epoch 9/20 600/600 [==============================] - 14s - loss: 0.0708 - acc: 0.9800 Epoch 10/20 600/600 [==============================] - 14s - loss: 0.0799 - acc: 0.9700 Epoch 11/20 600/600 [==============================] - 14s - loss: 0.0525 - acc: 0.9900 Epoch 12/20 600/600 [==============================] - 14s - loss: 0.0551 - acc: 0.9850 Epoch 13/20 600/600 [==============================] - 14s - loss: 0.0430 - acc: 0.9883 Epoch 14/20 600/600 [==============================] - 14s - loss: 0.0705 - acc: 0.9833 Epoch 15/20 600/600 [==============================] - 14s - loss: 0.0324 - acc: 0.9917 Epoch 16/20 600/600 [==============================] - 14s - loss: 0.0360 - acc: 0.9867 Epoch 17/20 600/600 [==============================] - 14s - loss: 0.0394 - acc: 0.9883 Epoch 18/20 600/600 [==============================] - 14s - loss: 0.0732 - acc: 0.9733 Epoch 19/20 600/600 [==============================] - 13s - loss: 0.0591 - acc: 0.9767 Epoch 20/20 600/600 [==============================] - 13s - loss: 0.0747 - acc: 0.9700 Note that if you run fit() again, the model will continue to train with the parameters it has already learnt instead of reinitializing them. Exercise: Implement step 4, i.e. test/evaluate the model. 123456### START CODE HERE ### (1 line)preds = happyModel.evaluate(x = X_test, y = Y_test);### END CODE HERE ###print()print ("Loss = " + str(preds[0]))print ("Test Accuracy = " + str(preds[1])) 150/150 [==============================] - 1s Loss = 0.49950652957 Test Accuracy = 0.800000001589 If your happyModel() function worked, you should have observed much better than random-guessing (50%) accuracy on the train and test sets. To give you a point of comparison, our model gets around 95% test accuracy in 40 epochs (and 99% train accuracy) with a mini batch size of 16 and “adam” optimizer. But our model gets decent accuracy after just 2-5 epochs, so if you’re comparing different models you can also train a variety of models on just a few epochs and see how they compare. If you have not yet achieved a very good accuracy (let’s say more than 80%), here’re some things you can play around with to try to achieve it: Try using blocks of CONV-&gt;BATCHNORM-&gt;RELU such as:123X = Conv2D(32, (3, 3), strides = (1, 1), name = 'conv0')(X)X = BatchNormalization(axis = 3, name = 'bn0')(X)X = Activation('relu')(X) until your height and width dimensions are quite low and your number of channels quite large (≈32 for example). You are encoding useful information in a volume with a lot of channels. You can then flatten the volume and use a fully-connected layer. You can use MAXPOOL after such blocks. It will help you lower the dimension in height and width. Change your optimizer. We find Adam works well. If the model is struggling to run and you get memory issues, lower your batch_size (12 is usually a good compromise) Run on more epochs, until you see the train accuracy plateauing. Even if you have achieved a good accuracy, please feel free to keep playing with your model to try to get even better results. Note: If you perform hyperparameter tuning on your model, the test set actually becomes a dev set, and your model might end up overfitting to the test (dev) set. But just for the purpose of this assignment, we won’t worry about that here. 3 - ConclusionCongratulations, you have solved the Happy House challenge! Now, you just need to link this model to the front-door camera of your house. We unfortunately won’t go into the details of how to do that here. What we would like you to remember from this assignment: Keras is a tool we recommend for rapid prototyping. It allows you to quickly try out different model architectures. Are there any applications of deep learning to your daily life that you’d like to implement using Keras? Remember how to code a model in Keras and the four steps leading to the evaluation of your model on the test set. Create-&gt;Compile-&gt;Fit/Train-&gt;Evaluate/Test. 4 - Test with your own image (Optional)Congratulations on finishing this assignment. You can now take a picture of your face and see if you could enter the Happy House. To do that: 1. Click on &quot;File&quot; in the upper bar of this notebook, then click &quot;Open&quot; to go on your Coursera Hub. 2. Add your image to this Jupyter Notebook&apos;s directory, in the &quot;images&quot; folder 3. Write your image&apos;s name in the following code 4. Run the code and check if the algorithm is right (0 is unhappy, 1 is happy)! The training/test sets were quite similar; for example, all the pictures were taken against the same background (since a front door camera is always mounted in the same position). This makes the problem easier, but a model trained on this data may or may not work on your own data. But feel free to give it a try! 1234567891011### START CODE HERE ###img_path = 'images/my_image.jpg'### END CODE HERE ###img = image.load_img(img_path, target_size=(64, 64))imshow(img)x = image.img_to_array(img)x = np.expand_dims(x, axis=0)x = preprocess_input(x)print(happyModel.predict(x)) [[ 2.04726325e-36]] 5 - Other useful functions in Keras (Optional)Two other basic features of Keras that you’ll find useful are: model.summary(): prints the details of your layers in a table with the sizes of its inputs/outputs plot_model(): plots your graph in a nice layout. You can even save it as “.png” using SVG() if you’d like to share it on social media ;). It is saved in “File” then “Open…” in the upper bar of the notebook. Run the following code. 1happyModel.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 64, 64, 3) 0 _________________________________________________________________ zero_padding2d_1 (ZeroPaddin (None, 70, 70, 3) 0 _________________________________________________________________ conv0 (Conv2D) (None, 64, 64, 32) 4736 _________________________________________________________________ bn0 (BatchNormalization) (None, 64, 64, 32) 128 _________________________________________________________________ activation_1 (Activation) (None, 64, 64, 32) 0 _________________________________________________________________ max_pool (MaxPooling2D) (None, 32, 32, 32) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 32768) 0 _________________________________________________________________ fc0 (Dense) (None, 1) 32769 ================================================================= Total params: 37,633 Trainable params: 37,569 Non-trainable params: 64 _________________________________________________________________ 12plot_model(happyModel, to_file='HappyModel.png')SVG(model_to_dot(happyModel).create(prog='dot', format='svg'))]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>convolutional-neural-networks</tag>
      </tags>
  </entry>
</search>
