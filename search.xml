<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Chapter23 Structured Streaming in Production]]></title>
    <url>%2F2019%2F08%2F10%2FChapter23_StructuredStreamingInProduction%2F</url>
    <content type="text"><![CDATA[The previous chapters of this part of the book have covered Structured Streaming from a user’s perspective. Naturally this is the core of your application. This chapter covers some of the operational tools needed to run Structured Streaming robustly in production after you’ve developed an application. 本书这一部分的前几章从用户的角度介绍了结构化流。当然，这是应用程序的核心。本章介绍在开发应用程序之后，在生产环境中可靠地运行结构化流所需的一些操作工具。 Structured Streaming was marked as production-ready in Apache Spark 2.2.0, meaning that this release has all the features required for production use and stabilizes the API. Many organizations are already using the system in production because, frankly, it’s not much different from running other production Spark applications. Indeed, through features such as transactional sources/sinks and exactly-once processing, the Structured Streaming designers sought to make it as easy to operate as possible. This chapter will walk you through some of the key operational tasks specific to Structured Streaming. This should supplement everything we saw and learned about Spark operations in Part II. 结构化流在 Apache Spark 2.2.0中被标记为生产就绪，这意味着该版本具有生产使用所需的所有功能，并稳定了API。许多组织已经在生产中使用该系统，因为坦率地说，它与运行其他生产Spark应用程序没有太大区别。事实上，通过事务性源/接收器和一次性处理等功能，结构化流设计人员力求使其尽可能易于操作。本章将引导您完成特定于结构化流的一些关键操作任务。这应该补充我们在第二部分中看到和学到的关于 Spark 操作的所有知识。 Fault Tolerance and Checkpointing 容错和检查点The most important operational concern for a streaming application is failure recovery. Faults are inevitable: you’re going to lose a machine in the cluster, a schema will change by accident without a proper migration, or you may even intentionally restart the cluster or application. In any of these cases, Structured Streaming allows you to recover an application by just restarting it. To do this, you must configure the application to use checkpointing and write-ahead logs, both of which are handled automatically by the engine. Specifically, you must configure a query to write to a checkpoint location on a reliable file system (e.g., HDFS, S3, or any compatible filesystem). Structured Streaming will then periodically save all relevant progress information (for instance, the range of offsets processed in a given trigger) as well as the current intermediate state values to the checkpoint location. In a failure scenario, you simply need to restart your application, making sure to point to the same checkpoint location, and it will automatically recover its state and start processing data where it left off. You do not have to manually manage this state on behalf of the application—Structured Streaming does it for you. 流应用程序最重要的操作问题是故障恢复。错误是不可避免的：您将失去集群中的一台机器，一个模式将在没有适当迁移的情况下意外更改，或者您甚至可能有意重新启动集群或应用程序。在这些情况下，结构化流允许您通过重新启动应用程序来恢复应用程序。为此，必须将应用程序配置为使用检查点和提前写入日志，这两个日志都由引擎自动处理。具体来说，您必须配置一个查询，以写入可靠文件系统（例如，HDFS、S3或任何兼容的文件系统）上的检查点位置。结构化流将定期将所有相关的进度信息（例如，在给定触发器中处理的偏移范围）以及当前中间状态值保存到检查点位置。在失败的情况下，只需重新启动应用程序，确保指向相同的检查点位置，它将自动恢复其状态，并在停止的地方开始处理数据。您不必代表应用程序手动管理此状态，结构化流为您做到了这一点。 To use checkpointing, specify your checkpoint location before starting your application through the checkpointLocation option on writeStream. You can do this as follows: 要使用检查点，请在通过 writeStream 上的检查点位置选项启动应用程序之前指定检查点位置。您可以这样做： 123456789101112131415// in Scalaval static = spark.read.json("/data/activity-data")val streaming = spark.readStream.schema(static.schema).option("maxFilesPerTrigger", 10).json("/data/activity-data").groupBy("gt").count()val query = streaming.writeStream.outputMode("complete").option("checkpointLocation", "/some/location/").queryName("test_stream").format("memory").start() 12345678910111213141516# in Pythonstatic = spark.read.json("/data/activity-data")streaming = spark\.readStream\.schema(static.schema)\.option("maxFilesPerTrigger", 10)\.json("/data/activity-data")\.groupBy("gt")\.count()query = streaming\.writeStream\.outputMode("complete")\.option("checkpointLocation", "/some/python/location/")\.queryName("test_python_stream")\.format("memory")\.start() If you lose your checkpoint directory or the information inside of it, your application will not be able to recover from failures and you will have to restart your stream from scratch. 如果丢失了检查点目录或其中的信息，应用程序将无法从失败中恢复，您必须从头开始重新启动流。 Updating Your Application 更新应用程序 Checkpointing is probably the most important thing to enable in order to run your applications in production. This is because the checkpoint will store all of the information about what your stream has processed thus far and what the intermediate state it may be storing is. However, checkpointing does come with a small catch—you’re going to have to reason about your old checkpoint data when you update your streaming application. When you update your application, you’re going to have to ensure that your update is not a breaking change. Let’s cover these in detail when we review the two types of updates: either an update to your application code or running a new Spark version. 为了在生产环境中运行应用程序，检查点可能是最重要的。这是因为检查点将存储到目前为止您的流处理的内容以及它可能存储的中间状态的所有信息。但是，检查点的出现只是一个小问题，当您更新流应用程序时，您必须考虑旧的检查点数据。当你更新你的应用程序时，你必须确保你的更新不是一个突破性的改变。当我们回顾这两种类型的更新时，让我们详细介绍一下这些：应用程序代码的更新或者运行一个新的 Spark 版本。 Updating Your Streaming Application Code 更新流应用程序代码Structured Streaming is designed to allow certain types of changes to the application code between application restarts. Most importantly, you are allowed to change user-defined functions (UDFs) as long as they have the same type signature. This feature can be very useful for bug fixes. For example, imagine that your application starts receiving a new type of data, and one of the data parsing functions in your current logic crashes. With Structured Streaming, you can recompile the application with a new version of that function and pick up at the same point in the stream where it crashed earlier. While small adjustments like adding a new column or changing a UDF are not breaking changes and do not require a new checkpoint directory, there are larger changes that do require an entirely new checkpoint directory. For example, if you update your streaming application to add a new aggregation key or fundamentally change the query itself, Spark cannot construct the required state for the new query from an old checkpoint directory. In these cases, Structured Streaming will throw an exception saying it cannot begin from a checkpoint directory, and you must start from scratch with a new (empty) directory as your checkpoint location. 结构化流的设计允许在应用程序重新启动之间对应用程序代码进行某些类型的更改。最重要的是，您可以更改用户定义函数（UDF），只要它们具有相同的类型签名。这个特性对于错误修复非常有用。例如，假设应用程序开始接收新类型的数据，并且当前逻辑崩溃时的一个数据解析函数。使用结构化流，您可以使用该函数的新版本重新编译应用程序，并在流中之前崩溃的同一点上继续进行。虽然诸如添加新列或更改UDF之类的小调整不是突破性的改变，也不需要新的检查点目录，但仍有较大的更改需要全新的检查点目录。例如，如果更新流应用程序以添加新的聚合键或从根本上更改查询本身，Spark将无法从旧的检查点目录构造新查询所需的状态。在这些情况下，结构化流将抛出一个异常，说明它不能从检查点目录开始，并且必须从头开始，使用一个新的（空）目录作为检查点位置。 Updating Your Spark Version 更新Spark版本Structured Streaming applications should be able to restart from an old checkpoint directory across patch version updates to Spark (e.g., moving from Spark 2.2.0 to 2.2.1 to 2.2.2). The checkpoint format is designed to be forward-compatible, so the only way it may be broken is due to critical bug fixes. If a Spark release cannot recover from old checkpoints, this will be clearly documented in its release notes. The Structured Streaming developers also aim to keep the format compatible across minor version updates (e.g., Spark 2.2.x to 2.3.x), but you should check the release notes to see whether this is supported for each upgrade. In either case, if you cannot start from a checkpoint, you will need to start your application again using a new checkpoint directory. 结构化流应用程序应该能够从旧的检查点目录跨补丁版本更新重新启动到 Spark（例如，从 Spark 2.2.0迁移到2.2.1到2.2.2）。检查点格式设计为向前兼容，因此唯一可能被破坏的方法是修复关键的错误。如果Spark发行版不能从旧的检查点恢复，那么它的发行说明中会清楚地记录这一点。结构化流式开发人员还致力于保持格式在次要版本更新（例如spark 2.2.x到2.3.x）之间的兼容性，但是您应该检查发行说明，以查看是否支持每次升级。在这两种情况下，如果无法从检查点启动，则需要使用新的检查点目录重新启动应用程序。 Sizing and Rescaling Your Application 调整应用程序的大小和重新缩放In general, the size of your cluster should be able to comfortably handle bursts above your data rate. The key metrics you should be monitoring in your application and cluster are discussed as follows. In general, if you see that your input rate is much higher than your processing rate (elaborated upon momentarily), it’s time to scale up your cluster or application. Depending on your resource manager and deployment, you may just be able to dynamically add executors to your application. When it comes time, you can scale-down your application in the same way—remove executors (potentially through your cloud provider) or restart your application with lower resource counts. These changes will likely incur some processing delay (as data is recomputed or partitions are shuffled around when executors are removed). In the end, it’s a business decision as to whether it’s worthwhile to create a system with more sophisticated resource management capabilities. 一般来说，集群的大小应该能够轻松地处理高于数据速率的突发事件。您应该在应用程序和集群中监控的关键指标讨论如下。一般来说，如果您看到您的输入速率远远高于您的处理速率（马上详细描述），那么是时候扩展集群或应用程序了。根据您的资源管理器和部署，您可能只能动态地向应用程序添加执行器。当遇到这种情况时，您可以用同样的方法缩小应用程序的规模，删除执行者（可能通过云提供商）或以较低的资源计数重新启动应用程序。这些更改可能会导致一些处理延迟（当执行器被删除时，数据会重新计算或分区会四处移动）。最后，对于是否值得创建一个具有更复杂资源管理功能的系统，这是一个业务决策。 While making underlying infrastructure changes to the cluster or application are sometimes necessary, other times a change may only require a restart of the application or stream with a new configuration. For instance, changing spark.sql.shuffle.partitions is not supported while a stream is currently running (it won’t actually change the number of shuffle partitions). This requires restarting the actual stream, not necessarily the entire application. Heavier weight changes, like changing arbitrary Spark application configurations, will likely require an application restart. 虽然有时需要对集群或应用程序进行基础结构更改，但在其他情况下，更改可能只需要用新配置重新启动应用程序或流。例如，当流当前正在运行时，不支持更改 spark.sql.shuffle.partitions（它实际上不会更改shuffle分区的数目）。这需要重新启动实际流，而不一定是整个应用程序。更重的重量变化，如改变任意的 Spark 应用程序配置，可能需要重新启动应用程序。 Metrics and Monitoring 量化指标和监控Metrics and monitoring in streaming applications is largely the same as for general Spark applications using the tools described in Chapter 18. However, Structured Streaming does add several more specifics in order to help you better understand the state of your application. There are two key APIs you can leverage to query the status of a streaming query and see its recent execution progress. With these two APIs, you can get a sense of whether or not your stream is behaving as expected. 流应用程序中的量化指标和监视与使用第18章中描述的工具的一般 Spark 应用程序基本相同。但是，结构化流确实添加了更多的细节，以帮助您更好地了解应用程序的状态。您可以利用两个关键API来查询流式查询的状态并查看其最近的执行进度。通过这两个API，您可以了解流是否按预期运行。 Query StatusThe query status is the most basic monitoring API, so it’s a good starting point. It aims to answer the question, “What processing is my stream performing right now?” This information is reported in the status field of the query object returned by startStream. For example, you might have a simple counts stream that provides counts of IOT devices defined by the following query (here we’re just using the same query from the previous chapter without the initialization code) : 查询状态是最基本的监控API，所以它是一个很好的起点。它的目的是回答这个问题，“我的流现在正在执行什么处理？”“此信息在 startStream 返回的查询对象的 status 字段中报告。例如，您可能有一个简单的计数流，它提供由以下查询定义的物联网设备计数（这里我们只使用上一章中的相同查询，而不使用初始化代码）： 1query.status To get the status of a given query, simply running the command query.status will return the current status of the stream. This gives us details about what is happening at that point in time in the stream. Here’s a sample of what you’ll get back when querying this status: 要获取给定查询的状态，只需运行命令 query.status 即可返回流的当前状态。这为我们提供了有关流中那个时间点发生的事情的详细信息。以下是查询此状态时将返回的示例： 12345&#123; &quot;message&quot; : &quot;Getting offsets from ...&quot;, &quot;isDataAvailable&quot; : true, &quot;isTriggerActive&quot; : true&#125; The above snippet describes getting the offsets from a Structured Streaming data source (hence the message describing getting offsets). There are a variety of messages to describe the stream’s status. 上面的代码段描述了从结构化流数据源获取偏移量（因此描述获取偏移量的消息）。有各种各样的消息来描述流的状态。 NOTE We have shown the status command inline here the way you would call it in a Spark shell. However, for a standalone application, you may not have a shell attached to run arbitrary code inside your process. In that case, you can expose its status by implementing a monitoring server, such as a small HTTP server that listens on a port and returns query.status when it gets a request. Alternatively, you can use the richer StreamingQueryListener API described later to listen to more events. 我们已经在这里显示了 status 命令，您可以在 Spark shell中调用它。但是，对于独立的应用程序，可能没有附加 shell 到进程内运行任意代码。在这种情况下，您可以通过实现监控服务器来公开其状态，例如在端口上侦听并在收到请求时返回 query.status 的小型 HTTP 服务器。或者，您可以使用后面描述的更丰富的 streamingQueryListener API来监听更多的事件。 Recent ProgressWhile the query’s current status is useful to see, equally important is an ability to view the query’s progress. The progress API allows us to answer questions like “At what rate am I processing tuples?” or “How fast are tuples arriving from the source?” By running query.recentProgress, you’ll get access to more time-based information like the processing rate and batch durations. The streaming query progress also includes information about the input sources and output sinks behind your stream. 虽然查询的当前状态很有用，但查看查询进度的能力同样重要。progress API 允许我们回答“我以什么速率处理元组（tuples）？”或者“元组（tuples）从源文件到达的速度有多快？”“通过运行 query.recentProgress，您可以访问更多基于时间的信息，如处理速率和批处理持续时间。流查询进度还包括有关流后面的输入源和输出接收器的信息。 1query.recentProgress Here’s the result of the Scala version after we ran the code from before; the Python one will be similar: 下面是 Scala 版本在运行之前的代码之后的结果；Python 版本将类似： 1234567891011121314151617181920212223242526272829303132Array(&#123; &quot;id&quot; : &quot;d9b5eac5-2b27-4655-8dd3-4be626b1b59b&quot;, &quot;runId&quot; : &quot;f8da8bc7-5d0a-4554-880d-d21fe43b983d&quot;, &quot;name&quot; : &quot;test_stream&quot;, &quot;timestamp&quot; : &quot;2017-08-06T21:11:21.141Z&quot;, &quot;numInputRows&quot; : 780119, &quot;processedRowsPerSecond&quot; : 19779.89350912779, &quot;durationMs&quot; : &#123; &quot;addBatch&quot; : 38179, &quot;getBatch&quot; : 235, &quot;getOffset&quot; : 518, &quot;queryPlanning&quot; : 138, &quot;triggerExecution&quot; : 39440, &quot;walCommit&quot; : 312 &#125;, &quot;stateOperators&quot; : [ &#123; &quot;numRowsTotal&quot; : 7, &quot;numRowsUpdated&quot; : 7 &#125; ], &quot;sources&quot; : [ &#123; &quot;description&quot; : &quot;FileStreamSource[/some/stream/source/]&quot;, &quot;startOffset&quot; : null, &quot;endOffset&quot; : &#123; &quot;logOffset&quot; : 0 &#125;, &quot;numInputRows&quot; : 780119, &quot;processedRowsPerSecond&quot; : 19779.89350912779 &#125; ], &quot;sink&quot; : &#123; &quot;description&quot; : &quot;MemorySink&quot; &#125;&#125;) As you can see from the output just shown, this includes a number of details about the state of the stream. It is important to note that this is a snapshot in time (according to when we asked for the query progress). In order to consistently get output about the state of the stream, you’ll need to query this API for the updated state repeatedly. The majority of the fields in the previous output should be selfexplanatory. However, let’s review some of the more consequential fields in detail. 正如您从刚刚显示的输出中看到的那样，这包括一些关于流状态的详细信息。需要注意的是，这是一个及时的快照（根据我们何时请求查询进度）。为了一致地获得有关流状态的输出，您需要反复查询此API以获取更新状态。上一个输出中的大多数字段都应该是一目了然的。但是，让我们详细回顾一些更重要的字段。 Input rate and processing rate 输入速率和处理速率The input rate specifies how much data is flowing into Structured Streaming from our input source. The processing rate is how quickly the application is able to analyze that data. In the ideal case, the input and processing rates should vary together. Another case might be when the input rate is much greater than the processing rate. When this happens, the stream is falling behind and you will need to scale the cluster up to handle the larger load. 输入速率指定从输入源流入结构化流的数据量。处理速度是应用程序分析数据的速度。在理想情况下，输入和处理速率应该同时变化。另一种情况可能是输入速率远远大于处理速率。当这种情况发生时，流将落在后面，您需要向上扩展集群以处理更大的负载。 Batch duration 批处理持续时间Nearly all streaming systems utilize batching to operate at any reasonable throughput (some have an option of high latency in exchange for lower throughput). Structured Streaming achieves both. As it operates on the data, you will likely see batch duration oscillate as Structured Streaming processes varying numbers of events over time. Naturally, this metric will have little to no relevance when the continuous processing engine is made an execution option. 几乎所有的流系统都利用批处理以任何合理的吞吐量运行（有些系统可以选择高延迟，以换取较低的吞吐量）。结构化流实现了这两个目标。当它对数据进行操作时，您可能会看到批处理持续时间随着结构化流处理时间的变化而波动。当然，当连续处理引擎成为一个执行选项时，这个量化指标几乎没有相关性。 TIP 提示 Generally it’s a best practice to visualize the changes in batch duration and input and processing rates. It’s much more helpful than simply reporting changes over time. 一般来说，将批处理持续时间、输入和处理速率的变化可视化是最佳实践。它比简单地报告随时间变化更有用。 Spark UI Spark用户界面The Spark web UI, covered in detail in Chapter 18, also shows tasks, jobs, and data processing metrics for Structured Streaming applications. On the Spark UI, each streaming application will appear as a sequence of short jobs, one for each trigger. However, you can use the same UI to see metrics, query plans, task durations, and logs from your application. One departure of note from the DStream API is that the Streaming Tab is not used by Structured Streaming. 第18章详细介绍了Spark Web 用户界面，它还显示了结构化流应用程序的任务、作业和数据处理指标。在Spark用户界面上，每个流式应用程序将显示为一系列短作业，每个触发器一个。但是，您可以使用同一个UI查看来自应用程序的量化指标、查询计划、任务工期和日志。与 DStream API 不同的一点是，结构化流不使用流选项卡。 Alerting 警告Understanding and looking at the metrics for your Structured Streaming queries is an important first step. However, this involves constantly watching a dashboard or the metrics in order to discover potential issues. You’re going to need robust automatic alerting to notify you when your jobs are failing or not keeping up with the input data rate without monitoring them manually. There are several ways to integrate existing alerting tools with Spark, generally building on the recent progress API we covered before. For example, you may directly feed the metrics to a monitoring system such as the open source Coda Hale Metrics library or Prometheus, or you may simply log them and use a log aggregation system like Splunk. In addition to monitoring and alerting on queries, you’re also going to want to monitor and alert on the state of the cluster and the overall application (if you’re running multiple queries together). 了解和查看结构化流式查询的指标是重要的第一步。但是，这需要不断观察仪表盘或指标，以发现潜在的问题。当你的工作失败或者没有手动监控就不能跟上输入数据速率时，你需要强大的自动警报来通知你。有几种方法可以将现有的警报工具与Spark集成在一起，通常基于我们之前介绍的新近发展的API。例如，您可以直接将量化指标输入监控系统，如开源 Coda Hale Metrics 库或 Prometheus ，也可以简单地将其记录并使用日志聚合系统，如Splunk。除了对查询进行监视和警报之外，您还需要对集群和整个应用程序的状态进行监视和发出警报（如果您一起运行多个查询）。 Advanced Monitoring with the Streaming Listener 使用流式侦听器进行高级监视We already touched on some of the high-level monitoring tools in Structured Streaming. With a bit of glue logic, you can use the status and queryProgress APIs to output monitoring events into your organization’s monitoring platform of choice (e.g., a log aggregation system or Prometheus dashboard). Beyond these approaches, there is also a lower-level but more powerful way to observe an application’s execution: the StreamingQueryListener class. 我们已经讨论了结构化流中的一些高级监控工具。使用一些粘合逻辑，您可以使用状态和 queryProgress API将监控事件输出到组织的监控平台（例如，日志聚合系统或 Prometheus 仪表板）。除了这些方法之外，还有一种更低阶但更强大的方法来观察应用程序的执行：StreamingQueryListener 类。 The StreamingQueryListener class will allow you to receive asynchronous updates from the streaming query in order to automatically output this information to other systems and implement robust monitoring and alerting mechanisms. You start by developing your own object to extend StreamingQueryListener, then attach it to a running SparkSession. Once you attach your custom listener with sparkSession.streams.addListener(), your class will receive notifications when a query is started or stopped, or progress is made on an active query. Here’s a simple example of a listener from the Structured Streaming documentation: StreamingQueryListener 类将允许您从流查询接收异步更新，以便自动将此信息输出到其他系统，并实现可靠的监视和警报机制。首先开发自己的对象来扩展 StreamingQueryListener，然后将其附加到正在运行的SparkSession。使用 sparkSession.streams.addListener（）附加自定义侦听器后，当查询启动或停止，或在活动查询上取得进展时，类将收到通知。以下是结构化流文档中侦听器的简单示例： 123456789101112131415val spark: SparkSession = ... spark.streams.addListener(new StreamingQueryListener() &#123; override def onQueryStarted(queryStarted: QueryStartedEvent): Unit = &#123; println("Query started: " + queryStarted.id) &#125; override def onQueryTerminated(queryTerminated: QueryTerminatedEvent): Unit = &#123; println("Query terminated: " + queryTerminated.id) &#125; override def onQueryProgress(queryProgress: QueryProgressEvent): Unit = &#123; println("Query made progress: " + queryProgress.progress) &#125;&#125;) Streaming listeners allow you to process each progress update or status change using custom code and pass it to external systems. For example, the following code for a StreamingQueryListener that will forward all query progress information to Kafka. You’ll have to parse this JSON string once you read data from Kafka in order to access the actual metrics: 流式侦听器（streaming listeners）允许您使用自定义代码处理每个进度更新或状态更改，并将其传递给外部系统。例如，下面的代码用于将所有查询进度信息转发到 Kafka 的 StreamingQueryListener。从Kafka读取数据后，必须解析这个JSON字符串，才能访问实际的量化指标： 12345678910111213141516171819class KafkaMetrics(servers: String) extends StreamingQueryListener &#123; val kafkaProperties = new Properties() kafkaProperties.put("bootstrap.servers", servers) kafkaProperties.put( "key.serializer", "kafkashaded.org.apache.kafka.common.serialization.StringSerializer") kafkaProperties.put( "value.serializer", "kafkashaded.org.apache.kafka.common.serialization.StringSerializer") val producer = new KafkaProducer[String, String](kafkaProperties) import org.apache.spark.sql.streaming.StreamingQueryListener import org.apache.kafka.clients.producer.KafkaProduceroverride def onQueryProgress(event : StreamingQueryListener.QueryProgressEvent): Unit = &#123; producer.send(new ProducerRecord("streaming-metrics", event.progress.json)) &#125; override def onQueryStarted(event: StreamingQueryListener.QueryStartedEvent) : Unit = &#123;&#125; override def onQueryTerminated(event: StreamingQueryListener.QueryTerminatedEvent) : Unit = &#123;&#125;&#125; Using the StreamingQueryListener interface, you can even monitor Structured Streaming applications on one cluster by running a Structured Streaming application on that same (or another) cluster. You could also manage multiple streams in this way. 使用streamingquerylistener接口，您甚至可以通过在同一个（或另一个）集群上运行结构化流应用程序来监视一个集群上的结构化流应用程序。您还可以用这种方式管理多个流。 Conclusion 结论In this chapter, we covered the main tools needed to run Structured Streaming in production: checkpoints for fault tolerance and various monitoring APIs that let you observe how your application is running. Lucky for you, if you’re running Spark in production already, many of the concepts and tools are similar, so you should be able to reuse a lot of your existing knowledge. Be sure to check Part IV to see some other helpful tools for monitoring Spark Applications. 在本章中，我们介绍了在生产环境中运行结构化流所需的主要工具：容错检查点和各种监视API，这些API允许您观察应用程序的运行情况。幸运的是，如果您已经在生产中运行了Spark，那么许多概念和工具都是类似的，因此您应该能够重用大量现有的知识。一定要检查第四部分，看看其他一些有助于监测 Spark 应用的工具。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter18_Monitoring-and-Debugging]]></title>
    <url>%2F2019%2F08%2F10%2FChapter18_Monitoring-and-Debugging(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 18 Monitoring and DebuggingThis chapter covers the key details you need to monitor and debug your Spark Applications. To do this, we will walk through the Spark UI with an example query designed to help you understand how to trace your own jobs through the execution life cycle. The example we’ll look at will also help you understand how to debug your jobs and where errors are likely to occur. 本章介绍了监视和调试Spark应用程序所需的关键详细信息。为此，我们将使用一个示例查询来浏览Spark UI，该查询旨在帮助您了解如何在执行生命周期中跟踪自己的作业。我们将看到的示例还将帮助您了解如何调试作业以及可能发生错误的位置。 The Monitoring Landscape 监控的宏观图At some point, you’ll need to monitor your Spark jobs to understand where issues are occuring in them. It’s worth reviewing the different things that we can actually monitor and outlining some of the options for doing so. Let’s review the components we can monitor (see Figure 18-1). 在某些时候，您需要监视您的Spark作业，以了解其中发生问题的位置。值得回顾一下我们可以实际监控的不同内容，并概述了这样做的一些选项。让我们回顾一下我们可以监控的组件（参见图18-1，在下文）。 Spark Applications and Jobs Spark应用程序和作业 The first thing you’ll want to begin monitoring when either debugging or just understanding better how your application executes against the cluster is the Spark UI and the Spark logs. These report information about the applications currently running at the level of concepts in Spark, such as RDDs and query plans. We talk in detail about how to use these Spark monitoring tools throughout this chapter. 在调试或只是更好地了解在集群背景下应用程序执行的方式时，您首先想要开始监视的是Spark UI和Spark日志。这些报告有关当前在Spark中概念级别运行的应用程序的信息，例如RDD和查询计划。我们将在本章中详细讨论如何使用这些Spark监视工具。JVMSpark runs the executors in individual Java Virtual Machines (JVMs). Therefore, the next level of detail would be to monitor the individual virtual machines (VMs) to better understand how your code is running. JVM utilities such as jstack for providing stack traces, jmap for creating heapdumps, jstat for reporting time–series statistics, and jconsole for visually exploring various JVM properties are useful for those comfortable with JVM internals. You can also use a tool like jvisualvm to help profile Spark jobs. Some of this information is provided in the Spark UI, but for very low-level debugging, the aforementioned tools can come in handy.Spark 在各个 Java 虚拟机（JVM）中运行执行程序（executor）。因此，下一级详细信息将是监视单个虚拟机（VM）以更好地了解代码的运行方式。 JVM实用程序（如用于提供堆栈跟踪的 jstack ，用于创建 heapdumps 的 jmap，用于报告时间序列统计信息的 jstat）以及用于可视化地探索各种 JVM 属性的 jconsole， 这些对于那些熟悉 JVM 内部的人来说非常有用。您还可以使用 jvisualvm 之类的工具来帮助分析 Spark 作业。其中一些信息在 Spark UI 中提供，但对于非常低级的调试，上述工具可以派上用场。OS/MachineThe JVMs run on a host operating system (OS) and it’s important to monitor the state of those machines ensure that they are healthy. This includes monitoring things like CPU, network, and I/O. These are often reported in cluster-level monitoring solutions; however, there are more specific tools that you can use, including dstat, iostat, and iotop.JVM 在主机操作系统（OS）上运行，监控这些机器的状态以确保它们是健康的非常重要。这包括监视 CPU，网络和 I/O 等内容。这些通常在集群级监控解决方案中报告；但是，您可以使用更多特定工具，包括 dstat，iostat 和iotop。ClusterNaturally, you can monitor the cluster on which your Spark Application(s) will run. This might be a YARN, Mesos, or standalone cluster. Usually it’s important to have some sort of monitoring solution here because, somewhat obviously, if your cluster is not working, you should probably know pretty quickly. Some popular cluster-level monitoring tools include Ganglia and Prometheus.当然，您可以监视运行 Spark 应用程序的集群。这可能是 YARN，Mesos 或独立集群。通常，在这里使用某种监控解决方案很重要，因为很明显，如果您的集群不工作，您应该很快就会知道。一些流行的集群级监控工具包括Ganglia 和 Prometheus。## What to Monitor 要监控什么After that brief tour of the monitoring landscape, let’s discuss how we can go about monitoring and debugging our Spark Applications. There are two main things you will want to monitor: the processes running your application (at the level of CPU usage, memory usage, etc.), and the query execution inside it (e.g., jobs and tasks).在简要介绍了监控环境之后，让我们讨论如何监控和调试我们的 Spark 应用程序。您需要监视两个主要内容：运行应用程序的进程（在 CPU 使用情况，内存使用情况的等级）以及在其中执行的查询（例如，作业和任务）。### Driver and Executor Processes 驱动程序和执行程序进程When you’re monitoring a Spark application, you’re definitely going to want to keep an eye on the driver. This is where all of the state of your application lives, and you’ll need to be sure it’s running in a stable manner. If you could monitor only one machine or a single JVM, it would definitely be the driver. With that being said, understanding the state of the executors is also extremely important for monitoring individual Spark jobs. To help with this challenge, Spark has a configurable metrics system based on the Dropwizard Metrics Library. The metrics system is configured via a configuration file that Spark expects to be present at $SPARK_HOME/conf/metrics.properties. A custom file location can be specified by changing the spark.metrics.conf configuration property. These metrics can be output to a variety of different sinks, including cluster monitoring solutions like Ganglia.当您监控Spark应用程序时，您肯定会想要关注驱动程序（driver）。这是您的应用程序的所有状态所在的位置，您需要确保它以稳定的方式运行。如果您只能监控一台机器或一台 JVM，那肯定是驱动程序（driver）。话虽如此，了解执行程序（executor）的状态对于监视各个 Spark 作业也非常重要。为了应对这一挑战，Spark 拥有一个基于 Dropwizard Metrics 库的可配置衡量系统。衡量系统通过 Spark 预期出现在 $SPARK_HOME/conf/metrics.properties 中的配置文件进行配置。可以通过更改 spark.metrics.conf 配置属性来指定自定义文件位置。这些指标可以输出到各种不同的接收器，包括像 Ganglia 这样的集群监控解决方案。### Queries, Jobs, Stages, and Tasks 查询，作业，阶段和任务 Although the driver and executor processes are important to monitor, sometimes you need to debug what’s going on at the level of a specific query. Spark provides the ability to dive into queries, jobs, stages, and tasks. (We learned about these in Chapter 15.) This information allows you to know exactly what’s running on the cluster at a given time. When looking for performance tuning or debugging, this is where you are most likely to start. Now that we know what we want to monitor, let’s look at the two most common ways of doing so: the Spark logs and the Spark UI. 虽然驱动程序（driver）和执行程序（executor）进程对于监视很重要，但有时您需要调试特定查询级别的进程。 Spark 提供了深入查询，工作，阶段和任务的能力。 （我们在第15章中了解了这些内容。）此信息可让您准确了解在给定时间情况下集群上正在运行的内容。在寻找性能调优或调试时，这是您最有可能开始的地方。现在我们知道了我们想要监控的内容，让我们看看这两种最常见的方式：Spark 日志和 Spark UI 。 Spark Logs Spark日志One of the most detailed ways to monitor Spark is through its log files. Naturally, strange events in Spark’s logs, or in the logging that you added to your Spark Application, can help you take note of exactly where jobs are failing or what is causing that failure. If you use the application template provided with the book, the logging framework we set up in the template will allow your application logs to show up along Spark’s own logs, making them very easy to correlate. One challenge, however, is that Python won’t be able to integrate directly with Spark’s Java-based logging library. Using Python’s logging module or even simple print statements will still print the results to standard error, however, and make them easy to find. 监视 Spark 的最详细方法之一是通过其日志文件。当然，Spark 的日志中或您添加到 Spark 应用程序的日志记录中的奇怪事件可以帮助您准确记录作业失败的原因或导致失败的原因。如果您使用本书提供的应用程序模板，我们在模板中设置的日志记录框架将允许您的应用程序日志显示在 Spark 自己的日志中，使它们非常容易关联。然而，一个挑战是 Python 无法直接与 Spark 的基于 Java 的日志库集成。但是，使用 Python 的日志记录模块甚至简单的打印语句仍然会将结果打印到标准错误，并使它们易于查找。To change Spark’s log level, simply run the following command :要更改 Spark 的日志级别，只需运行以下命令 : spark.sparkContext.setLogLevel(“INFO”)This will allow you to read the logs, and if you use our application template, you can log your own relevant information along with these logs, allowing you to inspect both your own application and Spark. The logs themselves will be printed to standard error when running a local mode application, or saved to files by your cluster manager when running Spark on a cluster. Refer to each cluster manager’s documentation about how to find them—typically, they are available through the cluster manager’s web UI.这将允许您阅读日志，如果您使用我们的应用程序模板，您可以记录您自己的相关信息以及这些日志，允许您检查自己的应用程序和 Spark。运行本地模式应用程序时，日志本身将打印为标准错误，或者在集群上运行 Spark 时由集群管理器保存到文件。请参阅每个集群管理器的文档，了解如何查找它们——通常，它们可通过集群管理器的Web UI 获得。You won’t always find the answer you need simply by searching logs, but it can help you pinpoint the given problem that you’re encountering and possibly add new log statements in your application to better understand it. It’s also convenient to collect logs over time in order to reference them in the future. For instance, if your application crashes, you’ll want to debug why, without access to the now crashed application. You may also want to ship logs off the machine they were written on to hold onto them if a machine crashes or gets shut down (e.g., if running in the cloud).您不会总是通过搜索日志找到所需的答案，但它可以帮助您查明您遇到的给定问题，并可能在您的应用程序中添加新的日志语句以更好地理解它。随着时间的推移收集日志以便将来引用它们也很方便。例如，如果您的应用程序崩溃，您将需要调试原因，而无需访问现在崩溃的应用程序。如果计算机崩溃或关闭（例如，如果在云中运行），您可能还希望将日志从他们写入的计算机上发送到其上。## The Spark UI The Spark UI provides a visual way to monitor applications while they are running as well as metrics about your Spark workload, at the Spark and JVM level. Every SparkContext running launches a web UI, by default on port 4040, that displays useful information about the application. When you run Spark in local mode, for example, just navigate to http://localhost:4040 to see the UI when running a Spark Application on your local machine. If you’re running multiple applications, they will launch web UIs on increasing port numbers (4041, 4042, …). Cluster managers will also link to each application’s web UI from their own UI. Spark UI 提供了一种可视化方式，用于在运行时监视应用程序，以及 Spark 和 JVM 级别的 Spark 工作负载指标。每个运行的 SparkContext 都会在端口 4040 上默认启动 Web UI，该UI显示有关应用程序的有用信息。例如，在本地模式下运行 Spark 时，只需导航到 http://localhost:4040，即可在本地计算机上运行 Spark 应用程序时查看UI 。如果您正在运行多个应用程序，他们将在增加端口号（4041,4042，…）时启动Web UI。集群管理器还将从其自己的 UI 链接到每个应用程序的 Web UI。 Figure 18-2 shows all of the tabs available in the Spark UI. 图18-2 显示了 Spark UI 中可用的所有选项卡。 These tabs are accessible for each of the things that we’d like to monitor. For the most part, each of these should be self-explanatory : 这些选项卡可供我们要监控的每个事项访问。在大多数情况下，每一个都应该是不言自明的： The Jobs tab refers to Spark jobs. “作业”选项卡指的是Spark作业。 The Stages tab pertains to individual stages (and their relevant tasks). 阶段选项卡适用于各个阶段（及其相关任务）。 The Storage tab includes information and the data that is currently cached in our Spark Application. “存储”选项卡包含当前在我们的 Spark 应用程序中缓存的信息和数据。 The Environment tab contains relevant information about the configurations and current settings of the Spark application. “环境”选项卡包含有关 Spark 应用程序的配置和当前设置的相关信息。 The SQL tab refers to our Structured API queries (including SQL and DataFrames). SQL选项卡引用我们的结构化API查询（包括 SQL 和 DataFrames）。 The Executors tab provides detailed information about each executor running our application. Executors选项卡提供有关运行我们的应用程序的每个执行程序（executor）的详细信息。 Let’s walk through an example of how you can drill down into a given query. Open a new Spark shell, run the following code, and we will trace its execution through the Spark UI: 让我们来看一个如何深入查看给定查询的示例。打开一个新的 Spark shell，运行以下代码，我们将通过 Spark UI 跟踪它的执行： 123456789# in Pythonspark.read\.option("header", "true")\.csv("/data/retail-data/all/online-retail-dataset.csv")\.repartition(2)\.selectExpr("instr(Description, 'GLASS') &gt;= 1 as is_glass")\.groupBy("is_glass")\.count()\.collect() This results in three rows of various values. The code kicks off a SQL query, so let’s navigate to the SQL tab, where you should see something similar to Figure 18-3. 这导致不同值的三行。 代码开始启动 SQL 查询，所以让我们导航到 SQL 选项卡，在那里你应该看到类似于图 18-3 的内容。 The first thing you see is aggregate statistics about this query: 您看到的第一件事是关于此查询的汇总统计信息： Submitted Time: 2017/04/08 16:24:41Duration: 2 sSucceeded Jobs: 2These will become important in a minute, but first let’s take a look at the Directed Acyclic Graph (DAG) of Spark stages. Each blue box in these tabs represent a stage of Spark tasks. The entire group of these stages represent our Spark job. Let’s take a look at each stage in detail so that we can better understand what is going on at each level, starting with Figure 18-4.这些将马上变得重要，但首先让我们来看看 Spark 阶段的有向无环图（DAG）。 这些选项卡中的每个蓝色框表示 Spark 任务的一个阶段。 这些阶段的整个组代表我们的 Spark 工作。 让我们详细了解每个阶段，以便我们可以更好地了解每个级别的情况，从图 18-4 开始。The box on top, labeled WholeStateCodegen, represents a full scan of the CSV file. The box below that represents a shuffle that we forced when we called repartition. This turned our original dataset (of a yet to be specified number of partitions) into two partitions.标记为 WholeStateCodegen 的顶部框表示 CSV 文件的完整扫描。 下面的框表示我们在调用重新分区时强制进行的随机洗牌(shuffle)。 这将我们的原始数据集（尚未指定的分区数）转换为两个分区。The next step is our projection (selecting/adding/filtering columns) and the aggregation. Notice that in Figure 18-5 the number of output rows is six. This conveniently lines up with the number of output rows multiplied by the number of partitions at aggregation time. This is because Spark performs an aggregation for each partition (in this case a hash-based aggregation) before shuffling the data around in preparation for the final stage.下一步是我们的投影（选择/添加/过滤列）和聚合。 请注意，在图18-5中，输出行数为6（final output * 2 = 6）。 这方便与输出行的数量乘以聚合时的分区数对齐。 这是因为 Spark 在为最终阶段做准备之前对数据进行洗牌(shuffle)之前，为每个分区执行聚合（在这种情况下是基于散列（hash-based） 的聚合）。The last stage is the aggregation of the subaggregations that we saw happen on a per-partition basis in the previous stage. We combine those two partitions in the final three rows that are the output of our total query (Figure 18-6)。最后一个阶段是我们在前一阶段基于每个分区发生的子聚合的聚合。 我们将这两个分区组合在最后三行中，这三行是我们总查询的输出（图18-6）。Let’s look further into the job’s execution. On the Jobs tab, next to Succeeded Jobs, click 2. As Figure 18-7 demonstrates, our job breaks down into three stages (which corresponds to what we saw on the SQL tab).让我们进一步了解作业的执行情况。 在 Jobs 选项卡上，单击 Succeeded Jobs，单击2. 如图18-7所示，我们的工作分为三个阶段（与我们在SQL选项卡上看到的相对应）。These stages have more or less the same information as what’s shown in Figure 18-6, but clicking the label for one of them will show the details for a given stage. In this example, three stages ran, with eight, two, and then two hundred tasks each. Before diving into the stage detail, let’s review why this is the case.这些阶段或多或少与图18-6中显示的信息相同，但单击其中一个阶段的标签将显示给定阶段的详细信息。 在这个例子中，运行了三个阶段，每个阶段分别有八个，两个，然后是两百个任务。 在深入了解阶段细节之前，让我们回顾一下为什么会这样。The first stage has eight tasks. CSV files are splittable, and Spark broke up the work to be distributed relatively evenly between the different cores on the machine. This happens at the cluster level and points to an important optimization: how you store your files. The following stage has two tasks because we explicitly called a repartition to move the data into two partitions. The last stage has 200 tasks because the default shuffle partitions value is 200.第一阶段有八个任务。 CSV 文件是可拆分的，Spark 分解了工作使其相对均匀分布在机器上不同核心之间。 这发生在集群级别，并指向一个重要的优化：如何存储文件。 下一个阶段有两个任务，因为我们显式调用了重新分区以将数据移动到两个分区中。 最后一个阶段有200个任务，因为默认的 shuffle 分区值为 200。Now that we reviewed how we got here, click the stage with eight tasks to see the next level of detail, as shown in Figure 18-8.现在我们回顾了我们如何到达这里，单击具有八个任务的阶段以查看下一个详细级别，如图18-8所示。Spark provides a lot of detail about what this job did when it ran. Toward the top, notice the Summary Metrics section. This provides a synopsis of statistics regarding various metrics. What you want to be on the lookout for is uneven distributions of the values (we touch on this in Chapter 19). In this case, everything looks very consistent; there are no wide swings in the distribution of values. In the table at the bottom, we can also examine on a per-executor basis (one for every core on this particular machine, in this case). This can help identify whether a particular executor is struggling with its workload.Spark 提供了很多关于这项工作在运行时所做些什么的细节。在顶部，请注意“摘要衡量标准（Summary Metrics）”部分。这提供了有关各种指标的统计数据的概要。你想要注意的是值的不均匀分布（我们在第19章中讨论）。在这种情况下，一切看起来都非常一致; 值的分布没有大幅波动。在底部的表中，我们还可以基于每个执行程序（executor）进行检查（在这种情况下，该特定计算机上的每个核心都有一个）。这有助于确定特定执行程序（executor）是否在努力应对其工作量。Spark also makes available a set of more detailed metrics, as shown in Figure 18-8, which are probably not relevant to the large majority of users. To view those, click Show Additional Metrics, and then either choose (De)select All or select individual metrics, depending on what you want to see.Spark还提供了一组更详细的指标，如图18-8所示，这些指标可能与绝大多数用户无关。要查看这些，请单击“显示其他衡量标准”，然后选择（取消）选择“全部”或选择单个衡量标准，具体取决于您要查看的内容。You can repeat this basic analysis for each stage that you want to analyze. We leave that as an exercise for the reader.您可以为要分析的每个阶段重复此基本分析。我们把它作为读者的练习。### Other Spark UI tabs 其他Spark UI选项卡 The remaining Spark tabs, Storage, Environment, and Executors, are fairly self-explanatory. The Storage tab shows information about the cached RDDs/DataFrames on the cluster. This can help you see if certain data has been evicted from the cache over time. The Environment tab shows you information about the Runtime Environment, including information about Scala and Java as well as the various Spark Properties that you configured on your cluster. 其余的 Spark 选项卡，存储，环境和执行程序（executor），都是不言自明的。 “存储”选项卡显示有关集群上缓存的 RDD / DataFrame 的信息。这可以帮助您查看某些数据是否随着时间的推移从缓存中逐出。 “环境”选项卡显示有关运行时环境的信息，包括有关 Scala 和 Java 的信息以及您在集群上配置的各种Spark属性。### Configuring the Spark user interface 配置Spark用户界面 There are a number of configurations that you can set regarding the Spark UI. Many of them are networking configurations such as enabling access control. Others let you configure how the Spark UI will behave (e.g., how many jobs, stages, and tasks are stored). Due to space limitations, we cannot include the entire configuration set here. Consult the relevant table on Spark UI Configurations in the Spark documentation. 您可以设置有关 Spark UI 的许多配置。其中许多是网络配置，例如启用访问控制。其他允许您配置 Spark UI 的行为方式（例如，存储了多少个作业，阶段和任务）。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中 Spark UI配置 的相关表。 Spark REST APIIn addition to the Spark UI, you can also access Spark’s status and metrics via a REST API. This is is available at http://localhost:4040/api/v1 and is a way of building visualizations and monitoring tools on top of Spark itself. For the most part this API exposes the same information presented in the web UI, except that it doesn’t include any of the SQL-related information. This can be a useful tool if you would like to build your own reporting solution based on the information available in the Spark UI. Due to space limitations, we cannot include the list of API endpoints here. Consult the relevant table on REST API Endpoints in the Spark documentation. 除了 Spark UI，您还可以通过 REST API 访问Spark的状态和指标。这可以在 http://localhost:4040/api/v1 上获得，它是一种在Spark本身之上构建可视化和监视工具的方法。在大多数情况下，此API公开Web UI中显示的相同信息，但它不包含任何与SQL相关的信息。如果您希望根据Spark UI中提供的信息构建自己的报告解决方案，这可能是一个有用的工具。由于篇幅限制，我们无法在此处包含API端点列表。请参阅Spark文档中有关REST API端点的相关表。### Spark UI History Server SparkUI历史记录服务器 Normally, the Spark UI is only available while a SparkContext is running, so how can you get to it after your application crashes or ends? To do this, Spark includes a tool called the Spark History Server that allows you to reconstruct the Spark UI and REST API, provided that the application was configured to save an event log. You can find up-to-date information about how to use this tool in the Spark documentation. 通常，Spark UI 仅在 SparkContext 运行时可用，因此在应用程序崩溃或结束后如何才能访问它？为此，Spark包含一个名为 Spark History Server 的工具，允许您重建 Spark UI 和 REST API，前提是应用程序已配置为保存事件日志。您可以在Spark文档中找到有关如何使用此工具的最新信息。To use the history server, you first need to configure your application to store event logs to a certain location. You can do this by by enabling and the event log location with the configuration spark.eventLog.dir. Then, once you have stored the events, you can run the history server as a standalone application, and it will automatically reconstruct the web UI based on these logs. Some cluster managers and cloud services also configure logging automatically and run a history server by default.要使用历史记录服务器，首先需要配置应用程序以将事件日志存储到特定位置。您可以通过启用spark.eventLog.enabled 和配置 spark.eventLog.dir 的事件日志位置来完成此操作。然后，一旦存储了事件，就可以将历史服务器作为独立应用程序运行，它将根据这些日志自动重建Web UI。某些集群管理器和云服务还会自动配置日志记录并默认运行历史记录服务器。There are a number of other configurations for the history server. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Spark History Server Configurations in the Spark documentation.历史服务器还有许多其他配置。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中有关Spark History Server配置的相关表。## Debugging and Spark First Aid 调试和Spark急救 The previous sections defined some core “vital signs”—that is, things that we can monitor to check the health of a Spark Application. For the remainder of the chapter we’re going to take a “first aid” approach to Spark debugging: We’ll review some signs and symptoms of problems in your Spark jobs, including signs that you might observe (e.g., slow tasks) as well as symptoms from Spark itself (e.g., OutOfMemoryError). There are many issues that may affect Spark jobs, so it’s impossible to cover everything. But we will discuss some of the more common Spark issues you may encounter. In addition to the signs and symptoms, we’ll also look at some potential treatments for these issues. 前面的部分定义了一些核心的“生命体征” ——也就是说，我们可以监视以检查Spark应用程序运行状况的事情。对于本章的其余部分，我们将对Spark调试采取“急救”方法： 我们将审查Spark作业中的一些问题迹象和症状，包括您可能观察到的迹象（例如，缓慢的任务）以及来自Spark本身的症状（例如，OutOfMemoryError）。有许多问题可能会影响Spark作业，因此无法涵盖所有内容。但我们将讨论您可能遇到的一些更常见的Spark问题。除了症状和体征外，我们还将研究这些问题的一些潜在治疗方法。 Most of the recommendations about fixing issues refer to the configuration tools discussed in Chapter 16. 有关修复问题的大多数建议都参考了第16章中讨论的配置工具。 Spark Jobs Not Starting Spark工作没有开始This issue can arise frequently, especially when you’re just getting started with a fresh deployment or environment. 这个问题可能经常出现，特别是当您刚刚开始全新部署或环境。 Signs and symptoms 迹象和症状 Spark jobs don’t start. Spark工作无法启动。 The Spark UI doesn’t show any nodes on the cluster except the driver. 除驱动程序（driver）外，Spark UI不显示集群上的任何节点。 The Spark UI seems to be reporting incorrect information. Spark UI 似乎报告了错误的信息。 Potential treatments 可能的疗法This mostly occurs when your cluster or your application’s resource demands are not configured properly. Spark, in a distributed setting, does make some assumptions about networks, file systems, and other resources. During the process of setting up the cluster, you likely configured something incorrectly, and now the node that runs the driver cannot talk to the executors. This might be because you didn’t specify what IP and port is open or didn’t open the correct one. This is most likely a cluster level, machine, or configuration issue. Another option is that your application requested more resources per executor than your cluster manager currently has free, in which case the driver will be waiting forever for executors to be launched. 当您的集群或应用程序的资源需求未正确配置时，通常会发生这种情况。 Spark 在分布式设置中确实对网络，文件系统和其他资源做出了一些假设。在设置集群的过程中，您可能错误地配置了某些内容，现在运行驱动程序（driver）的节点无法与执行程序（executor）通信。这可能是因为您未指定打开的IP和端口或未打开正确的IP和端口。这很可能是集群级别，计算机或配置问题。另一个选择是，您的应用程序为每个执行程序（executor）请求的资源比您的集群管理器当前有空的资源要多，在这种情况下，驱动程序（driver）将永远等待执行程序（executor）启动。 Ensure that machines can communicate with one another on the ports that you expect. Ideally, you should open up all ports between the worker nodes unless you have more stringent security constraints. 确保计算机可以在您期望的端口上相互通信。理想情况下，您应该打开工作节点之间的所有端口，除非您有更严格的安全约束。 Ensure that your Spark resource configurations are correct and that your cluster manager is properly set up for Spark. Try running a simple application first to see if that works. One common issue may be that you requested more memory per executor than the cluster manager has free to allocate, so check how much it is reporting free (in its UI) and your sparksubmit memory configuration. 确保Spark资源配置正确并且已正确设置集群管理器以用于 Spark。尝试先运行一个简单的应用程序，看看是否有效。一个常见问题可能是您为每个执行程序（executor）请求的内存多于集群管理器可以自由分配的内存，因此请检查它是报告空闲的多少（在其 UI 中）和 sparksubmit 内存配置。 Errors Before Execution 执行前的错误This can happen when you’re developing a new application and have previously run code on this cluster, but now some new code won’t work. 当您开发新应用程序并且之前在此集群上运行代码时，可能会发生这种情况，但现在某些新代码将无法运行。 Signs and symptoms 迹象和症状 Commands don’t run at all and output large error messages. 命令根本不运行并输出大的错误消息。 You check the Spark UI and no jobs, stages, or tasks seem to run. 您检查Spark UI并且似乎没有任何作业，阶段或任务运行。 Potential treatments 潜在的治疗方法After checking and confirming that the Spark UI environment tab shows the correct information for your application, it’s worth double-checking your code. Many times, there might be a simple typo or incorrect column name that is preventing the Spark job from compiling into its underlying Spark plan (when using the DataFrame API). 在检查并确认 Spark UI 环境选项卡显示应用程序的正确信息后，值得仔细检查您的代码。很多时候，可能会出现一个简单的拼写错误或不正确的列名，导致Spark作业无法编译到其基础 Spark 计划中（使用 DataFrame API 时）。 You should take a look at the error returned by Spark to confirm that there isn’t an issue in your code, such as providing the wrong input file path or field name. Double-check to verify that the cluster has the network connectivity that you expect between your driver, your workers, and the storage system you are using. 您应该查看Spark返回的错误，以确认代码中没有问题，例如提供错误的输入文件路径或字段名称。仔细检查以验证集群是否具有您期望的驱动程序（driver），工作人员和正在使用的存储系统之间的网络连接。 There might be issues with libraries or classpaths that are causing the wrong version of a library to be loaded for accessing storage. Try simplifying your application until you get a smaller version that reproduces the issue (e.g., just reading one dataset). 库或类路径可能存在导致加载库的错误版本以访问存储的问题。尝试简化您的应用程序，直到您获得重现问题的较小版本（例如，只读取一个数据集）。 Errors During Execution 执行期间的错误 This kind of issue occurs when you already are working on a cluster or parts of your Spark 当您已经在集群或部分Spark上工作时，会出现此类问题。 Application run before you encounter an error. This can be a part of a scheduled job that runs at some interval or a part of some interactive exploration that seems to fail after some time. 在遇到错误之前运行应用程序。这可以是某个时间间隔运行的已经安排作业的一部分，也可能是某些时间后似乎失败的某些交互式探索的一部分。 Signs and symptoms 迹象和症状One Spark job runs successfully on the entire cluster but the next one fails. 一个 Spark 作业在整个集群上成功运行，但下一个失败。 A step in a multistep query fails. 多步查询中的步骤失败。 A scheduled job that ran yesterday is failing today. 昨天运行的预定工作今天失败了。 Difficult to parse error message. 难以解析错误消息。 Potential treatments 可能的疗法 Check to see if your data exists or is in the format that you expect. This can change over time or some upstream change may have had unintended consequences on your application. If an error quickly pops up when you run a query (i.e., before tasks are launched), it is most likely an analysis error while planning the query. This means that you likely misspelled a column name referenced in the query or that a column, view, or table you referenced does not exist. 检查您的数据是否存在或是否符合您的预期格式。这可能会随着时间的推移而改变，或者某些上游更改可能会对您的应用程序产生意外后果。如果在运行查询时（即，在启动任务之前）快速弹出错误，则在计划查询时很可能是分析错误。这意味着您可能拼错了查询中引用的列名称，或者您引用的列，视图或表不存在&gt;。 Read through the stack trace to try to find clues about what components are involved (e.g., what operator and stage it was running in). Try to isolate the issue by progressively double-checking input data and ensuring the data conforms to your expectations. Also try removing logic until you can isolate the problem in a smaller version of your application. 读取堆栈跟踪以尝试查找涉及哪些组件的线索（例如，运行的算子和阶段）。尝试通过逐步检查输入数据并确保数据符合您的期望来隔离问题。还可以尝试删除逻辑，直到您可以在较小版本的应用程序中隔离问题。 If a job runs tasks for some time and then fails, it could be due to a problem with the input data itself, wherein the schema might be specified incorrectly or a particular row does not conform to the expected schema. For instance, sometimes your schema might specify that the data contains no nulls but your data does actually contain nulls, which can cause certain transformations to fail. 如果作业运行任务一段时间然后失败，则可能是由于输入数据本身存在问题，其中可能未正确指定模式或特定行不符合预期模式。例如，有时您的模式可能指定数据不包含空值，但您的数据确实包含空值，这可能导致某些转换失败。 It’s also possible that your own code for processing the data is crashing, in which case Spark will show you the exception thrown by your code. In this case, you will see a task marked as “failed” on the Spark UI, and you can also view the logs on that machine to understand what it was doing when it failed. Try adding more logs inside your code to figure out which data record was being processed. 您自己的处理数据的代码也可能崩溃，在这种情况下，Spark会向您显示代码抛出的异常。在这种情况下，您将在Spark UI上看到标记为“失败”的任务，您还可以查看该计算机上的日志以了解失败时正在执行的操作。尝试在代码中添加更多日志，以确定正在处理哪些数据记录。 Slow Tasks or Stragglers 缓慢的任务或StragglersThis issue is quite common when optimizing applications, and can occur either due to work not being evenly distributed across your machines (“skew”), or due to one of your machines being slower than the others (e.g., due to a hardware problem). 在优化应用程序时，此问题非常常见，并且可能由于工作不均匀分布在您的计算机上（“倾斜”），或者由于您的某台计算机比其他计算机慢（例如，由于硬件问题）而发生。 Signs and symptoms 迹象和症状Any of the following are appropriate symptoms of the issue : 以下任何一种都是该问题的适当症状：- Spark stages seem to execute until there are only a handful of tasks left. Those tasks then take a long time. Spark阶段似乎执行，直到只剩下少数任务。那些任务需要很长时间。- These slow tasks show up in the Spark UI and occur consistently on the same dataset(s). 这些缓慢的任务显示在 Spark UI 中，并在相同的数据集上一致地发生。- These occur in stages, one after the other. 这些是分阶段发生的，一个接一个。- Scaling up the number of machines given to the Spark Application doesn’t really help—some tasks still take much longer than others. 扩大提供给 Spark 应用程序的机器数量并没有多大帮助——某些任务仍然需要比其他任务更长的时间。- In the Spark metrics, certain executors are reading and writing much more data than others. 在Spark指标中，某些执行程序（executor）正在读取和写入比其他数据更多的数据。#### Potential treatments 可能的治疗方法- Slow tasks are often called “stragglers.” There are many reasons they may occur, but most often the source of this issue is that your data is partitioned unevenly into DataFrame or RDD partitions. When this happens, some executors might need to work on much larger amounts of work than others. One particularly common case is that you use a group-by-key operation and one of the keys just has more data than others. In this case, when you look at the Spark UI, you might see that the shuffle data for some nodes is much larger than for others. 缓慢的任务通常被称为“落后者”。它们可能出现的原因很多，但大多数情况下，这个问题的根源是您的数据被不均匀地划分为 DataFrame 或 RDD 分区。当发生这种情况时，一些执行程序（executor）可能需要处理比其他工作量大得多的工作。一个特别常见的情况是您使用逐个键操作，其中一个键只是比其他键更多的数据。在这种情况下，当您查看 Spark UI 时，您可能会看到某些节点的 shuffle 数据比其他节点大得多。- Try increasing the number of partitions to have less data per partition. 尝试增加分区数，以使每个分区的数据更少。- Try repartitioning by another combination of columns. For example, stragglers can come up when you partition by a skewed ID column, or a column where many values are null. In the latter case, it might make sense to first filter out the null values. Try increasing the memory allocated to your executors if possible. 尝试通过另一个列组合重新分区。例如，当您通过倾斜的 ID 列或许多值为 null 的列进行分区时，straggler 可能会出现。在后一种情况下，首先过滤掉空值可能是有意义的。如果可能，尝试增加分配给执行程序（executor）的内存。- Monitor the executor that is having trouble and see if it is the same machine across jobs; you might also have an unhealthy executor or machine in your cluster—for example, one whose disk is nearly full. If this issue is associated with a join or an aggregation, see “Slow Joins” or “Slow Aggregations”. 监视有问题的执行程序（executor），看看它是否是跨不同作业的同一台机器；您的集群中可能还有一个不健康的执行程序（executor）或计算机——例如，磁盘几乎已满的计算机。如果此问题与连接或聚合相关联，请参阅“慢速连接”或“慢速聚合”。- Check whether your user-defined functions (UDFs) are wasteful in their object allocation or business logic. Try to convert them to DataFrame code if possible. Ensure that your UDFs or User-Defined Aggregate Functions (UDAFs) are running on a small enough batch of data. Oftentimes an aggregation can pull a lot of data into memory for a common key, leading to that executor having to do a lot more work than others. 检查用户定义的函数（UDF）在对象分配或业务逻辑中是否浪费。如果可能，尝试将它们转换为 DataFrame 代码。确保您的 UDF 或用户定义的聚合函数（UDAF）在足够小的数据批量上运行。通常，聚合可以将大量数据拉入内存以用于普通的键，从而导致执行程序（executor）必须比其他人执行更多的工作。- Turning on speculation, which we discuss in “Slow Reads and Writes”, will have Spark run a second copy of tasks that are extremely slow. This can be helpful if the issue is due to a faulty node because the task will get to run on a faster one. Speculation does come at a cost, however, because it consumes additional resources. In addition, for some storage systems that use eventual consistency, you could end up with duplicate output data if your writes are not idempotent . (We discussed speculation configurations in Chapter 17.) 打开我们在“慢速读取和写入”中讨论的推测（speculation），将使 Spark 运行极其缓慢的第二个任务副本。如果问题是由于故障节点引起的，这可能会有所帮助，因为任务将以更快的速度运行。然而，推测（speculation）确实需要付出代价，因为它消耗了额外的资源。此外，对于某些使用最终一致性的存储系统，如果写入不是幂等的，则最终可能会出现重复的输出数据。 （我们在第17章讨论了推测(speculation)配置）- Another common issue can arise when you’re working with Datasets. Because Datasets perform a lot of object instantiation to convert records to Java objects for UDFs, they can cause a lot of garbage collection. If you’re using Datasets, look at the garbage collection metrics in the Spark UI to see if they’re consistent with the slow tasks. 当您使用 Datasets 时，可能会出现另一个常见问题。由于 Datasets 执行大量对象实例化以将记录转换为UDF 的 Java 对象，因此它们可能导致大量垃圾回收。如果您正在使用 Datasets，请查看Spark UI中的垃圾收集指标，以查看它们是否与缓慢的任务一致。- Stragglers can be one of the most difficult issues to debug, simply because there are so many possible causes. However, in all likelihood, the cause will be some kind of data skew, so definitely begin by checking the Spark UI for imbalanced amountsimbalanced amounts of data across tasks. Stragglers可能是最难调试的问题之一，因为有很多可能的原因。但是，很有可能，原因将是某种数据偏差，因此必须首先检查Spark UI以查找跨任务的不平衡数据量。### Slow Aggregations 慢的聚合If you have a slow aggregation, start by reviewing the issues in the “Slow Tasks” section before proceeding. Having tried those, you might continue to see the same problem.如果您的聚合速度较慢，请先继续查看“慢速任务”部分中的问题，然后再继续。尝试过这些后，您可能会继续看到同样的问题。#### Signs and symptoms 迹象和症状- Slow tasks during a groupBy call. 在 groupBy 调用期间缓慢执行任务。- Jobs after the aggregation are slow, as well. 聚合后的工作也很慢。#### Potential treatments 可能的疗法 Unfortunately, this issue can’t always be solved. Sometimes, the data in your job just has some skewed keys, and the operation you want to run on them needs to be slow. Increasing the number of partitions, prior to an aggregation, might help by reducing the number of different keys processed in each task. 不幸的是，这个问题并不总能解决。有时，作业中的数据只有一些倾斜的键，您想要在它们上运行的操作需要很慢。在聚合之前增加分区数可能有助于减少每个任务中处理的不同键的数量。 Increasing executor memory can help alleviate this issue, as well. If a single key has lots of data, this will allow its executor to spill to disk less often and finish faster, although it may still be much slower than executors processing other keys. 增加执行程序（executor）内存也有助于缓解此问题。如果单个键有大量数据，这将允许其执行程序（executor）更少地溢出到磁盘并更快地完成，尽管它可能仍然比处理其他键的执行程序（executor）慢得多。 If you find that tasks after the aggregation are also slow, this means that your dataset might have remained unbalanced after the aggregation. Try inserting a repartition call to partition it randomly. 如果您发现聚合后的任务也很慢，这意味着聚合后您的数据集可能仍然不平衡。尝试插入重新分区调用以随机分区。 Ensuring that all filters and SELECT statements that can be are above the aggregation can help to ensure that you’re working only on the data that you need to be working on and nothing else. Spark’s query optimizer will automatically do this for the structured APIs. 确保可以在聚合之上的所有过滤器和 SELECT 语句可以帮助确保您仅处理您需要处理的数据而不是其他任何内容。 Spark的查询优化器将自动为结构化API执行此操作。 Ensure null values are represented correctly (using Spark’s concept of null) and not as some default value like “ “ or “EMPTY”. Spark often optimizes for skipping nulls early in the job when possible, but it can’t do so for your own placeholder values. 确保正确表示空值（使用Spark的null概念）而不是像“”或“EMPTY”那样的默认值。 Spark通常会尽可能优化在作业的早期跳过空值，但是对于您自己的占位符值，它不能这样做。 Some aggregation functions are also just inherently slower than others. For instance, collect_list and collect_set are very slow aggregation functions because they must return all the matching objects to the driver, and should be avoided in performance-critical code. 某些聚合函数本身也比其他函数慢。例如，collect_list和collect_set是非常慢的聚合函数，因为它们必须将所有匹配的对象返回给驱动程序（driver），并且应该在性能关键代码中避免使用。 Slow Joins 慢加入Joins and aggregations are both shuffles, so they share some of the same general symptoms as well as treatments. 连接和聚合都是随机洗牌(shuffle)，因此它们共享一些相同的症状和应对方法。 Signs and symptoms 迹象和症状 A join stage seems to be taking a long time. This can be one task or many tasks. l连接阶段似乎需要很长时间。这可以是一个任务或许多任务。 Stages before and after the join seem to be operating normally. 连接之前和之后的阶段似乎正常运行。 Potential treatments 可能的疗法 Many joins can be optimized (manually or automatically) to other types of joins. We covered how to select different join types in Chapter 8. 许多连接可以优化（手动或自动）到其他类型的连接。我们在第8章介绍了如何选择不同的连接类型。 Experimenting with different join orderings can really help speed up jobs, especially if some of those joins filter out a large amount of data; do those first. 尝试不同的连接顺序可以真正帮助加快工作，特别是如果其中一些连接过滤掉大量数据; 先做那些。 Partitioning a dataset prior to joining can be very helpful for reducing data movement across the cluster, especially if the same dataset will be used in multiple join operations. It’s worth experimenting with different prejoin partitioning. Keep in mind, again, that this isn’t “free” and does come at the cost of a shuffle. 在连接之前对数据集进行分区对于减少集群中的数据移动非常有用，尤其是在多个连接操作中将使用相同的数据集时。值得尝试不同的预连接（prejoin）分区。请记住，这不是“免费”，而是以洗牌(shuffle)为代价。 Slow joins can also be caused by data skew. There’s not always a lot you can do here, but sizing up the Spark application and/or increasing the size of executors can help, as described in earlier sections. 数据倾斜也可能导致慢连接。你可以在这里做很多事情，但是调整 Spark 应用程序和/或增加执行程序（executor）的数量可以提供帮助，如前面部分所述。 Ensuring that all filters and select statements that can be are above the join can help to ensure that you’re working only on the data that you need for the join. 确保可以在连接之上的所有筛选器和选择语句可以帮助确保您仅处理连接所需的数据。 Ensure that null values are handled correctly (that you’re using null) and not some default value like “ “ or “EMPTY”, as with aggregations. 确保正确处理空值（您使用的是null），而不是像聚合一样处理某些默认值，如“”或“EMPTY”。 Sometimes Spark can’t properly plan for a broadcast join if it doesn’t know any statistics about the input DataFrame or table. If you know that one of the tables that you are joining is small, you can try to force a broadcast (as discussed in Chapter 8), or use Spark’s statistics collection commands to let it analyze the table. 如果 Spark 不知道有关输入DataFrame或表的任何统计信息，Spark有时无法正确规划广播连接（broadcast join）。如果您知道要加入的其中一个表很小，则可以尝试强制广播（如第8章中所述），或使用Spark的统计信息收集命令让它分析表。 Slow Reads and Writes 慢的读和写Slow I/O can be difficult to diagnose, especially with networked file systems. 慢速 I/O 可能难以诊断，尤其是对于网络文件系统。 Signs and symptoms 迹象和症状Slow reading of data from a distributed file system or external system. Slow writes from network file systems or Blob storage. 从分布式文件系统或外部系统缓慢读取数据。从网络文件系统或 Blob 存储缓慢写入。 Potential treatments 潜在的治疗Turning on speculation (set spark.speculation to true) can help with slow reads and writes. This will launch additional tasks with the same operation in an attempt to see whether it’s just some transient issue in the first task. Speculation is a powerful tool and works well with consistent file systems. However, it can cause duplicate data writes with some eventually consistent cloud services, such as Amazon S3, so check whether it is supported by the storage system connector you are using. 打开推测（将 spark.speculation 设置为 true）可以帮助减慢读取和写入。这将使用相同的操作启动其他任务，以尝试查看它是否只是第一个任务中的一些短暂问题。推测(speculation)是一种功能强大的工具，适用于一致的文件系统。但是，它可能导致重复数据写入与一些最终一致的云服务（如Amazon S3），因此请检查您使用的存储系统连接器是否支持它。 Ensuring sufficient network connectivity can be important—your Spark cluster may simply not have enough total network bandwidth to get to your storage system. 确保足够的网络连接非常重要——您的 Spark 集群可能根本没有足够的总网络带宽来访问您的存储系统。 For distributed file systems such as HDFS running on the same nodes as Spark, make sure Spark sees the same hostnames for nodes as the file system. This will enable Spark to do locality-aware scheduling, which you will be able to see in the “locality” column in the Spark UI. We’ll talk about locality a bit more in the next chapter. 对于与Spark在相同节点上运行的分布式文件系统（如HDFS），请确保Spark看到与文件系统相同的节点主机名。这将使Spark能够进行关注局部性的调度，您可以在Spark UI的“locality”列中看到该调度。我们将在下一章中讨论一下局部性。 Driver OutOfMemoryError or Driver Unresponsive 驱动程序OutOfMemoryError或驱动程序无响应This is usually a pretty serious issue because it will crash your Spark Application. It often happens due to collecting too much data back to the driver, making it run out of memory. 这通常是一个相当严重的问题，因为它会使您的Spark应用程序崩溃。它经常发生，因为收集了太多的数据回到驱动程序，使其耗尽内存。 Signs and symptoms 迹象和症状 Spark Application is unresponsive or crashed. OutOfMemoryErrors or garbage collection messages in the driver logs. Spark应用程序无响应或崩溃。驱动程序日志中的OutOfMemoryErrors或垃圾回收消息。 Commands take a very long time to run or don’t run at all.命令需要很长时间才能运行或根本不运行。 Interactivity is very low or non-existent.交互性很低或根本不存在。 Memory usage is high for the driver JVM.驱动程序JVM的内存使用率很高。 Potential treatments 可能的治疗方法There are a variety of potential reasons for this happening, and diagnosis is not always straightforward. 这种情况有多种可能的原因，诊断并不总是直截了当的。 Your code might have tried to collect an overly large dataset to the driver node using operations such as collect. 您的代码可能尝试使用诸如collect之类的操作将过大的数据集收集到驱动程序节点。 You might be using a broadcast join where the data to be broadcast is too big. Use Spark’s maximum broadcast join configuration to better control the size it will broadcast. 您可能正在使用广播连接，其中要广播的数据太大。使用Spark的最大广播连接配置可以更好地控制它将广播的大小。 A long-running application generated a large number of objects on the driver and is unable to release them. Java’s jmap tool can be useful to see what objects are filling most of the memory of your driver JVM by printing a histogram of the heap. However, take note that jmap will pause that JVM while running. Increase the driver’s memory allocation if possible to let it work with more data. 长时间运行的应用程序在驱动程序上生成了大量对象，无法释放它们。 Java 的 jmap 工具可以通过打印堆的直方图来查看哪些对象填充了驱动程序 JVM 的大部分内存。但请注意，jmap 会在运行时暂停该JVM。如果可能的话，增加驱动程序的内存分配，让它可以处理更多数据。 Issues with JVMs running out of memory can happen if you are using another language binding, such as Python, due to data conversion between the two requiring too much memory in the JVM. Try to see whether your issue is specific to your chosen language and bring back less data to the driver node, or write it to a file instead of bringing it back as in-memory objects. 如果您使用其他语言绑定（如Python），JVM内存不足会出现问题，因为两者之间的数据转换需要JVM中的内存过多。尝试查看您的问题是否特定于您选择的语言，并将较少的数据带回驱动程序节点，或将其写入文件而不是将其作为内存中对象重新引入。 If you are sharing a SparkContext with other users (e.g., through the SQL JDBC server and some notebook environments), ensure that people aren’t trying to do something that might be causing large amounts of memory allocation in the driver (like working overly large arrays in their code or collecting large datasets). 如果您与其他用户共享 SparkContext（例如，通过SQL JDBC服务器和某些 notebook 环境），请确保人们不会尝试执行可能导致驱动程序中大量内存分配的操作（例如，过度工作代码中的大型数组或收集大型数据集）。 Executor OutOfMemoryError or Executor Unresponsive Executor OutOfMemoryError或Executor无响应Spark applications can sometimes recover from this automatically, depending on the true underlyingissue. Spark应用程序有时可以自动从中恢复，具体取决于真正的底层问题。 Signs and symptoms 迹象和症状 OutOfMemoryErrors or garbage collection messages in the executor logs. You can find these in the Spark UI. 执行程序(executor)日志中的OutOfMemoryErrors或垃圾回收消息。您可以在Spark UI中找到它们。 Executors that crash or become unresponsive. 崩溃或无响应的执行程序（executor）。 Slow tasks on certain nodes that never seem to recover. 某些节点上的缓慢任务似乎永远无法恢复。 Potential treatments 潜在的疗法 Try increasing the memory available to executors and the number of executors. 尝试增加执行程序(executor)可用的内存和执行程序(executor)的数量。 Try increasing PySpark worker size via the relevant Python configurations. 尝试通过相关的Python配置增加PySpark工作者大小。 Look for garbage collection error messages in the executor logs. Some of the tasks that are running, especially if you’re using UDFs, can be creating lots of objects that need to be garbage collected. Repartition your data to increase parallelism, reduce the amount of records per task, and ensure that all executors are getting the same amount of work. 在执行程序(executor)日志中查找垃圾收集错误消息。正在运行的某些任务（尤其是在使用UDF时）可能会创建大量需要进行垃圾回收的对象。重新分区数据以增加并行度，减少每个任务的记录数量，并确保所有执行程序(executor)获得相同的工作量。 Ensure that null values are handled correctly (that you’re using null) and not some default value like “ “ or “EMPTY”, as we discussed earlier. 确保正确处理空值（您正在使用null）而不是像我们之前讨论的那样的默认值，如“”或“EMPTY”。 This is more likely to happen with RDDs or with Datasets because of object instantiations. 由于对象实例化，这更有可能发生在 RDD 或 Datasets 中。 Try using fewer UDFs and more of Spark’s structured operations when possible. 尽可能尝试使用更少的UDF和更多Spark的结构化操作。 Use Java monitoring tools such as jmap to get a histogram of heap memory usage on your executors, and see which classes are taking up the most space. 使用 jmap 等Java监视工具获取执行程序(executor)堆内存使用情况的直方图，并查看哪些类占用的空间最多。 If executors are being placed on nodes that also have other workloads running on them, such as a key-value store, try to isolate your Spark jobs from other jobs. 如果将执行程序(executor)放置在也运行其他工作负载的节点上（例如键值存储），请尝试将Spark作业与其他作业隔离开来。 Unexpected Nulls in Results 结果中出现意外空白Signs and symptoms 迹象和症状 Unexpected null values after transformations. 转换后出现意外的空值。 Scheduled production jobs that used to work no longer work, or no longer produce the right results. 过去工作安排的生产作业不再起作用，或者不再产生正确的结果。 Potential treatments 潜在的治疗 It’s possible that your data format has changed without adjusting your business logic. This means that code that worked before is no longer valid. 您的数据格式可能已更改，而无需调整业务逻辑。这意味着以前工作的代码不再有效。 Use an accumulator to try to count records or certain types, as well as parsing or processing errors where you skip a record. This can be helpful because you might think that you’re parsing data of a certain format, but some of the data doesn’t. Most often, users will place the accumulator in a UDF when they are parsing their raw data into a more controlled format and perform the counts there. This allows you to count valid and invalid records and then operate accordingly after the fact. 使用累加器尝试计算记录或某些类型，以及解析或处理跳过记录的错误。这可能很有用，因为您可能认为您正在解析某种格式的数据，但有些数据却没有。大多数情况下，用户在将原始数据解析为更受控制的格式并在那里执行计数时，会将累加器放在 UDF 中。这允许您计算有效和无效的记录，然后在事后进行相应的操作。 Ensure that your transformations actually result in valid query plans. Spark SQL sometimes does implicit type coercions that can cause confusing results. For instance, the SQL expression SELECT 5“23” results in 115 because the string “25” converts to an the value 25 as an integer, but the expression SELECT 5 “ “ results in null because casting the empty string to an integer gives null. Make sure that your intermediate datasets have the schema you expect them to (try using printSchema on them), and look for any CAST operations in the final query plan. 确保您的转换实际上产生有效的查询计划。 Spark SQL有时会执行隐式类型强制，这可能会导致混乱的结果。例如，SQL表达式SELECT 5 “23”导致115，因为字符串“25”将整数转换为值25，但表达式SELECT 5 “”导致null，因为将空字符串转换为整数给出null。确保您的中间数据集具有您期望的模式（尝试对它们使用printSchema），并在最终查询计划中查找任何CAST操作。 ​ No Space Left on Disk Errors 磁盘错误没有剩余空间Signs and symptoms 迹象和症状 You see “no space left on disk” errors and your jobs fail. 您看到“磁盘上没有剩余空间”错误，您的作业失败。 Potential treatments 潜在的治疗 The easiest way to alleviate this, of course, is to add more disk space. You can do this by sizing up the nodes that you’re working on or attaching external storage in a cloud environment. 当然，减轻这种情况的最简单方法是添加更多磁盘空间。您可以通过调整正在处理的节点或在云环境中连接外部存储来实现此目的。 If you have a cluster with limited storage space, some nodes may run out first due to skew. 如果您的集群存储空间有限，则某些节点可能会由于数据倾斜而首先耗尽。 Repartitioning the data as described earlier may help here. 如前所述重新分区数据可能对此有所帮助。 There are also a number of storage configurations with which you can experiment. Some of these determine how long logs should be kept on the machine before being removed. For more information, see the Spark executor logs rolling configurations in Chapter 16. 您还可以使用许多存储配置进行试验。其中一些决定了在移除之前应该在机器上保留多长时间的日志。有关更多信息，请参阅第16章中的Spark执行程序(executor)日志滚动配置。 Try manually removing some old log files or old shuffle files from the machine(s) in question. This can help alleviate some of the issue although obviously it’s not a permanent fix. 尝试从相关机器手动删除一些旧的日志文件或旧的随机洗牌文件。这可以帮助减轻一些问题，虽然显然它不是永久性的修复。 Serialization Errors 序列化错误Signs and symptoms 迹象和症状 You see serialization errors and your jobs fail.您看到序列化错误，您的作业失败。 Potential treatments潜在的治疗方法 This is very uncommon when working with the Structured APIs, but you might be trying to perform some custom logic on executors with UDFs or RDDs and either the task that you’re trying to serialize to these executors or the data you are trying to share cannot be serialized. This often happens when you’re working with either some code or data that cannot be serialized into a UDF or function, or if you’re working with strange data types that cannot be serialized. If you are using (or intend to be using Kryo serialization), verify that you’re actually registering your classes so that they are indeed serialized. 这在使用结构化API时非常罕见，但您可能尝试使用UDF或RDD在执行程序(executor)上执行某些自定义逻辑，以及您尝试序列化到这些执行程序(executor)的任务或您尝试共享的数据无法序列化。当您使用某些无法序列化为UDF或函数的代码或数据时，或者您正在使用无法序列化的奇怪数据类型时，通常会发生这种情况。如果您正在使用（或打算使用Kryo序列化），请验证您实际上是在注册类，以便它们确实是序列化的。 Try not to refer to any fields of the enclosing object in your UDFs when creating UDFs inside a Java or Scala class. This can cause Spark to try to serialize the whole enclosing object, which may not be possible. Instead, copy the relevant fields to local variables in the same scope as closure and use those. 在Java或Scala类中创建UDF时，尽量不要引用UDF中封闭对象的任何字段。这可能导致Spark尝试序列化整个封闭对象，这可能是不可能的。相反，将相关字段复制到与闭包相同的范围内的局部变量并使用它们。## Conclusion 结论 This chapter covered some of the main tools that you can use to monitor and debug your Spark jobs and applications, as well as the most common issues we see and their resolutions. As with debugging any complex software, we recommend taking a principled, step-by-step approach to debug issues. Add logging statements to figure out where your job is crashing and what type of data arrives at each stage, try to isolate the problem to the smallest piece of code possible, and work up from there. For data skew issues, which are unique to parallel computing, use Spark’s UI to get a quick overview of how much work each task is doing. In Chapter 19, we discuss performance tuning in particular and various tools you can use for that. 本章介绍了一些可用于监视和调试Spark作业和应用程序的主要工具，以及我们看到的最常见问题及其解决方案。与调试任何复杂软件一样，我们建议采用有原则的逐步方法来调试问题。添加日志记录语句以确定作业崩溃的位置以及每个阶段到达的数据类型，尝试将问题隔离到可能的最小代码段，并从那里开始工作。对于并行计算所特有的数据偏差问题，请使用Spark的UI快速了解每项任务的工作量。在第19章中，我们特别讨论了性能调优以及可以使用的各种工具。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter17_Deploying-Spark]]></title>
    <url>%2F2019%2F08%2F07%2FChapter17_Deploying-Spark(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 17 Deploying SparkThis chapter explores the infrastructure you need in place for you and your team to be able to run Spark Applications: 本章将探讨您和您的团队能够运行Spark应用程序所需的基础架构： Cluster deployment choices 集群部署选择 Spark’s different cluster managers Spark的不同集群管理器 Deployment considerations and configuring deployments 部署考虑事项和配置部署 For the most, part Spark should work similarly with all the supported cluster managers ; however, customizing the setup means understanding the intricacies of each of the cluster management systems. The hard part is deciding on the cluster manager (or choosing a managed service). Although we would be happy to include all the minute details about how you can configure different cluster with different cluster managers, it’s simply impossible for this book to provide hyper-specific details for every situation in every single environment. The goal of this chapter, therefore, is not to discuss each of the cluster managers in full detail, but rather to look at their fundamental differences and to provide a reference for a lot of the material already available on the Spark website. Unfortunately, there is no easy answer to “which is the easiest cluster manager to run” because it varies so much by use case, experience, and resources. The Spark documentation site offers a lot of detail about deploying Spark with actionable examples. We do our best to discuss the most relevant points. 对于大多数人来说，Spark应该与所有受支持的集群管理器类似地工作；但是，自定义设置意味着要了解每个集群管理系统的复杂性。困难的部分是决定集群管理器（或选择托管服务）。虽然我们很乐意提供有关如何使用不同集群管理器配置不同集群的所有细节，但本书根本不可能为每个环境中的每种情况提供超特定的详细信息。因此，本章的目标不是详细讨论每个集群管理器，而是要查看它们的基本差异，并为Spark网站上已有的许多资料提供参考。不幸的是，“哪个是最容易运行的集群管理器”没有简单的答案，因为它因用例，经验和资源而变化很大。 Spark文档站点提供了有关使用可操作示例部署 Spark 的大量详细信息。我们会尽力讨论最相关的观点。 As of this writing, Spark has three officially supported cluster managers : 在开始撰写本文时，Spark有三个官方支持的集群管理器： Standalone mode 独立模式 Hadoop YARN Apache Mesos These cluster managers maintain a set of machines onto which you can deploy Spark Applications. Naturally, each of these cluster managers has an opinionated view toward management, and so there are trade-offs and semantics that you will need to keep in mind. However, they all run Spark applications the same way (as covered in Chapter 16). Let’s begin with the first point: where to deploy your cluster. 这些集群管理器维护一组可以部署Spark应用程序的计算机。当然，这些集群管理者中的每一个都对管理有一种自以为是的观点，因此需要记住权衡和语义。但是，它们都以相同的方式运行 Spark 应用程序（如第16章所述）。让我们从第一点开始：部署集群的位置。 Where to Deploy Your Cluster to Run Spark Applications 在何处部署集群以运行Spark应用程序There are two high-level options for where to deploy Spark clusters: deploy in an on-premises cluster or in the public cloud. This choice is consequential and is therefore worth discussing. 在哪里部署Spark集群有两个高级选项：在内部部署集群或公共云中部署。这种选择是重要的，因此值得讨论。 On-Premises Cluster Deployments 内部部署集群部署Deploying Spark to an on-premises cluster is sometimes a reasonable option, especially for organizations that already manage their own datacenters. As with everything else, there are trade-offs to this approach. An on-premises cluster gives you full control over the hardware used, meaning you can optimize performance for your specific workload. However, it also introduces some challenges, especially when it comes to data analytics workloads like Spark. First, with on-premises deployment, your cluster is fixed in size, whereas the resource demands of data analytics workloads are often elastic. If you make your cluster too small, it will be hard to launch the occasional very large analytics query or training job for a new machine learning model, whereas if you make it large, you will have resources sitting idle. Second, for on-premises clusters, you need to select and operate your own storage system, such as a Hadoop file system or scalable key-value store. This includes setting up georeplication and disaster recovery if required. 将 Spark 部署到内部部署集群有时是一种合理的选择，特别是对于已经管理自己的数据中心的组织。与其他一切一样，这种方法存在权衡取舍。内部部署集群使您可以完全控制所使用的硬件，这意味着您可以针对特定工作负载优化性能。但是，它也带来了一些挑战，特别是在Spark等数据分析工作负载方面。首先，通过内部部署，您的集群的大小是固定的，而数据分析工作负载的资源需求通常是弹性的。如果您的集群太小，则很难为新的机器学习模型启动偶尔的非常大的分析查询或训练工作，而如果您将其扩大，则会使资源闲置。其次，对于内部部署集群，您需要选择并运行自己的存储系统，例如 Hadoop 文件系统或可伸缩键值存储。这包括在需要时设置地理复制和灾难恢复。 If you are going to deploy on-premises, the best way to combat the resource utilization problem is to use a cluster manager that allows you to run many Spark applications and dynamically reassign resources between them, or even allows non-Spark applications on the same cluster. All of Spark’s supported cluster managers allow multiple concurrent applications, but YARN and Mesos have better support for dynamic sharing and also additionally support non-Spark workloads. Handling on-premisesresource sharing is likely going to be the biggest difference your users see day to day with Spark on-premises versus in the cloud: in public clouds, it’s easy to give each application its own cluster of exactly the required size for just the duration of that job. 如果要部署内部部署，解决资源利用率问题的最佳方法是使用集群管理器，它允许您运行许多 Spark 应用程序并在它们之间动态重新分配资源，甚至允许在相同集群上使用非 Spark 应用程序。所有 Spark 支持的集群管理器都允许多个并发应用程序，但 YARN 和 Mesos 可以更好地支持动态共享，还可以支持非Spark工作负载。处理资源共享可能是您的用户每天使用Spark内部部署与云中看到的最大差异：在公共云中，很容易为每个应用程序提供自己的集群，其中包含完全所需的大小那份工作。For storage, you have several different options, but covering all the trade-offs and operational details in depth would probably require its own book. The most common storage systems used for Spark are distributed file systems such as Hadoop’s HDFS and key-value stores such as Apache Cassandra. Streaming message bus systems such as Apache Kafka are also often used for ingesting data. All these systems have varying degrees of support for management, backup, and georeplication, sometimes built into the system and sometimes only through third-party commercial tools. Before choosing a storage option, we recommend evaluating the performance of its Spark connector and evaluating the available management tools.对于存储，您有几种不同的选择，但是深入讨论所有权衡和操作细节可能需要它自己的书。用于 Spark 的最常见存储系统是分布式文件系统，例如 Hadoop 的 HDFS 和键值存储，例如 Apache Cassandra。诸如 Apache Kafka之类的流式消息总线系统也经常用于摄取数据。所有这些系统都对管理，备份和地理复制有不同程度的支持，有时内置于系统中，有时仅通过第三方商业工具。在选择存储选项之前，我们建议您评估其Spark连接器的性能并评估可用的管理工具。### Spark in the Cloud 云端Spark While early big data systems were designed for on-premises deployment, the cloud is now an increasingly common platform for deploying Spark. The public cloud has several advantages when it comes to big data workloads. First, resources can be launched and shut down elastically, so you can run that occasional “monster” job that takes hundreds of machines for a few hours without having to pay for them all the time. Even for normal operation, you can choose a different type of machine and cluster size for each application to optimize its cost performance—for example, launch machines with Graphics Processing Units (GPUs) just for your deep learning jobs. Second, public clouds include low-cost, georeplicated storage that makes it easier to manage large amounts of data. 虽然早期的大数据系统是为内部部署而设计的，但云现在是部署Spark的日益普遍的平台。在涉及大数据工作负载时，公共云有几个优点。首先，资源可以弹性地启动和关闭，因此您可以运行偶尔的“怪物”工作，这需要数百台机器几个小时，而无需一直为它们付费。即使是正常操作，您也可以为每个应用程序选择不同类型的计算机和群集大小，以优化其性价比——例如，仅为您的深度学习作业启动具有图形处理单元（GPU）的计算机。其次，公共云包括低成本，地理复制的存储，可以更轻松地管理大量数据。 Many companies looking to migrate to the cloud imagine they’ll run their applications in the same way that they run their on-premises clusters. All the major cloud providers (Amazon Web Services [AWS], Microsoft Azure, Google Cloud Platform [GCP], and IBM Bluemix) include managed Hadoop clusters for their customers, which provide HDFS for storage as well as Apache Spark. This is actually not a great way to run Spark in the cloud, however, because by using a fixed-size cluster and file system, you are not going to be able to take advantage of elasticity. Instead, it is generally a better idea to use global storage systems that are decoupled from a specific cluster, such as Amazon S3, Azure Blob Storage, or Google Cloud Storage and spin up machines dynamically for each Spark workload. With decoupled compute and storage, you will be able to pay for computing resources only when needed, scale them up dynamically, and mix different hardware types. Basically, keep in mind that running Spark in the cloud need not mean migrating an on-premises installation to virtual machines: you can run Spark natively against cloud storage to take full advantage of the cloud’s elasticity, cost-saving benefit, and management tools without having to manage an on-premise computing stack within your cloud environment. 许多希望迁移到云的公司想象他们将以运行其内部部署集群的方式运行其应用程序。所有主要云提供商（Amazon Web Services [AWS]，Microsoft Azure，Google Cloud Platform [GCP]和IBM Bluemix）都为其客户提供托管Hadoop集群，这些集群为存储和Apache Spark提供HDFS。然而，这实际上不是在云中运行Spark的好方法，因为通过使用固定大小的集群和文件系统，您将无法利用弹性。而是，通常最好使用与特定集群（例如Amazon S3，Azure Blob存储或Google云存储）分离的全局存储系统，并为每个Spark工作负载动态启动计算机。通过分离计算和存储，您将能够仅在需要时为计算资源付费，动态扩展计算资源，并混合使用不同的硬件类型。基本上，请记住，在云中运行Spark并不意味着将本地安装迁移到虚拟机：您可以针对云存储本地运行Spark以充分利用云的弹性，成本节约优势和管理工具，而无需必须在云环境中管理内部部署计算堆栈。 Several companies provide “cloud-native” Spark-based services, and all installations of Apache Spark can of course connect to cloud storage. Databricks, the company started by the Spark team from UC Berkeley, is one example of a service provider built specifically for Spark in the cloud. Databricks provides a simple way to run Spark workloads without the heavy baggage of a Hadoop installation. The company provides a number of features for running Spark more efficiently in the cloud, such as auto-scaling, auto-termination of clusters, and optimized connectors to cloud storage, as well as a collaborative environment for working on notebooks and standalone jobs. The company also provides a free Community Edition for learning Spark where you can run notebooks on a small cluster and share them live with others. A fun fact is that this entire book was written using the free Community Edition of Databricks, because we found the integrated Spark notebooks, live collaboration, and cluster management the easiest way to produce and test this content. 有几家公司提供“基于云原生”的基于 Spark 的服务，Apache Spark的所有安装当然都可以连接到云存储。由加州大学伯克利分校的 Spark 团队发起的公司 Databricks 是专门为云中的 Spark 构建的服务提供商的一个例子。 Databricks 提供了一种运行Spark工作负载的简单方法，而无需承担 Hadoop 安装的沉重负担。该公司提供了许多功能，可以在云中更有效地运行Spark，例如自动扩展，集群自动终止，云存储的优化连接器，以及用于处理 notebook 和独立作业的协作环境。该公司还提供免费的社区版学习 Spark，您可以在小型集群上运行 notebook 并与他人分享。一个有趣的事实是，整本书是使用免费的 Databricks 社区版编写的，因为我们发现集成的 Spark notebook，实时协作和集群管理是生成和测试此内容的最简单方法。If you run Spark in the cloud, much of the content in this chapter might not be relevant because you can often create a separate, short-lived Spark cluster for each job you execute. In that case, the standalone cluster manager is likely the easiest to use. However, you may still want to read this content if you’d like to share a longer-lived cluster among many applications, or to install Spark on virtual machines yourself.如果您在云中运行Spark，本章中的大部分内容可能都不相关，因为您通常可以为您执行的每个作业创建一个单独的，短期的Spark集群。在这种情况下，独立集群管理器可能是最容易使用的。但是，如果您希望在许多应用程序之间共享一个较长期的集群，或者您自己在虚拟机上安装Spark，则可能仍希望阅读此内容。## Cluster Managers 集群管理器 Unless you are using a high-level managed service, you will have to decide on the cluster manager to use for Spark. Spark supports three aforementioned cluster managers: standalone clusters, Hadoop YARN, and Mesos. Let’s review each of these. 除非您使用的是高级托管服务，否则您必须决定要用于Spark的集群管理器。Spark 支持三个上述集群管理器：独立集群（standalone clusters），Hadoop YARN 和 Mesos。让我们回顾一下这些。 Standalone Mode 独立模式Spark’s standalone cluster manager is a lightweight platform built specifically for Apache Spark workloads. Using it, you can run multiple Spark Applications on the same cluster. It also provides simple interfaces for doing so but can scale to large Spark workloads. The main disadvantage of the standalone mode is that it’s more limited than the other cluster managers—in particular, your cluster can only run Spark. It’s probably the best starting point if you just want to quickly get Spark running on a cluster, however, and you do not have experience using YARN or Mesos. Spark 的独立集群管理器是专为 Apache Spark 工作负载构建的轻量级平台。使用它，您可以在同一个集群上运行多个 Spark 应用程序。它还提供了简单的界面，但可以扩展到大型 Spark 工作负载。独立模式的主要缺点是它比其他集群管理器更有限——特别是，您的集群只能运行 Spark。如果您只想快速让 Spark 在群集上运行，那么这可能是最好的起点，但是您没有使用 YARN 或 Mesos 的经验。 Starting a standalone cluster 启动独立集群Starting a standalone cluster requires provisioning the machines for doing so. That means starting them up, ensuring that they can talk to one another over the network, and getting the version of Spark you would like to run on those sets of machines. After that, there are two ways to start the cluster: by hand or using built-in launch scripts. 启动独立群集需要配置计算机。这意味着启动它们，确保它们可以通过网络相互通信，并获得您希望在这些机器上运行的 Spark 版本。之后，有两种方法可以启动集群：手动或使用内置启动脚本。 Let’s first launch a cluster by hand. The first step is to start the master process on the machine that we want that to run on, using the following command : 让我们首先手动启动一个集群。第一步是使用以下命令在我们希望运行的机器上启动主进程： 1$SPARK_HOME/sbin/start-master.sh When we run this command, the cluster manager master process will start up on that machine. Once started, the master prints out a spark://HOST:PORT URI. You use this when you start each of the worker nodes of the cluster, and you can use it as the master argument to your SparkSession on application initialization. You can also find this URI on the master’s web UI, which is http://masterip-address:8080 by default. With that URI, start the worker nodes by logging in to each machine and running the following script using the URI you just received from the master node. The master machine must be available on the network of the worker nodes you are using, and the port must be open on the master node, as well: 运行此命令时，集群管理器主进程将在该计算机上启动。一旦启动，master 就打印出一个 spark://HOST:PORT URI。在启动集群的每个工作节点时使用它，并且可以在应用程序初始化时将其用作 SparkSession 的主参数。您还可以在 master 的 Web UI 上找到此 URI，默认情况下为 http://masterip-address:8080 。使用该 URI，通过登录到每台计算机并使用刚从主节点收到的 URI 运行以下脚本来启动工作节点。主机必须在您正在使用的工作节点的网络上可用，并且端口必须在主节点上打开，以及 ： 1$SPARK_HOME/sbin/start-slave.sh &lt;master-spark-URI&gt; As soon as you’ve run that on another machine, you have a Spark cluster running! This process is naturally a bit manual; thankfully there are scripts that can help to automate this process. 只要你在另一台机器上运行它，就会运行一个Spark集群！这个过程自然需要一点手动; 谢天谢地，有些脚本可以帮助自动化这个过程。 Cluster launch scripts 集群启动脚本You can configure cluster launch scripts that can automate the launch of standalone clusters. To do this, create a file called conf/slaves in your Spark directory that will contain the hostnames of all the machines on which you intend to start Spark workers, one per line. If this file does not exist, everything will launch locally. When you go to actually start the cluster, the master machine will access each of the worker machines via Secure Shell (SSH). By default, SSH is run in parallel and requires that you configure password-less (using a private key) access. If you do not have a password-less setup, you can set the environment variable SPARK_SSH_FOREGROUND and serially provide a password for each worker. 您可以配置可以自动启动独立集群的集群启动脚本。为此，在Spark目录中创建一个名为 conf/slaves 的文件，该文件将包含要在其上启动 Spark 工作的所有计算机的主机名，每行一个。如果此文件不存在，则所有内容都将在本地启动。当您真正启动集群时，主计算机将通过 Secure Shell(SSH) 访问每个工作计算机。默认情况下，SSH 是并行运行的，需要您配置无密码（使用私钥）访问。如果您没有无密码设置，则可以设置环境变量SPARK_SSH_FOREGROUND 并为每个工作人员连续提供密码。 After you set up this file, you can launch or stop your cluster by using the following shell scripts, based on Hadoop’s deploy scripts, and available in $SPARK_HOME/sbin: 设置此文件后，可以使用以下基于 Hadoop 部署脚本的 shell 脚本启动或停止集群，并在 $SPARK_HOME/sbin 中提供： $SPARK_HOME/sbin/start-master.sh Starts a master instance on the machine on which the script is executed. 在执行脚本的计算机上启动主实例。 $SPARK_HOME/sbin/start-slaves.sh Starts a slave instance on each machine specified in the conf/slaves file. 在conf / slaves文件中指定的每台计算机上启动从属实例。 $SPARK_HOME/sbin/start-slave.sh Starts a slave instance on the machine on which the script is executed. 在执行脚本的计算机上启动从属实例。 $SPARK_HOME/sbin/start-all.sh Starts both a master and a number of slaves as described earlier. 如前所述，启动主服务器和多个从服务器。 $SPARK_HOME/sbin/stop-master.sh Stops the master that was started via the bin/start-master.sh script. 停止通过bin / start-master.sh脚本启动的主服务器。 $SPARK_HOME/sbin/stop-slaves.sh Stops all slave instances on the machines specified in the conf/slaves file. 停止conf / slaves文件中指定的计算机上的所有从属实例。 $SPARK_HOME/sbin/stop-all.sh Stops both the master and the slaves as described earlier. 如前所述，停止主站和从站。 Standalone cluster configurations 独立集群配置Standalone clusters have a number of configurations that you can use to tune your application. These control everything from what happens to old files on each worker for terminated applications to the worker’s core and memory resources. These are controlled via environment variables or via application properties. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Standalone Environment Variables in the Spark documentation. 独立集群具有许多可用于调整应用程序的配置。它们控制从终止应用程序的每个工作程序上的旧文件发生到工作程序的核心和内存资源的所有内容。这些是通过环境变量或应用程序属性控制的。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中有关独立环境变量的相关表。 Submitting applications 提交申请After you create the cluster, you can submit applications to it using the spark://URI of the master. You can do this either on the master node itself or another machine using spark-submit. There are some specific command-line arguments for standalone mode, which we covered in “Launching Applications”. 创建集群后，可以使用主服务器的 spark://URI 向其提交应用程序。您可以在主节点本身或使用 spark-submit 的其他计算机上执行此操作。独立模式有一些特定的命令行参数，我们在“启动应用程序”中介绍了这些参数。 Spark on YARN 运行在YARN上SparkHadoop YARN is a framework for job scheduling and cluster resource management. Even though Spark is often (mis)classified as a part of the “Hadoop Ecosystem,” in reality, Spark has little to do with Hadoop. Spark does natively support the Hadoop YARN cluster manager but it requires nothing from Hadoop itself. Hadoop YARN 是一个用于作业调度和集群资源管理的框架。尽管 Spark 经常（错误地）归类为 “Hadoop生态系统” 的一部分，但事实上，Spark与Hadoop几乎没有关系。 Spark 本身支持 Hadoop YARN 集群管理器，但它不需要Hadoop本身。 You can run your Spark jobs on Hadoop YARN by specifying the master as YARN in the spark-submit command-line arguments. Just like with standalone mode, there are a number of knobs that you are able to tune according to what you would like the cluster to do. The number of knobs is naturally larger than that of Spark’s standalone mode because Hadoop YARN is a generic scheduler for a large number of different execution frameworks. 您可以通过在 spark-submit 命令行参数中将 master 指定为 YARN 来在 Hadoop YARN 上运行 Spark 作业。就像独立模式一样，有许多旋钮可以根据您希望集群执行的操作进行调整。旋钮的数量自然大于 Spark 的独立模式，因为 Hadoop YARN 是大量不同执行框架的通用调度程序。 Setting up a YARN cluster is beyond the scope of this book, but there are some great books on the topic as well as managed services that can simplify this experience. 设置 YARN 集群超出了本书的范围，但是有一些关于该主题的优秀书籍以及可以简化此体验的托管服务。 Submitting applications 提交申请When submitting applications to YARN, the core difference from other deployments is that --master will become yarn as opposed the master node IP, as it is in standalone mode. Instead, Spark will find the YARN configuration files using the environment variable HADOOP_CONF_DIR or YARN_CONF_DIR. Once you have set those environment variables to your Hadoop installation’s configuration directory, you can just run spark-submit like we saw in Chapter 16. 在向 YARN 提交申请时，与其他部署的核心区别在于 --master 将成为 YARN 而不是主节点 IP，正如它处于独立模式的那样。相反，Spark 将使用环境变量 HADOOP_CONF_DIR 或 YARN_CONF_DIR 找到 YARN 配置文件。一旦将这些环境变量设置到 Hadoop 安装的配置目录中，就可以像我们在第16章中看到的那样运行 spark-submit。 NOTE 注意There are two deployment modes that you can use to launch Spark on YARN. As discussed in previous chapters, cluster mode has the spark driver as a process managed by the YARN cluster, and the client can exit after creating the application. In client mode, the driver will run in the client process and therefore YARN will be responsible only for granting executor resources to the application, not maintaining the master node. Also of note is that in cluster mode, Spark doesn’t necessarily run on the same machine on which you’re executing. Therefore libraries and external jars must be distributed manually or through the --jars command-line argument. 您可以使用两种部署模式在 YARN 上启动 Spark。如前几章所述，集群模式将 Spark 驱动程序作为 YARN 集群管理的进程，客户端可以在创建应用程序后退出。在客户端模式下，驱动程序将在客户端进程中运行，因此 YARN 仅负责向应用程序授予执行器（executor）资源，而不是维护主节点。另外值得注意的是，在集群模式下，Spark不一定在您正在执行的同一台机器上运行。因此，必须手动或通过 --jars 命令行参数分发库和外部jar。 There are a few YARN-specific properties that you can set by using spark-submit. These allow you to control priority queues and things like keytabs for security. We covered these in “Launching Applications” in Chapter 16. 您可以使用 spark-submit 设置一些特定于 YARN 的属性。这些允许您控制优先级队列和诸如 keytabs 之类的东西以确保安全性。我们在第16章的“启动应用程序”中介绍了这些内容。 Configuring Spark on YARN Applications 在YARN应用程序上配置SparkDeploying Spark as YARN applications requires you to understand the variety of different configurations and their implications for your Spark applications. This section covers some best practices for basic configurations and includes references to some of the important configuration for running your Spark applications. 将Spark部署为YARN应用程序需要您了解各种不同的配置及其对 Spark 应用程序的影响。本节介绍了一些基本配置的最佳实践，并包括对运行Spark应用程序的一些重要配置的引用。 Hadoop configurations Hadoop配置If you plan to read and write from HDFS using Spark, you need to include two Hadoop configuration files on Spark’s classpath: hdfs-site.xml, which provides default behaviors for the HDFS client; and core-site.xml, which sets the default file system name. The location of these configuration files varies across Hadoop versions, but a common location is inside of /etc/hadoop/conf. Some tools create these configurations on the fly, as well, so it’s important to understand how your managed service might be deploying these, as well. 如果您计划使用 Spark 从 HDFS 读取和写入，则需要在 Spark 的类路径中包含两个Hadoop配置文件：hdfs-site.xml，它为 HDFS 客户端提供默认行为; 和 core-site.xml，它设置默认文件系统名称。这些配置文件的位置因 Hadoop 版本而异，但常见位置在 /etc/hadoop/conf 中。有些工具也可以动态创建这些配置，因此了解托管服务如何部署这些配置也很重要。 To make these files visible to Spark, set HADOOP_CONF_DIR in $SPARK_HOME/spark-env.sh to a location containing the configuration files or as an environment variable when you go to spark–submit your application. 要使这些文件对 Spark 可见，请将 $SPARK_HOME/spark-env.sh 中的 HADOOP_CONF_DIR 设置为包含配置文件的位置，或者当您转到 spark-submit 应用程序时将其设置为环境变量。#### Application properties for YARN YARN的应用程序属性 There are a number of Hadoop-related configurations and things that come up that largely don’t have much to do with Spark, just running or securing YARN in a way that influences how Spark runs. Due to space limitations, we cannot include the configuration set here. Refer to the relevant table on YARN Configurations in the Spark documentation. 有许多与 Hadoop 相关的配置和出现的东西很大程度上与 Spark 没什么关系，只是以影响 Spark 运行或保护 YARN 的方式。 由于空间限制，我们不能在此处包含配置集。 请参阅 Spark文档 中有关 YARN配置的相关表。 Spark on Mesos 在Mesos运行的SparkApache Mesos is another clustering system that Spark can run on. A fun fact about Mesos is that the project was also started by many of the original authors of Spark, including one of the authors of this book. In the Mesos project’s own words : Apache Mesos 是 Spark 可以运行的另一个集群系统。关于 Mesos 的一个有趣的事实是该项目也是由Spark的许多原作者创建的，包括本书的作者之一。在Mesos项目中用自己的话来说 ： Apache Mesos abstracts CPU, memory, storage, and other compute resources away from machines (physical or virtual), enabling fault-tolerant and elastic distributed systems to easily be built and run effectively. Apache Mesos将CPU，内存，存储和其他计算资源从机器（物理或虚拟）中抽象出来，使容错和弹性分布式系统能够轻松构建并有效运行。For the most part, Mesos intends to be a datacenter scale-cluster manager that manages not just short-lived applications like Spark, but long-running applications like web applications or other resource interfaces. Mesos is the heaviest-weight cluster manager, simply because you might choose this cluster manager only if your organization already has a large-scale deployment of Mesos, but it makes for a good cluster manager nonetheless.在大多数情况下，Mesos打算成为一个数据中心规模集群管理器，它不仅管理像Spark这样的短期应用程序，而且管理长期运行的应用程序，如Web应用程序或其他资源接口。 Mesos是最重的集群管理器，仅仅因为您可能只在您的组织已经大规模部署Mesos时才选择此集群管理器，但它仍然是一个优秀的集群管理器。Mesos is a large piece of infrastructure, and unfortunately there’s simply too much information for us to cover how to deploy and maintain Mesos clusters. There are many great books on the subject for that, including Dipa Dubhashi and Akhil Das’s Mastering Mesos (O’Reilly, 2016). The goal here is to bring up some of the considerations that you’ll need to think about when running Spark Applications on Mesos.Mesos 是一个很大的基础架构，不幸的是，我们有太多的信息来介绍如何部署和维护 Mesos 集群。有很多关于这个主题的好书，包括 Dipa Dubhashi 和 Akhil Das 的 《Mastering Mesos》（O’Reilly，2016）。这里的目标是提出在 Mesos 上运行 Spark 应用程序时需要考虑的一些注意事项。For instance, one common thing you will hear about Spark on Mesos is fine-grained versus coarse-grained mode. Historically Mesos supported a variety of different modes (fine-grained and coarse-grained), but at this point, it supports only coarse-grained scheduling (fine-grained has been deprecated). Coarse-grained mode means that each Spark executor runs as a single Mesos task. Spark executors are sized according to the following application properties :例如，你会听到关于 Spark on Mesos 的一个常见的事情是细粒度和粗粒度模式。从历史上看，Mesos支持各种不同的模式（细粒度和粗粒度），但此时，它仅支持粗粒度调度（细粒度已被弃用）。粗粒度模式意味着每个Spark执行程序作为单个Mesos任务运行。 Spark执行程序根据以下应用程序属性调整大小：123spark.executor.memoryspark.executor.coresspark.cores.max/spark.executor.cores#### Submitting applicationsSubmitting applications to a Mesos cluster is similar to doing so for Spark’s other cluster managers. For the most part you should favor cluster mode when using Mesos. Client mode requires some extra configuration on your part, especially with regard to distributing resources around the cluster. For instance, in client mode, the driver needs extra configuration information in spark-env.sh to work with Mesos.将应用程序提交到 Mesos 集群与 Spark 的其他集群管理器类似。 在大多数情况下，您应该在使用 Mesos 时使用集群模式。 客户端模式需要您进行一些额外配置，尤其是在集群上分配资源方面。 例如，在客户端模式下，驱动程序需要 spark-env.sh 中的额外配置信息才能使用Mesos。In spark-env.sh set some environment variables :在spark-env.sh中设置一些环境变量：1234export MESOS_NATIVE_JAVA_LIBRARY=&lt;path to libmesos.so&gt;This path is typically &lt;prefix&gt;/lib/libmesos.so where the prefix is /usr/local by default. On Mac OSX, the library is called libmesos.dylib instead of libmesos.so:export SPARK_EXECUTOR_URI=&lt;URL of spark-2.2.0.tar.gz uploaded above&gt;Finally, set the Spark Application property spark.executor.uri to . Now, when starting a Spark application against the cluster, pass a mesos://URL as the master when creating a SparkContex , and set that property as a parameter in your SparkConf variable or the initialization of a SparkSession :最后，将 Spark Application 属性 spark.executor.uri 设置为 。 现在，在针对集群启动 Spark 应用程序时，在创建 SparkContext 时将 mesos://URL 作为 master 传递，并将该属性设置为 SparkConf 变量中的参数或 SparkSession 的初始化：1234567// in Scalaimport org.apache.spark.sql.SparkSessionval spark = SparkSession.builder.master("mesos://HOST:5050").appName("my app").config("spark.executor.uri", "&lt;path to spark-2.2.0.tar.gz uploaded above&gt;").getOrCreate()Submitting cluster mode applications is fairly straightforward and follows the same spark-submit structure you read about before. We covered these in “Launching Applications”.提交集群模式应用程序非常简单，并遵循您之前阅读的相同的 spark-submit 结构。 我们在“启动应用程序”中介绍了这些内容。#### Configuring Mesos 配置Mesos Just like any other cluster manager, there are a number of ways that we can configure our Spark Applications when they’re running on Mesos. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Mesos Configurations in the Spark documentation. 就像任何其他集群管理器一样，我们可以通过多种方式在 Spark 应用程序运行时配置它们。 由于篇幅限制，我们无法在此处包含整个配置集。 请参阅 Spark文档 中有关 Mesos配置的相关表 。 Secure Deployment Configurations 安全部署配置Spark also provides some low-level ability to make your applications run more securely, especially in untrusted environments. Note that the majority of this setup will happen outside of Spark. These configurations are primarily network-based to help Spark run in a more secure manner. This means authentication, network encryption, and setting TLS and SSL configurations. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Security Configurations in the Spark documentation. Spark 还提供了一些低阶功能，使您的应用程序运行更安全，尤其是在不受信任的环境中。请注意，此设置的大部分将在 Spark 之外发生。这些配置主要基于网络，以帮助 Spark 以更安全的方式运行。这意味着身份验证，网络加密以及设置 TLS 和 SSL 配置。由于篇幅限制，我们无法在此处包含整个配置集。请参阅 Spark文档 中有关 安全配置的相关表 。 Cluster Networking Configurations 集群网络配置Just as shuffles are important, there can be some things worth tuning on the network. This can also be helpful when performing custom deployment configurations for your Spark clusters when you need to use proxies in between certain nodes. If you’re looking to increase Spark’s performance, these should not be the first configurations you go to tune, but may come up in custom deployment scenarios. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Networking Configurations in the Spark documentation. 正如洗牌（shuffle）很重要，网络上可能会有一些值得调整的东西。当您需要在某些节点之间使用代理时，在为Spark 集群执行自定义部署配置时，这也很有用。如果您希望提高Spark的性能，这些不应该是您要调整的第一个配置，但可能会出现在自定义部署方案中。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档 中的网络配置相关表。### Application Scheduling 应用程序调度 Spark has several facilities for scheduling resources between computations. First, recall that, as described earlier in the book, each Spark Application runs an independent set of executor processes. Cluster managers provide the facilities for scheduling across Spark applications. Second, within each Spark application, multiple jobs (i.e., Spark actions) may be running concurrently if they were submitted by different threads. This is common if your application is serving requests over the network. Spark includes a fair scheduler to schedule resources within each application. We introduced this topic in the previous chapter. Spark 有几种用于在计算之间调度资源的工具。首先，回想一下，如本书前面所述，每个 Spark 应用程序都运行一组独立的执行器（executor）进程。集群管理器提供跨Spark应用程序进行调度的工具。其次，在每个Spark应用程序中，如果它们是由不同的线程提交的，则多个作业（即 Spark actions ）可以同时运行。如果您的应用程序通过网络提供请求，这种情况很常见。 Spark包含一个公平的调度程序（fair scheduler）来安排每个应用程序中的资源。我们在前一章介绍了这个主题。 If multiple users need to share your cluster and run different Spark Applications, there are different options to manage allocation, depending on the cluster manager. The simplest option, available on all cluster managers, is static partitioning of resources. With this approach, each application is given a maximum amount of resources that it can use, and holds onto those resources for the entire duration. In spark-submit there are a number of properties that you can set to control the resource allocation of a particular application. Refer to Chapter 16 for more information. In addition, dynamic allocation (described next) can be turned on to let applications scale up and down dynamically based on their current number of pending tasks. If, instead, you want users to be able to share memory and executor resources in a fine-grained manner, you can launch a single Spark Application and use thread scheduling within it to serve multiple requests in parallel. 如果多个用户需要共享您的集群并运行不同的Spark应用程序，则可以使用不同的选项来管理分配，具体取决于集群管理器。所有集群管理器上都可以使用的最简单的选项是资源的静态分区（static partitioning of resources）。通过这种方法，每个应用程序都可以使用最多的资源，并在整个持续时间内保留这些资源。在spark-submit中，您可以设置许多属性来控制特定应用程序的资源分配。有关更多信息，请参阅第16章。此外，可以打开动态分配（下面描述），让应用程序根据当前挂起的任务数量动态扩展和缩小。相反，如果您希望用户能够以细粒度的方式共享内存和执行程序资源，则可以启动单个Spark应用程序并在其中使用线程调度来并行处理多个请求。 Dynamic allocation 动态分配If you would like to run multiple Spark Applications on the same cluster, Spark provides a mechanism to dynamically adjust the resources your application occupies based on the workload. This means that your application can give resources back to the cluster if they are no longer used, and request them again later when there is demand. This feature is particularly useful if multiple applications share resources in your Spark cluster. 如果您希望在同一个集群上运行多个Spark应用程序，Spark会提供一种机制，根据工作负载动态调整应用程序占用的资源。这意味着如果不再使用，您的应用程序可以将资源提供给集群，并在需要时稍后再次请求它们。如果多个应用程序共享Spark集群中的资源，则此功能特别有用。 This feature is disabled by default and available on all coarse-grained cluster managers; that is, standalone mode, YARN mode, and Mesos coarse-grained mode. There are two requirements for using this feature. First, your application must set spark.dynamicAllocation.enabled to true. Second, you must set up an external shuffle service on each worker node in the same cluster and set spark.shuffle.service.enabled to true in your application. The purpose of the external shuffle service is to allow executors to be removed without deleting shuffle files written by them. This is set up differently for each cluster manager and is described in the job scheduling configuration. Due to space limitations, we cannot include the configuration set for dynamic allocation. Refer to the relevant table on Dynamic Allocation Configurations. 默认情况下禁用此功能，并且所有粗粒度集群管理器均可使用此功能; 也就是独立模式，YARN模式和Mesos粗粒度模式。使用此功能有两个要求。首先，您的应用程序必须将 spark.dynamicAllocation.enabled 设置为true。其次，必须在同一群集中的每个工作节点上设置外部洗牌（shuffle） 服务，并在应用程序中将 spark.shuffle.service.enabled 设置为 true。外部 shuffle 服务的目的是允许删除执行程序而不删除它们写入的shuffle文件。对于每个集群管理器，此设置不同，并在作业调度配置中进行了描述。由于空间限制，我们不能包含动态分配的配置集。请参阅动态分配配置的相关表。 Miscellaneous Considerations 各种各样的考虑因素There several other topics to consider when deploying Spark applications that may affect your choice of cluster manager and its setup. These are just things that you should think about when comparing different deployment options. 在部署可能影响您选择的集群管理器及其设置的 Spark 应用程序时，还需要考虑其他几个主题。在比较不同的部署选项时，您应该考虑这些事项。 One of the more important considerations is the number and type of applications you intend to be running. For instance, YARN is great for HDFS-based applications but is not commonly used for much else. Additionally, it’s not well designed to support the cloud, because it expects information to be available on HDFS. Also, compute and storage is largely coupled together, meaning that scaling your cluster involves scaling both storage and compute instead of just one or the other. Mesos does improve on this a bit conceptually, and it supports a wide range of application types, but it still requires pre-provisioning machines and, in some sense, requires buy-in at a much larger scale. For instance, it doesn’t really make sense to have a Mesos cluster for only running Spark Applications. Spark standalone mode is the lightest-weight cluster manager and is relatively simple to understand and take advantage of, but then you’re going to be building more application management infrastructure that you could get much more easily by using YARN or Mesos. 其中一个更重要的考虑因素是您打算运行的应用程序的数量和类型。例如，YARN 非常适合基于 HDFS 的应用程序，但并不常用于其他许多应用程序。此外，它还没有很好地支持云，因为它希望在 HDFS 上提供信息。此外，计算和存储在很大程度上是耦合在一起的，这意味着扩展集群涉及扩展存储和计算，而不仅仅是一个或另一个。 Mesos 在概念上确实有所改进，它支持多种应用类型，但它仍然需要预配置机器，从某种意义上说，需要更大规模的支持。例如，只运行 Spark 应用程序的 Mesos 集群没有真正发挥它的价值。 Spark 独立模式是最轻量级的集群管理器，并且相对易于理解和利用，但随后您将构建更多应用程序管理基础架构，使用 YARN 或 Mesos 可以更轻松地获得这些基础架构。 Another challenge is managing different Spark versions. Your hands are largely tied if you want to try to run a variety of different applications running different Spark versions, and unless you use a well-managed service, you’re going to need to spend a fair amount of time either managing different setup scripts for different Spark services or removing the ability for your users to use a variety of different Spark applications. 另一个挑战是管理不同的 Spark 版本。如果你想尝试运行运行不同 Spark 版本的各种不同应用程序，你的手很大程度上是捆绑在一起的，除非你使用管理良好的服务，否则你需要花费相当多的时间来管理不同的设置脚本用于不同的 Spark 服务或删除用户使用各种不同 Spark 应用程序的能力。 Regardless of the cluster manager that you choose, you’re going to want to consider how you’re going to set up logging, store logs for future reference, and allow end users to debug their applications. These are more “out of the box” for YARN or Mesos and might need some tweaking if you’re using standalone. 无论您选择哪个集群管理器，您都会想要考虑如何设置日志记录，存储日志以供将来参考，以及允许最终用户调试其应用程序。对于 YARN 或 Mesos 来说，这些更“开箱即用”，如果您使用独立的话，可能需要进行一些调整。 One thing you might want to consider—or that might influence your decision making—is maintaining a metastore in order to maintain metadata about your stored datasets, such as a table catalog. We saw how this comes up in Spark SQL when we are creating and maintaining tables. Maintaining an Apache Hive metastore, a topic beyond the scope of this book, might be something that’s worth doing to facilitate more productive, cross-application referencing to the same datasets. Depending on your workload, it might be worth considering using Spark’s external shuffle service. Typically Spark stores shuffle blocks (shuffle output) on a local disk on that particular node. An external shuffle service allows for storing those shuffle blocks so that they are available to all executors, meaning that you can arbitrarily kill executors and still have their shuffle outputs available to other applications. 您可能想要考虑的一件事——或者可能影响您决策的一件事——是维护一个 Metastore，以维护有关您存储的数据集的元数据，例如表目录。我们在创建和维护表时看到了如何在 Spark SQL 中出现这种情况。维护 Apache Hive Metastore 是一个超出本书范围的主题，可能值得做些什么来促进对同一数据集的更高效，跨应用程序的引用。根据您的工作量，可能值得考虑使用 Spark 的外部 shuffle 服务。通常，Spark 将 shuffle 块（ shuffle 输出）存储在该特定节点上的本地磁盘上。外部 shuffle 服务允许存储这些 shuffle 块，以便它们可供所有执行程序使用，这意味着您可以任意杀死执行程序，并且仍然可以将其 shuffle 输出提供给其他应用程序。 Finally, you’re going to need to configure at least some basic monitoring solution and help users debug their Spark jobs running on their clusters. This is going to vary across cluster management options and we touch on some of the things that you might want to set up in Chapter 18. 最后，您将需要至少配置一些基本监视解决方案，并帮助用户调试在其集群上运行的 Spark 作业。这在集群管理选项中会有所不同，我们会讨论您可能希望在第18章中设置的一些内容。 Conclusion 结论This chapter looked at the world of configuration options that you have when choosing how to deploy Spark. Although most of the information is irrelevant to the majority of users, it is worth mentioning if you’re performing more advanced use cases. It might seem fallacious, but there are other configurations that we have omitted that control even lower-level behavior. You can find these in the Spark documentation or in the Spark source code. Chapter 18 talks about some of the options that we have when monitoring Spark Applications. 本章介绍了在选择部署 Spark 时所具有的配置选项的世界。虽然大多数信息与大多数用户无关，但如果您正在执行更高级的用户案例，则值得一提。它可能看起来很荒谬，但是我们已经省略了其他配置来控制甚至更低阶的行为。您可以在 Spark 文档或 Spark 源代码中找到它们。第18章讨论了监视Spark应用程序时的一些选项。]]></content>
      <categories>
        <category>English,中文</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 16 Developing Spark Applications]]></title>
    <url>%2F2019%2F08%2F05%2FChapter16_DevelopingSparkApplications(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 16 Developing Spark ApplicationsTesting Spark Applications 测试Spark应用程序You now know what it takes to write and run a Spark Application, so let’s move on to a less exciting but still very important topic: testing. Testing Spark Applications relies on a couple of key principles and tactics that you should keep in mind as you’re writing your applications. 您现在知道编写和运行Spark应用程序需要什么，所以让我们继续讨论一个不太令人兴奋但仍然非常重要的主题：测试。测试Spark应用程序依赖于在编写应用程序时应该记住的几个关键原则和策略。 Strategic Principles 战略原则Testing your data pipelines and Spark Applications is just as important as actually writing them. This is because you want to ensure that they are resilient to future change, in data, logic, and output. In this section, we’ll first discuss what you might want to test in a typical Spark Application, then discuss how to organize your code for easy testing. 测试数据管道和 Spark 应用程序与实际编写它们一样重要。这是因为您希望确保它们能够适应未来的变化，包括数据，逻辑和输出。在本节中，我们将首先讨论您可能希望在典型的 Spark 应用程序中测试的内容，然后讨论如何组织代码以便于轻松测试。 Input data resilience 输入数据弹性Being resilient to different kinds of input data is something that is quite fundamental to how you write your data pipelines. The data will change because the business needs will change. Therefore your Spark Applications and pipelines should be resilient to at least some degree of change in the input data or otherwise ensure that these failures are handled in a graceful and resilient way. For the most part this means being smart about writing your tests to handle those edge cases of different inputs and making sure that the pager only goes off when it’s something that is truly important. 对不同类型的输入数据具有弹性对于编写数据管道非常重要。数据将发生变化，因为业务需求将发生变化。因此，您的Spark应用程序和管道应该能够适应输入数据的至少某种程度的变化，或者确保以优雅和弹性的方式处理这些故障。在大多数情况下，这意味着要聪明地编写测试来处理不同输入的边缘情况（极端情况），并确保报警器仅在真正重要的事情发生时才会发出警报。 Business logic resilience and evolution 业务逻辑弹性和演变The business logic in your pipelines will likely change as well as the input data. Even more importantly, you want to be sure that what you’re deducing from the raw data is what you actually think that you’re deducing. This means that you’ll need to do robust logical testing with realistic data to ensure that you’re actually getting what you want out of it. One thing to be wary of here is trying to write a bunch of “Spark Unit Tests” that just test Spark’s functionality. You don’t want to be doing that; instead, you want to be testing your business logic and ensuring that the complex business pipeline that you set up is actually doing what you think it should be doing. 管道中的业务逻辑可能会随输入数据而变化。更重要的是，您希望确保从原始数据中推断出的内容是您实际认为正在推导的内容。这意味着您需要使用真实数据进行强大的逻辑测试，以确保您实际上从中获得了您想要的内容。需要注意的一件事是尝试编写一组只测试Spark功能的“Spark Unit Tests”。你不想这样做;相反，您希望测试业务逻辑并确保您设置的复杂业务管道实际上正在执行您认为应该执行的操作。 Resilience in output and atomicity 输出的弹性和原子性Assuming that you’re prepared for departures in the structure of input data and that your business logic is well tested, you now want to ensure that your output structure is what you expect. This means you will need to gracefully handle output schema resolution. It’s not often that data is simply dumped in some location, never to be read again—most of your Spark pipelines are probably feeding other Spark pipelines. For this reason you’re going to want to make certain that your downstream consumers understand the “state” of the data—this could mean how frequently it’s updated as well as whether the data is “complete” (e.g., there is no late data) or that there won’t be any last-minute corrections to the data. All of the aforementioned issues are principles that you should be thinking about as you build your data pipelines (actually, regardless of whether you’re using Spark). This strategic thinking is important for laying down the foundation for the system that you would like to build. 假设您已准备好在输入数据结构中离开，并且您的业务逻辑已经过充分测试，那么您现在需要确保您的输出结构符合您的预期。这意味着您需要优雅地处理输出模式解析。通常不会将数据简单地转储到某个位置，永远不会再次读取——大多数Spark管道可能正在为其他 Spark 管道提供服务。因此，您需要确保下游消费者了解数据的“状态”——这可能意味着更新频率以及数据是否“完整”（例如，没有后期数据）或者不会对数据进行任何最后修正。所有上述问题都是您在构建数据管道时应该考虑的原则（实际上，无论您是否使用Spark）。这种战略思维对于为您希望构建的系统奠定基础非常重要。 Tactical Takeaways 战术外卖Although strategic thinking is important, let’s talk a bit more in detail about some of the tactics that you can actually use to make your application easy to test. The highest value approach is to verify that your business logic is correct by employing proper unit testing and to ensure that you’re resilient to changing input data or have structured it so that schema evolution will not become unwielding in the future. The decision for how to do this largely falls on you as the developer because it will vary according to your business domain and domain expertise. 虽然战略思维很重要，但让我们更详细地谈谈您可以实际使用的一些策略，以使您的应用程序易于测试。最有价值的方法是通过采用适当的单元测试来验证您的业务逻辑是否正确，并确保您能够适应不断变化的输入数据，或者对其进行结构化，以便模式演变在未来不会变得无法使用。关于如何做到这一点的决定很大程度上取决于您作为开发人员，因为它将根据您的业务领域和领域专业知识而有所不同。 Managing SparkSessions 管理SparkSessionsTesting your Spark code using a unit test framework like JUnit or ScalaTest is relatively easy because of Spark’s local mode—just create a local mode SparkSession as part of your test harness to run it. However, to make this work well, you should try to perform dependency injection as much as possible when managing SparkSessions in your code. That is, initialize the SparkSession only once and pass it around to relevant functions and classes at runtime in a way that makes it easy to substitute during testing. This makes it much easier to test each individual function with a dummy SparkSession in unit tests. 使用单元测试框架（如 JUnit 或 ScalaTest ）测试Spark代码相对容易，因为Spark的本地模式——只需创建一个本地模式 SparkSession 作为测试工具的一部分来运行它。但是，为了使这项工作更好，您应该在代码中管理SparkSessions 时尽可能多地执行依赖项注入。也就是说，只将 SparkSession 初始化一次并在运行时将其传递给相关的函数和类，以便在测试期间轻松替换。这使得在单元测试中使用仿真的SparkSession测试每个单独的函数变得更加容易。#### Which Spark API to Use? 使用哪种Spark API？Spark offers several choices of APIs, ranging from SQL to DataFrames and Datasets, and each of these can have different impacts for maintainability and testability of your application. To be perfectly honest, the right API depends on your team and its needs: some teams and projects will need the less strict SQL and DataFrame APIs for speed of development, while others will want to use type-safe Datasets or RDDs.Spark提供了多种API选择，从 SQL 到 DataFrames 和 Datasets，每种API都会对应用程序的可维护性和可测试性产生不同的影响。说实话，正确的API取决于您的团队及其需求：一些团队和项目需要不太严格的SQL和DataFrame API来提高开发速度，而其他团队和项目则需要使用类型安全的 DataSet 或 RDD 。In general, we recommend documenting and testing the input and output types of each function regardless of which API you use. The type-safe API automatically enforces a minimal contract for your function that makes it easy for other code to build on it. If your team prefers to use DataFrames or SQL, then spend some time to document and test what each function returns and what types of inputs it accepts to avoid surprises later, as in any dynamically typed programming language. While the lower-level RDD API is also statically typed, we recommend going into it only if you need low-level features such as partitioning that are not present in Datasets, which should not be very common; the Dataset API allows more performance optimizations and is likely to provide even more of them in the future.通常，我们建议记录和测试每个函数的输入和输出类型，无论您使用哪个API。类型安全的API会自动为您的函数强制执行最小的约定（可以理解为：可使用的条款少，灵活度低），以便在其上构建其他代码。如果您的团队更喜欢使用DataFrames或SQL，那么花一些时间来记录和测试每个函数返回的内容以及它接受哪些类型的输入以避免以后出现意外，就像在任何动态类型编程语言中一样。虽然较低阶的RDD API也是静态类型的，但我们建议只有在需要低阶功能（例如数据集中不存在的分区）时才进入它，这应该不常见; DataSet API允许更多性能优化，并且可能在将来提供更多性能优化。A similar set of considerations applies to which programming language to use for your application: there certainly is no right answer for every team, but depending on your needs, each language will provide different benefits. We generally recommend using statically typed languages like Scala and Java for larger applications or those where you want to be able to drop into low-level code to fully control performance, but Python and R may be significantly better in other cases—for example, if you need to use some of their other libraries. Spark code should easily be testable in the standard unit testing frameworks in every language.一组类似的注意事项适用于您的应用程序使用哪种编程语言：每个团队肯定没有正确的答案，但根据您的需求，每种语言都会提供不同的好处。我们通常建议对大型应用程序使用静态类型语言（如Scala和Java），或者希望能够放入低阶代码以完全控制性能的语言，但在其他情况下，Python和R可能会明显更好——例如，如果你需要使用他们的一些其他库。 Spark代码应该可以在每种语言的标准单元测试框架中轻松测试。### Connecting to Unit Testing Frameworks 连接到单元测试框架To unit test your code, we recommend using the standard frameworks in your langage (e.g., JUnit or ScalaTest ), and setting up your test harnesses to create and clean up a SparkSession for each test. Different frameworks offer different mechanisms to do this, such as “before” and “after” methods. We have included some sample unit testing code in the application templates for this chapter.要对代码进行单元测试，我们建议您使用标准框架（例如，JUnit 或 ScalaTest），并设置测试工具来为每个测试创建和清理 SparkSession。不同的框架提供了不同的机制来实现这一点，例如“之前”和“之后”方法。我们在本章的应用程序模板中包含了一些示例单元测试代码。### Connecting to Data Sources 连接到数据源As much as possible, you should make sure your testing code does not connect to production data sources, so that developers can easily run it in isolation if these data sources change. One easy way to make this happen is to have all your business logic functions take DataFrames or Datasets as input instead of directly connecting to various sources; after all, subsequent code will work the same way no matter what the data source was. If you are using the structured APIs in Spark, another way to make this happen is named tables: you can simply register some dummy datasets (e.g., loaded from small text file or from in-memory objects) as various table names and go from there.您应尽可能确保测试代码不会连接到生产数据源，以便开发人员可以在这些数据源发生更改时轻松地单独运行它。实现这一目标的一种简单方法是让所有业务逻辑功能将DataFrames或Datasets作为输入，而不是直接连接到各种源; 毕竟，无论数据源是什么，后续代码都将以相同的方式工作。如果您在Spark中使用结构化API，另一种实现此目的的方法是命名表：您只需将一些仿真数据集（例如，从小文本文件或内存中对象加载）注册为各种表名，然后从那里开始。## The Development Process 开发过程The development process with Spark Applications is similar to development workflows that you have probably already used. First, you might maintain a scratch space, such as an interactive notebook or some equivalent thereof, and then as you build key components and algorithms, you move them to a more permanent location like a library or package. The notebook experience is one that we often recommend (and are using to write this book) because of its simplicity in experimentation. There are also some tools, such as Databricks, that allow you to run notebooks as production applications as well.Spark应用程序的开发过程类似于您可能已经使用过的开发工作流程。首先，您可以维护一个临时空间，例如交互式笔记本或其等效物，然后在构建关键组件和算法时，将它们移动到更永久的位置，如库或包。notebook（与jupter notebook类似）的体验是我们经常推荐的（并且正在用来编写本书），因为它的实验非常简单。还有一些工具，如 Databricks，允许您将笔记本作为生产应用程序运行。When running on your local machine, the spark-shell and its various language-specific implementations are probably the best way to develop applications. For the most part, the shell is for interactive applications, whereas spark-submit is for production applications on your Spark cluster. You can use the shell to interactively run Spark, just as we showed you at the beginning of this book. This is the mode with which you will run PySpark, Spark SQL, and SparkR. In the bin folder, when you download Spark, you will find the various ways of starting these shells. Simply run sparkshell(for Scala), spark-sql, pyspark, and sparkR. After you’ve finished your application and created a package or script to run, spark-submit will become your best friend to submit this job to a cluster.在本地计算机上运行时，spark-shell 及其各种特定于语言的实现可能是开发应用程序的最佳方式。在大多数情况下，shell用于交互式应用程序，而 spark-submit 用于Spark集群上的生产应用程序。您可以使用shell以交互方式运行Spark，就像我们在本书开头部分向您展示的那样。这是运行 PySpark，Spark SQL 和 SparkR 的模式。在bin文件夹中，当您下载 Spark 时，您将找到启动这些 shell 的各种方法。只需运行 sparkshell（适用于Scala），spark-sql，pyspark 和 sparkR。在您完成应用程序并创建要运行的包或脚本后，spark-submit 将成为您将此作业提交到群集的最佳朋友。## Launching Applications 启动应用程序 The most common way for running Spark Applications is through spark-submit. Previously in this chapter, we showed you how to run spark-submit; you simply specify your options, the application JAR or script, and the relevant arguments: 运行Spark应用程序的最常用方法是通过spark-submit。 在本章的前面，我们向您展示了如何运行spark-submit; 您只需指定选项，应用程序 JAR 或脚本以及相关参数： 12345678./bin/spark-submit \--class &lt;main-class&gt; \--master &lt;master-url&gt; \--deploy-mode &lt;deploy-mode&gt; \--conf &lt;key&gt;=&lt;value&gt; \... # other options&lt;application-jar-or-script&gt; \[application-arguments] You can always specify whether to run in client or cluster mode when you submit a Spark job with spark-submit. However, you should almost always favor running in cluster mode (or in client mode on the cluster itself) to reduce latency between the executors and the driver.当您使用 spark-submit 提交 Spark 作业时，您始终可以指定是以客户端还是群集模式运行。 但是，您几乎总是倾向于在群集模式下运行（或在集群本身的客户端模式下）以减少执行程序和驱动程序之间的延迟。When submitting applciations, pass a .py file in the place of a .jar, and add Python .zip, .egg, or .py to the search path with –py-files.提交 applciations 时，在 .jar 的位置传递 .py 文件，并使用 –py-files 将 Python .zip，.egg 或 .py 添加到搜索路径。For reference, Table 16-1 lists all of the available spark-submit options, including those that are particular to some cluster managers. To enumerate all these options yourself, run spark-submit with –help.作为参考，表16-1列出了所有可用的 spark-submit 选项，包括某些集群管理器特有的选项。 要自己枚举所有这些选项，请使用 –help 运行 spark-submit。Table 16-1. Spark submit help text| Parameter | Description || —————————- | ———————————————————— || –masterMASTER_URL | spark://host:port, mesos://host:port, yarn, or localSpark 连接的资源管理器 || –deploymodeDEPLOY_MODE | Whether to launch the driver program locally (“client”) or on one of the worker machines inside the cluster (“cluster”) (Default: client)是在本地（“client”）还是在集群内（“cluster”）的某个工作机器上启动驱动程序（默认值：client） || –classCLASS_NAME | Your application’s main class (for Java / Scala apps).您的应用程序的主类（适用于Java / Scala应用程序）。 || –name NAME | A name of your application.你的应用程序主类。 || –jars JARS | Comma-separated list of local JARs to include on the driver and executor classpaths.以逗号分隔的本地 JAR 列表，包含在驱动程序和执行程序类路径中。 || –packages | search the local Maven repo, then Maven Central and any additional remote repositories given by –repositories. The format for the coordinates should be groupId:artifactId:version.搜索本地 Maven 仓库，然后搜索 Maven Central 以及 --repositories 给出的任何其他远程仓库。 坐标的格式应为 groupId:artifactId:version。 || –exclude-packages | Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in –packages to avoid dependency conflicts.逗号分隔的 groupId:artifactId 列表，在解析 --packages 中提供的依赖项时排除，以避免依赖性冲突。 || –repositories | Comma-separated list of additional remote repositories to search for the Maven coordinates given with –packages.以逗号分隔的其他远程仓库列表，用于搜索 --packages 给出的Maven坐标 || –py-filesPY_FILES | Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps.以逗号分隔的 .zip，.egg 或 .py 文件列表，放在 Python 应用程序的 PYTHONPATH 上。 || –filesFILES | Comma-separated list of files to be placed in the working directory of each executor.以逗号分隔的文件列表，放在每个执行程序的工作目录中。 || –confPROP=VALUE | Arbitrary Spark configuration property.任意Spark配置属性 || –propertiesfile FILE | Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf.从中加载额外属性的文件的路径。 如果未指定，则会查找 conf/spark-defaults.conf。 || –driver-memory MEM | Memory for driver (e.g., 1000M, 2G) (Default: 1024M).驱动器的内存（默认：1024M）。 || –driver-java-options | Extra Java options to pass to the driver.传递给驱动器的额外 Java 选项。 || –driver-library-path | Extra library path entries to pass to the driver.要传递给驱动程序的额外库路径条目。 || –driver-class-path | Extra class path entries to pass to the driver. Note that JARs added with –jars are automatically included in the classpath.要传递给驱动程序的额外类路径条目。请注意，添加了 --jars 的 JAR 会自动包含在类路径中。 || –executor-memory MEM | Memory per executor (e.g., 1000M, 2G) (Default: 1G).每个执行器的内存（例如，1000M，2G）（默认值：1G）。 || –proxy-user NAME | User to impersonate when submitting the application. This argument does not work with –principal / keytab.用户在提交申请时进行冒充。此参数不适用于 --principal/keytab。 || –help, -h | Show this help message and exit.显示此帮助消息并退出。 || –verbose, -v | Print additional debug output.打印其他调试输出。 || –version | Print the version of current Spark.打印当前Spark的版本。 |There are some deployment-specific configurations as well (see Table 16-2).还有一些特定于部署的配置（参见表16-2）。| Cluster Managers | Modes | Conf | Description || —————- | ——- | ————————— | ———————————————————— || Standalone | Cluster | –driver-cores NUM | Cores for driver (Default: 1)驱动核心（默认值：1） || Standalone/Mesos | Cluster | –supervise | If given, restarts the driver on failure.如果给定，则在失败时重新启动驱动程序。 || Standalone/Mesos | Cluster | –killSUBMISSION_ID | If given, kills the driver specified.如果给定，则杀死指定的驱动程序。 || Standalone/Mesos | Cluster | –statusSUBMISSION_ID | If given, requests the status of the driver specified.如果给定，请求指定的驱动程序的状态。 || Standalone/Mesos | Either | –total-executor-cores NUM | Total cores for all executors.所有执行器（executor）的核心总数。 || Standalone/YARN | Either | –total-executor-cores NUM1 | Number of cores per executor. (Default: 1 in YARN mode or all available cores on the worker in standalone mode)每个执行器（executor）的核心数。 （默认值：YARN模式下为1或独立模式下工作线程上的所有可用核心） || YARN | Either | –driver-cores NUM | Number of cores used by the driver, only in cluster mode (Default: 1).驱动程序使用的核心数，仅在集群模式下（默认值：1）。 || YARN | Either | queue QUEUE_NAME | The YARN queue to submit to (Default: “default”).要提交的YARN队列（默认值：“默认”）。 || YARN | Either | –num-executors NUM | Number of executors to launch (Default: 2). If dynamic allocation is enabled, the initial number of executors will be at least NUM.要启动的执行程序数（默认值：2）。如果启用了动态分配，则执行程序的初始数量将至少为NUM。 || YARN | Either | –archivesARCHIVES | Comma-separated list of archives to be extracted into the working directory of each executor.以逗号分隔的档案列表，提取到每个执行程序的工作目录中。 || YARN | Either | –principal PRINCIPAL | Principal to be used to log in to KDC, while running on secure HDFS.Principal 用于在安全 HDFS 上运行时登录 KDC。 || YARN | Either | –keytabKEYTAB | The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the Application Master via the Secure Distributed Cache, for renewing the login tickets and the delegation tokens periodically.包含上面指定的主体的keytab的文件的完整路径。此密钥表将通过安全分布式缓存复制到运行Application Master的节点，以定期更新登录票证和委派令牌。 |最新的 Spark 应用程序提交帮助文档### Application Launch ExamplesWe already covered some local-mode application examples previously in this chapter, but it’s worth looking at how we use some of the aforementioned options, as well. Spark also includes several examples and demonstration applications in the examples directory that is included when you download Spark. If you’re stuck on how to use certain parameters, simply try them first on your local machine and use the SparkPi class as the main class:我们已经介绍了本章前面的一些本地模式应用程序示例，但是值得一看的是我们如何使用上述一些选项。 Spark还包含下载Spark时包含的示例目录中的几个示例和演示应用程序。 如果你坚持使用某些参数，只需先在本地机器上尝试它们，然后使用SparkPi类作为主类：1234567./bin/spark-submit \--class org.apache.spark.examples.SparkPi \--master spark://207.184.161.138:7077 \--executor-memory 20G \--total-executor-cores 100 \replace/with/path/to/examples.jar \1000The following snippet does the same for Python. You run it from the Spark directory and this will allow you to submit a Python application (all in one script) to the standalone cluster manager. You can also set the same executor limits as in the preceding example:以下代码段对Python也是如此。 您可以从Spark目录运行它，这将允许您将Python应用程序（所有在一个脚本中）提交给独立的集群管理器。 您还可以设置与前面示例中相同的执行器（executor）限制：1234./bin/spark-submit \--master spark://207.184.161.138:7077 \examples/src/main/python/pi.py \1000You can change this to run in local mode as well by setting the master to local or local[] to run on all the cores on your machine. You will also need to change the /path/to/examples.jar to the relevant Scala and Spark versions you are running.您可以将此更改为在本地模式下运行，方法是将主服务器设置为 local 或 local[] 以在计算机上的所有核心上运行。 您还需要将 /path/to/examples.jar 更改为您正在运行的相关Scala和Spark版本。## Configuring Applications 配置应用程序Spark includes a number of different configurations, some of which we covered in Chapter 15. There are many different configurations, depending on what you’re hoping to achieve. This section covers those very details. For the most part, this information is included for reference and is probably worth skimming only, unless you’re looking for something in particular. The majority of configurations fall into the following categories:Spark包含许多不同的配置，其中一些我们在第15章中介绍过。根据您希望实现的目标，有许多不同的配置。本节介绍了这些细节。在大多数情况下，这些信息仅供参考，可能仅值得略读，除非您特别寻找某些内容。大多数配置分为以下几类：- Application properties 应用属性- Runtime environment 运行环境- Shuffle behavior 洗牌行为- Spark UI- Compression and serialization 解压缩- Memory management 内存管理- Execution behavior 执行行为- Networking 网络- Scheduling 调度- Dynamic allocation 动态分配- Security 安全- Encryption 加密- Spark SQL- Spark streaming Spark流- SparkRSpark provides three locations to configure the system:Spark提供三个位置来配置系统：- Spark properties control most application parameters and can be set by using a SparkConf object Spark 属性控制大多数应用程序参数，可以使用 SparkConf 对象进行设置- Java system properties Java系统属性- Hardcoded configuration files 硬编码配置文件There are several templates that you can use, which you can find in the /conf directory available in the root of the Spark home folder. You can set these properties as hardcoded variables in your applications or by specifying them at runtime. You can use environment variables to set per-machine settings, such as the IP address, through the conf/spark-env.sh script on each node. Lastly, you can configure logging through log4j.properties.您可以使用几个模板，您可以在Spark主文件夹的根目录中的 /conf 目录中找到这些模板。您可以在应用程序中将这些属性设置为硬编码变量，也可以在运行时指定它们。您可以使用环境变量通过每个节点上的 conf/spark-env.sh 脚本设置每台计算机设置，例如IP地址。最后，您可以通过 log4j.properties 配置日志记录。### The SparkConfThe SparkConf manages all of our application configurations. You create one via the import statement, as shown in the example that follows. After you create it, the SparkConf is immutable for that specific Spark Application:SparkConf 管理我们的所有应用程序配置。您可以通过 import 语句创建一个，如下面的示例所示。创建它之后，SparkConf 对于特定的Spark应用程序是不可变的：1234// in Scalaimport org.apache.spark.SparkConfval conf = new SparkConf().setMaster("local[2]").setAppName("DefinitiveGuide").set("some.conf", "to.some.value")1234# in Pythonfrom pyspark import SparkConfconf = SparkConf().setMaster("local[2]").setAppName("DefinitiveGuide")\.set("some.conf", "to.some.value")You use the SparkConf to configure individual Spark Applications with Spark properties. These Spark properties control how the Spark Application runs and how the cluster is configured. The example that follows configures the local cluster to have two threads and specifies the application name that shows up in the Spark UI.您可以使用 SparkConf 使用 Spark 属性配置各个 Spark 应用程序。这些 Spark 属性控制 Spark 应用程序的运行方式以及集群的配置方式。以下示例将本地集群配置为具有两个线程，并指定在 Spark UI 中显示的应用程序名称。You can configure these at runtime, as you saw previously in this chapter through command-line arguments. This is helpful when starting a Spark Shell that will automatically include a basic Spark Application for you; for instance:您可以在运行时配置它们，如本章前面通过命令行参数所见。这在启动Spark Shell时非常有用，它将自动包含一个基本的Spark应用程序；例如：1./bin/spark-submit --name "DefinitiveGuide" --master local[4] ...Of note is that when setting time duration-based properties, you should use the following format:值得注意的是，在设置基于持续时间的属性时，您应该使用以下格式：- 25ms (milliseconds 毫秒)- 5s (seconds 秒)- 10m or 10min (minutes 分钟)- 3h (hours 小时)- 5d (days 天)- 1y (years 年)### Application Properties 应用属性Application properties are those that you set either from spark-submit or when you create your Spark Application. They define basic application metadata as well as some execution characteristics. Table 16-3 presents a list of current application properties.应用程序属性是您通过 spark-submit 或创建 Spark 应用程序时设置的属性。它们定义了基本的应用程序元数据以及一些执行特性。表 16-3 列出了当前的应用程序属性。Table 16-3. Application properties| Property name 属性名 | Default默认值 | Meaning 意思 || ————————– | ——————- | ———————————————————— || spark.app.name | (none) | The name of your application. This will appear in the UI and in log data.您的应用程序的名称。 这将显示在UI和日志数据中。 || spark.driver.cores | 1 | Number of cores to use for the driver process, only in cluster mode.仅在集群模式下用于驱动程序进程的核心数。 || spark.driver.maxResultSize | 1g | Limit of total size of serialized results of all partitions for each Spark action (e.g., collect). Should be at least 1M, or 0 for unlimited. Jobs will be aborted if the total size exceeds this limit. Having a high limit can cause OutOfMemoryErrors in the driver (depends on spark.driver.memory and memory overhead of objects in JVM). Setting a proper limit can protect the driver from OutOfMemoryErrors.每个Spark操作的所有分区的序列化结果的总大小限制（例如，collect）。 应至少为1M，或0为无限制。 如果总大小超过此限制，则将中止作业。 具有上限可能会导致驱动程序中的OutOfMemoryErrors（取决于spark.driver.memory和JVM中对象的内存开销）。 设置适当的限制可以保护驱动程序免受OutOfMemoryErrors的影响。 || spark.driver.memory | 1g | Amount of memory to use for the driver process, where SparkContext is initialized. (e.g. 1g, 2g). Note: in client mode, this must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, set this through the –driver-memory command-line option or in your default properties file.用于初始化 SparkContext 的驱动程序进程的内存量。 （例如1g，2g）。 注意：在客户端模式下，不能直接在应用程序中通过 SparkConf 设置，因为驱动程序JVM已在此时启动。 而是通过–driver-memory命令行选项或在默认属性文件中设置它。 || spark.executor.memory | 1g | Amount of memory to use per executor process (e.g., 2g, 8g).每个执行程序进程使用的内存量（例如，2g，8g）。 || spark.extraListeners | (none) | A comma-separated list of classes that implement SparkListener; when initializing SparkContext, instances of these classes will be created and registered with Spark’s listener bus. If a class has a single-argument constructor that accepts a SparkConf, that constructor will be called; otherwise, a zero-argument constructor will be called. If no valid constructor can be found, the SparkContext creation will fail with an exception.以逗号分隔的实现SparkListener的类列表; 在初始化SparkContext时，将创建这些类的实例并使用Spark的侦听器总线进行注册。 如果一个类有一个接受SparkConf的单参数构造函数，那么将调用该构造函数; 否则，将调用零参数构造函数。 如果找不到有效的构造函数，SparkContext创建将失败并出现异常。 || spark.logConf | (false) | Logs the effective SparkConf as INFO when a SparkContext is started.启动 SparkContext 时，将有效的 SparkConf 记录为INFO。 || spark.master | (none) | The cluster manager to connect to. See the list of allowed master URLs.要连接的集群管理器。 请参阅允许的主URL列表。 || spark.submit.deployMode | (none) | The deploy mode of the Spark driver program, either “client” or “cluster,” which means to launch driver program locally (“client”) or remotely (“cluster”) on one of the nodes inside the cluster.Spark驱动程序的部署模式，“客户端”或“集群”，这意味着在集群内的一个节点上本地（“客户端”）或远程（“集群”）启动驱动程序。 || spark.log.callerContext | (none) | Application information that will be written into Yarn RM log/HDFS audit log when running on Yarn/HDFS. Its length depends on the Hadoop configuration hadoop.caller.context.max.size. It should be concise, and typically can have up to 50 characters.在 Yarn/HDFS 上运行时将写入Yarn RM log / HDFS 审核日志的应用程序信息。 它的长度取决于Hadoop配置hadoop.caller.context.max.size。 它应该简洁，通常最多可包含50个字符。 || spark.driver.supervise | (false) | If true, restarts the driver automatically if it fails with a non-zero exit status. Only has effect in Spark standalone mode or Mesos cluster deploy mode.如果为true，则在失败且退出状态为非零时自动重新启动驱动程序。 仅在Spark独立模式或Mesos集群部署模式下有效。 |You can ensure that you’ve correctly set these values by checking the application’s web UI on port 4040 of the driver on the “Environment” tab. Only values explicitly specified through sparkdefaults.conf, SparkConf, or the command line will appear. For all other configuration properties, you can assume the default value is used.您可以通过在“环境”选项卡上检查驱动程序的端口4040上的应用程序的Web UI来确保您正确设置了这些值。仅显示那些通过 sparkdefaults.conf，SparkConf或命令行显式指定的值。对于所有其他配置属性，您可以假设使用默认值。### Runtime Properties 运行时属性Although less common, there are times when you might also need to configure the runtime environment of your application. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on the Runtime Environment in the Spark documentation. These properties allow you to configure extra classpaths and python paths for both drivers and executors, Python worker configurations, as well as miscellaneous logging properties.虽然不太常见，但有时您可能还需要配置应用程序的运行时环境。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中的 Runtime Environment 上的相关表。这些属性允许您对驱动程序（driver）和执行器（excutor），Python工作程序进行配置以及各种日志记录属性配置额外的类路径和python路径。### Execution Properties 执行属性These configurations are some of the most relevant for you to configure because they give you finer-grained control on actual execution. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Execution Behavior in the Spark documentation. The most common configurations to change are spark.executor.cores (to control the number of available cores) and spark.files.maxPartitionBytes (maximum partition size when reading files).这些配置与您配置最相关，因为它们可以为您提供对实际执行的精细控制。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中的执行行为相关表。要更改的最常见配置是 spark.executor.cores（用于控制可用内核的数量）和 spark.files.maxPartitionBytes（读取文件时的最大分区大小）。### Configuring Memory Management 配置内存管理There are times when you might need to manually manage the memory options to try and optimize your applications. Many of these are not particularly relevant for end users because they involve a lot of legacy concepts or fine-grained controls that were obviated in Spark 2.X because of automatic memory management. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Memory Management in the Spark documentation.有时您可能需要手动管理内存选项以尝试和优化您的应用程序。 其中许多与最终用户并不特别相关，因为它们涉及很多遗留概念或由于自动内存管理而在Spark 2.X中避免的细粒度控制。 由于篇幅限制，我们无法在此处包含整个配置集。 请参阅Spark文档中的内存管理相关表。### Configuring Shuffle Behavior 配置洗牌行为We’ve emphasized how shuffles can be a bottleneck in Spark jobs because of their high communication overhead. Therefore there are a number of low-level configurations for controlling shuffle behavior. Due to space limitations, we cannot include the entire configuration set here. Refer to the relevant table on Shuffle Behavior in the Spark documentation.我们已经强调了洗牌如何成为Spark工作的瓶颈，因为它们的通信开销很高。因此，存在许多用于控制洗牌行为的低阶配置。由于篇幅限制，我们无法在此处包含整个配置集。请参阅Spark文档中有关洗牌行为的相关表。### Environmental Variables 环境变量You can configure certain Spark settings through environment variables, which are read from the conf/spark-env.sh script in the directory where Spark is installed (or conf/spark-env.cmd on Windows). In Standalone and Mesos modes, this file can give machine-specific information such as hostnames. It is also sourced when running local Spark Applications or submission scripts. Note that conf/spark-env.sh does not exist by default when Spark is installed. However, you can copy conf/spark-env.sh.template to create it. Be sure to make the copy executable.您可以通过环境变量配置某些Spark设置，这些环境变量是从安装Spark的目录中的 conf/spark-env.sh 脚本（或Windows上的 conf/spark-env.cmd）中读取的。 在Standalone和Mesos模式下，此文件可以提供特定于机器的信息，例如主机名。 它还在运行本地Spark应用程序或提交脚本时获取。 请注意，安装Spark时默认情况下不存在conf/spark-env.sh。 但是，您可以复制 conf/spark-env.sh.template 来创建它。 务必使副本可执行。The following variables can be set in spark-env.sh:可以在spark-env.sh中设置以下变量：JAVA_HOME ​ Location where Java is installed (if it’s not on your default PATH).​ 安装Java的位置（如果它不在您的默认PATH上）。 PYSPARK_PYTHON ​ Python binary executable to use for PySpark in both driver and workers (default is python2.7 if available; otherwise, python). Property spark.pyspark.python takes precedence if it is set.​ 在驱动程序和工作程序中用于 PySpark 的 Python 二进制可执行文件（如果可用，默认为 python2.7; 否则为python）。 如果设置了属性 spark.pyspark.python，则优先级。 PYSPARK_DRIVER_PYTHON ​ Python binary executable to use for PySpark in driver only (default is PYSPARK_PYTHON). Property spark.pyspark.driver.python takes precedence if it is set.​ Python二进制可执行文件仅用于驱动程序中的PySpark（默认为PYSPARK_PYTHON）。 如果设置了属性spark.pyspark.driver.python，则优先级。 SPARKR_DRIVER_R ​ R binary executable to use for SparkR shell (default is R). Property spark.r.shell.command takes precedence if it is set.​ 用于SparkR shell的R二进制可执行文件（默认为R）。 如果设置了属性spark.r.shell.command优先。 SPARK_LOCAL_IP ​ IP address of the machine to which to bind.​ 要绑定的计算机的IP地址。 SPARK_PUBLIC_DNS ​ Hostname your Spark program will advertise to other machines.​ 您的Spark程序的主机名将通告给其他计算机。 In addition to the variables ust listed, there are also options for setting up the Spark standalone cluster scripts, such as number of cores to use on each machine and maximum memory. Because spark-env.sh is a shell script, you can set some of these programmatically; for example, you might compute SPARK_LOCAL_IP by looking up the IP of a specific network interface. 除了列出的变量之外，还有用于设置Spark独立集群脚本的选项，例如每台计算机上使用的核心数和最大内存。因为spark-env.sh是一个shell脚本，你可以通过编程方式设置其中一些;例如，您可以通过查找特定网络接口的IP来计算SPARK_LOCAL_IP。—NOTE 注意When running Spark on YARN in cluster mode, you need to set environment variables by using the spark.yarn.appMasterEnv.[EnvironmentVariableName] property in your conf/spark-defaults.conf file. Environment variables that are set in spark-env.sh will not be reflected in the YARN Application Master process in cluster mode. See the YARN-related Spark Properties for more information.在集群模式下在 YARN 上运行Spark时，需要使用 conf/spark-defaults.conf 文件中的spark.yarn.appMasterEnv.[EnvironmentVariableName] 属性设置环境变量。在 spark-env.sh 中设置的环境变量不会在集群模式下反映在 YARN Application Master 进程中。有关更多信息，请参阅与 YARN 相关的 Spark属性。—### Job Scheduling Within an Application 应用程序内的作业调度Within a given Spark Application, multiple parallel jobs can run simultaneously if they were submitted from separate threads. By job, in this section, we mean a Spark action and any tasks that need to run to evaluate that action. Spark’s scheduler is fully thread-safe and supports this use case to enable applications that serve multiple requests (e.g., queries for multiple users). By default, Spark’s scheduler runs jobs in FIFO fashion. If the jobs at the head of the queue don’t need to use the entire cluster, later jobs can begin to run right away, but if the jobs at the head of the queue are large, later jobs might be delayed significantly.在给定的 Spark 应用程序中，如果从不同的的线程提交多个并行作业，则它们可以同时运行。按照作业，在本节中，我们指的是 Spark 操作以及需要运行以评估该操作的任何任务。 Spark 的调度程序是完全线程安全的，并支持此用户案例以支持提供多个请求的应用程序（例如，为多个用户进行查询）。默认情况下，Spark 的调度程序以FIFO方式运行作业。如果队列头部的作业不需要使用整个集群，则以后的作业可以立即开始运行，但如果队列头部的作业很大，则后续作业可能会显着延迟。It is also possible to configure fair sharing between jobs. Under fair sharing, Spark assigns tasks between jobs in a round-robin fashion so that all jobs get a roughly equal share of cluster resources. This means that short jobs submitted while a long job is running can begin receiving resources right away and still achieve good response times without waiting for the long job to finish. This mode is best for multiuser settings.也可以在作业之间配置公平共享。在公平共享下，Spark以循环方式在作业之间分配任务，以便所有作业获得大致相等的集群资源份额。这意味着当长期运行的工作正在执行时所提交的短期工作可以立即开始接收资源，并且仍然可以实现良好的响应时间，而无需等待长时间的工作完成。此模式最适合多用户设置。To enable the fair scheduler, set the spark.scheduler.mode property to FAIR when configuring a SparkContext.要启用公平调度器，请在配置 SparkContext 时将 spark.scheduler.mode 属性设置为 FAIR。123val conf = new SparkConf().setMaster(...).setAppName(...)conf.set("spark.scheduler.mode", "FAIR")val sc = new SparkContext(conf)The fair scheduler also supports grouping jobs into pools, and setting different scheduling options, or weights, for each pool. This can be useful to create a high-priority pool for more important jobs or to group the jobs of each user together and give users equal shares regardless of how many concurrent jobs they have instead of giving jobs equal shares. This approach is modeled after the Hadoop Fair Scheduler.公平调度器还支持将作业分组到池中，并为每个池设置不同的调度选项或权重。这对于为更重要的作业创建高优先级池或将每个用户的作业组合在一起并为用户提供相同的份额非常有用，无论他们有多少并发作业而不是给予作业相等的份额。此方法模拟Hadoop Fair Scheduler。Without any intervention, newly submitted jobs go into a default pool, but jobs pools can be set by adding the spark.scheduler.pool local property to the SparkContext in the thread that’s submitting them. This is done as follows (assuming sc is your SparkContext )：在没有任何干预的情况下，新提交的作业将进入默认池，但可以通过将 spark.scheduler.pool 本地属性添加到提交它们的线程中的 SparkContext 来设置作业池。这是完成如下（假设sc是你的 SparkContext )： 1sc.setLocalProperty("spark.scheduler.pool", "pool1") After setting this local property, all jobs submitted within this thread will use this pool name. The setting is per-thread to make it easy to have a thread run multiple jobs on behalf of the same user. If you’d like to clear the pool that a thread is associated with, set it to null. 设置此本地属性后，此线程中提交的所有作业都将使用此池名称。该设置是每个线程，以便让线程代表同一个用户运行多个作业变得容易。如果要清除与线程关联的池，请将其设置为null。 Conclusion 结论This chapter covered a lot about Spark Applications; we learned how to write, test, run, and configure them in all of Spark’s languages. In Chapter 17, we talk about deploying and the cluster management options you have when it comes to running Spark Applications. 本章介绍了 Spark 应用程序；我们学习了如何使用Spark的所有语言编写，测试，运行和配置它们。在第17章中，我们将讨论在运行 Spark 应用程序时的部署和集群管理选项。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译 Chapter 15 How Spark Runs on a Cluster]]></title>
    <url>%2F2019%2F08%2F05%2FChapter15_HowSparkRuns-on-a-Cluster(SparkTheDefinitiveGuide)_online%2F</url>
    <content type="text"><![CDATA[Chapter 15 How Spark Runs on a Cluster Spark如何在集群上的运行Thus far in the book, we focused on Spark’s properties as a programming interface. We have discussed how the structured APIs take a logical operation, break it up into a logical plan, and convert that to a physical plan that actually consists of Resilient Distributed Dataset (RDD) operations that execute across the cluster of machines. This chapter focuses on what happens when Spark goes about executing that code. We discuss this in an implementation-agnostic way—this depends on neither the cluster manager that you’re using nor the code that you’re running. At the end of the day, all Spark code runs the same way. 到目前为止，在书中，我们将重点放在Spark作为编程接口的属性上。我们已经讨论了结构化API如何执行逻辑操作，将其分解为逻辑计划，并将其转换为实际由跨机器集群执行的弹性分布式数据集（RDD）操作组成的物理计划。本章主要讨论 Spark 执行该代码时会发生什么。我们以一种不知实现的方式讨论这个问题，这既不依赖于您正在使用的集群管理器，也不依赖于您正在运行的代码。一天结束时，所有 Spark 代码都以相同的方式运行。 This chapter covers several key topics: 本章包括几个关键主题： The architecture and components of a Spark Application Spark应用程序的体系结构和组件 The life cycle of a Spark Application inside and outside of Spark Spark 内外 Spark 应用的生命周期 Important low-level execution properties, such as pipelining 重要的低级执行属性，如管道 What it takes to run a Spark Application, as a segue into Chapter 16. 运行Spark应用程序需要什么，作为转到第16章的桥接。 Let’s begin with the architecture. 让我们从架构开始 The Architecture of a Spark Application Spark应用程序的架构In Chapter 2, we discussed some of the high-level components of a Spark Application. Let’s review those again: 在第2章中，我们讨论了 Spark 应用程序的一些高级组件。让我们再次回顾一下： The Spark driver Spark驱动器 The driver is the process “in the driver seat” of your Spark Application. It is the controller of the execution of a Spark Application and maintains all of the state of the Spark cluster (the state and tasks of the executors). It must interface with the cluster manager in order to actually get physical resources and launch executors. At the end of the day, this is just a process on a physical machine that is responsible for maintaining the state of the application running on the cluster. 驱动器是 Spark 应用程序处在“驾驶员席位”的进程。它是Spark应用程序执行的控制器，维护Spark集群的所有状态（执行器的状态和任务）。它必须与群集管理器接口，以便实际获得物理资源和启动执行器。最后，这只是一个物理机器上的进程，负责维护集群上运行的应用程序的状态。 The Spark executors Spark执行器 Spark executors are the processes that perform the tasks assigned by the Spark driver. Executors have one core responsibility: take the tasks assigned by the driver, run them, and report back their state (success or failure) and results. Each Spark Application has its own separate executor processes. Spark 执行器是执行 Spark 驱动程序分配的任务的进程。执行者有一个核心责任：承担驱动程序分配的任务，运行它们，并报告它们的状态（成功或失败）和结果。每个Spark应用程序都有自己的独立执行器进程。 The cluster manager 集群管理员 The Spark Driver and Executors do not exist in a void, and this is where the cluster manager comes in. The cluster manager is responsible for maintaining a cluster of machines that will run your Spark Application(s). Somewhat confusingly, a cluster manager will have its own “driver” (sometimes called master) and “worker” abstractions. The core difference is that these are tied to physical machines rather than processes (as they are in Spark). Figure 15-1 shows a basic cluster setup. The machine on the left of the illustration is the Cluster Manager Driver Node. The circles represent daemon processes running on and managing each of the individual worker nodes. There is no Spark Application running as of yet—these are just the processes from the cluster manager. Spark驱动程序和执行器不存在于一个空间，这就是集群管理器所处的位置。集群管理器负责维护运行Spark应用程序的机器集群。有些令人困惑的是，集群管理器将有自己的“驱动程序（driver）”（有时称为master）和“工作者（worker）”的抽象结构。核心区别在于，它们与物理机器而不是进程（如 Spark 中的进程）联系在一起。图15-1显示了一个基本的集群设置。图左侧的机器是群集管理器驱动程序节点。圆圈表示运行在每个工作节点上并管理每个工作节点的守护进程。到目前为止还没有运行spark应用程序，这些只是来自集群管理器的进程。When it comes time to actually run a Spark Application, we request resources from the cluster manager to run it. Depending on how our application is configured, this can include a place to run the Spark driver or might be just resources for the executors for our Spark Application. Over the course of Spark Application execution, the cluster manager will be responsible for managing the underlying machines that our application is running on.在实际运行Spark应用程序时，我们从集群管理器请求资源来运行它。根据应用程序的配置方式，这可能包括一个运行Spark驱动程序的位置，或者可能只是Spark应用程序的执行者的资源。在Spark应用程序执行过程中，集群管理员将负责管理应用程序运行的底层机器。Spark currently supports three cluster managers: a simple built-in standalone cluster manager, Apache Mesos, and Hadoop YARN. However, this list will continue to grow, so be sure to check the documentation for your favorite cluster manager. Now that we’ve covered the basic components of an application, let’s walk through one of the first choices you will need to make when running your applications: choosing the execution mode.Spark目前支持三个集群管理器：一个简单的内置独立集群管理器、Apache Mesos 和 Hadoop Yarn。但是，这个列表将继续增长，因此一定要检查您最喜欢的集群管理器的文档。既然我们已经介绍了应用程序的基本组件，那么让我们来看看在运行应用程序时需要做的第一个选择：选择执行模式。### Execution Modes 执行模式An execution mode gives you the power to determine where the aforementioned resources are physically located when you go to run your application. You have three modes to choose from:执行模式使您能够在运行应用程序时确定上述资源的物理位置。您有三种模式可供选择：- Cluster mode- Client mode- Local modeWe will walk through each of these in detail using Figure 15-1 as a template. In the following section, rectangles with solid borders represent Spark driver process whereas those with dotted borders represent the executor processes.我们将使用图15-1作为模板详细介绍每种方法。在下面的部分中，带实心边框的矩形表示 Spark 驱动程序进程，而带虚线边框的矩形表示执行程序进程。#### Cluster mode 集群模式Cluster mode is probably the most common way of running Spark Applications. In cluster mode, a user submits a pre-compiled JAR, Python script, or R script to a cluster manager. The cluster manager then launches the driver process on a worker node inside the cluster, in addition to the executor processes. This means that the cluster manager is responsible for maintaining all Spark Application–related processes. Figure 15-2 shows that the cluster manager placed our driver on a worker node and the executors on other worker nodes.集群模式可能是运行Spark应用程序的最常见方式。在集群模式下，用户向集群管理器提交预编译的JAR、Python脚本或R脚本。然后，除了执行器进程之外，集群管理员在集群内的工作节点上启动驱动程序进程。这意味着集群管理员负责维护所有与Spark应用程序相关的流程。图15-2显示集群管理器将我们的驱动程序放在一个工作节点上，而执行器放在其他工作节点上。#### Client mode 客户端模式Client mode is nearly the same as cluster mode except that the Spark driver remains on the client machine that submitted the application. This means that the client machine is responsible for maintaining the Spark driver process, and the cluster manager maintains the executor processses. In Figure 15-3, we are running the Spark Application from a machine that is not colocated on the cluster. These machines are commonly referred to as gateway machines or edge nodes. In Figure 15-3, you can see that the driver is running on a machine outside of the cluster but that the workers are located on machines in the cluster.客户端模式与集群模式几乎相同，只是Spark驱动程序保留在提交应用程序的客户端上。这意味着客户端负责维护Spark 驱动程序进程，集群管理员维护执行器进程。在图15-3中，我们运行的Spark应用程序来自一台未在集群上并置的机器。这些机器通常被称为网关机器或边缘节点。在图15-3中，您可以看到驱动程序（driver）在集群外部的一台机器上运行，但工作人员（worker）位于集群中的机器上。#### Local mode 当地模式Local mode is a significant departure from the previous two modes: it runs the entire Spark Application on a single machine. It achieves parallelism through threads on that single machine. This is a common way to learn Spark, to test your applications, or experiment iteratively with local development. However, we do not recommend using local mode for running production applications.本地模式与前两种模式有很大的不同：它在一台机器上运行整个Spark应用程序。它通过单个机器上的线程实现并行性。这是学习Spark、测试应用程序或使用本地开发进行迭代实验的常用方法。但是，我们不建议在运行生产应用程序时使用本地模式。## The Life Cycle of a Spark Application (Outside Spark) Spark 应用的生命周期（Spark外部）This chapter has thus far covered the vocabulary necessary for discussing Spark Applications. It’s now time to talk about the overall life cycle of Spark Applications from “outside” the actual Spark code. We will do this with an illustrated example of an application run with spark-submit (introduced in Chapter 3). We assume that a cluster is already running with four nodes, a driver (not a Spark driver but cluster manager driver) and three worker nodes. The actual cluster manager does not matter at this point: this section uses the vocabulary from the previous section to walk through a step-by-step Spark Application life cycle from initialization to program exit.本章迄今为止涵盖了讨论 Spark 应用程序所需的词汇。现在是时候从实际的 Spark 代码“外部”来讨论 Spark 应用程序的整个生命周期了。我们将通过一个使用 spark-submit 运行的应用程序的示例（在第3章中介绍）来实现这一点。我们假设一个集群已经运行了四个节点、一个驱动程序（不是 Spark 驱动程序，而是集群管理器驱动程序）和三个工作节点。此时，实际的集群管理器并不重要：本节使用上一节中的词汇表逐步遍历从初始化到程序退出的 Spark 应用程序生命周期。—NOTE 注意This section also makes use of illustrations and follows the same notation that we introduced previously. Additionally, we now introduce lines that represent network communication. Darker arrows represent communication by Spark or Spark related processes, whereas dashed lines represent more general communication (like cluster management communication).本节还使用了插图，并遵循我们前面介绍的相同的符号。此外，我们现在引入表示网络通信的线。较暗的箭头表示通过 Spark 或 Spark 相关进程进行的通信，而虚线表示更一般的通信（如集群管理通信）。—### Client Request 客户端请求The first step is for you to submit an actual application. This will be a pre-compiled JAR or library. At this point, you are executing code on your local machine and you’re going to make a request to the cluster manager driver node (Figure 15-4). Here, we are explicitly asking for resources for the Spark driver process only. We assume that the cluster manager accepts this offer and places the driver onto a node in the cluster. The client process that submitted the original job exits and the application is off and running on the cluster.第一步是提交实际的申请。这将是一个预编译的 JAR 或库。此时，您正在本地计算机上执行代码，并将向集群管理员驱动程序节点发出请求（图15-4）。这里，我们明确地要求只为 Spark 驱动程序进程提供资源。我们假设集群管理员接受这个提议，并将驱动程序放在集群中的一个节点上。提交原始作业的客户端进程退出，应用程序在集群上关闭并运行。To do this, you’ll run something like the following command in your terminal:为此，您将在终端中运行如下命令：12345678./bin/spark-submit \--class &lt;main-class&gt; \--master &lt;master-url&gt; \--deploy-mode cluster \--conf &lt;key&gt;=&lt;value&gt; \... # other options&lt;application-jar&gt; \[application-arguments]### Launch 启动（应用程序）Now that the driver process has been placed on the cluster, it begins running user code (Figure 15-5). This code must include a SparkSession that initializes a Spark cluster (e.g., driver + executors). The SparkSession will subsequently communicate with the cluster manager (the darker line), asking it to launch Spark executor processes across the cluster (the lighter lines). The number of executors and their relevant configurations are set by the user via the command-line arguments in the original spark-submit call.现在驱动程序进程已经放置在集群上，它开始运行用户代码（图15-5）。此代码必须包含初始化 Spark 集群的SparkSession（例如，驱动程序+执行器）。SparkSession 随后将与集群管理器（较暗的线）通信，要求它在集群中启动 Spark executor进程（较亮的线）。执行器（executor）的数量及其相关配置由用户通过原始 spark-submit 调用中的命令行参数设置。The cluster manager responds by launching the executor processes (assuming all goes well) and sends the relevant information about their locations to the driver process. After everything is hooked upcorrectly, we have a “Spark Cluster” as you likely think of it today.集群管理器通过启动执行器进程（假设一切正常）进行响应，并将有关其位置的相关信息发送到驱动程序进程。在所有的东西都连接正确之后，我们就有了一个“Spark 集群”，就像你今天想象的那样。### Execution 执行Now that we have a “Spark Cluster,” Spark goes about its merry way executing code, as shown in Figure 15-6. The driver and the workers communicate among themselves, executing code and moving data around. The driver schedules tasks onto each worker, and each worker responds with the status of those tasks and success or failure. (We cover these details shortly.)既然我们有了一个“Spark 集群”，Spark就会以一种愉快的方式执行代码，如图15-6所示。驱动程序和工作人员（workers ）之间进行通信，执行代码并移动数据。驱动程序将任务调度到每个工作人员（workers ）身上，每个工作人员对这些任务的状态以及成功或失败作出响应。（我们将很快介绍这些细节。）### Completion 完成 After a Spark Application completes, the driver processs exits with either success or failure (Figure 15-7). The cluster manager then shuts down the executors in that Spark cluster for the driver. At this point, you can see the success or failure of the Spark Application by asking the cluster manager for this information. Spark应用程序完成后，驱动程序进程以成功或失败退出（图15-7）。然后，集群管理员为驱动程序关闭该 Spark 集群中的执行器。此时，通过向集群管理器询问这些信息，您可以看到 Spark 应用程序的成功或失败。 The Life Cycle of a Spark Application (Inside Spark) Spark应用程序的生命周期（Spark内部）We just examined the life cycle of a Spark Application outside of user code (basically the infrastructure that supports Spark), but it’s arguably more important to talk about what happens within Spark when you run an application. This is “user-code” (the actual code that you write that defines your Spark Application). Each application is made up of one or more Spark jobs. Spark jobs within an application are executed serially (unless you use threading to launch multiple actions in parallel). 我们刚刚研究了 Spark 应用程序在用户代码之外的生命周期（基本上是支持 Spark 的基础设施），但是讨论运行应用程序时 Spark 内发生的事情可能更重要。这是“用户代码”（定义 Spark 应用程序的实际代码）。每个应用程序由一个或多个 Spark 作业组成。应用程序中的 Spark 作业是串行执行的（除非使用线程并行启动多个操作）。 The SparkSessionThe first step of any Spark Application is creating a SparkSession. In many interactive modes, this is done for you, but in an application, you must do it manually. Some of your legacy code might use the new SparkContext pattern. This should be avoided in favor of the builder method on the SparkSession, which more robustly instantiates the Spark and SQL Contexts and ensures that there is no context conflict, given that there might be multiple libraries trying to create a session in the same Spark Appication: 任何Spark应用程序的第一步都是创建 SparkSession。在许多交互模式中，这是为您完成的，但在应用程序中，您必须手动完成。一些遗留代码可能使用新的 SparkContext 模式。应该避免这样做，因为 SparkSession 上的builder方法更能有力地实例化 Spark 和 SQL 上下文，并确保没有上下文冲突，因为可能有多个库试图在同一Spark应用程序中创建会话：1234// Creating a SparkSession in Scalaimport org.apache.spark.sql.SparkSessionval spark = SparkSession.builder().appName("Databricks Spark Example").config("spark.sql.warehouse.dir", "/user/hive/warehouse").getOrCreate()12345# Creating a SparkSession in Pythonfrom pyspark.sql import SparkSessionspark = SparkSession.builder.master("local").appName("Word Count")\.config("spark.some.config.option", "some-value")\.getOrCreate()After you have a SparkSession, you should be able to run your Spark code. From the SparkSession, you can access all of low-level and legacy contexts and configurations accordingly, as well. Note that the SparkSession class was only added in Spark 2.X. Older code you might find would instead directly create a SparkContext and a SQLContext for the structured APIs.在进行 SparkSession 之后，您应该能够运行spark代码。通过 SparkSession，您还可以相应地访问所有低阶和遗留上下文和配置。请注意，SparkSession 类只添加在 Spark 2.x 中。您可能会发现，较旧的代码将直接为结构化API创建 SparkContext 和 sqlContext。#### The SparkContext A SparkContext object within the SparkSession represents the connection to the Spark cluster. This class is how you communicate with some of Spark’s lower-level APIs, such as RDDs. It is commonly stored as the variable sc in older examples and documentation. Through a SparkContext, you can create RDDs, accumulators, and broadcast variables, and you can run code on the cluster. For the most part, you should not need to explicitly initialize a SparkContext; you should just be able to access it through the SparkSession. If you do want to, you should create it in the most general way, through the getOrCreate method: SparkSession 中的 SparkContext 对象表示与 Spark 群集的连接。这个类是如何与Spark的一些低阶API（如RDD）通信的。在旧的示例和文档中，它通常存储为变量 sc 。通过 SparkContext，您可以创建RDD、累加器（accumulators）和广播（broadcast）变量，并且可以在集群上运行代码。在大多数情况下，您不需要显式初始化 SparkContext；您只需要能够通过 SparkSession 访问它。如果您愿意，您应该通过 getOrCreate 方法以最一般的方式创建它：123// in Scalaimport org.apache.spark.SparkContextval sc = SparkContext.getOrCreate()THE SPARKSESSION, SQLCONTEXT, AND HIVECONTEXT In previous versions of Spark, the SQLContext and HiveContext provided the ability to work with DataFrames and Spark SQL and were commonly stored as the variable sqlContext in examples, documentation, and legacy code. As a historical point, Spark 1.X had effectively two contexts. The SparkContext and the SQLContext. These two each performed different things. The former focused on more fine-grained control of Spark’s central abstractions, whereas the latter focused on the higher-level tools like Spark SQL. In Spark 2.X, the communtiy combined the two APIs into the centralized SparkSession that we have today. However, both of these APIs still exist and you can access them via the SparkSession. It is important to note that you should never need to use the SQLContext and rarely need to use the SparkContext. 在Spark的早期版本中，SQLContext 和 HiveContext提供了使用 DataFrame 和 Spark SQL的能力，并且通常作为变量SQLContext存储在示例、文档和旧代码中。作为一个历史点，spark 1.x实际上有两个上下文。SparkContext和SQLContext。这两个人的表现各不相同。前者侧重于对Spark的中心抽象进行更细粒度的控制，而后者则侧重于更高级的工具，如Spark SQL。在spark 2.x中，社区将这两个API合并到了我们今天使用的集中式 SparkSession 中。但是，这两个API仍然存在，您可以通过SparkSession访问它们。需要注意的是，您不应该需要使用 SQLContext，而且很少需要使用 SparkContext。After you initialize your SparkSession, it’s time to execute some code. As we know from previous chapters, all Spark code compiles down to RDDs. Therefore, in the next section, we will take some logical instructions (a DataFrame job) and walk through, step by step, what happens over time.初始化 SparkSession 之后，该执行一些代码了。正如我们从前面的章节所知道的，所有 Spark 代码都编译成RDD。因此，在下一节中，我们将接受一些逻辑指令（一个 DataFrame 作业）并逐步了解随着时间的推移会发生什么。### Logical Instructions 逻辑指令As you saw in the beginning of the book, Spark code essentially consists of transformations and actions. How you build these is up to you—whether it’s through SQL, low-level RDD manipulation, or machine learning algorithms. Understanding how we take declarative instructions like DataFrames and convert them into physical execution plans is an important step to understanding how Spark runs on a cluster. In this section, be sure to run this in a fresh environment (a new Spark shell) to follow along with the job, stage, and task numbers.正如您在书的开头所看到的，Spark代码基本上由转换（transformation ）和动作（action）组成。无论是通过SQL、低阶的RDD操作还是机器学习算法，如何构建这些都取决于您。了解我们如何使用声明性指令（如DataFrame）并将其转换为物理执行计划是了解Spark如何在集群上运行的重要步骤。在本节中，请确保在新的环境（新的 Spark shell）中运行此程序，以跟踪作业（job）、阶段（stage）和任务（task）编号。### Logical instructions to physical execution 物理执行的逻辑指令 We mentioned this in Part II, but it’s worth reiterating so that you can better understand how Spark takes your code and actually runs the commands on the cluster. We will walk through some more code, line by line, explain what’s happening behind the scenes so that you can walk away with a better understanding of your Spark Applications. In later chapters, when we discuss monitoring, we will perform a more detailed tracking of a Spark job through the Spark UI. In this current example, we’ll take a simpler approach. We are going to do a three-step job: using a simple DataFrame, we’ll repartition it, perform a value-by-value manipulation, and then aggregate some values and collect the final result. 我们在第二部分中提到了这一点，但是值得重申，这样您就可以更好地理解 Spark 是如何使用代码并在集群上实际运行命令的。我们将一行一行地介绍更多的代码，解释幕后发生的事情，以便您能够更好地了解 Spark 应用程序。在后面的章节中，当我们讨论监控时，我们将通过 Spark UI 对 Spark 作业执行更详细的跟踪。在当前的示例中，我们将采用更简单的方法。我们要做一个三步的工作：使用一个简单的数据框架，我们将对它重新分区，执行一个值一个值的操作，然后聚合一些值并收集最终的结果。 NOTE 注意 This code was written and runs with Spark 2.2 in Python (you’ll get the same result in Scala, so we’ve omitted it). The number of jobs is unlikely to change drastically but there might be improvements to Spark’s underlying optimizations that change physical execution strategies. 这段代码是用 Python 中的 Spark 2.2 编写和运行的（您将在 Scala 中得到相同的结果，所以我们省略了它）。工作数量不太可能大幅度改变，但 Spark 的底层优化可能会有所改进，从而改变物理执行策略。 123456789# in Pythondf1 = spark.range(2, 10000000, 2)df2 = spark.range(2, 10000000, 4)step1 = df1.repartition(5)step12 = df2.repartition(6)step2 = step1.selectExpr("id * 5 as id")step3 = step2.join(step12, ["id"])step4 = step3.selectExpr("sum(id)")step4.collect() # 2500000000000 When you run this code, we can see that your action triggers one complete Spark job. Let’s take a look at the explain plan to ground our understanding of the physical execution plan. We can access this information on the SQL tab (after we actually run a query) in the Spark UI, as well: 当您运行此代码时，我们可以看到您的操作触发了一个完整的Spark作业。让我们看一下解释计划，以加深我们对实际执行计划的理解。我们可以在Spark UI中的SQL选项卡（在实际运行查询之后）上访问这些信息，以及： 1step4.explain() 123456789101112131415== Physical Plan ==*HashAggregate(keys=[], functions=[sum(id#15L)])+- Exchange SinglePartition +- *HashAggregate(keys=[], functions=[partial_sum(id#15L)]) +- *Project [id#15L] +- *SortMergeJoin [id#15L], [id#10L], Inner :- *Sort [id#15L ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(id#15L, 200) : +- *Project [(id#7L * 5) AS id#15L] : +- Exchange RoundRobinPartitioning(5) : +- *Range (2, 10000000, step=2, splits=8) +- *Sort [id#10L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(id#10L, 200) +- Exchange RoundRobinPartitioning(6) +- *Range (2, 10000000, step=4, splits=8) What you have when you call collect (or any action) is the execution of a Spark job that individually consist of stages and tasks. Go to localhost:4040 if you are running this on your local machine to see the Spark UI. We will follow along on the “jobs” tab eventually jumping to stages and tasks as we proceed to further levels of detail. 当您调用 collect（或任何操作）时，您所拥有的是 Spark作业的执行，它分别由阶段（stage）和任务（task）组成。如果您在本地机器上运行这个程序，请转到 localhost:4040 查看Spark用户界面。我们将继续关注“jobs”选项卡，最终跳到阶段（stage）和任务（task），继续深入到更详细的层次。 A Spark Job 一个Spark作业In general, there should be one Spark job for one action. Actions always return results. Each job breaks down into a series of stages, the number of which depends on how many shuffle operations need to take place. 通常，一个动作（action）应该有一个Spark作业。操作始终返回结果。每项工作分为一系列阶段（stage），其数量取决于需要进行多少次洗牌（shuffle）操作。 This job breaks down into the following stages and tasks:这项工作分为以下几个阶段（stage）和任务（task）： Stage 1 with 8 Tasks第1阶段，8个任务 Stage 2 with 8 Tasks第2阶段，8个任务 Stage 3 with 6 Tasks第3阶段有6个任务 Stage 4 with 5 Tasks第4阶段有5个任务 Stage 5 with 200 Tasks第5阶段，200个任务 Stage 6 with 1 Task第6阶段，1个任务 I hope you’re at least somewhat confused about how we got to these numbers so that we can take the time to better understand what is going on! 我希望你至少对我们如何得到这些数字感到困惑，以便我们可以花时间更好地了解正在发生的事情！ Stages 阶段Stages in Spark represent groups of tasks that can be executed together to compute the same operation on multiple machines. In general, Spark will try to pack as much work as possible (i.e., as many transformations as possible inside your job) into the same stage, but the engine starts new stages after operations called shuffles. A shuffle represents a physical repartitioning of the data—for example, sorting a DataFrame, or grouping data that was loaded from a file by key (which requires sending records with the same key to the same node). This type of repartitioning requires coordinating across executors to move data around. Spark starts a new stage after each shuffle, and keeps track of what order the stages must run in to compute the final result. Spark中的阶段（stage）表示可以一起执行以在多台计算机上计算相同操作的任务（task）组。一般来说，Spark会尝试将尽可能多的工作（即工作中尽可能多的转换）打包到同一个阶段（stage），但引擎会在称为洗牌（shuffle）的操作后启动新的阶段（stage）。 shuffle 表示数据的物理重新分区——例如，对 DataFrame 进行排序，或者根据键（key）分组从文件加载的数据（这需要将具有相同键的记录发送到同一节点）。这种类型的重新分区需要跨执行器（executor）进行协调以移动数据。 Spark在每次shuffle之后开始一个新阶段（stage），并跟踪阶段（stage）必须运行的顺序以计算最终结果。 In the job we looked at earlier, the first two stages correspond to the range that you perform in order to create your DataFrames. By default when you create a DataFrame with range, it has eight partitions. The next step is the repartitioning. This changes the number of partitions by shuffling the data. These DataFrames are shuffled into six partitions and five partitions, corresponding to the number of tasks in stages 3 and 4. 在我们之前查看的工作中，前两个阶段（stage）对应于您为创建DataFrame而执行的范围（range）。默认情况下，当您使用范围（range）创建DataFrame时，它有八个分区。下一步是重新分区（repartitioning）。这会通过对数据洗牌（shuffle）来更改分区数。这些DataFrame被洗牌到六个分区和五个分区，对应于阶段3和4中的任务数。 Stages 3 and 4 perform on each of those DataFrames and the end of the stage represents the join (a shuffle). Suddenly, we have 200 tasks. This is because of a Spark SQL configuration. The spark.sql.shuffle.partitions default value is 200, which means that when there is a shuffle performed during execution, it outputs 200 shuffle partitions by default. You can change this value, and the number of output partitions will change. 阶段（stage）3和4对每个DataFrame执行，阶段（stage）结束表示连接（join）。突然间，我们有200个任务（task）。这是因为Spark SQL配置。 spark.sql.shuffle.partitions 默认值为200，这意味着当执行期间执行了洗牌（shuffle）时，它默认输出200个洗牌（shuffle）分区。您可以更改此值，并且输出分区的数量将更改。 TIP 提示We cover the number of partitions in a bit more detail in Chapter 19 because it’s such an important parameter. This value should be set according to the number of cores in your cluster to ensure efficient execution. Here’s how to set it: 我们在第19章中更详细地介绍了分区的数量，因为它是一个非常重要的参数。这个值应该根据集群中核心的数量来设置，以确保高效执行。设置方法如下： 1spark.conf.set("spark.sql.shuffle.partitions", 50) A good rule of thumb is that the number of partitions should be larger than the number of executors on your cluster, potentially by multiple factors depending on the workload. If you are running code on your local machine, it would behoove you to set this value lower because your local machine is unlikely to be able to execute that number of tasks in parallel. This is more of a default for a cluster in which there might be many more executor cores to use. Regardless of the number of partitions, that entire stage is computed in parallel. The final result aggregates those partitions individually, brings them all to a single partition before finally sending the final result to the driver. We’ll see this configuration several times over the course of this part of the book. 一个好的经验法则是分区数应该大于集群上执行器（executor）的数量，可能由多个因素决定，具体取决于工作负载。如果您在本地计算机上运行代码，那么您可以将此值设置得更低，因为本地计算机不太可能并行执行该数量的任务。对于可能需要使用更多执行程序核心的集群，这更像是一个默认设置。无论分区数量如何，整个阶段都是并行计算的。最终结果单独聚合这些分区，在最后将最终结果发送给驱动程序之前将它们全部带到一个分区。在本书的这一部分过程中，我们会多次看到这种配置。 Tasks 任务Stages in Spark consist of tasks. Each task corresponds to a combination of blocks of data and a set of transformations that will run on a single executor. If there is one big partition in our dataset, we will have one task. If there are 1,000 little partitions, we will have 1,000 tasks that can be executed in parallel. A task is just a unit of computation applied to a unit of data (the partition). Partitioning your data into a greater number of partitions means that more can be executed in parallel. This is not a panacea, but it is a simple place to begin with optimization. Spark中的阶段（stage）由任务（task）组成。每个任务（task）对应于将在单个执行器（executor）上运行的数据块和一组转换的组合。如果我们的数据集中有一个大分区，我们将有一个任务。如果有1000个小分区，我们将有1,000个可以并行执行的任务。任务只是应用于数据单元（分区）的计算单位。将数据划分为更多数量的分区意味着可以并行执行更多数据。这不是灵丹妙药，但它是一个简单的开始优化的入手之处。 Execution Details 执行细节Tasks and stages in Spark have some important properties that are worth reviewing before we close out this chapter. First, Spark automatically pipelines stages and tasks that can be done together, such as a map operation followed by another map operation. Second, for all shuffle operations, Spark writes the data to stable storage (e.g., disk), and can reuse it across multiple jobs. We’ll discuss these concepts in turn because they will come up when you start inspecting applications through the Spark UI. 在我们结束本章之前，Spark中的任务和阶段具有一些值得检查的重要属性。首先，Spark自动管理可以一起完成的阶段和任务，例如映射（map）操作，然后是另一个映射（map）操作。其次，对于所有洗牌（shuffle）操作，Spark将数据写入稳定存储（例如，磁盘），并且可以在多个作业中重复使用它。我们将依次讨论这些概念，因为当您开始通过Spark UI检查应用程序时，它们会出现。 Pipelining 管道化An important part of what makes Spark an “in-memory computation tool” is that unlike the tools that came before it (e.g., MapReduce), Spark performs as many steps as it can at one point in time before writing data to memory or disk. One of the key optimizations that Spark performs is pipelining, which occurs at and below the RDD level. With pipelining, any sequence of operations that feed data directly into each other, without needing to move it across nodes, is collapsed into a single stage of tasks that do all the operations together. For example, if you write an RDD-based program that does a map, then a filter, then another map, these will result in a single stage of tasks that immediately read each input record, pass it through the first map, pass it through the filter, and pass it through the last map function if needed. This pipelined version of the computation is much faster than writing the intermediate results to memory or disk after each step. The same kind of pipelining happens for a DataFrame or SQL computation that does a select, filter, and select. 使Spark成为“内存计算工具”的一个重要部分是，与之前的工具（例如，MapReduce）不同，Spark在将数据写入内存或磁盘之前的一个时间点执行尽可能多的步骤。 Spark执行的一个关键优化是流水线操作，它发生在RDD级别和低于RDD级别。通过流水线操作，将数据直接相互馈送而无需跨节点移动的任何操作序列都会折叠为一起完成所有操作的任务。例如，如果你编写一个基于RDD的程序来执行一个映射（map），然后是一个过滤器（filter），然后是另一个映射（map），这些将导致一个阶段的任务立即读取每个输入记录，将其传递通过第一个映射，传递给它过滤器，如果需要，将其传递给最后一个映射（map）函数。这个流水线版的计算比在每个步骤之后将中间结果写入内存或磁盘要快得多。对于执行select，filter和select的DataFrame或SQL计算，会发生相同类型的流水线操作。 From a practical point of view, pipelining will be transparent to you as you write an application—the Spark runtime will automatically do it—but you will see it if you ever inspect your application through the Spark UI or through its log files, where you will see that multiple RDD or DataFrame operations were pipelined into a single stage. 从实际的角度来看，在编写应用程序时，流水线操作对您来说是透明的——Spark运行时会自动执行——但如果您通过Spark UI或其日志文件检查应用程序，您将看到它将看到多个RDD或DataFrame操作被流水线化为单个阶段。 Shuffle Persistence 洗牌的持久化The second property you’ll sometimes see is shuffle persistence. When Spark needs to run an operation that has to move data across nodes, such as a reduce-by-key operation (where input data for each key needs to first be brought together from many nodes), the engine can’t perform pipelining anymore, and instead it performs a cross-network shuffle. Spark always executes shuffles by first having the “source” tasks (those sending data) write shuffle files to their local disks during their execution stage. Then, the stage that does the grouping and reduction launches and runs tasks that fetch their corresponding records from each shuffle file and performs that computation (e.g., fetches and processes the data for a specific range of keys). Saving the shuffle files to disk lets Spark run this stage later in time than the source stage (e.g., if there are not enough executors to run both at the same time), and also lets the engine re-launch reduce tasks on failure without rerunning all the input tasks. 你有时会看到的第二个属性是随机持久性。当 Spark 需要运行必须跨节点移动数据的操作时，例如 reduce-by-key 操作（每个键的输入数据需要首先从许多节点聚集在一起），引擎不能再执行流水线操作了，而是它执行交叉网络随机洗牌（shuffle）。 在执行阶段，Spark总是首先通过让“源”任务（那些发送数据的任务）将洗牌（shuffle）文件写入本地磁盘来执行洗牌（shuffle）操作。然后，执行分组和减少启动项，并运行从每个洗牌文件获取其相应记录的任务并执行该计算（例如，获取和处理特定范围的键的数据）。将洗牌（shuffle）文件保存到磁盘允许Spark比源阶段更晚地运行此阶段（例如，如果没有足够的执行器（executor）同时运行两者），并且还允许引擎重新启动以在故障时且不用重新运行所有输入任务的情况下减少任务。 One side effect you’ll see for shuffle persistence is that running a new job over data that’s already been shuffled does not rerun the “source” side of the shuffle. Because the shuffle files were already written to disk earlier, Spark knows that it can use them to run the later stages of the job, and it need not redo the earlier ones. In the Spark UI and logs, you will see the pre-shuffle stages marked as “skipped”. This automatic optimization can save time in a workload that runs multiple jobs over the same data, but of course, for even better performance you can perform your own caching with the DataFrame or RDD cache method, which lets you control exactly which data is saved and where. You’ll quickly grow accustomed to this behavior after you run some Spark actions on aggregated data and inspect them in the UI. 您将看到的随机持久性的一个副作用是，对已经被洗牌的数据运行新作业不会重新运行“源”端的洗牌操作。因为洗牌（shuffle）文件早先已经写入磁盘，所以Spark知道它可以使用它们来运行作业的后期阶段，并且它不需要重做早期的那些（任务）。在Spark UI和日志中，您将看到标记为“已跳过”的预洗牌阶段。这种自动优化可以节省在同一数据上运行多个作业的工作负载的时间，但当然，为了获得更好的性能，您可以使用DataFrame或RDD缓存方法执行自己的缓存，这样您就可以精确控制保存的数据和哪里。在对聚合数据运行一些Spark操作并在UI中检查它们之后，您将很快习惯于此行为。 Conclusion 结论In this chapter, we discussed what happens to Spark Applications when we go to execute them on a cluster. This means how the cluster will actually go about running that code as well as what happens within Spark Applications during the process. At this point, you should feel quite comfortable understanding what happens within and outside of a Spark Application. This will give you a starting point for debugging your applications. Chapter 16 will discuss writing Spark Applications and the things you should consider when doing so. 在本章中，我们讨论了当我们在集群上执行它们时Spark应用程序会发生什么。 这意味着集群将如何实际运行该代码以及在此过程中Spark应用程序中发生的事情。 此时，您应该非常自如地了解Spark应用程序内部和外部发生的情况。 这将为您调试应用程序提供一个起点。 第16章将讨论编写Spark应用程序以及执行此操作时应考虑的事项。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chapter 20 Stream Processing Fundamentals]]></title>
    <url>%2F2019%2F06%2F02%2FChapter20_StreamProcessingFundamentals(SparkTheDefinitiveGuide)%2F</url>
    <content type="text"><![CDATA[Stream processing is a key requirement in many big data applications. As soon as an application computes something of value—say, a report about customer activity, or a new machine learning model —an organization will want to compute this result continuously in a production setting. As a result, organizations of all sizes are starting to incorporate stream processing, often even in the first version of a new application. 流处理是许多大数据应用程序的关键要求。一旦一个应用程序计算出一些有价值的东西，比如关于客户活动的报告，或者一个新的机器学习模型，一个组织就会想要在生产环境中连续计算这个结果。因此，各种规模的组织都开始合并流处理，甚至在新应用程序的第一个版本中也是如此。 Luckily, Apache Spark has a long history of high-level support for streaming. In 2012, the project incorporated Spark Streaming and its DStreams API, one of the first APIs to enable stream processing using high-level functional operators like map and reduce. Hundreds of organizations now use DStreams in production for large real-time applications, often processing terabytes of data per hour. Much like the Resilient Distributed Dataset (RDD) API, however, the DStreams API is based on relatively low-level operations on Java/Python objects that limit opportunities for higher-level optimization. Thus, in 2016, the Spark project added Structured Streaming, a new streaming API built directly on DataFrames that supports both rich optimizations and significantly simpler integration with other DataFrame and Dataset code. The Structured Streaming API was marked as stable in Apache Spark 2.2, and has also seen swift adoption throughout the Spark community. 幸运的是，Apache Spark 有很长的高级流支持历史。2012年，该项目整合了 Spark Streaming 及其 DStreams API，这是第一批使用诸如 Map 和 Reduce 之类的高级功能操作符实现流处理的 API 之一。数百个组织现在在生产中使用数据流来处理大型实时应用程序，通常每小时处理数兆字节的数据。与弹性分布式数据集（RDD）API非常相似，但是，DStreams API 是基于 Java/Python 对象上相对较低级别的操作，这些对象限制了更高级别优化的机会。因此，在2016年，Spark项目添加了结构化流式处理（Structured Streaming），这是一种直接在 DataFrames 上构建的新流式API，它既支持丰富的优化，也支持与其他 DataFrame 和 Dataset 代码的显著简化集成。结构化流式API在ApacheSpark2.2中被标记为稳定的，并且在整个Spark社区中也得到了迅速的采用。 In this book, we will focus only on the Structured Streaming API, which integrates directly with the DataFrame and Dataset APIs we discussed earlier in the book and is the framework of choice for writing new streaming applications. If you are interested in DStreams, many other books cover that API, including several dedicated books on Spark Streaming only, such as Learning Spark Streaming by Francois Garillot and Gerard Maas (O’Reilly, 2017). Much as with RDDs versus DataFrames, however, Structured Streaming offers a superset of the majority of the functionality of DStreams, and will often perform better due to code generation and the Catalyst optimizer. Before we discuss the streaming APIs in Spark, let’s more formally define streaming and batch processing. This chapter will discuss some of the core concepts in this area that we will need throughout this part of the book. It won’t be a dissertation on this topic, but will cover enough of the concepts to let you make sense of systems in this space. 在本书中，我们将只关注结构化流式API（Structured Streaming API），它直接与本书前面讨论的 DataFrame 和 Dataset API 集成，是编写新流式应用程序的首选框架。如果您对 DStream 感兴趣，那么许多其他书籍都涉及该 API，其中包括一些专门的关于 Spark Streaming 的书籍，例如 Francois Garillot和Gerard Maas的 Learning Spark Streaming（O’Reilly，2017）。然而，与RDD与 DataFrame 相比，结构化流提供了数据流大部分功能的超集，并且由于代码生成和Catalyst优化器，通常性能会更好。 在讨论Spark中的流式API之前，让我们更正式地定义流式处理和批处理。本章将讨论本书这一部分中我们需要的这一领域的一些核心概念。这不是一篇关于这个主题的论文，但将涵盖足够多的概念，使您能够理解这个空间中的系统。 What Is Stream Processing 什么是流处理? Stream processing is the act of continuously incorporating new data to compute a result. In stream processing, the input data is unbounded and has no predetermined beginning or end. It simply forms a series of events that arrive at the stream processing system (e.g., credit card transactions, clicks on a website, or sensor readings from Internet of Things [IoT] devices). User applications can then compute various queries over this stream of events (e.g., tracking a running count of each type of event or aggregating them into hourly windows). The application will output multiple versions of the result as it runs, or perhaps keep it up to date in an external “sink” system such as a key-value store. 流处理是不断合并新数据以计算结果的行为。在流处理中，输入数据是无边界的，没有预先确定的开始或结束。它只是形成一系列到达流处理系统的事件（例如，信用卡交易、网站点击或物联网设备的传感器读数）。然后，用户应用程序可以计算对该事件流的各种查询（例如，跟踪每种类型事件的运行计数，或将其聚合到每小时一次的窗口中）。应用程序将在运行时输出结果的多个版本，或者在外部的“接收器”系统（如键值存储）中使其事件保持最新。 Naturally, we can compare streaming to batch processing, in which the computation runs on a fixedinput dataset. Oftentimes, this might be a large-scale dataset in a data warehouse that contains all the historical events from an application (e.g., all website visits or sensor readings for the past month). Batch processing also takes a query to compute, similar to stream processing, but only computes the result once. 当然，我们可以将流式处理与批处理进行比较，在批处理中，计算运行在固定的输入数据集上。通常，这可能是数据仓库中的大型数据集，其中包含应用程序的所有历史事件（例如，过去一个月的所有网站访问或传感器读数）。批处理也需要一个查询来计算，类似于流处理，但只计算一次结果。 Although streaming and batch processing sound different, in practice, they often need to work together. For example, streaming applications often need to join input data against a dataset written periodically by a batch job, and the output of streaming jobs is often files or tables that are queried in batch jobs. Moreover, any business logic in your applications needs to work consistently across streaming and batch execution: for example, if you have a custom code to compute a user’s billing amount, it would be harmful to get a different result when running it in a streaming versus batch fashion! To handle these needs, Structured Streaming was designed from the beginning to interoperate easily with the rest of Spark, including batch applications. Indeed, the Structured Streaming developers coined the term continuous applications to capture end-to-end applications that consist of streaming, batch, and interactive jobs all working on the same data to deliver an end product. Structured Streaming is focused on making it simple to build such applications in an end-to-end fashion instead of only handling stream-level per-record processing. 虽然流式处理和批处理听起来不同，但在实践中，它们通常需要一起工作。例如，流式处理应用程序通常需要将输入数据与批处理作业定期写入的数据集连接起来，而流式处理作业的输出通常是批处理作业中查询的文件或表。此外，应用程序中的任何业务逻辑都需要在流式处理和批处理执行之间始终如一地工作：例如，如果您有一个自定义代码来计算用户的账单金额，那么以流式处理与批处理方式运行时获得不同的结果将是有害的！为了处理这些需求，从一开始就设计了结构化流（Structured Streaming），以便与Spark的其余部分（包括批处理应用程序）轻松地进行互操作。实际上，结构化流式开发人员创造了术语“连续应用程序”，以捕获由流式、批处理和交互式作业组成的端到端应用程序，这些作业都处理相同的数据以交付最终产品。结构化流的重点是使端到端的方式构建此类应用程序变得简单，而不是只应对流级别的每个记录处理。 Stream Processing Use Cases 流处理使用案例We defined stream processing as the incremental processing of unbounded datasets, but that’s a strange way to motivate a use case. Before we get into advantages and disadvantages of streaming, let’s explain why you might want to use streaming. We’ll describe six common use cases with varying requirements from the underlying stream processing system. 我们将流处理定义为对无边界数据集的增量处理，但这是一种激活使用案例的奇怪方式。在我们讨论流的优点和缺点之前，让我们解释一下为什么您可能想要使用流。我们将描述来自底层流处理系统的具有不同需求的六个常见用户案例。 Notifications and alerting 通知和警报Probably the most obvious streaming use case involves notifications and alerting. Given some series of events, a notification or alert should be triggered if some sort of event or series of events occurs. This doesn’t necessarily imply autonomous or preprogrammed decision making; alerting can also be used to notify a human counterpart of some action that needs to be taken. An example might be driving an alert to an employee at a fulfillment center that they need to get a certain item from a location in the warehouse and ship it to a customer. In either case, the notification needs to happen quickly. 可能最明显的流式用户案例涉及通知和警报。对于某些事件系列，如果发生某种事件或事件系列，则应触发通知或警报。这并不一定意味着自主或预先编程的决策；警报也可以用来通知人类对应方需要采取的某些行动。例如，向履行中心的员工发出警报，提醒他们需要从仓库中的某个位置获取某个项目并将其发送给客户。在这两种情况下，通知都需要快速发生。 Real-time reporting 实时报告Many organizations use streaming systems to run real-time dashboards that any employee can look at. For example, this book’s authors leverage Structured Streaming every day to run real-time reporting dashboards throughout Databricks (where both authors of this book work). We use these dashboards to monitor total platform usage, system load, uptime, and even usage of new features as they are rolled out, among other applications. 许多组织使用流媒体系统来运行任何员工都可以查看的实时仪表盘。例如，本书的作者利用结构化流媒体每天在整个 Databricks（本书的两位作者都在这里工作）中运行实时报告仪表盘。我们使用这些仪表盘来监控平台的总使用量、系统负载、正常运行时间，甚至在推出新功能时对它们的使用情况，以及其他应用程序。 Incremental ETL 增量的ETLOne of the most common streaming applications is to reduce the latency companies must endure while retreiving information into a data warehouse—in short, “my batch job, but streaming.” Spark batch jobs are often used for Extract, Transform, and Load (ETL) workloads that turn raw data into a structured format like Parquet to enable efficient queries. Using Structured Streaming, these jobs can incorporate new data within seconds, enabling users to query it faster downstream. In this use case, it is critical that data is processed exactly once and in a fault-tolerant manner: we don’t want to lose any input data before it makes it to the warehouse, and we don’t want to load the same data twice. Moreover, the streaming system needs to make updates to the data warehouse transactionally so as not to confuse the queries running on it with partially written data. 最常见的流式应用程序之一是减少公司在将信息检索到数据仓库（简而言之，“我的批处理作业，但流式处理”）时必须忍受的延迟。Spark批处理作业通常用于提取、转换和加载（ETL）的工作，将原始数据转换为类似 Parquet的结构化格式，以实现高效查询。使用结构化流，这些作业可以在几秒钟内合并新数据，使用户能够更快地向下游查询数据。在这个用户案例中，以一种容错的方式处理数据且有且仅且处理一次是非常关键的：我们不希望在数据到达仓库之前丢失任何输入数据，也不希望加载相同的数据两次。此外，流媒体系统需要对数据仓库进行事务性更新，以免将运行在数据仓库上的查询与部分写入的数据混淆。 Update data to serve in real time 更新数据以提供实时服务Streaming systems are frequently used to compute data that gets served interactively by another application. For example, a web analytics product such as Google Analytics might continuously track the number of visits to each page, and use a streaming system to keep these counts up to date. When users interact with the product’s UI, this web application queries the latest counts. Supporting this use case requires that the streaming system can perform incremental updates to a key–value store (or other serving system) as a sync, and often also that these updates are transactional, as in the ETL case, to avoid corrupting the data in the application. 流系统通常用于计算由其他应用程序交互提供服务的数据。例如，像Google Analytics这样的Web Analytics产品可能会持续跟踪每个页面的访问次数，并使用流式系统保持这些计数的最新。当用户与产品的UI交互时，此Web应用程序查询最新计数。支持此用户案例要求流系统可以同步对键值存储（或其他服务系统）执行增量更新，并且通常这些更新是事务性的，如ETL情况，以避免损坏应用程序中的数据。 Real-time decision making 实时决策Real-time decision making on a streaming system involves analyzing new inputs and responding to them automatically using business logic. An example use case would be a bank that wants to automatically verify whether a new transaction on a customer’s credit card represents fraud based on their recent history, and deny the transaction if the charge is determined fradulent. This decision needs to be made in real-time while processing each transaction, so developers could implement this business logic in a streaming system and run it against the stream of transactions. This type of application will likely need to maintain a significant amount of state about each user to track their current spending patterns, and automatically compare this state against each new transaction. 流媒体系统的实时决策涉及到分析新的输入并使用业务逻辑自动响应它们。例如，一家银行希望根据客户最近的历史自动验证其信用卡上的新交易是否代表欺诈行为，并在确定费用过期时拒绝该交易。这个决策需要在处理每个事务时实时做出，因此开发人员可以在流系统中实现这个业务逻辑，并针对事务流运行它。这种类型的应用程序可能需要维护每个用户的大量状态，以跟踪他们当前的支出模式，并自动将这种状态与每个新事务进行比较。 Online machine learning 在线机器学习A close derivative of the real-time decision-making use case is online machine learning. In this scenario, you might want to train a model on a combination of streaming and historical data from multiple users. An example might be more sophisticated than the aforementioned credit card transaction use case: rather than reacting with hardcoded rules based on one customer’s behavior, the company may want to continuously update a model from all customers’ behavior and test each transaction against it. This is the most challenging use case of the bunch for stream processing systems because it requires aggregation across multiple customers, joins against static datasets, integration with machine learning libraries, and low-latency response times. 实时决策用户案例的一个密切派生是在线机器学习。在这个场景中，您可能希望对来自多个用户的流数据和历史数据组合的模型进行训练。一个例子可能比前面提到的信用卡交易用户案例更复杂：公司可能希望从所有客户的行为中不断更新一个模型，并根据它测试每个交易，而不是根据一个客户的行为对硬编码规则做出反应。对于流处理系统来说，这是最具挑战性的用户案例，因为它需要跨多个客户进行聚合、针对静态数据集进行连接、与机器学习库集成以及低延迟响应时间。 Advantages of Stream ProcessingNow that we’ve seen some use cases for streaming, let’s crystallize some of the advantages of stream processing. For the most part, batch is much simpler to understand, troubleshoot, and write applications in for the majority of use cases. Additionally, the ability to process data in batch allows for vastly higher data processing throughput than many streaming systems. However, stream processing is essential in two cases. First, stream processing enables lower latency: when your application needs to respond quickly (on a timescale of minutes, seconds, or milliseconds), you will need a streaming system that can keep state in memory to get acceptable performance. Many of the decision making and alerting use cases we described fall into this camp. Second, stream processing can also be more efficient in updating a result than repeated batch jobs, because it automatically incrementalizes the computation. For example, if we want to compute web traffic statistics over the past 24 hours, a naively implemented batch job might scan all the data each time it runs, always processing 24 hours’ worth of data. In contrast, a streaming system can remember state from the previous computation and only count the new data. If you tell the streaming system to update your report every hour, for example, it would only need to process 1 hour’s worth of data each time (the new data since the last report). In a batch system, you would have to implement this kind of incremental computation by hand to get the same performance, resulting in a lot of extra work that the streaming system will automatically give you out of the box. 既然我们已经看到了流的一些用例，那么让我们具体说明流处理的一些优势。在大多数情况下，对于大多数用例，批处理更容易理解、排除故障和编写应用程序。此外，批量处理数据的能力使得数据处理吞吐量大大高于许多流系统。然而，流处理在两种情况下是必要的。首先，流处理可以降低延迟：当应用程序需要快速响应（以分钟、秒或毫秒为时间刻度）时，您将需要一个流系统，它可以在内存中保持状态以获得可接受的性能。我们描述的许多决策和警报用例都属于这个阵营。第二，流处理在更新结果方面也比重复的批处理作业更有效，因为它会自动增加计算量。例如，如果我们想计算过去24小时内的Web流量统计数据，一个简单实现的批处理作业可能会在每次运行时扫描所有数据，总是处理24小时的数据。相反，流系统可以记住以前计算的状态，并且只计算新数据。例如，如果您告诉流系统每小时更新一次报告，则每次只需要处理1小时的数据（自上次报告以来的新数据）。在批处理系统中，您必须手工实现这种增量计算，以获得相同的性能，从而导致流系统自动提供的大量额外工作。 Challenges of Stream ProcessingWe discussed motivations and advantages of stream processing, but as you likely know, there’s never a free lunch. Let’s discuss some of the challenges of operating on streams. To ground this example, let’s imagine that our application receives input messages from a sensor (e.g., inside a car) that report its value at different times. We then want to search within this stream for certain values, or certain patterns of values. One specific challenge is that the input records might arrive to our application out-of-order: due to delays and retransmissions, for example, we might receive the following sequence of updates in order, where the time field shows the time when the value was actually measured: 我们讨论了流处理的动机和优势，但正如您可能知道的，从来没有免费的午餐。让我们讨论在流上操作的一些挑战。为了使这个例子更加简单，让我们假设我们的应用程序从一个传感器（例如，在车内）接收输入消息，该传感器在不同的时间报告其值。然后我们希望在这个流中搜索特定的值或值的特定模式。一个具体的挑战是，输入记录可能会无序到达我们的应用程序：例如，由于延迟和重新传输，我们可能会按顺序接收以下更新序列，其中时间字段显示实际测量值的时间： 12345&#123;value: 1, time: "2017-04-07T00:00:00"&#125;&#123;value: 2, time: "2017-04-07T01:00:00"&#125;&#123;value: 5, time: "2017-04-07T02:00:00"&#125;&#123;value: 10, time: "2017-04-07T01:30:00"&#125;&#123;value: 7, time: "2017-04-07T03:00:00"&#125; In any data processing system, we can construct logic to perform some action based on receiving the single value of “5.” In a streaming system, we can also respond to this individual event quickly. However, things become more complicated if you want only to trigger some action based on a specific sequence of values received, say, 2 then 10 then 5. In the case of batch processing, this is not particularly difficult because we can simply sort all the events we have by time field to see that 10 did come between 2 and 5. However, this is harder for stream processing systems. The reason is that the streaming system is going to receive each event individually, and will need to track some state across events to remember the 2 and 5 events and realize that the 10 event was between them. The need to remember such state over the stream creates more challenges. For instance, what if you have a massive data volume (e.g., millions of sensor streams) and the state itself is massive? What if a machine in the sytem fails, losing some state? What if the load is imbalanced and one machine is slow? And how can your application signal downstream consumers when analysis for some event is “done” (e.g., the pattern 2-10-5 did not occur)? Should it wait a fixed amount of time or remember some state indefinitely? All of these challenges and others—such as making the input and the output of the system transactional—can come up when you want to deploy a streaming application. 在任何数据处理系统中，我们都可以在接收到单个值“5”的基础上构造逻辑来执行某些操作。在流系统中，我们还可以快速响应这个单独的事件。但是，如果您只想根据接收到的特定值序列（例如2，然后10，然后5）触发一些操作，那么事情会变得更加复杂。在批处理的情况下，这并不特别困难，因为我们可以简单地按时间字段对所有事件进行排序，以查看10是否在2到5之间。然而，对于流处理系统来说，这是很困难的。**原因是流系统将单独接收每个事件，并且需要跨事件跟踪一些状态，以记住值为2和5的事件，并认识到值为10的事件介于两者之间。需要记住这样的状态会带来更多的挑战。例如，如果你有一个巨大的数据量（例如，数百万个传感器流），而状态本身也是巨大的呢？如果系统中的一台机器发生故障，失去某种状态，会怎么样？如果负载不平衡，一台机器运行缓慢怎么办？当某些事件的分析“完成”时（例如，模式2-10-5没有发生），您的应用程序如何向下游消费者发出信号？它应该等待固定的时间还是无限期地记住某个状态？当您想要部署流式应用程序时，所有这些挑战和其他挑战（如使系统事务性的输入和输出）都会出现。 To summarize, the challenges we described in the previous paragraph and a couple of others, are as follows: 综上所述，我们在上一段和其他几段中描述的挑战如下： Processing out-of-order data based on application timestamps (also called event time) 基于应用程序时间戳（也称为事件时间）处理无序数据 Maintaining large amounts of state 维持大量的状态 Supporting high-data throughput 支持大数据吞吐量 Processing each event exactly once despite machine failures 尽管机器出现故障，但每个事件处理一次 Handling load imbalance and straggler 处理负载不平衡和散乱 Responding to events at low latency 响应低延迟事件 Joining with external data in other storage systems 与其他存储系统中的外部数据连接 Determining how to update output sinks as new events arrive 确定新事件到达时如何更新输出接收器 Writing data transactionally to output systems 以事务方式将数据写入输出系统 Updating your application’s business logic at runtime 在运行时更新应用程序的业务逻辑 Each of these topics are an active area of research and development in large-scale streaming systems. To understand how different streaming systems have tackled these challenges, we describe a few of the most common design concepts you will see across them. 这些课题中的每一个都是大规模流系统研究和开发的活跃领域。为了了解不同的流系统如何应对这些挑战，我们将介绍一些您将看到的最常见的设计概念。 Stream Processing Design Points 流处理设计点To support the stream processing challenges we described, including high throughput, low latency, and out-of-order data, there are multiple ways to design a streaming system. We describe the most common design options here, before describing Spark’s choices in the next section. 为了支持我们描述的流处理挑战，包括高吞吐量、低延迟和无序数据，设计流系统有多种方法。在下一节描述Spark的选择之前，我们先在这里描述最常见的设计选项。 Record-at-a-Time Versus Declarative APIs 记录一次与声明性APIThe simplest way to design a streaming API would be to just pass each event to the application and let it react using custom code. This is the approach that many early streaming systems, such as Apache Storm, implemented, and it has an important place when applications need full control over the processing of data. Streaming that provide this kind of record-at-a-time API just give the user a collection of “plumbing” to connect together into an application. However, the downside of these systems is that most of the complicating factors we described earlier, such as maintaining state, are solely governed by the application. For example, with a record-at-a-time API, you are responsible for tracking state over longer time periods, dropping it after some time to clear up space, and responding differently to duplicate events after a failure. Programming these systems correctly can be quite challenging. At its core, low-level APIs require deep expertise to be develop and maintain. 设计流式API的最简单方法是将每个事件传递给应用程序，并让它使用自定义代码进行响应。这是许多早期流媒体系统（如 Apache Storm）实现的方法，在应用程序需要完全控制数据处理时，它具有重要的地位。流式处理提供了这种一次记录的API，它只给用户一个“管道”集合，将它们连接到一个应用程序中。但是，这些系统的缺点是，我们前面描述的大多数复杂因素（如维护状态）都是由应用程序单独控制的。例如，对于记录一次的API，您负责在较长的时间段内跟踪状态，在一段时间后将其丢弃以清除空间，并对失败后的重复事件做出不同的响应。对这些系统进行正确的编程是非常有挑战性的。核心层面来说，低阶API需要深厚的专业知识去执行开发和维护。 As a result, many newer streaming systems provide declarative APIs, where your application specifies what to compute but not how to compute it in response to each new event and how to recover from failure. Spark’s original DStreams API, for example, offered functional API based on operations like map, reduce and filter on streams. Internally, the DStream API automatically tracked how much data each operator had processed, saved any relevant state reliably, and recovered the computation from failure when needed. Systems such as Google Dataflow and Apache Kafka Streams provide similar, functional APIs. Spark’s Structured Streaming actually takes this concept even further, switching from functional operations to relational (SQL-like) ones that enable even richer automatic optimization of the execution without programming effort. 因此，许多较新的流系统都提供声明性API（declarative APIs），其中应用程序指定要计算什么，而不是如何响应每个新事件以及如何从故障中恢复。例如，Spark 最初的 DStreams API 提供了基于 map、reduce 和 filter 等操作的函数式 API。在内部，DStream API自动跟踪每个操作员处理了多少数据，可靠地保存了任何相关状态，并在需要时从失败中恢复计算。 Google Dataflow 和 Apache Kafka 流等系统提供类似的功能性API。Spark的结构化流实际上更进一步地采用了这一概念，从功能操作转换为关系操作（类似于SQL），从而在不需要编程的情况下实现更丰富的自动执行优化。 Event Time Versus Processing Time 事件时间与处理时间For the systems with declarative APIs, a second concern is whether the system natively supports event time. Event time is the idea of processing data based on timestamps inserted into each record at the source, as opposed to the time when the record is received at the streaming application (which is called processing time). In particular, when using event time, records may arrive to the system out of order (e.g., if they traveled back on different network paths), and different sources may also be out of sync with each other (some records may arrive later than other records for the same event time). If your application collects data from remote sources that may be delayed, such as mobile phones or IoT devices, event-time processing is crucial: without it, you will miss important patterns when some data is late. In contrast, if your application only processes local events (e.g., ones generated in the same datacenter), you may not need sophisticated event-time processing. 对于具有声明性API的系统，第二个问题是系统本身是否支持事件时间。事件时间是基于在源位置插入到每个记录中的时间戳来处理数据的概念，而不是在流应用程序接收记录的时间（称为处理时间）。特别是，在使用事件时间时，记录可能会无序到达系统（例如，如果它们返回到不同的网络路径上），并且不同的源也可能不同步（某些记录可能比同一事件时间的其他记录晚到达）。如果您的应用程序从可能延迟的远程源（如移动电话或物联网设备）收集数据，事件时间处理至关重要：如果没有它，当某些数据延迟时，您将错过重要的模式。相反，如果应用程序只处理本地事件（例如，在同一个数据中心生成的事件），则可能不需要复杂的事件时间处理。 When using event-time, several issues become common concerns across applications, including tracking state in a manner that allows the system to incorporate late events, and determining when it is safe to output a result for a given time window in event time (i.e., when the system is likely to have received all the input up to that point). Because of this, many declarative systems, including Structured Streaming, have “native” support for event time integrated into all their APIs, so that these concerns can be handled automatically across your whole program. 当使用事件时间时，几个问题成为应用程序中常见的问题，包括以允许系统合并延迟事件的方式跟踪状态，以及在给定的事件时间窗口内，确定安全输出结果的时间（即，系统可能具有接收到该点之前的所有输入）。因此，许多声明性系统（包括结构化流）都支持将事件时间集成到其所有API中，以便在整个程序中自动处理这些问题。 Continuous Versus Micro-Batch Execution 连续与微批量执行The final design decision you will often see come up is about continuous versus micro-batch execution. In continuous processing-based systems, each node in the system is continually listening to messages from other nodes and outputting new updates to its child nodes. For example, suppose that your application implements a map-reduce computation over several input streams. In a continuous processing system, each of the nodes implementing map would read records one by one from an input source, compute its function on them, and send them to the appropriate reducer. The reducer would then update its state whenever it gets a new record. The key idea is that this happens on each individual record, as illustrated in Figure 20-1. 您经常会看到的最终设计决策是关于连续与微批量执行的。在基于连续处理的系统中，系统中的每个节点都在不断地监听来自其他节点的消息，并将新的更新输出到其子节点。例如，假设您的应用程序在多个输入流上实现了一个map reduce计算。在连续处理系统中，实现映射的每个节点将从输入源中逐个读取记录，计算其功能，并将其发送到相应的reducer。然后，每当有新的记录时，reducer就会更新其状态。关键的想法是这发生在每个单独的记录上，如图20-1所示。 Continuous processing has the advantage of offering the lowest possible latency when the total input rate is relatively low, because each node responds immediately to a new message. However, continuous processing systems generally have lower maximum throughput, because they incur a significant amount of overhead per-record (e.g., calling the operating system to send a packet to a downstream node). In addition, continous systems generally have a fixed topology of operators that cannot be moved at runtime without stopping the whole system, which can introduce load balancing issues. 当总输入速率相对较低时，连续处理的优点是提供尽可能低的延迟，因为每个节点都会立即响应新消息。但是，连续处理系统通常具有较低的最大吞吐量，因为它们在每条记录上都会产生大量开销（例如，调用操作系统向下游节点发送数据包）。此外，连续系统通常具有固定的算子（ operator: [mathematics] a symbol or function which represents an operation in mathematics）拓扑结构，如果不停止整个系统，就无法在运行时移动这些算子，这会引入负载平衡问题。 In contrast, micro-batch systems wait to accumulate small batches of input data (say, 500 ms’ worth), then process each batch in parallel using a distributed collection of tasks, similar to the execution of a batch job in Spark. Micro-batch systems can often achieve high throughput per node because they leverage the same optimizations as batch systems (e.g., vectorized processing), and do not incur any extra per-record overhead, as illustrated in Figure 20-2. 相反，微批处理系统等待积累小批量的输入数据（比如500毫秒的值），然后使用分布式任务集合并行处理每一批，类似于Spark中批处理作业的执行。微批处理系统通常可以实现每个节点的高吞吐量，因为它们利用与批处理系统相同的优化（例如，矢量化处理），并且不会产生任何额外的每记录开销，如图20-2所示。 Thus, they need fewer nodes to process the same rate of data. Micro-batch systems can also use dynamic load balancing techniques to handle changing workloads (e.g., increasing or decreasing the number of tasks). The downside, however, is a higher base latency due to waiting to accumulate a micro-batch. In practice, the streaming applications that are large-scale enough to need to distribute their computation tend to prioritize throughput, so Spark has traditionally implemented micro-batch processing. In Structured Streaming, however, there is an active development effort to also support a continuous processing mode beneath the same API. 因此，它们需要更少的节点来处理相同的数据速率。微批处理系统还可以使用动态负载平衡技术来处理不断变化的工作负载（例如，增加或减少任务数量）。但是，缺点是，由于等待积累一个微批处理，所以基本延迟更高。在实践中，流应用程序的规模足够大，需要分配它们的计算（资源），倾向于优先考虑吞吐量，因此Spark传统上已经实现了微批处理。然而，在结构化流中，也有一项积极的开发工作来支持同一API下的连续处理模式。 When choosing between these two execution modes, the main factors you should keep in mind are your desired latency and total cost of operation (TCO). Micro-batch systems can comfortably deliver latencies from 100 ms to a second, depending on the application. Within this regime, they will generally require fewer nodes to achieve the same throughput, and hence lower operational cost (including lower maintenance cost due to less frequent node failures). For much lower latencies, you should consider a continuous processing system, or using a micro-batch system in conjunction with a fast serving layer to provide low-latency queries (e.g., outputting data into MySQL or Apache Cassandra, where it can be served to clients in milliseconds). 在这两种执行模式之间进行选择时，您应该记住的主要因素是所需的延迟和总操作成本（TCO: total cost of operation）。根据应用程序的不同，微批处理系统可以轻松地将延迟从100毫秒传递到一秒钟。在这种情况下，它们通常需要更少的节点来实现相同的吞吐量，从而降低运营成本（包括由于节点故障频率较低而导致的维护成本）。对于较低的延迟，您应该考虑使用连续处理系统，或者将微批处理系统与快速服务层结合使用，以提供低延迟查询（例如，将数据输出到 MySQL 或 Apache Cassandra 中，在那里它可以以毫秒为单位提供给客户机）。 In general, Structured Streaming is meant to be an easier-to use and higher-performance evolution of Spark Streaming’s DStream API, so we will focus solely on this new API in this book. Many of the concepts, such as building a computation out of a graph of transformations, also apply to DStreams, but we leave the exposition of that to other books. Spark’s Streaming APIs Spark的流式APIWe covered some high-level design approaches to stream processing, but thus far we have not discussed Spark’s APIs in detail. Spark includes two streaming APIs, as we discussed at the beginning of this chapter. The earlier DStream API in Spark Streaming is purely micro-batch oriented. It has a declarative (functional-based) API but no support for event time. The newer Structured Streaming API adds higher-level optimizations, event time, and support for continuous processing. 我们介绍了流处理的一些高级设计方法，但到目前为止，我们还没有详细讨论Spark的API。Spark包含两个流式API，正如我们在本章开头所讨论的。Spark Streaming 中早期的 DStream API 纯粹是面向微批处理的。它有一个声明性（基于函数的）API，但不支持事件时间。更新的结构化流式API增加了更高级的优化、事件时间和对连续处理的支持。 The DStream API 数据流APISpark’s original DStream API has been used broadly for stream processing since its first release in 2012. For example, DStreams was the most widely used processing engine in Datanami’s 2016 survey. Many companies use and operate Spark Streaming at scale in production today due to its highlevel API interface and simple exactly-once semantics. Interactions with RDD code, such as joins with static data, are also natively supported in Spark Streaming. Operating Spark Streaming isn’t much more difficult than operating a normal Spark cluster. However, the DStreams API has several limitations. First, it is based purely on Java/Python objects and functions, as opposed to the richer concept of structured tables in DataFrames and Datasets. This limits the engine’s opportunity to perform optimizations. Second, the API is purely based on processing time—to handle event-time operations, applications need to implement them on their own. Finally, DStreams can only operate in a micro-batch fashion, and exposes the duration of micro-batches in some parts of its API, making it difficult to support alternative execution modes. Spark 的原始 DStream API 自2012年首次发布以来，已广泛用于流处理。例如，DStreams是Datanami 2016年调查中使用最广泛的处理引擎。由于其高级API接口和简单的一次性语义，许多公司现在在生产中大规模使用和操作Spark流。与 RDD 代码的交互（如与静态数据的连接）也在Spark流中得到原生支持。操作 Spark 流并不比操作普通的 Spark 集群更困难。但是，DStreams API有几个限制。首先，它纯粹基于Java/Python 对象和函数，而不是 DataFrames 和 Datasets 中结构化表的更丰富的概念。这限制了引擎执行优化的机会。第二，API 纯粹是基于处理时间来处理事件时间操作，应用程序需要自己实现它们。最后，DStream 只能以微批量方式操作，并在其API的某些部分公开微批量的持续时间，这使得支持替代执行模式变得困难。 Structured Streaming 结构化流Structured Streaming is a higher-level streaming API built from the ground up on Spark’s Structured APIs. It is available in all the environments where structured processing runs, including Scala, Java, Python, R, and SQL. Like DStreams, it is a declarative API based on high-level operations, but by building on the structured data model introduced in the previous part of the book, Structured Streaming can perform more types of optimizations automatically. However, unlike DStreams, Structured Streaming has native support for event time data (all of its the windowing operators automatically support it). As of Apache Spark 2.2, the system only runs in a micro-batch model, but the Spark team at Databricks has announced an effort called Continuous Processing to add a continuous execution mode. This should become an option for users in Spark 2.3. 结构化流是在Spark的结构化API基础上构建的更高阶的流式API。它可以在结构化处理运行的所有环境中使用，包括Scala、Java、Python、R和SQL。与数据流一样，它是一个基于高阶操作的声明性API，但是通过构建本书上一部分介绍的结构化数据模型，结构化流可以自动执行更多类型的优化。但是，与数据流不同，结构化流具有对事件时间数据的原生支持（它的所有窗口化算子都自动支持它）。从 Apache Spark 2.2开始，系统只运行在一个微批量模型中，但 Databricks 的 Spark团队已经宣布了一项称为“连续处理”的工作，以添加连续执行模式。这应该成为 Spark 2.3 中用户的一个选项。 More fundamentally, beyond simplifying stream processing, Structured Streaming is also designed to make it easy to build end-to-end continuous applications using Apache Spark that combine streaming, batch, and interactive queries. For example, Structured Streaming does not use a separate API from DataFrames: you simply write a normal DataFrame (or SQL) computation and launch it on a stream. Structured Streaming will automatically update the result of this computation in an incremental fashion as data arrives. This is a major help when writing end-to-end data applications: developers do not need to maintain a separate streaming version of their batch code, possibly for a different execution system, and risk having these two versions of the code fall out of sync. As another example, Structured Streaming can output data to standard sinks usable by Spark SQL, such as Parquet tables, making it easy to query your stream state from another Spark applications. In future versions of Apache Spark, we expect more and more components of the project to integrate with Structured Streaming, including online learning algorithms in MLlib. 更重要的是，除了简化流处理之外，结构化流还设计为使用结合流、批处理和交互式查询的Apache Spark轻松构建端到端连续应用程序。例如，结构化流不使用不同于 DataFrames 的API：您只需编写一个普通的 DataFrame（或SQL）计算并在流上启动它。当数据到达时，结构化流将以增量方式自动更新计算结果。在编写端到端数据应用程序时，主要的帮助是：可能是针对不同的执行系统，开发人员不需要维护其批处理代码的不同的流式版本，并且拥有两个版本的代码将有不同步的风险。作为另一个例子，结构化流可以将数据输出到 Spark Sql 可用的标准接收器，例如 Parquet 表，这样就可以很容易地从另一个Spark应用程序查询流状态。在 Apache Spark的未来版本中，我们期望项目中越来越多的组件与结构化流集成，包括MLLIB中的在线学习算法。 Conclusion 总结This chapter covered the basic concepts and ideas that you’re going to need to understand stream processing. The design approaches introduced in this chapter should clarify how you can evaluate streaming systems for a given application. You should also feel comfortable understanding what trade-offs the authors of DStreams and Structured Streaming have made, and why the direct support for DataFrame programs is a big help when using Structured Streaming: there is no need to duplicate your application logic. In the upcoming chapters, we’ll dive right into Structured Streaming to understand how to use it. 本章介绍了理解流处理所需的基本概念和想法。本章介绍的设计方法应该阐明如何评估给定应用程序的流系统。您还应该理解数据流和结构化流的作者所做的权衡，以及为什么在使用结构化流时直接支持 DataFrame 程序是一个很大的帮助：不需要复制应用程序逻辑。在接下来的章节中，我们将直接深入到结构化流，了解如何使用它。]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop YARN Federation]]></title>
    <url>%2F2019%2F05%2F14%2FHadoopYARNFederation%2F</url>
    <content type="text"><![CDATA[Preface写这个阅读笔记之前，Hadoop YARN 的稳定发行版是：2.9.2 。 YARN Federation 思想源自：HDFS Federation，官方文档介绍，HDFS Federation 解决的是 NameNode 的横向扩展，HDFS HA 解决的是 NameNode 的单点问题。 PurposeYARN is known to scale to thousands of nodes. The scalability of YARN is determined by the Resource Manager, and is proportional to number of nodes, active applications, active containers, and frequency of heartbeat (of both nodes and applications). Lowering heartbeat can provide scalability increase, but is detrimental to utilization (see old Hadoop 1.x experience). This document described a federation-based approach to scale a single YARN cluster to tens of thousands of nodes, by federating multiple YARN sub-clusters. The proposed approach is to divide a large (10-100k nodes) cluster into smaller units called sub-clusters, each with its own YARN RM and compute nodes. The federation system will stitch these sub-clusters together and make them appear as one large YARN cluster to the applications. The applications running in this federated environment will see a single massive YARN cluster and will be able to schedule tasks on any node of the federated cluster. Under the hood, the federation system will negotiate with sub-clusters resource managers and provide resources to the application. The goal is to allow an individual job to “span” sub-clusters seamlessly. YARN 可以伸缩到数千个节点。 YARN 的可伸缩性由资源管理器决定，与节点数量、活动的应用程序、活动的容器和心跳频率（节点和应用程序）成正比。降低心跳可以提高可伸缩性，但不利于利用率（请参阅旧的Hadoop1.x体验）。本文描述了一种基于联邦（federation）的方法，通过将多个 YARN 子集群结成（federate）联邦，将单个 YARN 集群扩展到数万个节点。该方法将一个大的（10-100K节点）集群划分为更小的子集群单元，每个子集群都有自己的 YARN RM 和计算节点。联邦系统（federation system）将这些子集群结合（stitch）在一起，使它们成为应用程序中的一个大型 YARN 集群。在这个联合环境中运行的应用程序将看到一个单个巨大的 YARN 集群，并且能够在联邦集群的任何节点上调度任务。在幕后，联邦系统将与子集群资源管理器协商，并向应用程序提供资源。目标是允许单个作业无缝地“跨越”子集群。 This design is structurally scalable, as we bound the number of nodes each RM is responsible for, and appropriate policies, will try to ensure that the majority of applications will reside within a single sub-cluster, thus the number of applications each RM will see is also bounded. This means we could almost linearly scale, by simply adding sub-clusters (as very little coordination is needed across them). This architecture can provide very tight enforcement of scheduling invariants within each sub-cluster (simply inherits from YARN), while continuous rebalancing across subcluster will enforce (less strictly) that these properties are also respected at a global level (e.g., if a sub-cluster loses a large number of nodes, we could re-map queues to other sub-clusters to ensure users running on the impaired sub-cluster are not unfairly affected). 这种设计在结构上是可伸缩的，因为我们限制了了每个 RM 负责的节点的数量，并且适当的策略将尝试确保大多数应用程序将驻留在单个子集群中，因此每个 RM 将看到的应用程序的数量也是有界的。这意味着我们可以通过简单地添加子集群（因为在它们之间几乎不需要协调）来线性扩展。这种体系结构可以在每个子集群中提供对调度不变量进行非常严格执行（简单地继承自 YARN），而跨子集群的连续重新平衡将强制（不太严格）在全局级别上也遵守这些属性（例如，如果子集群丢失了大量的节点，我们可以将队列重新映射到其他子集群，以确保在受损子集群上运行的用户不会受到不公平的影响）。 Federation is designed as a “layer” atop of existing YARN codebase, with limited changes in the core YARN mechanisms. 联邦被设计为现有 YARN 代码库的顶“层”，核心 YARN 机制的变化有限。 Assumptions: We assume reasonably good connectivity across sub-clusters (e.g., we are not looking to federate across DC yet, though future investigations of this are not excluded). 我们假设子集群之间具有相当好的连通性（例如，我们还不希望在整个DC之间建立联邦，尽管未来对此的调查并未排除在外）。 We rely on HDFS federation (or equivalently scalable DFS solutions) to take care of scalability of the store side. 我们依赖 HDFS 联邦（或同等可扩展的 HDFS 解决方案）来处理存储端的可扩展性。 ArchitectureOSS YARN has been known to scale up to about few thousand nodes. The proposed architecture leverages the notion of federating a number of such smaller YARN clusters, referred to as sub-clusters, into a larger federated YARN cluster comprising of tens of thousands of nodes. The applications running in this federated environment see a unified large YARN cluster and will be able to schedule tasks on any nodes in the cluster. Under the hood, the federation system will negotiate with sub-clusters RMs and provide resources to the application. The logical architecture in Figure 1 shows the main components that comprise the federated cluster, which are described below. 据了解，OSS YARN 可以扩展到大约几千个节点。所提出的架构利用了一些较小的 YARN 集群（称为子集群）联合成由数万个节点组成的较大的联邦 YARN 集群的概念。在这个联邦环境中运行的应用程序可以看到一个统一的大型 YARN 集群，并且能够在集群中的任何节点上调度任务。在这种情况下，联邦系统将与子集群的 RMs 进行协商，并为应用程序提供资源。图1中的逻辑架构显示了组成联邦集群的主要组件，如下所述。 YARN Sub-clusterA sub-cluster is a YARN cluster with up to few thousands nodes. The exact size of the sub-cluster will be determined considering ease of deployment/maintenance, alignment with network or availability zones and general best practices. 子集群是一个具有数千个节点的 YARN 集群。考虑到易于部署或维护、与网络或区域可用性以及通用最佳实践，将确定子集群的准确大小。 The sub-cluster YARN RM will run with work-preserving high-availability turned-on, i.e., we should be able to tolerate YARN RM, NM failures with minimal disruption. If the entire sub-cluster is compromised, external mechanisms will ensure that jobs are resubmitted in a separate sub-cluster (this could eventually be included in the federation design). 子集群 YARN RM 将在保持高可用性的情况下运行，即，我们应该能够承受 YARN RM、NM故障，且受损最小。如果整个子集群受到破坏，外部机制将确保在不同的子集群中重新提交作业（这可能最终包括在联邦集群设计中）。 Sub-cluster is also the scalability unit in a federated environment. We can scale out the federated environment by adding one or more sub-clusters. 子集群也是联邦环境中的可伸缩性单元。我们可以通过添加一个或多个子集群来扩展联邦环境。 Note: by design each sub-cluster is a fully functional YARN RM, and its contribution to the federation can be set to be only a fraction of its overall capacity, i.e. a sub-cluster can have a “partial” commitment to the federation, while retaining the ability to give out part of its capacity in a completely local way. 注：根据设计，每个子集群都是一个功能齐全的 YARN RM，其对联邦的贡献可以设置为其总容量的一小部分，即子集群可以对联邦“部分”承诺，同时保留了部分容量完全给本地运行的的能力。 RouterYARN applications are submitted to one of the Routers, which in turn applies a routing policy (obtained from the Policy Store), queries the State Store for the sub-cluster URL and redirects the application submission request to the appropriate sub-cluster RM. We call the sub-cluster where the job is started the “home sub-cluster”, and we call “secondary sub-clusters” all other sub-cluster a job is spanning on. The Router exposes the ApplicationClientProtocol to the outside world, transparently hiding the presence of multiple RMs. To achieve this the Router also persists the mapping between the application and its home sub-cluster into the State Store. This allows Routers to be soft-state while supporting user requests cheaply, as any Router can recover this application to home sub-cluster mapping and direct requests to the right RM without broadcasting them. For performance caching and session stickiness might be advisable. The state of the federation (including applications and nodes) is exposed through the Web UI. YARN 应用程序被提交到其中一个路由器（Router），该路由器依次应用路由策略（从 Policy Store 中获得），查询 Policy Store 得到子集群 URL，并将应用程序提交的请求重定向到相应的子集群 RM。我们将启动作业的子集群称为“home sub-cluster”，并将作业所跨越的所有其他子集群称为“secondary sub-cluster”。路由器向外界公开ApplicationClientProtocol，透明（transparently ）地隐藏多个 RMs 的存在。为了实现这一点，路由器（Router）还将应用程序与其 home sub-cluster 之间的映射一直保存到 State Store 中。这允许路由器处于软状态（soft-state），同时以较低的成本支持用户请求，因为任何路由器都可以将此应用程序恢复到 home sub-cluster 映射，并将请求直接发送到正确的 RM，而无需广播它们。对于性能缓存（performance caching）和会话粘性（session stickiness）可能是明智的。联邦状态（包括应用程序和节点）通过Web UI公开。 AMRMProxyThe AMRMProxy is a key component to allow the application to scale and run across sub-clusters. The AMRMProxy runs on all the NM machines and acts as a proxy to the YARN RM for the AMs by implementing the ApplicationMasterProtocol. Applications will not be allowed to communicate with the sub-cluster RMs directly. They are forced by the system to connect only to the AMRMProxy endpoint, which would provide transparent access to multiple YARN RMs ( by dynamically routing/splitting/merging the communications ). At any one time, a job can span across one home sub-cluster and multiple secondary sub-clusters, but the policies operating in the AMRMProxy try to limit the footprint of each job to minimize overhead on the scheduling infrastructure (more in section on scalability/load). The interceptor chain architecture of the ARMMProxy is showing in figure. AMRMProxy 是允许应用程序在子集群之间扩展和运行的关键组件。AMRMProxy 运行在所有 NM 机器上，通过实现 ApplicationMasterProtocol 作为 AMs 的 YARN RM的代理。不允许应用程序直接与子集群 RMs 通信。它们被系统强制只连接到 AMRMProxy 端点，这将提供对多个 YARN RMs 的透明（transparently）访问（通过动态路由/拆分/合并通信）。在任何时候，一个作业都可以跨越一个 home sub-cluster 和多个 secondary sub-clusters，但是 AMRMProxy 中运行的策略试图限制每个作业的占用空间，以最小化在负责调度的基础结构（scheduling infrastructure）上的开销（更多内容请参见可伸缩性/负载部分）。AMRMProxy 的拦截器链结构如图所示。 Role of AMRMProxy Protect the sub-cluster YARN RMs from misbehaving AMs. The AMRMProxy can prevent DDOS attacks by throttling/killing AMs that are asking too many resources. 保护 sub-cluster YARN RMs 不受不良 AMs 的影响。AMRMProxy 可以通过限制/杀死请求过多资源的 AMs 来防止DDO攻击。 Mask the multiple YARN RMs in the cluster, and can transparently allow the AM to span across sub-clusters. All container allocations are done by the YARN RM framework that consists of the AMRMProxy fronting the home and other sub-cluster RMs. 遮掩集群中的多个 YARN RMs ，并透明地（transparently）允许 AM 跨越子群。所有的容器分配都是通过YARN RM 框架完成的，该框架由 home sub-cluster RM 和 other sub-cluster RMs 的 AMRMProxy 组成。 Intercepts all the requests, thus it can enforce application quotas, which would not be enforceable by sub-cluster RM (as each only see a fraction of the AM requests). 截取所有请求，因此它可以强制应用程序配额，而子集群RM将无法强制应用程序配额（因为每个请求只看到AM请求的一部分）。 The AMRMProxy can enforce load-balancing / overflow policies. AMRMProxy 可以强制执行负载平衡/溢出策略。 Global Policy GeneratorGlobal Policy Generator overlooks the entire federation and ensures that the system is configured and tuned properly all the time. A key design point is that the cluster availability does not depends on an always-on GPG. The GPG operates continuously but out-of-band from all cluster operations, and provide us with a unique vantage point, that allows to enforce global invariants, affect load balancing, trigger draining of sub-clusters that will undergo maintenance, etc. More precisely the GPG will update user capacity allocation-to-subcluster mappings, and more rarely change the policies that run in Routers, AMRMProxy (and possible RMs). 全局策略生成器（Global Policy Generator: GPG）整体把控（overlook）整个联邦系统，并确保系统始终正确配置和调优。一个关键的设计点是集群的可用性并不依赖于一个始终在线的 GPG。GPG连续运行，但在所有集群的操作都是out-of-band，并为我们提供了一个独特的优势点，允许强制执行全局不变量、影响负载平衡、触发以下动作：排除要进行维护的子集群等。更准确地说，GPG将更新：用户与分配给子集群的容量之间的映射关系，并且很少更改在路由器、AMRMProxy （以及可能的 RMs）中运行的策略。 In case the GPG is not-available, cluster operations will continue as of the last time the GPG published policies, and while a long-term unavailability might mean some of the desirable properties of balance, optimal cluster utilization and global invariants might drift away, compute and access to data will not be compromised. 如果 GPG 不可用，则集群操作将按照在 GPG 上次发布的策略继续进行，而长期不可用可能意味着平衡、最佳集群利用率和全局不变量的某些理想属性可能会偏移，计算以及访问数据不会被妥协。 NOTE: In the current implementation the GPG is a manual tuning process, simply exposed via a CLI (YARN-3657).注意：在当前的实现中，GPG 是一个手动调优过程，只需通过 CLI（yarn-3657）公开即可。 This part of the federation system is part of future work in YARN-5597.联邦系统的这一部分是 YARN-5597 未来工作的一部分。 Federation State-StoreThe Federation State defines the additional state that needs to be maintained to loosely couple multiple individual sub-clusters into a single large federated cluster. This includes the following information: 联邦状态（ Federation State）定义了需要维护的额外状态，以便将多个子集群松散地耦合到单个大型联邦集群中。这包括以下信息： 1. Sub-cluster MembershipThe member YARN RMs continuously heartbeat to the state store to keep alive and publish their current capability/load information. This information is used by the Global Policy Generator (GPG) to make proper policy decisions. Also this information can be used by routers to select the best home sub-cluster. This mechanism allows us to dynamically grow/shrink the “cluster fleet” by adding or removing sub-clusters. This also allows for easy maintenance of each sub-cluster. This is new functionality that needs to be added to the YARN RM but the mechanisms are well understood as it’s similar to individual YARN RM HA. 成员 YARN RMs 发出连续心跳到 State Store来保持激活状态并发布其当前能力值（capability）/负载信息（load information）。全局策略生成器（GPG）使用此信息做出正确的策略决策。路由器也可以利用这些信息来选择最佳的 home sub-cluster。这个机制允许我们通过添加或删除子集群来动态地增长（grow）/收缩（shrink）“集群的机群（cluster fleet）”。这还允许轻松维护每个子集群。这是需要添加到 YARN RM 中的新功能，但机制已被很好地理解，因为它类似于单个 YARN RM HA。 2. Application’s Home Sub-clusterThe sub-cluster on which the Application Master (AM) runs is called the Application’s “home sub-cluster”. The AM is not limited to resources from the home sub-cluster but can also request resources from other sub-clusters, referred to as secondary sub-clusters. The federated environment will be configured and tuned periodically such that when an AM is placed on a sub-cluster, it should be able to find most of the resources on the home sub-cluster. Only in certain cases it should need to ask for resources from other sub-clusters. 运行 Application Master（AM）的子集群称为应用程序的 “home sub-cluster” 。AM不限于来自 home sub-cluster 的资源，还可以从其他子集群（被称为 secondary sub-clusters）请求资源。联邦环境将定期进行配置和调优，这样当 AM 放置在一个子集群上时，它应该能够找到 home sub-cluster 上的大部分资源。只有在某些情况下，它才需要从其他子集群请求资源。 Federation Policy StoreThe federation Policy Store is a logically separate store (while it might be backed by the same physical component), which contains information about how applications and resource requests are routed to different sub-clusters. The current implementation provides several policies, ranging from random/hashing/round robin/priority to more sophisticated ones which account for sub-cluster load, and request locality needs. 联邦 Policy Store 是一个逻辑上独立的存储（虽然它可能由同一物理组件支持），其中包含如何将应用程序和资源请求路由到不同子群集的信息。当前的实现提供了几个策略，从随机/哈希/循环/优先级到更复杂的策略，这些策略负责子集群负载和请求位置需求。 Running Applications across Sub-ClustersWhen an application is submitted, the system will determine the most appropriate sub-cluster to run the application, which we call as the application’s home sub-cluster. All the communications from the AM to the RM will be proxied via the AMRMProxy running locally on the AM machine. AMRMProxy exposes the same ApplicationMasterService protocol endpoint as the YARN RM. The AM can request containers using the locality information exposed by the storage layer. In ideal case, the application will be placed on a sub-cluster where all the resources and data required by the application will be available, but if it does need containers on nodes in other sub-clusters, AMRMProxy will negotiate with the RMs of those sub-clusters transparently and provide the resources to the application, thereby enabling the application to view the entire federated environment as one massive YARN cluster. AMRMProxy, Global Policy Generator (GPG) and Router work together to make this happen seamlessly. 提交应用程序时，系统将确定运行应用程序的最合适的子集群，我们称之为应用程序的 home sub-cluster。从 AM 到 RM 的所有通信都将通过在 AM 机器上运行的 AMRMProxy 进行代理。 AMRMProxy 开放了相同的 ApplicationMasterService 协议端点作为与 YARN RM。AM可以使用存储层公开的位置信息请求容器。理想情况下，应用程序将被放置在子集群上，应用程序所需的所有资源和数据都将可用，但如果它确实需要其他子集群中节点上的容器， AMRMProxy 将透明地与这些子集群的 RMs 协商，并提供资源。源到应用程序，从而使应用程序能够将整个联邦环境视为一个巨大的 YARN 集群。 AMRMProxy 、全局策略生成器（gpg）和路由器协同工作，实现无缝连接。 The figure shows a sequence diagram for the following job execution flow: 该图显示了以下作业执行流程的序列图： The Router receives an application submission request that is complaint to the YARN Application Client Protocol. 路由器接收到一个应用程序提交请求，这是对 YARN 应用客户端协议的投诉。 The router interrogates a routing table / policy to choose the “ “home RM” ” for the job (the policy configuration is received from the state-store on heartbeat). 路由器询问路由表/策略以选择作业的“ “home RM” ”（有心跳的时候，策略配置从状态存储 (state-store) 接收）。 The router queries the membership state to determine the endpoint of the “home RM” . 路由器查询成员身份状态以确定 “home RM” 的端点。 The router then redirects the application submission request to the “home RM” . 然后，路由器将应用程序提交请求重定向到 “home RM” 。 The router updates the application state with the home sub-cluster identifier. 路由器使用 home sub-cluster 标识更新应用程序状态。 Once the application is submitted to the “home RM” , the stock YARN flow is triggered, i.e. the application is added to the scheduler queue and its AM started in the home sub-cluster, on the first NodeManager that has available resources. 一旦应用程序提交到 “home RM” ，库存的 YARN 流就会被触发，即将应用程序添加到调度器队列中，并在home sub-cluster 中具有可用资源的第一个节点管理器（NM） 来启动它的 AM。 a. During this process, the AM environment is modified by indicating that the address of the AMRMProxy as the YARN RM to talk to. a. 在此过程中，通过指出 AMRMProxy 的地址作为要通信的 YARN RM 来修改 AM 环境（变量）。 b. The security tokens are also modified by the NM when launching the AM, so that the AM can only talk with the AMRMProxy. Any future communication from AM to the YARN RM is mediated by the AMRMProxy. b.安全令牌（security tokens）在启动AM时也会被NM修改，因此 AM 只能与 AMRMproxy 通信。从 AM 到 YARN RM 的任何未来通信都由 Amrmproxy 做媒。 The AM will then request containers using the locality information exposed by HDFS. 然后，AM 将使用 HDFS 公开的位置信息请求容器。 Based on a policy the AMRMProxy can impersonate the AM on other sub-clusters, by submitting an Unmanaged AM, and by forwarding the AM heartbeats to relevant sub-clusters. 根据策略， AMRMProxy 可以通过提交还未被管理的AM（Unmanaged AM）以及将 AM 心跳转发到相关子群集来扮演其他子群集上的 AM。 a. Federation supports multiple application attempts with AMRMProxy HA. AM containers will have different attempt id in home sub-cluster, but the same Unmanaged AM in secondaries will be used across attempts. a. 联邦支持使用 AMRMProxy HA 进行多个应用程序尝试。AM 容器 在 home sub-cluster 中具有不同的尝试ID，但在不同的尝试之间将使用secondaries中相同的 UAM (Unmanaged AM)。 b. When AMRMProxy HA is enabled, UAM token will be stored in Yarn Registry. In the registerApplicationMaster call of each application attempt, AMRMProxy will go fetch existing UAM tokens from registry (if any) and re-attached to the existing UAMs. b. 启用 AMRMProxy HA 后，UAM(Unmanaged AM) 令牌将存储在 YARN 注册表中。在每次应用程序尝试的registerApplicationMaster 调用中， AMRMProxy 将从注册表（如果有）中获取现有的UAM令牌，并重新连接到现有的UAM。 The AMRMProxy will use both locality information and a pluggable policy configured in the state-store to decide whether to forward the resource requests received by the AM to the Home RM or to one (or more) Secondary RMs. In Figure 1, we show the case in which the AMRMProxy decides to forward the request to the secondary RM. AMRMProxy 将同时使用位置信息和状态存储（state-store）中配置的可插拔策略来决定是将AM接收到的资源请求转发到 Home RM 还是一个（或多个）Secondary RMs。在图1中，我们展示了 AMRMProxy 决定将请求转发到 secondary RM 的情况。 The secondary RM will provide the AMRMProxy with valid container tokens to start a new container on some node in its sub-cluster. This mechanism ensures that each sub-cluster uses its own security tokens and avoids the need for a cluster wide shared secret to create tokens. The AMRMProxy forwards the allocation response back to the AM. secondary RM 将向 AMRMProxy 提供有效的容器令牌，以便在其子集群中的某个节点上启动新的容器。此机制确保每个子集群使用自己的安全令牌（security tokens），并避免需要集群范围的共享机密来创建令牌。 AMRMProxy 将分配响应转发回 AM。 The AM starts the container on the target NodeManager (on sub-cluster 2) using the standard YARN protocols.AM使用标准 YARN 协议在目标节点管理器（在子集群2）上启动容器。 ConfigurationTo configure the YARN to use the Federation, set the following property in the conf/yarn-site.xml: 要将 YARN 配置为使用 Federation，请在 conf/yarn-site.xml 中设置以下属性： EVERYWHERE:These are common configurations that should appear in the conf/yarn-site.xml at each machine in the federation. 这些是常见的配置，应该出现在联邦中每台机器的 conf/yarn-site.xml 中。 Property Example Description yarn.federation.enabled true Whether federation is enabled or not是否启用联邦机制 yarn.resourcemanager.cluster-id &lt;unique-subcluster-id&gt; The unique subcluster identifier for this RM (same as the one used for HA).为这个 RM 使用子集群的标识符（HA模式下，互为备份的 RM 也是同样的配置） State-Store:Currently, we support ZooKeeper and SQL based implementations of the state-store.目前，我们支持状态存储的ZooKeeper和基于SQL的实现。 Note: The State-Store implementation must always be overwritten with one of the below.注意：状态存储实现必须始终被下面的某个覆盖。 ZooKeeper: one must set the ZooKeeper settings for Hadoop:动物园管理员：必须为Hadoop设置 ZooKeeper： Property Example Description yarn.federation.state-store.class org.apache.hadoop.yarn.&lt;br /&gt;server.federation.store.&lt;br /&gt;impl.ZookeeperFederationStateStore The type of state-store to use. hadoop.zk.address host:port The address for the ZooKeeper ensemble. SQL: one must setup the following parameters: SQL: 必须设置以下参数： Property Example Description yarn.federation.state-store.class org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore The type of state-store to use. yarn.federation.state-store.sql.url jdbc:mysql://&lt;host&gt;:&lt;port&gt;/FederationStateStore For SQLFederationStateStore the name of the DB where the state is stored. yarn.federation.state-store.sql.jdbc-class com.mysql.jdbc.jdbc2.optional.MysqlDataSource For SQLFederationStateStore the jdbc class to use. yarn.federation.state-store.sql.username &lt;dbuser&gt; For SQLFederationStateStore the username for the DB connection. yarn.federation.state-store.sql.password &lt;dbpass&gt; For SQLFederationStateStore the password for the DB connection. We provide scripts for MySQL and Microsoft SQL Server. 我们为MySQL和Microsoft SQL Server提供脚本。 For MySQL, one must download the latest jar version 5.x from MVN Repository and add it to the CLASSPATH. Then the DB schema is created by executing the following SQL scripts in the database: 对于MySQL，必须从MVN存储库下载最新的JAR5.x版本，并将其添加到类路径。然后，通过在数据库中执行以下SQL脚本来创建 DB schema： sbin/FederationStateStore/MySQL/FederationStateStoreDatabase.sql sbin/FederationStateStore/MySQL/FederationStateStoreUser.sql sbin/FederationStateStore/MySQL/FederationStateStoreTables.sql sbin/FederationStateStore/MySQL/FederationStateStoreStoredProcs.sql In the same directory we provide scripts to drop the Stored Procedures, the Tables, the User and the Database. 在同一个目录中，我们提供了删除存储过程、表、用户和数据库的脚本。 Note: the defines a default user/password for the DB that you are highly encouraged to set this to a proper strong password. 注意：FederationStateStoreUser.sql 定义了数据库的默认用户/密码，强烈建议您将其设置为正确的强密码。 For SQL-Server, the process is similar, but the jdbc driver is already included. SQL-Server scripts are located in sbin/FederationStateStore/SQLServer/. 对于SQL Server，过程类似，但已经包含了JDBC驱动程序。SQL Server脚本位于sbin/federationstatestore/sql server/中。 Optional: Property Example Description yarn.federation.failover.enabled true Whether should retry considering RM failover within each subcluster.考虑到每个子群集中的RM故障转移，是否应重试。 yarn.federation.blacklist-subclusters &lt;subcluster-id&gt; A list of black-listed sub-clusters, useful to disable a sub-cluster黑名单子群集的列表，用于关闭子集群 yarn.federation.policy-manager org.apache.hadoop.yarn.&lt;br /&gt;server.federation.policies.&lt;br /&gt;manager.&lt;br /&gt;WeightedLocalityPolicyManager The choice of policy manager determines how Applications and ResourceRequests are routed through the system.策略管理器的选择决定了应用程序和资源请求在系统中的路由方式。 yarn.federation.policy-manager-params &lt;binary&gt; The payload that configures the policy. In our example a set of weights for router and amrmproxy policies. This is typically generated by serializing a policymanager that has been configured programmatically, or by populating the state-store with the .json serialized form of it.配置策略的有效负载。在我们的示例中，路由器和 AMRMProxy 策略的一组权重。这通常是通过序列化已通过编程方式配置的 PolicyManager 或使用.json序列化形式填充状态存储来生成的。 yarn.federation.subcluster-resolver.class org.apache.hadoop.&lt;br /&gt;yarn.server.federation.&lt;br /&gt;resolver.&lt;br /&gt;DefaultSubClusterResolverImpl The class used to resolve which subcluster a node belongs to, and which subcluster(s) a rack belongs to.用于解析节点所属的子群集和机架所属的子群集的类。 yarn.federation.machine-list node1,subcluster1,rack1\n&lt;br /&gt; node2 , subcluster2, RACK1\n&lt;br /&gt; node3,subcluster3, rack2\n&lt;br /&gt; node4, subcluster3, rack2\n a list of Nodes, Sub-clusters, Rack, used by the DefaultSubClusterResolverImpl默认子集群使用的节点、子群集、机架列表。 ON RMs:These are extra configurations that should appear in the conf/yarn-site.xml at each ResourceManager.这些是额外的配置，应该出现在每个资源管理器的conf/yarn-site.xml中。 Property Example Description yarn.resourcemanager.epoch &lt;unique-epoch&gt; The seed value for the epoch. This is used to guarantee uniqueness of container-IDs generate by different RMs. It must therefore be unique among sub-clusters and well-spaced to allow for failures which increment epoch. Increments of 1000 allow for a large number of sub-clusters and practically ensure near-zero chance of collisions (a clash will only happen if a container is still alive for 1000 restarts of one RM, while the next RM never restarted, and an app requests more containers).epoch的种子值。这是为了保证不同 RMs 生成的容器ID的唯一性。因此，它在子集群中必须是唯一的，并且具有良好的间隔以允许增加epoch出现的失败。增量1000允许大量的子集群，并且实际上可以确保几乎没有发生冲突的机会（只有当容器在1000次重新启动一个 RM 时仍处于活动状态，而下一个 RM 从未重新启动，并且应用程序请求更多容器时，才会发生冲突）。 Optional: Property Example Description yarn.federation.state-store.heartbeat-interval-secs 60 The rate at which RMs report their membership to the federation to the central state-store.RMs 向中央状态存储报告其联盟成员身份的时间间隔。 ON ROUTER:These are extra configurations that should appear in the conf/yarn-site.xml at each Router.这些是额外的配置，应该出现在每个路由器的 conf/yarn-site.xml 中。 Property Example Description yarn.router.bind-host 0.0.0.0 Host IP to bind the router to. The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in yarn.router.*.address respectively. This is most useful for making Router listen to all interfaces by setting to 0.0.0.0.路由器绑定的主机IP。服务器将绑定到的实际地址。如果设置了此可选地址，则 RPC 和 webapp 服务器将分别绑定到此地址和 yarn.router.*.address 中指定的端口。这对于通过设置为0.0.0.0将路由器列表设置为所有接口最有用。 yarn.router.clientrm.&lt;br /&gt;interceptor-class.pipeline org.apache.hadoop.yarn.&lt;br /&gt;server.router.clientrm.&lt;br /&gt;FederationClientInterceptor A comma-seperated list of interceptor classes to be run at the router when interfacing with the client. The last step of this pipeline must be the Federation Client Interceptor.与客户端接口交互时要在路由器上运行的截断类的逗号分隔列表。此管道的最后一步必须是Federation Client拦截器。 Optional: Property Example Description yarn.router.hostname 0.0.0.0 Router host name. yarn.router.clientrm.address 0.0.0.0:8050 路由主机名 yarn.router.webapp.address 0.0.0.0:8089 Webapp address at the router.webapp在路由器的地址 yarn.router.admin.address 0.0.0.0:8052 Admin address at the router.路由器的管理地址 yarn.router.webapp.https.address 0.0.0.0:8091 Secure webapp address at the router.在路由器上安全的webapp地址 yarn.router.submit.retry 3 The number of retries in the router before we give up.在放弃之前，在路由器上的尝试次数。 yarn.federation.statestore.max-connections 10 This is the maximum number of parallel connections each Router makes to the state-store.每个路由器连接到state-store的最大并发数 yarn.federation.cache-ttl.secs 60 The Router caches informations, and this is the time to leave before the cache is invalidated.路由器缓存信息，这是缓存的有效时间。 yarn.router.webapp.&lt;br /&gt;interceptor-class.pipeline org.apache.hadoop.yarn.&lt;br /&gt;server.router.webapp.&lt;br /&gt;FederationInterceptorREST A comma-seperated list of interceptor classes to be run at the router when interfacing with the client via REST interface. The last step of this pipeline must be the Federation Interceptor REST.当通过 REST 接口与客户机交互时，要在路由器上运行的拦截器类的逗号分隔列表。此管道的最后一步必须是 Federation Interceptor REST 。 ON NMs:These are extra configurations that should appear in the conf/yarn-site.xml at each NodeManager.这些额外的配置应该出现在每个节点管理器的 conf/yarn-site.xml 中。 Property Example Description yarn.nodemanager.&lt;br /&gt;amrmproxy.enabled true Whether or not the AMRMProxy is enabled.是否启用 AMRMProxy yarn.nodemanager.amrmproxy.&lt;br /&gt;interceptor-class.pipeline org.apache.hadoop.yarn.&lt;br /&gt;server.nodemanager.amrmproxy.&lt;br /&gt;FederationInterceptor A comma-separated list of interceptors to be run at the amrmproxy. For federation the last step in the pipeline should be the FederationInterceptor.要在 AMRMProxy 上运行的拦截器的逗号分隔列表。对于联邦，管道中的最后一步应该是联合拦截器。 yarn.client.&lt;br /&gt;failover-proxy-provider org.apache.hadoop.yarn.&lt;br /&gt;server.federation.failover&lt;br /&gt;.FederationRMFailoverProxyProvider The class used to connect to the RMs by looking up the membership information in federation state-store. This must be set if federation is enabled, even if RM HA is not enabled.通过在联邦 state-store 中查找成员身份信息来连接到 RMs的类。如果启用了联邦，即使未启用RM HA，也必须设置此选项。 Optional: Property Example Description yarn.nodemanager.amrmproxy.ha.enable true Whether or not the AMRMProxy HA is enabled for multiple application attempt suppport.是否为多个应用程序尝试支持启用 AMRMProxy HA。 yarn.federation.statestore.max-connections 1 The maximum number of parallel connections from each AMRMProxy to the state-store. This value is typically lower than the router one, since we have many AMRMProxy that could burn-through many DB connections quickly.从每个 AMRMProxy 到状态存储的最大并行连接数。这个值通常低于Router在这个属性的值，因为我们有许多 AMRMProxy 可以快速通过许多 DB 连接。 yarn.federation.cache-ttl.secs 300 The time to leave for the AMRMProxy cache. Typically larger than at the router, as the number of AMRMProxy is large, and we want to limit the load to the centralized state-store.离开 Amrmproxy 缓存的时间。通常比 Router在这个属性上设置的值要大，因为AMRMProxy 的数量很大，我们希望将负载限制到集中式状态存储。 Running a Sample JobIn order to submit jobs to a Federation cluster one must create a seperate set of configs for the client from which jobs will be submitted. In these, the conf/yarn-site.xml should have the following additional configurations: 为了将作业提交到联邦集群，必须为将从中提交作业的客户端创建一组单独的配置。在这些配置中，conf/yarn-site.xml 应该具有以下附加配置： Property Example Description yarn.resourcemanager.address &lt;router_host&gt;:8050 Redirects jobs launched at the client to the router’s client RM port.将在客户端启动的作业重定向到路由器的客户端 RM 端口。 yarn.resourcemanger.scheduler.address localhost:8049 Redirects jobs to the federation AMRMProxy port.将作业重定向到联邦的AMRMProxy 端口 Any YARN jobs for the cluster can be submitted from the client configurations described above. In order to launch a job through federation, first start up all the clusters involved in the federation as described here. Next, start up the router on the router machine with the following command: 集群的任何 YARN job 都可以从上面描述的客户机配置提交。为了通过联邦启动一个作业，首先启动联邦中涉及的所有集群，如本文所述。接下来，使用以下命令在路由器计算机上启动路由器： 1$HADOOP_HOME/bin/yarn --daemon start router Now with $HADOOP_CONF_DIR pointing to the client configurations folder that is described above, run your job the usual way. The configurations in the client configurations folder described above will direct the job to the router’s client RM port where the router should be listening after being started. Here is an example run of a Pi job on a federation cluster from the client: 现在，当 $HADOOP_CONF_DIR 指向上面描述的客户端配置文件夹时，以通常的方式运行您的作业。上面描述的客户端配置文件夹中的配置将把作业引导到路由器的客户端 RM 端口，路由器在启动后应该在该端口上进行侦听。下面是从客户端在联邦集群上运行 Pi 作业的示例： 1$HADOOP_HOME/bin/yarn jar hadoop-mapreduce-examples-3.0.0.jar pi 16 1000 This job is submitted to the router which as described above, uses a generated policy from the GPG to pick a home RM for the job to which it is submitted. 此作业将提交给路由器，如上文所述，路由器使用GPG生成的策略为其提交到的作业选择一个 Home RM。 The output from this particular example job should be something like: 这个特定示例作业的输出应该类似于： 1234567891011122017-07-13 16:29:25,055 INFO mapreduce.Job: Job job_1499988226739_0001 running in uber mode : false2017-07-13 16:29:25,056 INFO mapreduce.Job: map 0% reduce 0%2017-07-13 16:29:33,131 INFO mapreduce.Job: map 38% reduce 0%2017-07-13 16:29:39,176 INFO mapreduce.Job: map 75% reduce 0%2017-07-13 16:29:45,217 INFO mapreduce.Job: map 94% reduce 0%2017-07-13 16:29:46,228 INFO mapreduce.Job: map 100% reduce 100%2017-07-13 16:29:46,235 INFO mapreduce.Job: Job job_1499988226739_0001 completed successfully...Job Finished in 30.586 secondsEstimated value of Pi is 3.14250000...... The state of the job can also be tracked on the Router Web UI at routerhost:8089. Note that no change in the code or recompilation of the input jar was required to use federation. Also, the output of this job is the exact same as it would be when run without federation. Also, in order to get the full benefit of federation, use a large enough number of mappers such that more than one cluster is required. That number happens to be 16 in the case of the above example. 作业的状态也可以在路由器 Web UI 上的 routerhost:8089上跟踪。注意，使用联邦不需要更改代码或重新编译输入的jar包。此外，此作业的输出与在没有联合的情况下运行时的输出完全相同。此外，为了充分利用联合，请使用足够多的映射器，以便多个集群的需求。在上面的例子中，这个数字恰好是16。 A list of References 官方关于 YARN Federation issue: https://issues.apache.org/jira/browse/YARN-2915 HDFS基于路由的Federation方案 YARN Federation的架构设计]]></content>
      <categories>
        <category>中文, english</category>
      </categories>
      <tags>
        <tag>Hadoop YARN</tag>
        <tag>Distributed System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transformer论文简记]]></title>
    <url>%2F2019%2F03%2F10%2FTransformer_learning-resources%2F</url>
    <content type="text"><![CDATA[资源Transformer来自论文: All Attention Is You Need 别人的总结资源： 谷歌官方AI博客: Transformer: A Novel Neural Network Architecture for Language Understanding Attention机制详解（二）——Self-Attention与Transformer 谷歌软件工程师 一个是Jay Alammar可视化地介绍Transformer的博客文章 The Illustrated Transformer，非常容易理解整个机制，建议先从这篇看起，这是中文翻译版本； 放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较 中科院软件所 · 自然语言处理 /搜索 10年工作经验的博士（阿里，微博）； Calvo的博客：Dissecting BERT Part 1: The Encoder，尽管说是解析Bert，但是因为Bert的Encoder就是Transformer，所以其实它是在解析Transformer，里面举的例子很好； 再然后可以进阶一下，参考哈佛大学NLP研究组写的“The Annotated Transformer. ”，代码原理双管齐下，讲得也很清楚。 《Attention is All You Need》浅读（简介+代码） 这个总结的角度也很棒。 总结这里总结的思路：自顶向下方法 model architecture一图胜千言，6层编码器和解码器，论文中没有说为什么是6这个特定的数字 Encoder Decoder如果我们想做堆叠了2个Encoder和2个Decoder的Transformer，那么它可视化就会如下图所示： 翻译输出的时候，前一个时间步的输出，要作为下一个时间步的解码器端的输入，下图展示第2~6步： 下面是一个单层：Nx 表示 N1, … , N6 层 partsMulti-head Attention其实就是多个Self-Attention结构的结合，每个head学习到在不同表示空间中的特征，所谓“多头”（Multi-Head），就是做h次同样的事情（参数不共享），然后把结果拼接。 Self-Attention实际上是scaled dot-product attention 缩放的点积注意力： Addresidual connection: skip connection 跳跃了解 Normlayer norm 归一化层 Positional encodinggoogle的这个位置编码很魔幻，是两个周期函数：sine cosine数学系出生的博主的解释：《Attention is All You Need》浅读（简介+代码），相比之下Bert的位置编码直观的多。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[了解Hadoop YARN架构]]></title>
    <url>%2F2019%2F02%2F01%2FAchitecture_of_Apache_Hadoop_YARN%2F</url>
    <content type="text"><![CDATA[前言本文是对 官网：Architecture of Apache Hadoop YARN 的翻译，记录本篇的时候，版本为 2.9.2。本文简单过一下，深入了解可以看看列出的参考资料。 正文The fundamental idea of YARN is to split up the functionalities of resource management and job scheduling/monitoring into separate daemons. The idea is to have a global ResourceManager (RM) and per-application ApplicationMaster (AM). An application is either a single job or a DAG of jobs. YARN 的基本思想是将资源管理和作业调度/监视功能拆分为单独的守护进程。其想法是拥有一个全局资源管理器（ResourceManager: RM）和每个应用程序的应用程序主控器（ApplicationMaster：AM）。应用程序可以是单个作业，也可以是一个DAG作业。 The ResourceManager and the NodeManager form the data-computation framework. The ResourceManager is the ultimate authority that arbitrates resources among all the applications in the system. The NodeManager is the per-machine framework agent who is responsible for containers, monitoring their resource usage (cpu, memory, disk, network) and reporting the same to the ResourceManager/Scheduler. 资源管理器和节点管理器构成了数据计算框架。资源管理器是在系统中所有应用程序之间仲裁资源的最高权威（机构）。节点管理器是每台计算机框架的代理负责容器、监视其资源使用情况（CPU、内存、磁盘、网络），并向资源管理器（ResourceManager）/调度程序（Scheduler）报告这些情况。 The per-application ApplicationMaster is, in effect, a framework specific library and is tasked with negotiating resources from the ResourceManager and working with the NodeManager(s) to execute and monitor the tasks. 实际上，每个应用程序的 ApplicationMaster 是一个特定于框架的库，它的任务是与ResourceManager协商资源，并与节点管理器一起执行和监视任务。 The ResourceManager has two main components: Scheduler and ApplicationsManager. ResourceManager有两个主要组件：调度器（Scheduler ）和应用程序管理器（ApplicationsManager）。 The Scheduler is responsible for allocating resources to the various running applications subject to familiar constraints of capacities, queues etc. The Scheduler is pure scheduler in the sense that it performs no monitoring or tracking of status for the application. Also, it offers no guarantees about restarting failed tasks either due to application failure or hardware failures. The Scheduler performs its scheduling function based on the resource requirements of the applications; it does so based on the abstract notion of a resource Container which incorporates elements such as memory, cpu, disk, network etc. 调度器（Scheduler）负责根据熟悉的容量、队列等限制将资源分配给各种正在运行的应用程序。调度器是纯粹的调度程序，在某种意义上，它不执行对应用程序状态的监视或跟踪。此外，它不能保证由于应用程序故障或硬件故障而重新启动失败的任务。调度器根据应用程序的资源需求来执行其调度功能；它是基于资源容器的抽象概念来执行的，该资源容器包含内存、CPU、磁盘、网络等元素。 The Scheduler has a pluggable policy which is responsible for partitioning the cluster resources among the various queues, applications etc. The current schedulers such as the CapacityScheduler and the FairScheduler would be some examples of plug-ins. 调度器有一个可插拔的策略，负责在不同的队列、应用程序等之间划分集群资源。当前的调度器（如CapacityScheduler 和 FairScheduler）是插件的一些示例。 The ApplicationsManager is responsible for accepting job-submissions, negotiating the first container for executing the application specific ApplicationMaster and provides the service for restarting the ApplicationMaster container on failure. The per-application ApplicationMaster has the responsibility of negotiating appropriate resource containers from the Scheduler, tracking their status and monitoring for progress. ApplicationsManager 负责接受作业的提交过程，协商得到用来执行应用程序相关的 ApplicationMaster 的第一个容器，并提供在失败时重新启动 ApplicationMaster 容器的服务。每个应用程序的 ApplicationMaster 负责从调度器协商适当的资源容器，跟踪它们的状态并监控进度。 MapReduce in hadoop-2.x maintains API compatibility with previous stable release (hadoop-1.x). This means that all MapReduce jobs should still run unchanged on top of YARN with just a recompile. hadoop-2.x中的 MapReduce 与以前的稳定版本（hadoop-1.x）保持API兼容性。这意味着，所有 MapReduce 作业都应该在 YARN 上保持不变，只需重新编译即可。 YARN supports the notion of resource reservation via the ReservationSystem, a component that allows users to specify a profile of resources over-time and temporal constraints (e.g., deadlines), and reserve resources to ensure the predictable execution of important jobs.The ReservationSystem tracks resources over-time, performs admission control for reservations, and dynamically instruct the underlying scheduler to ensure that the reservation is fullfilled. YARN 支持通过 ReservationSystem 保留资源的概念，ReservationSystem 是一个组件，允许用户指定一个临时和时间限制的资源配置文件（例如截止日期），并保留资源以确保重要作业的可预测执行。ReservationSystem 跟踪随着时间的推移，资源将对预订执行许可控制，并动态指示基础调度器以确保预订得到执行。 In order to scale YARN beyond few thousands nodes, YARN supports the notion of Federation via the YARN Federation feature. Federation allows to transparently wire together multiple yarn (sub-)clusters, and make them appear as a single massive cluster. This can be used to achieve larger scale, and/or to allow multiple independent clusters to be used together for very large jobs, or for tenants who have capacity across all of them. 为了将YARN扩展到数千个节点之外，YARN 通过 YARN Federation 特性支持 Federation 的概念。联邦（Federation）允许透明地将多个YARN（子）集群连接在一起，并使它们看起来像一个单独的大集群。这可以用于实现更大的规模，和/或允许将多个独立的集群一起用于非常大的工作，或用于具有所有能力使用所有集群的租户（tennant: 用户。租户）。 YARN Federation 详细参考的另一篇读记：Hadoop YARN Federation 参考资料 Hadoop In Action, 2nd Edition Hadoop: The Definitive Guide, 4th Edition Apache Hadoop YARN-Moving beyond MapReduce and Batch Processing with Apache Hadoop 2 YARN Architecture 笔记小结 - 葛尧的文章 - 知乎 YARN Architecture 笔记二 - 葛尧的文章 - 知乎 Hadoop技术内幕：深入解析YARN架构设计与实现原理]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Hadoop YARN</tag>
        <tag>Distributed System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle首战Titanic 0.82275-Top3% & 0.83732-Top2%]]></title>
    <url>%2F2019%2F01%2F06%2FTitanic_with_name_sex_age_and_ticket_features-0.82275-0.83732%2F</url>
    <content type="text"><![CDATA[本文用数据分析探索规律，效果好于一堆的随机森林和xgboost，超过参加这个比赛的很多ensemble模型，至少排在前156/10021（Top 2%），最终只选择 name，sex，age，Ticket 4个特征，构建出新的特征，然后进行规则判断，即多个嵌套的if-else，再一次感受到了特征工程的强大。省了数据缺失弥补，其他繁琐的数据预处理，数据清洗，后续的调参和集成模型。需要注意的是：需要自己定制交叉验证函数。 具体方案细节，查看我的jupyter notebook： Titanic_with_name_sex_age_and_ticket_features-0.82275.ipynb Titanic_with_name_sex_age_and_ticket_features-0.83732.ipynb]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[极大化似然估计与贝叶斯估计区别]]></title>
    <url>%2F2019%2F01%2F01%2FMaximum-Likelihood-Estimation_VS_Bayes-Estimation%2F</url>
    <content type="text"><![CDATA[即使学了很久，很多人都没弄清楚极大化似然估计与贝叶斯估计区别，本文将简要概述一下区别，如果要详细搞清楚，建议食用MIT概率论教材《Introduction to probability》的第8, 9两章，本文只是简单总结和《统计学习方法》第4章——朴素贝叶斯方法中的例子。 正文下图摘录自教材《Introduction to probability》的第8章 参考 http://www.cnblogs.com/little-YTMM/p/5399532.html]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>probability</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark编程基础笔记]]></title>
    <url>%2F2018%2F12%2F28%2Ffoundations_of_spark_programming%2F</url>
    <content type="text"><![CDATA[花了4天时间快速过了一遍这个课程，这个课程好处都是有代码实例，毕竟再复杂的工程都是一个个模块堆积起来的。笔记 pdf百度链接链接：提取码：d72o，自己做的书签。 第2章 scala语言基础 第3章 spark的设计与运行原理 第4章 Spark环境搭建和使用方法 第5章 RDD编程 第6章 Spark SQL 第7章 Spark Streaming 第8章 Spark MLlib]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从特征工程到XGBoost参数调优]]></title>
    <url>%2F2018%2F12%2F18%2Fget_started_feature-engineering%2F</url>
    <content type="text"><![CDATA[前言本文陈述脉络：理论结合kaggle上一个具体的比赛。 正文数据科学的一般流程 指南 特征工程 评价指标 XGBoost参数调优 XGBoost并行处理 特征工程结合以下案例分析： Two Sigma Connect: Rental Listing Inquiries 任务：根据公寓的listing 内容，预测纽约市某公寓租赁listing的受欢迎程度标签： interest_level，该listing被咨询的次数 选择这个案例是因为小而精，虽然只有14维特征，但是基本上都涉及各种类型特征。 有三个取值：: ‘high’, ‘medium’, ‘low’，是一个多类分类任务 Listing内容有： 浴室和卧室的数目bathrooms， bedrooms 地理位置（ longitude 、 latitude ） 地址： display_address、 street_address building_id、 listing_id、 manager_id Created：创建日期 Description：更多描述信息 features: 公寓的一些特征描述 photos: a list of photo links 价格：price 数据分析方法对数据进行探索性的分析的工具包：pandas、 matplotlib／seaborn 读取训练数据，取少量样本进行观测，并查看数据规模和数据类型 标签、特征意义、特征类型等 分析每列特征的分布 直方图 包括标签列（对分类问题，可看出类别样本是否均衡） 检测奇异点（outliers） 分析每两列特征之间的相关性 – 特征与特征之间信息是否冗余 – 特征与标签是否线性相关 histogram 直方图 直方图：每个取值在数据集中出现的次数，可视为概率函 数（PDF）的估计（seaborn可视化工具比较简单） 123import seaborn as sns%matplotlib inline（ seaborn 是基于matplotlib 的）sns.distplot(train.price.values, bins=50, kde=True) 核密度估计 Kernel Density Estimation, KDE 对直方图的加窗平滑 在分类任务中，我们关心不同类别的特征分布 violinplot 提供不同类别条件下特征更多的分部信息 核密度估计（KDE） 三个4分位数（quartile）：1/4, 1/2, 3/4 1.5倍四分数间距（nterquartile range, IQR） IQR ：第三四分位数和第一分位数的区别（即Q1~Q3的差距），表示变量的分散情况，播放差更稳健的统计量 12order = ['low', 'medium', 'high']sns.violinplot(x='interest_level', y='price', data=train, order = order) outliers 奇异点奇异点：或称离群点，指远离大多数样本的样本点。通常认为这些点是噪声，对模型有坏影响 可以通过直方图或散点图发现奇异点 直方图的尾巴 散点图上孤立的点 12345plt.figure(figsize=(8,6))plt.scatter(range(train_df.shape[0]), train_df.price.values, color = color[6])plt.xlabel('the number of train data', fontsize=12)plt.ylabel('price', fontsize=12)plt.show() 可以通过只保留某些分位数内的点去掉奇异点 如0.5%-99.5%，或&gt;99% 12ulimit = np.percentile(train.price.values, 99)train['price'].loc[train['price']&gt;ulimit] = ulimit correlation 相关性 相关性可以通过计算相关系数或打印散点图来发现 相关系数：两个随机变量x,y之间的线性相关程度，不线性相关并不代表不相关，可能高阶相关，如 $y=x^2$ $r = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=0}^{n}}(x_i-\bar{x})^2\sum_{i=0}^{n}(y_i-\bar{y})^2}, -1\le r \le 1$ 通常 $|r| &gt; 0.5$ ，认为两者相关性比较强 $r\cases{=0, &amp;\text{完全线性不相关}\\ &gt;0, &amp;\text{正相关}\\ &lt;0, &amp;\text{负相关}}$ 相关性只能是数值型特征之间相关性 我们希望特征与标签强相关，分类直方图可以从某种程度上看出特征与标签的相关性：不同类别的直方图差异大 1234order = ['low', 'medium', 'high']sns.stripplot(train_df.interest_level, train_df.price.values, jitter=True, order=order)plt.title("Price VS Interest Level")plt.show() 特征与特征之间强相关的话意味着信息冗余 可以两个特征可以只保留一个特征 或采用主成分分析（PCA）等降维 123456789101112contFeaturelist = []contFeaturelist.append('bathrooms')contFeaturelist.append('bedrooms')contFeaturelist.append('price')correlationMatrix = train_df[contFeaturelist].corr().abs()plt.subplots()sns.heatmap(correlationMatrix, annot=True)#Mask unimportant featuressns.heatmap(correlationMatrix, mask=correlationMatrix &lt; 1, cbar = False)plt.show() 数据类型XGBoost 模型内部将所有的问题都建模成一个回归预测问题，输入特征只能是数值型。如果给定的数据是不同的类型，必须先将数据变成数值型。 类别型特征（ categorical features） LabelEncoder： 对不连续的数字或者文本进行编号 可用在对字符串型的标签编码（测试结果需进行反变换） 编号默认有序数关系 存储量小 如不希望有序数关系： OneHotEncoder：将类别型整数输入从 1 维 K 维的稀疏编码（K 种类别） 对XGBoost，OneHotEncoder不是必须，因为XGBoost对特征进行排序从而进行分裂建树；如果用OneHotEncoder得到稀疏编码，XGBoost建树过程中对稀疏特征处理速度块 输入必须是数值型数据（对字符串输入，先调用LabelEncoder变成数字，再用OneHotEncoder ） 存储要求高 低基数（low-cardinality ）类别型特征： OneHotEncoder 1维到K维， K为该特征不同的取值数目 通常在K &lt;10的情况下采用 高基数（high-cardinality）类别型特征：通常有成百上千个不同的取值，可先降维，如：邮政编码、街道名称… 聚类（Clustering）： 1 维 到 K维，K为聚类的类别数 主成分分析（principle component analysis, PCA）：但对大矩阵操作费资源 均值编码：在贝叶斯的架构下，利用标签变量，有监督地确定最适合特定特征的编码方式。均值编码详细参考： Mean Encoding: A Preprocessing Scheme for High-Cardinality Categorical Features 平均数编码：针对高基数定性特征（类别特征）的数据预处理/特征工程 日期型特征 日期特征：年月日 时间特征：小时分秒 时间段：早中晚 星期，工作日／周末 123456789train_test['Date'] = pd.to_datetime(train_test['created'])train_test['Year'] = train_test['Date'].dt.yeartrain_test['Month'] = train_test['Date'].dt.monthtrain_test['Day'] = train_test['Date'].dt.daytrain_test['Wday'] = train_test['Date'].dt.dayofweektrain_test['Yday'] = train_test['Date'].dt.dayofyeartrain_test['hour'] = train_test['Date'].dt.hour train_test = train_test.drop(['Date', 'created'], axis=1) 文本型特征 可用词云（wordcloud）可视化 文本词频统计函数，自动统计词的个数，以字典形式内部存储，在显示的时候词频大的词的字体更大 123456789# wordcloud for street addressplt.figure()wordcloud = WordCloud(background_color='white', width=600, height=300, max_font_size=50, max_words=40)wordcloud.generate(text_street)wordcloud.recolor(random_state=0)plt.imshow(wordcloud)plt.title("Wordcloud for street address", fontsize=30)plt.axis("off")plt.show() TF-IDF 通俗易懂原理参考：廖雪峰老师的TF-IDF，概率解释参考：CoolShell 陈皓的 TF-IDF 实战参考官网和使用sklearn提取文本的tfidf特征 下面是个例子123456789101112from sklearn.feature_extraction.text import TfidfVectorizerX_train = ['This is the first document.', 'This is the second document.']X_test = ['This is the third document.']vectorizer = TfidfVectorizer()# 用X_train数据来fitvectorizer.fit(X_train)# 得到tfidf的矩阵tfidf_train = vectorizer.transform(X_train)tfidf_test = vectorizer.transform(X_test)tfidf_train.toarray() 数据预处理from sklearn.preprocessing import … 数据标准化 数据归一化 数据二值化 数据缺失 XGBoost对数据预处理要求少，以上操作都不是必须 特征工程小结 如果知道数据的物理意义（领域专家），可能可以设计更多特征 如Higgs Boson任务中有几维特征是物理学家设计的，还有些有高能物理 研究经验的竞赛者设计了其他一些特征 如房屋租赁任务中，利用常识可设计出一些特征，例子：租金/卧室数目=单价 如果不是领域专家，一些通用的规则： 字符串型特征：Label编码 时间特征：年月日、时间段（早中晚）… 数值型特征：加减乘除，多项式，log, exp 低基数类别特征：one-hot编码 高基数类别特征：先降维，再one-hot编码；均值编码 非结构化特征 文本 语音 图像／视频 fMRI … 利用领域知识设计特征 如曾经流行的图像目标检测特征HOG… 利用深度学习从数据中学习特征表示 采用end-to-end方式一起学习特征和分类／回归／排序 学习好特征可以送入XGBoost学习器 信息泄漏训练数据特征不应该包含标签的信息– 如Rent Listing Inquries任务中图片压缩文件里文件夹的创造时间：加入这个特征后，模型普遍能提高0.01的public LB分数 特征工程案例实践这是我的 jupyter notebook: Rent Listing Inquries 评价指标回归问题的评价指标损失函数可以作为评价指标，以下约定俗成： $\hat{y_i}$ 是预测值，$y$ 是标签值 L1: mean absolute error (MAE) $MAE = \frac{1}{N}\sum_{i=0}^{N}|\hat{y_i}-y_i|$ L2: Root Mean Squared Error(RMSE) $RMSE = \sqrt{\frac{1}{N}\sum_{i=0}^{N}|\hat{y_i}-y_i|}$ Root Mean Sqared Logarithmic Error (RMSLE) $RMLSE = \sqrt{\frac{1}{N}\sum_{i=0}^{N} \big( \log(\hat{y_i}+1) - \log(y_i+1) \big) ^2}$ https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError 当不想在给预测值与真值差距施加很大惩罚时，采用RMSLE 分类任务的评价指标同样地，损失函数可以作为评价指标 logistic/负log似然损失 $\text{logloss}= -{1\over N}\sum_{i=0}^{N}\sum_{j=0}^{M}y_{ij}\log{P_{ij}}$，M是类别数，$y_{ij}$ 为二值函数，当 i 个样本是第 j 类时 $y_{ij}=1$ ，否则取 $0$ ；$p_{ij}$ 为模型预测的第 i 个样本为第 j 类的概率。当 $M=2$ 时， $\text{logloss} = -{1\over N}\sum_{i=0}^{N}\big(y_i\log p_i + (1-y_i)\log(1-p_i)\big)$ ，$y_i$ 为第 i 个样本类别，$p_i$ 为模型预测的第 i 个样本为第 1 类的概率。 0-1损失对应的Mean Consequential Error (MCME) $\text{MCE}=-\frac{1}{N}\sum\limits_{\hat{y_i}\ne y_i}1$ 两类分类任务中更多评价指标 ROC／AUC PR曲线 MAP@n 0-1损失：假设两种错误的代价相等 ​ False Positive （FP） &amp; False Negative（FN） 有些任务中可能某一类错误的代价更大 如蘑菇分类中将毒蘑菇误分为可食用代价更大 因此单独列出每种错误的比例：混淆矩阵 混淆矩阵（confusion matrix） 真正的正值（true positives） 假的正值（false positives） 真正的负值（true negatives） 假的负值（false negatives ） SciKit-Learn实现了多类分类任务的混淆矩阵 sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None) y_true： N个样本的标签真值 y_pred： N个样本的预测标签值 labels：C个类别在矩阵的索引顺序，缺省为y_true或y_pred类别出现的顺序 sample_weight： N个样本的权重 Receiver Operating Characteristic (ROC)上面我们讨论给定阈值 $τ$ 的TPR和FPR 如果不是只考虑一个阈值，而是在一些列阈值上运行检测器，并画出TPR和FPR为阈值 $τ$ 的隐式函数，得到ROC曲线在此处键入公式。 PR曲线Precision and Recall (PR曲线)：用于稀有事件检测，如目标检测、信息检索 负样本非常多，因此 FPR = FP /N_ 很小，比较 TPR 和 FPR 不是很有用的信息 （ROC曲线中只有左边很小一部分有意义） $\rightarrow$ 只讨论正的样本 Precision（精度，查准率，正确 率）：以信息检索为例，对于一个查询，返回了一系列的文档，正确率指的是返回结果中相关文档占的比例 $\text{precision}= TP /\hat{N}_+$ 预测结果真正为正的比例 Recall（召回率，查全率）：返回结果中相关文档占所有相关文档的比例 $\text{Recall}=TP/N_+$ 被正确预测到正样本的比例 Precision and Recall (PR曲线) 阈值变化时的P 和R ，只考虑了返回结果中相关文档的个数，没有考虑文档之间的序。 对一个搜索引擎或推荐系统而言，返回的结果必然是有序的，而且越相关的文档排的越靠前越好，于是有了 AP 的概念。 AP: Average Precision，对不同召回率点上的正确率进行平均。 Average PrecisionPrecision只考虑了返回结果中相关文档的个数，没有考虑文档之间的序。 对一个搜索引擎或推荐系统而言，返回的结果必然是有序的，而且越相关的文档排的越靠前越好，于是有了AP的概念。 AP: Average Precision，对不同召回率点上的正确率进行平均 $AP = \int_{0}^{1}p(k)dr = \sum_{k=0}^{n}p(k)\Delta r(k)$ 即 PR 曲线下的面积（Recall: AUC 为 ROC 下的面积） 其中 k 为返回文档中序位，n 为返回文档的数目，$p(k)$ 为列表中k截至点的precision，$\Delta r(k)$ 表示从 $k-1$ 到 $k$ 召回率的变化 上述离散求和表示等价于 $AP=\sum_{k=0}^{n}p(k)rel(k)/\text{相关文档的数目}$ ，其中 $rel(k)$ 为示性函数，即第 $k$ 个位置为相关文档则取1，否则取0。 计算每个位置上的 precision，如果该位置的文档是不相关的则该位置 precision=0，然后对所有的位置的precision 再做 average 。 MAP: Mean Average Precision $MAP = (\sum_{q=0}^{Q}AP(q)/(Q))$ ，其中 $Q$ 为查询的数目，$n$ 为文档数目。 MAP@K （MAPK） 在现代web信息检索中，recall其实已经没有意义，因为相关文档有成千上万个，很少有人会关心所有文档 Precision@K：在第K个位置上的Precision 对于搜索引擎，考虑到大部分作者只关注前一、两页的结果，所以Precision @10， Precision @20对大规模搜索引擎非常有效 MAP@K：多个查询Precision@K的平均 F1 分数/调和平均 亦被称为F1 score, balanced F-score or F-measure Precision 和 Recall 加权平均： $F1=\frac{2(\text{Precision Recall)}}{(\text{Precision + Recall)}}$ 最好为1，最差为0 多类：每类的F1平均值 Scikit-Learn: Scoring 用交叉验证（cross_val_score和GridSearchCV）评价模型性能时，用scoring参数定义评价指标。 评价指标是越高越好，因此用一些损失函数当评价指标时，需要再加负号，如neg_log_loss，neg_mean_squared_error 详见sklearn文档：http://scikit-learn.org/stable/modules/model_evaluation.html Scoring Function Comment Classification ‘accuracy’ metrics.accuracy_score ‘balanced_accuracy’ metrics.balanced_accuracy_score for binary targets ‘average_precision’ metrics.average_precision_score ‘brier_score_loss’ metrics.brier_score_loss ‘f1’ metrics.f1_score for binary targets ‘f1_micro’ metrics.f1_score micro-averaged ‘f1_macro’ metrics.f1_score macro-averaged ‘f1_weighted’ metrics.f1_score weighted average ‘f1_samples’ metrics.f1_score by multilabel sample ‘neg_log_loss’ metrics.log_loss requires predict_proba support ‘precision’ etc. metrics.precision_score suffixes apply as with ‘f1’ ‘recall’ etc. metrics.recall_score suffixes apply as with ‘f1’ ‘roc_auc’ metrics.roc_auc_score Clustering ‘adjusted_mutual_info_score’ metrics.adjusted_mutual_info_score ‘adjusted_rand_score’ metrics.adjusted_rand_score ‘completeness_score’ metrics.completeness_score ‘fowlkes_mallows_score’ metrics.fowlkes_mallows_score ‘homogeneity_score’ metrics.homogeneity_score ‘mutual_info_score’ metrics.mutual_info_score ‘normalized_mutual_info_score’ metrics.normalized_mutual_info_score ‘v_measure_score’ metrics.v_measure_score Regression ‘explained_variance’ metrics.explained_variance_score ‘neg_mean_absolute_error’ metrics.mean_absolute_error ‘neg_mean_squared_error’ metrics.mean_squared_error ‘neg_mean_squared_log_error’ metrics.mean_squared_log_error ‘neg_median_absolute_error’ metrics.median_absolute_error ‘r2’ metrics.r2_score SciKit-Learn：sklearn.metrics metrics模块还提供为其他目的而实现的预测误差评估函数 分类任务的评估函数如表所示，其他任务评估函数请见：http://scikitlearn.org/stable/modules/classes.html#module-sklearn.metrics Classification metricsSee the Classification metrics section of the user guide for further details. metrics.accuracy_score(y_true, y_pred[, …]) Accuracy classification score. metrics.auc(x, y[, reorder]) Compute Area Under the Curve (AUC) using the trapezoidal rule metrics.average_precision_score(y_true, y_score) Compute average precision (AP) from prediction scores metrics.balanced_accuracy_score(y_true, y_pred) Compute the balanced accuracy metrics.brier_score_loss(y_true, y_prob[, …]) Compute the Brier score. metrics.classification_report(y_true, y_pred) Build a text report showing the main classification metrics metrics.cohen_kappa_score(y1, y2[, labels, …]) Cohen’s kappa: a statistic that measures inter-annotator agreement. metrics.confusion_matrix(y_true, y_pred[, …]) Compute confusion matrix to evaluate the accuracy of a classification metrics.f1_score(y_true, y_pred[, labels, …]) Compute the F1 score, also known as balanced F-score or F-measure metrics.fbeta_score(y_true, y_pred, beta[, …]) Compute the F-beta score metrics.hamming_loss(y_true, y_pred[, …]) Compute the average Hamming loss. metrics.hinge_loss(y_true, pred_decision[, …]) Average hinge loss (non-regularized) metrics.jaccard_similarity_score(y_true, y_pred) Jaccard similarity coefficient score metrics.log_loss(y_true, y_pred[, eps, …]) Log loss, aka logistic loss or cross-entropy loss. metrics.matthews_corrcoef(y_true, y_pred[, …]) Compute the Matthews correlation coefficient (MCC) metrics.precision_recall_curve(y_true, …) Compute precision-recall pairs for different probability thresholds metrics.precision_recall_fscore_support(…) Compute precision, recall, F-measure and support for each class metrics.precision_score(y_true, y_pred[, …]) Compute the precision metrics.recall_score(y_true, y_pred[, …]) Compute the recall metrics.roc_auc_score(y_true, y_score[, …]) Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores. metrics.roc_curve(y_true, y_score[, …]) Compute Receiver operating characteristic (ROC) metrics.zero_one_loss(y_true, y_pred[, …]) Zero-one classification loss. Regression metricsSee the Regression metrics section of the user guide for further details. metrics.explained_variance_score(y_true, y_pred) Explained variance regression score function metrics.mean_absolute_error(y_true, y_pred) Mean absolute error regression loss metrics.mean_squared_error(y_true, y_pred[, …]) Mean squared error regression loss metrics.mean_squared_log_error(y_true, y_pred) Mean squared logarithmic error regression loss metrics.median_absolute_error(y_true, y_pred) Median absolute error regression loss metrics.r2_score(y_true, y_pred[, …]) R^2 (coefficient of determination) regression score function. Multilabel ranking metricsSee the Multilabel ranking metrics section of the user guide for further details. metrics.coverage_error(y_true, y_score[, …]) Coverage error measure metrics.label_ranking_average_precision_score(…) Compute ranking-based average precision metrics.label_ranking_loss(y_true, y_score) Compute Ranking loss measure XGBoost 原理部分参考： XGBoost第一课 XGBoost支持的目标函数objective参数，这个参数在 XGBoost 里面属于任务参数（Learning Task Parameters） [default=reg:linear] reg:linear: linear regression reg:logistic: logistic regression binary:logistic: logistic regression for binary classification, output probability binary:logitraw: logistic regression for binary classification, output score before logistic transformation binary:hinge: hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities. count:poisson –poisson regression for count data, output mean of poisson distribution 计数问题的poisson回归，输出结果为poisson分布。 max_delta_step is set to 0.7 by default in poisson regression (used to safeguard optimization) survival:cox: Cox regression for right censored survival time data (negative values are considered right censored). Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function h(t) = h0(t) * HR). multi:softmax: set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes) 让XGBoost采用softmax目标函数处理多分类问题 multi:softprob: same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata * nclass matrix. The result contains predicted probability of each data point belonging to each class. 和softmax一样，但是输出的是ndata * nclass的向量，可以将该向量 reshape成ndata行nclass列的矩阵。没行数据表示样本所属于每个类别的概率。 rank:pairwise: Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized rank:ndcg: Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized rank:map: Use LambdaMART to perform list-wise ranking where Mean Average Precision (MAP) is maximized reg:gamma: gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed. reg:tweedie: Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed. XGBoost自定义目标函数 在GBDT训练过程，当每步训练得到一棵树，要调用目标函数得到其梯度作为下一棵树拟合的目标 XGBoost在调用obj函数时会传入两个参数：preds和dtrain preds为当前模型完成训练时，所有训练数据的预测值 dtrain为训练集，可以通过dtrain.get_label()获取训练样本的label 同时XGBoost规定目标函数需返回当前preds基于训练label的一阶和二阶梯度 例子 参考官网：https://github.com/dmlc/xgboost/blob/master/demo/guide-python/custom_objective.py 12345678#user define objective function, given prediction, return gradient and second order gradient#this is log likelihood lossdef logregobj(preds, dtrain): #自定义损失函数 labels = dtrain.get_label() preds = 1.0 / (1.0 + np.exp(-preds)) grad = preds - labels #梯度 hess = preds * (1.0-preds) #2阶导数 return grad, hess 调用的时候：123# training with customized objective, we can also do step by step training# simply look at xgboost.py's implementation of trainbst = xgb.train(param, dtrain, num_round, watchlist, obj=logregobj, feval=evalerror) XGBoost支持的评价函数eval_metric参数，这个参数在 XGBoost 里面属于任务参数（Learning Task Parameters） [default according to objective] Evaluation metrics for validation data, a default metric will be assigned according to objective (rmse for regression, and error for classification, mean average precision for ranking) User can add multiple evaluation metrics. Python users: remember to pass the metrics in as list of parameters pairs instead of map, so that latter eval_metric won’t override previous one The choices are listed below: rmse: root mean square error mae: mean absolute error logloss: negative log-likelihood error: Binary classification error rate. It is calculated as #(wrong cases)/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances. error@t: a different than 0.5 binary classification threshold value could be specified by providing a numerical value through ‘t’. merror: Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases). mlogloss: Multiclass logloss. auc: Area under the curve aucpr: Area under the PR curve ndcg: Normalized Discounted Cumulative Gain map: Mean Average Precision ndcg@n, map@n: ‘n’ can be assigned as an integer to cut off the top positions in the lists for evaluation. ndcg-, map-, ndcg@n-, map@n-: In XGBoost, NDCG and MAP will evaluate the score of a list without any positive samples as 1. By adding “-” in the evaluation metric XGBoost will evaluate these score as 0 to be consistent under some conditions. poisson-nloglik: negative log-likelihood for Poisson regression gamma-nloglik: negative log-likelihood for gamma regression cox-nloglik: negative partial log-likelihood for Cox proportional hazards regression gamma-deviance: residual deviance for gamma regression tweedie-nloglik: negative log-likelihood for Tweedie regression (at a specified value of the tweedie_variance_power parameter) XGBoost自定义评价函数例子参考官网：https://github.com/dmlc/xgboost/blob/master/demo/guide-python/custom_objective.py 123456789101112131415# user defined evaluation function, return a pair metric_name, result# NOTE: when you do customized loss function, the default prediction value is margin# this may make builtin evaluation metric not function properly# for example, we are doing logistic loss, the prediction is score before logistic transformation# the builtin evaluation error assumes input is after logistic transformation# Take this in mind when you use the customization, and maybe you need write customized evaluation functiondef evalerror(preds, dtrain): labels = dtrain.get_label() # return a pair metric_name, result. The metric name must not contain a colon (:) or a space # since preds are margin(before logistic transformation, cutoff at 0) return 'my-error', float(sum(labels != (preds &gt; 0.0))) / len(labels)# training with customized objective, we can also do step by step training# simply look at xgboost.py's implementation of trainbst = xgb.train(param, dtrain, num_round, watchlist, obj=logregobj, feval=evalerror) 调用的时候： 123# training with customized objective, we can also do step by step training# simply look at xgboost.py's implementation of trainbst = xgb.train(param, dtrain, num_round, watchlist, obj=logregobj, feval=evalerror) XGBoost参数调优XGBoost参数列表 参数 说明 max_depth 树的最大深度。树越深通常模型越复杂，更容易过拟合。 learning_rate 学习率或收缩因子。学习率和迭代次数／弱分类器数目n_estimators相关。 缺省：0.1 n_estimators 弱分类器数目. 缺省:100 slient 参数值为1时，静默模式开启，不输出任何信息 objective 待优化的目标函数，常用值有： binary:logistic 二分类的逻辑回归，返回预测的概率 multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。 multi:softprob 和 multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。支持用户自定义目标函数 nthread 用来进行多线程控制。 如果你希望使用CPU全部的核，那就不用缺省值-1，算法会自动检测它。 booster 选择每次迭代的模型，有两种选择： gbtree：基于树的模型，为缺省值。 gbliner：线性模型 gamma 节点分裂所需的最小损失函数下降值 min_child_weight 叶子结点需要的最小样本权重（hessian）和 max_delta_step 允许的树的最大权重 subsample 构造每棵树的所用样本比例（样本采样比例），同GBM colsample_bytree 构造每棵树的所用特征比例 colsample_bylevel 树在每层每个分裂的所用特征比例 reg_alpha L1/L0正则的惩罚系数 reg_lambda L2正则的惩罚系数 scale_pos_weight 正负样本的平衡 base_score 每个样本的初始估计，全局偏差 random_state 随机种子 seed 随机种子 missing 当数据缺失时的填补值。缺省为np.nan kwargs XGBoost Booster的Keyword参数 参数类别 通用参数：这部分参数通常我们不需要调整，默认值就好 学习目标参数：与任务有关，定下来后通常也不需要调整 booster参数：弱学习器相关参数，需要仔细调整，会影响模型性能 通用参数 booster：弱学习器类型 可选gbtree（树模型）或gbliner（线性模型） 或 dart （参考我的另一篇博文： XGBoost第一课） 默认为gbtree（树模型为非线性模型，能处理更复杂的任务） silent：是否开启静默模式 1：静默模式开启，不输出任何信息 默认值为0：输出一些中间信息，以助于我们了解模型的状态 nthread：线程数 默认值为-1，表示使用系统所有CPU核 学习目标参数 objective: 损失函数 支持分类／回归／排 eval_metric：评价函数 seed：随机数的种子 默认为0 设置seed可复现随机数据的结果，也可以用于调整参数 booster参数弱学习器的参数，尽管有两种booster可供选择，这里只介绍gbtree learning_rate : 收缩步长 vs. n_estimators：树的数目 较小的学习率通常意味着更多弱分学习器 通常建议学习率较小（ $\eta &lt; 0.1$ ）弱学习器数目n_estimators大 $f_m(x_i)=f_{m-1}(x_i)+\eta\beta_m WeakLearner_m(x_i) $ 可以设置较小的学习率，然后用交叉验证确定n_estimators 行（subsample）列（colsample_bytree、colsample_bylevel）下采样比例 默认值均为1，即不进行下采样，使用所有数据 随机下采样通常比用全部数据的确定性过程效果更好，速度更快 建议值：0.3 - 0.8 树的最大深度： max_depth max_depth越大，模型越复杂，会学到更具体更局部的样本 需要使用交叉验证进行调优，默认值为6，建议3-10 min_child_weight ：孩子节点中最小的样本权重和 如果一个叶子节点的样本权重和小于min_child_weight则分裂过程 结束 Kaggle竞赛优胜者的建议 Tong He（XGBoost R语言版本开发者）： 三个最重要的参数为：树的数目、树的深度和学习率。建议参数调整策略为： 采用默认参数配置试试 如果系统过拟合了，降低学习率 如果系统欠拟合，加大学习率 油管上作者视频：Kaggle Winning Solution Xgboost Algorithm - Learn from Its Author, Tong He Owen Zhang （常使用XGBoost）建议： n_estimators和learning_rate：固定n_estimators为100（数目不大，因为树的深度较大，每棵树比较复杂），然后调整learning_rate 树的深度max_depth：从6开始，然后逐步加大 $\text{min_child_weight}={1\over\sqrt{\text{rare_events}}}$ ，其中 rare_events 为稀有事件的数目 列采样 ${\text{colsample_bytree}\over \text{colsample_bylevel}}$ ：在 $[0.3,0.5]$ 之间进行网格搜索 行采样subsample：固定为1 gamma: 固定为0.0 油管上大神的视频：Learn Kaggle techniques from Kaggle #1, Owen Zhang 参数调优的一般方法 选择较高的学习率(learning rate)，并选择对应于此学习率的理想的树数量 学习率以工具包默认值为0.1。 XGBoost直接引用函数“cv”可以在每一次迭代中使用交叉验证，并返回理想的树数量（因为交叉验证很慢，所以可以import两种XGBoost：直接引用xgboost（用“cv”函数调整树的数目）和XGBClassifier —xgboost的sklearn包（用GridSearchCV调整其他参数 ）。 对于给定的学习率和树数量，进行树参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree, colsample_bylevel) xgboost的正则化参数(lambda, alpha)的调优 降低学习率，确定理想参数 XGBoost参数调优案例分析 竞赛官网：Otto Group Product Classification Challenge 是关于电商商品分类的案例，其中 Target：共9个商品类别 93个特征：整数型特征 详细请看我的jupyter notebook: kaggle Titanic 案例 详细请看我的jupyter notebook: XGBoost并行处理XGBoost工程实现 XGBoost用C++实现，显示地采用OpenMP API做并行处理 建单棵树时并行（Random Forest在建不同树时并行，但Boosting增加树是一个串行操作） XGBoost的scikit-learn接口中的参数 nthread 可指定线程数 -1 表示使用系统所有的核资源 model = XGBClassifier(nthread=-1) 在准备建树数据时高效（近似建树、稀疏、 Cache、数据分块） 交叉验证也支持并行（由scikit-learn 提供支持） scikit-learn 支持的k折交叉验证也支持多线程 cross_val_score() 函数中的参数：n_ jobs = -1 表示使用系统所有的CPU核 results = cross_val_score(model, X, label_encoded_y, cv=kfold, scoring= ’neg_log_loss’ , n_jobs=-1, verbose=1) 并行处理的三种配置 交叉验证并行，XGBoost建树不并行 交叉验证不并行，XGBoost建树并行 交叉验证并行，XGBoost建树并行 Otto数据集上的10折交叉验证实验结果： Single Thread XGBoost, Parallel Thread CV: 359.854589 Parallel Thread XGBoost, Single Thread CV: 330.498101 Parallel Thread XGBoost and CV: 313.382301，并行 XGBoost 比并行交叉验证好，两者都并行更好 例子查看 12345678910# evaluate the effect of the number of threads results = [] num_threads = [1, 2, 3, 4] for n in num_threads: start = time.time() model = XGBClassifier(nthread=n) model.fit(X_train, y_train) elapsed = time.time() - start print(n, elapsed) results.append(elapsed) XGBoost总结 XGBoost是一个用于监督学习的非参数模型 目标函数（损失函数、正则项） 参数（树的每个分支分裂特征及阈值） 优化：梯度下降 参数优化 决定模型复杂度的重要参数：learning_rate, n_estimators, max_depth, min_child_weight, gamma, reg_alpha, reg_lamba 随机采样参数也影响模型的推广性： subsample, colsample_bytree, colsample_bylevel 其他未涉及的部分 分布式XGBoost AWS YARN Cluster … GPU加速 并行计算与内存优化的细节 主要关注XGBoost的对外接口 XGBoost资源 XGBoost官方文档：https://xgboost.readthedocs.io/en/latest/ Python API：http://xgboost.readthedocs.io/en/latest/python/python_api.html Github： https://github.com/dmlc/xgboost 很多有用的资源：https://github.com/dmlc/xgboost/blob/master/demo/README.md GPU加速：https://github.com/dmlc/xgboost/blob/master/plugin/updater_gpu/README.md XGBoost原理：XGBoost: A Scalable Tree Boosting System https://arxiv.org/abs/1603.02754 其他资源 XGBoost参数调优： https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuningxgboost-with-codes-python/ 中文版：http://blog.csdn.net/u010657489/article/details/51952785 Owen Zhang, Winning Data Science Competitions https://www.slideshare.net/OwenZhang2/tips-for-data-sciencecompetitions?from_action=save XGBoost User Group： https://groups.google.com/forum/#!forum/xgboost-user/]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning， feature engineering</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost原理和底层实现剖析]]></title>
    <url>%2F2018%2F10%2F02%2Fget-started-XGBoost%2F</url>
    <content type="text"><![CDATA[前言在深度学习火起来之前，集成学习 （ensemble learning 包括 boosting: GBDT, XGBoost）是 kaggle 等比赛中的利器，所以集成学习是机器学习必备的知识点，如果提升树或者GBDT不熟悉，最好先看一下我的另一文： 《统计学习方法》第8章 提升方法之AdaBoost\BoostingTree\GBDT ，陈天奇 的 XGBoost (eXtreme Gradient Boosting) 和 微软的 lightGBM 是 GBDT 算法模型的实现，非常巧妙，是比赛的屠龙之器，算法不仅仅是数学，还涉及系统设计和工程优化。以下引用陈天奇 XGBoost论文 的一段话： Among the 29 challenge winning solutions 3 published at Kaggle’s blog during 2015, 17 solutions used XGBoost. Among these solutions, eight solely used XGBoost to train the model, while most others combined XGBoost with neural nets in ensembles. For comparison, the second most popular method, deep neural nets, was used in 11 solutions. The success of the system was also witnessed in KDDCup 2015, where XGBoost was used by every winning team in the top-10. Moreover, the winning teams reported that ensemble methods outperform a well-configured XGBoost by only a small amount [1]. 正文分成以下几个部分 快速了解：来自陈天奇的ppt XGBoost的设计精髓：来自陈天奇的关于XGBoost的论文 参数详解：结合原理+XGBoost官网API的翻译 正文XGBoost快速了解这部分内容基本上是对陈天奇幻灯片：官网幻灯片 outlook 幻灯片大纲• 监督学习的主要概念的回顾• 回归树和集成模型 (What are we Learning)• 梯度提升 (How do we Learn)• 总结 Review of key concepts of supervised learning 监督学习的关键概念的回顾概念 符号 含义 $R^d$ 特征维度为d的数据集 $x_i∈R^d$ 第i个样本 $w_j$ 第j个特征的权重 $\hat{y}_i$ $x_i$ 的预测值 $y_i$ 第i个训练集的对应的标签 $\Theta$ 特征权重的集合 模型 基本上相关的所有模型都是在下面这个线性式子上发展起来的$$\hat y_i = \sum_{j = 0}^{d} w_j x_{ij}$$上式中 $x_0=1$，就是引入了一个偏差量，或者说加入了一个常数项。由该式子可以得到一些模型： 线性模型，最后的得分就是 $\hat{y}_i$ 。 logistic模型，最后的得分是sigmoid函数 $\frac{1}{1+e^{−\hat{y}_i}}$ 。然后设置阀值，转为正负实例。 其余的大部分也是基于 $\hat{y}_i$ 做了一些运算得到最后的分数 参数 参数就是 $\Theta=\{w_j|j=1,…,d\}$ ，这也正是我们所需要通过训练得出的。 训练时的目标函数 训练时通用的目标函数如下：$$Obj(\Theta)=L(\Theta)+Ω(\Theta)$$在上式中 $L(\Theta)$ 代表的是训练误差，表示该模型对于训练集的匹配程度。$Ω(\Theta)$ 代表的是正则项，表明的是模型的复杂度。 训练误差可以用 $L = \sum_{i = 1}^n l(y_i, \hat y_i)$ 来表示，一般有方差和logistic误差。 方差: $l(y_i,\hat y_i) = (y_i - \hat y_i)^2$ logstic误差: $l(y_i, \hat y_i) = y_i ln(1 + e^{- \hat y_i}) + (1 - y_i)ln(1 + e^{\hat y_i})$ 正则项按照Andrew NG的话来说，就是避免过拟合的。为什么能起到这个作用呢？正是因为它反应的是模型复杂度。模型复杂度，也就是我们的假设的复杂度，按照奥卡姆剃刀的原则，假设越简单越好。所以我们需要这一项来控制。 L2 范数: $Ω(w)=λ||w||_2$ L1 范数(lasso): $Ω(w)=λ||w||_1$ 常见的优化函数有有岭回归，logstic回归和Lasso，具体的式子如下​： 岭回归，这是最常见的一种，由线性模型，方差和L2范数构成。具体式子为 $\sum\limits^n_{i=1}(y_i−w^Tx_i)^2+λ||w||_2$ logstic回归，这也是常见的一种，主要是用于二分类问题，比如爱还是不爱之类的。由线性模型，logistic 误差和L2范数构成。具体式子为 $\sum\limits^n_{i=1} [y_iln(1+e^{−w^Tx_i})+(1−y_i)ln(1+e^{w^Tx_i})]+λ||w||_2$ lasso比较少见，它是由线性模型，方差和L1范数构成的。具体式子为 $\sum\limits_{i = 1}^n (y_i - w^T x_i)^2 + \lambda \vert \vert w \vert \vert _1$ 我们的目标的就是让 $Obj(\Theta)$ 最小。那么由上述分析可见，这时必须让 $L(\Theta$ ) 和 $Ω(\Theta)$ 都比较小。而我们训练模型的时候，要在 bias 和 variance 中间找平衡点。bias 由 $L(\Theta)$ 控制，variance 由 $Ω(\Theta)$ 控制。欠拟合，那么 $L(\Theta)$ 和 $Ω(\Theta)$ 都会比较大，过拟合的话 $Ω(\Theta)$ 会比较大，因为模型的扩展性不强，或者说稳定性不好。 回归树和集成模型 (What are we Learning)Regression Tree (CART) 回归树，也叫做分类与回归树，我认为就是一个叶子节点具有权重的二叉决策树。它具有以下两点特征 决策规则与决策树的一样。 每个叶子节点上都包含了一个权重，也有人叫做分数。 下图就是一个回归树的示例： 回归树的集成模型 回归 小男孩落在第一棵树的最左叶子和第二棵树的最左叶子，所以它的得分就是这两片叶子的权重之和，其余也同理。 树有以下四个优点： 使用范围广，像GBM，随机森林等。(PS: 据陈天奇大神的统计，至少有超过半数的竞赛优胜者的解决方案都是用回归树的变种) 对于输入范围不敏感，所以并不需要对输入归一化 能学习特征之间更高级别的相互关系 很容易对其扩展 模型和参数 假设我们有 $K$ 棵树，那么$$\hat y_i = \sum_{k = 1}^K f_k(x_i),\ \ f_k \in \cal F$$上式中 $\cal F$ 表示的是回归森林中的所有函数空间。$f_k(x_i)$ 表示的就是第 $i$ 个样本在第 $k$ 棵树中落在的叶子的权重。那么现在我们需要求的参数就是每棵树的结构和每片叶子的权重，或者简单的来说就是求 $f_k$ 。那么为了和上一节所说的通用结构统一，可以设$$\Theta = \lbrace f_1,f_2,f_3, \cdots ,f_k \rbrace$$ 在单一变量上学习一棵树 定义一个目标对象，优化它。 例如： 考虑这样一个问题：在输入只有时间（t）的回归树 我想预测在时间是t的时候，我是否喜欢浪漫风格的音乐？ 可见分段函数的分割点就是回归树的非叶子节点，分段函数每一段的高度就是回归树叶子的权重。那么就可以直观地看到欠拟合和过拟合曲线所对应的回归树的结构。根据我们上一节的讨论，$Ω(f)$ 表示模型复杂度，那么在这里就对应着分段函数的琐碎程度。$L(f)$ 表示的就是函数曲线和训练集的匹配程度。 学习阶跃函数 第二幅图：太多的分割点，$\Omega(f)$ 即模型复杂度很高；第三幅图：错误的分割点，$L(f)$ 即损失函数很高。第四幅图：在模型复杂度和损失函数之间取得很好的平衡。 综上所述 模型：假设我们有k棵树，那么模型的表达式 $\hat{y}_i = \sum\limits_{k=1}^{K}f_k(x_i), f_k\in \cal{F}$ 目标函数：$Obj =\underbrace{\sum_{i=1}^{n}l(y_i, \hat{y_i})}_{训练误差} +\underbrace{\sum_{k=1}^{K}\Omega(f_k)}_{树的复杂度}$ 定义树的复杂度几种方式 树的节点数或深度 树叶子节点的L2范式 …（后面会介绍有更多的细节） 目标函数 vs 启发式当你讨论决策树，它通常是启发式的 按信息增益 对树剪枝 最大深度 对叶子节点进行平滑 大多数启发式可以很好地映射到目标函数 信息增益 -&gt; 训练误差 剪枝 -&gt; 按照树节点的数目定义的正则化项 最大深度 -&gt; 限制函数空间 对叶子值进行平滑操作 -&gt; 叶子权重的L2正则化项 回归树不仅仅用于回归 回归树的集成模型定义了你如何创建预测的分数，它能够用于 分类，回归，排序 … … 回归树的功能取决于你怎么定义目标函数 目前为止我们已经学习过 使用方差损失（Square Loss） $l(y_i, \hat{y_i})=(y_i-\hat{y}_i)$ ，这样就产生了普通的梯度提升机（common gradient boosted machine） 使用逻辑损失（Logistic loss）$l(y, \hat{y}_i)=y_i\ln(1+e^{-\hat{y}_i}) + (1-y_i)\ln(1+e^{\hat{y}_i})$ ，这样就产生了逻辑梯度提升（LogitBoost）。 梯度提升Gradient Boosting (How do we Learn) 那怎么学习？ 目标对象：$\sum_{i=1}^{n}l(y_i,\hat{y_i}) + \sum_k\Omega(f_k), f_k \in \cal{F}$ 我们不能用像SGD（随机梯度下降）这样的方法去找到 f，因为他们是树而不是仅仅是数值向量。 解决方案：加法训练 Additive Training（提升方法boosting） 从常量方法开始，每一次（轮）添加一个新的方法 这个算法的思想很简单，一棵树一棵树地往上加，一直到 $K$ 棵树停止。过程可以用下式表达：$$\begin{align}\hat y_i^{(0)} &amp;= 0 \\\hat y_i^{(1)} &amp;= f_1(x_i) = \hat y_i^{(0)} + f_1(x_i) \\\hat y_i^{(2)} &amp;= f_1(x_i) + f_2(x_i) = \hat y_i^{(1)} + f_2(x_i) \\&amp; \cdots \\\hat y_i^{(t)} &amp;= \sum_{k = 1}^t f_k(x_i) = \hat y_i^{(t - 1)} + f_t(x_i)\end{align}$$ 加法训练 我们如何决定什么样的 $f$ 加到模型中？ 优化目标 在 $t$ 轮的预测是：$\hat y_i^{(t)} = \hat y_i^{(t - 1)} + f_t(x_i) $ 加号右边这一项就是我们在 t 轮需要决定的东西 $$ \begin{align} Obj^{(t)} &amp;= \sum_{i = 1}^n l(y_i, \hat y_i^{(t)}) + \sum_{i = 1}^t \Omega (f_i) \\ &amp;= \sum_{i = 1}^n l(y_i, \hat y_i^{(t - 1)} + f_t(x_i)) + \Omega (f_t) + constant \end{align} $$ 考虑平方误差 $$ \begin{align} Obj^{(t)} &amp;= \sum_{i=1}^{n} \left \{y_i-(\hat{y}^{(t-1)}_i)+f_t(x_i)\right \}^2 +\Omega(f_t)+const \\ &amp;= \sum_{i=1}^{n} \left \{2(\hat{y}^{(t-1)}_i-y_i)+f_t(x_i)^2\right \} +\Omega(f_t)+const \\ \end{align} $$ $(\hat{y}^{(t-1)}_i-y_i)$ 称为残差。 损失函数的泰勒展开可由泰勒公式得到下式$$f(x + \Delta x) \approx f(x) +f^{\prime}(x) \Delta x + \frac 1 2 f^{\prime \prime}(x) \Delta x^2$$那么现在可以把 $y^{(t)}_i$看成上式中的 $f(x+Δx)$ ，$y^{(t−1)}_i$ 就是 $f(x)$ ，$f_t(x_i)$ 为 $Δx$ 。然后设 $g_i$ 代表 $f′(x)$ ，也就是 $g_i = {\partial}_{\hat y^{(t - 1)}} \ l(y_i, \hat y^{(t - 1)})$ 用 $h_i$ 代表 $f′′(x)$， 于是 $h_i = {\partial}_{\hat y^{(t - 1)}}^2 \ l(y_i, \hat y^{(t - 1)})$ 于是现在目标函数就为下式:$$\begin{align}Obj^{(t)} &amp;\approx \sum_{i = 1}^n [l(y_i, \hat y_i^{(t - 1)}) + g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t) + constant \\&amp;= \sum_{i = 1}^n [g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t) + [\sum_{i = 1}^n l(y_i, \hat y_i^{(t - 1)}) + constant]\end{align}$$可以用平方误差的例子进行泰勒展开看看结果是否一致，很明显，上式中后面那项 $[\sum_{i = 1}^n l(y_i, \hat y_i^{(t - 1)}) + constant]$ 对于该目标函数我们求最优值点的时候并无影响，所以，现在有了新的优化目标$$Obj^{(t)} \approx \sum_{i = 1}^n [g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t)$$ 这么苦逼图啥？ 改进树的定义 Refine the definition of tree上一节讨论了 $f_t(x)$ 的物理意义，现在我们对其进行数学公式化。设 $w∈R^T$ ， $w$ 为树叶的权重序列，$q:R^d \rightarrow \lbrace 1,2, \cdots ,T \rbrace$ ，$q$ 为树的结构。那么 $q(x)$ 表示的就是样本 $x$ 所落在树叶的位置。可以用下图形象地表示 现在对训练误差部分的定义已经完成。那么对模型的复杂度应该怎么定义呢？ 定义树的复杂度 Define Complexity of a Tree树的深度？最小叶子权重？叶子个数？叶子权重的平滑程度？等等有许多选项都可以描述该模型的复杂度。为了方便，现在用叶子的个数和叶子权重的平滑程度来描述模型的复杂度。可以得到下式：$$\Omega(f_t) = \gamma T + \frac 1 2 \lambda \sum_{j = 1}^T w_j^2$$说明：上式中前一项用叶子的个数乘以一个收缩系数，后一项用L2范数来表示叶子权重的平滑程度。 下图就是计算复杂度的一个示例： 修改目标函数 Revisit the Objectives最后再增加一个定义，用 $I_j$ 来表示第 $j$ 个叶子里的样本集合。也就是上图中，第 $j$ 个圈，就用 $I_j$ 来表示。$$I_j = \lbrace i|q(x_i) = j \rbrace$$好了，最后把优化函数重新按照每个叶子组合,并舍弃常数项：$$\begin{align}Obj^{(t)} &amp;\approx \sum_{i = 1}^n [g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t) \\ &amp;= \sum_{i = 1}^n [ g_i w_{q(x_i)} + \frac 1 2 h_i w_{q(x)}^2] + \gamma T + \frac 1 2 \lambda \sum_{j = 1}^T w_j^2 \\ &amp;= \sum_{j = 1}^T [(\sum_{i \in I_j } g_i)w_j + \frac 1 2 (\sum_{i \in I_j}h_i + \lambda)w_j^2] + \gamma T\end{align}$$ 这是 $T$ 个独立的二次函数的和。 结构分 The Structure Score初中时所学的二次函数的最小值可以推广到矩阵函数里$$\mathop{\min_x}\{Gx+ \frac 1 2 Hx^2\} = - \frac 1 2 \frac {G^2} H, \quad H \gt 0 \\\mathop{\arg\min_x}\{Gx+\frac{1}{2}Hx^2\} = -\frac{G}{H}，H \ge 0$$设 $G_j = \sum_{i \in I_j } g_i,\ H_j = \sum_{i \in I_j}h_i$ ，那么$$\begin{align}Obj^{(t)} &amp;= \sum_{j = 1}^T [(\sum_{i \in I_j } g_i)w_j + \frac 1 2 (\sum_{i \in I_j}h_i + \lambda)w_j^2] + \gamma T \\ &amp;= \sum_{j = 1}^T [G_j w_j + \frac 1 2 (H_j + \lambda)w_j^2] + \gamma T\end{align}$$因此，若假设我们的树的结构已经固定，就是 $q(x)$ 已经固定，那么$$\begin{align}W_j^* &amp;= - \frac {G_j}{H_j + \lambda} \\Obj &amp;= - \frac 1 2 \sum_{j = 1}^T \frac {G_j^2}{H_j + \lambda} + \gamma T\end{align}$$例子 用于单棵树的搜索算法 Searching Algorithm for Single Tree现在只要知道树的结构，就能得到一个该结构下的最好分数。可是树的结构应该怎么确定呢？ 枚举可能的树结构 q 使用分数公式来计算 q 的结构分： $Obj = -\frac{1}{2} \sum\limits_{j=1}^{T}\frac{G_j^2}{H_j+\lambda} + \gamma T$ 找到最好的树结构，然后使用优化的叶子权重： $w^*_j=-\frac{G_j}{H_j+\lambda}$ 但是这可能有无限多个可能的树结构 树的贪婪学习 Greedy Learning of the Tree 从深度为 0 的树开始 对树的每个叶子节点，试着添加一个分裂点。添加这个分裂点后目标函数（即损失函数）的值变化 $$ \begin{align} Obj_{split} &amp;= - \frac{1}{2}[\underbrace{\frac{G_L^2}{H_L+\lambda}}_{左孩子节点分数} + \underbrace {\frac{G^2_R}{H_R+\lambda}}_{右孩子节点分数}] + \gamma T_{split} \\ Obj_{unsplit} &amp;= - \frac{1}{2}\underbrace{\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}}_{分裂前的分数} + \gamma T_{unsplit} \\ Gain &amp;= Obj_{unsplit} - Obj_{split} \\ &amp;= \frac 1 2 [\frac {G_L^2}{H_L + \lambda} + \frac {G_R^2}{H_R + \lambda} - \frac {(G_L + G_R)^2}{H_L + H_R + \lambda}] - \gamma(T_{split} - T_{unsplit}) \end{align} $$ 剩下的问题：我们如何找到最好的分裂点？ 最好分裂点的查找 Efficient Finding of the Best Split 当分裂规则是 $x_j&lt;a$ 时，树的增益是 ? 假设 $x_j$ 是年龄 我们所需要就是上图的两边 $g$ 和 $h$ 的和，然后计算 $$ Gain = \frac{G_L^2}{H_L+\lambda} + \frac{G_L^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda} - \gamma $$ 在一个特征上，从左至右对已经排序的实例进行线性扫描能够决定哪个是最好的分裂点。 分裂点查找算法 An Algorithm for Split Finding 对于每个节点，枚举所有的特征 对于每个特征，根据特征值对实例（样本）进行排序 在这个特征上，使用线性扫描决定哪个是最好的分裂点 在所有特征上采用最好分裂点的方案 深度为 $K$ 的生长树的时间复杂度 $O(K\ d\ n\log n)$ ：每一层需要 $O(n\ \log n)$ 时间去排序，且需要在 $d$ 个特征上排序，我们需要在 $K$ 层进行这些排序。（补充：$O(n)$ 时间计算当前特征的最佳分裂点，即最后实际上 $O(d\ K\ (n\log n +n)$） 这些可以进一步优化（例如：使用近似算法和缓存已经排序的特征） 能够拓展到非常大的数据集 类变量（categorical variables） 有一些树处理分开处理类变量和连续值的变量 xgboost可以简单地使用之前推导的分数公式去计算基于类变量的分裂分数 实际上，没有必要分开处理类变量 我们可以使用独热编码（one-hot encoding）将类变量编码成数值向量。分配一个维度为类数量的向量。 $$ z_j=\cases{1,\quad &amp;\text{if $x$ is in category $j$}\\ 0,\quad &amp;otherwise} $$ 如果有很多类变量，这个数值向量将是稀疏的，xgboost学习算法被设计成偏爱处理稀疏数据。 补充：对某个节点的分割时，是需要按某特征的值排序，那么对于无序的类别变量，就需要进行one-hot化。否则，举个例子：假设某特征有1，2，3三种变量，进行比较时，就会只比较左子树为1, 2或者右子树为2, 3，或者不分割，哪个更好，但是左子树为 1,3 的分割的这种情况就会忘记考虑。因为 $Gain$ 于特征的值范围是无关的，它采用的是已经生成的树的结构与权重来计算的。所以不需要对特征进行归一化处理。 剪枝和正则化 Pruning and Regularization 回忆一下增益公式： $Gain=\underbrace{\frac{G^2_L}{H_L+\lambda} + \frac{G^2_R}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda}}_{训练损失的减少量} - \underbrace{\gamma}_{正则项}$ 当训练损失减少量小于正则项的时候，分裂后的增益就变成负的。 在树的简化度（simplicity）和预测性能（predictiveness）的权衡（trade-off） 提早终止（Pre-stopping） 如果最好的分裂产生的增益计算出来是负的，那么停止分裂。 但是（当前的）一个分裂可能对未来的分裂有益。 后剪枝 （Post-Prunning） 生长一棵树到最大深度，再递归地剪枝所有具有负增益的叶子分裂节点。 回顾提升树算法 Recap: Boosted Tree Algorithm 每一轮添加一棵树 每一轮开始的时候，计算 $g_i=\partial_{\hat{y}_i^{(t-1)}}l(y_i,\hat{y}^{(t-1)}), h_i=\partial_{\hat{y}^{(t-1)}}l(y_i, \hat{y}^{(t-1)})$ 使用统计学知识（统计所有分裂点信息：一节梯度和二阶梯度），用贪婪的方式生长一棵树 $f_t(x)$ ： $$ Obj = -\frac{1}{2}\sum\limits_{j=1}^{T}\frac{G_j^2}{H_j+\lambda} + \gamma T $$ 添加 $f_t(x)$ 到模型 $\hat{y}_i^{(t)}=\hat{y}_i^{(t-1)} + f_t(x_i)$ 通常，我们这么做令 $\hat{y}_i^{(t)}=\hat{y}_i^{(t-1)} + \epsilon f_t(x_i)$ $\epsilon$ 称为步伐大小（step-size）或者缩放（shrinkage），通常设置为大约 0.1 这意味着在每一步我们做完全优化，是为了给未来的轮次保留机会（去进一步优化），这样做有助于防止过拟合。 —————————————————————幻灯片内容结束———————————————————————- XGBoost 系统设计的精髓这部分内容主要来自陈天奇的论文 XGBoost: A Scalable Tree Boosting System 缩放和列抽样 shrinkage and column subsampling随机森林中的用法和目的一样，用来防止过拟合，主要参考论文2.3节 这个xgboost与现代的gbdt一样，都有shrinkage参数 （最原始的gbdt没有这个参数）类似于梯度下降算法中的学习速率，在每一步tree boosting之后增加了一个参数 $\eta$（被加入树的权重），通过这种方式来减小每棵树的影响力，给后面的树提供空间去优化模型。 column subsampling 列（特征）抽样，这个经常用在随机森林，不过据XGBoost的使用者反馈，列抽样防止过拟合的效果比传统的行抽样还好（xgboost也提供行抽样的参数供用户使用），并且有利于后面提到的并行化处理算法。 查找分裂点的近似算法 Approximate Algorithm主要参考论文3.2节 当数据量十分庞大，以致于不能全部放入内存时，精确的贪婪算法就不可能很有效率，通样的问题也出现在分布式的数据集中，为了高效的梯度提升算法，在这两种背景下，近似的算法被提出使用，算法的伪代码如下图所示 概括一下：枚举所有特征，根据特征，比如是第 $k$ 个特征的分布的分位数来决定出 $l$ 个候选切分点 $S_k = \{s_{k1},s_{k2},\cdots s_{kl}\}$ ，然后根据这些候选切分点把相应的样本映射到对应的桶中，对每个桶的 $G,H$ 进行累加。最后在候选切分点集合上贪心查找，和Exact Greedy Algorithm类似。 特征分布的分位数的理解 此图来自知乎weapon大神的《 GBDT算法原理与系统设计简介》 论文给出近似算法的2种变体，主要是根据候选点的来源不同区分： 在建树之前预先将数据进行全局（global）分桶，需要设置更小的分位数间隔，这里用 ϵ 表示，3分位的分位数间隔就是 $1/3$，产生更多的桶，特征分裂查找基于候选点多，计算较慢，但只需在全局执行一次，全局分桶多次使用。 每次分裂重新局部（local）分桶，可以设置较大的 $ϵ$ ，产生更少的桶，每次特征分裂查找基于较少的候选点，计算速度快，但是需要每次节点分裂后重新执行，论文中说该方案更适合树深的场景。 论文给出Higgs案例下，方案1全局分桶设置 $ϵ=0.05$ 与精确算法效果差不多，方案2局部分桶设置 $ϵ=0.3$ 与精确算法仅稍差点，方案1全局分桶设置 $ϵ=0.3$ 则效果极差，如下图： 由此可见，局部选择的近似算法的确比全局选择的近似算法优秀的多，所得出的结果和贪婪算法几乎不相上下。 最后很重的是：使用哪种方案，xgboost用户可以自由选择。 Notably, it is also possible to directly construct approximate histograms of gradient statistics. Our system efficiently supports exact greedy for the single machine setting, as well as approximate algorithm with both local and global proposal methods for all settings. Users can freely choose between the methods according to their needs. 这里直方图算法，常用于GPU的内存优化算法，leetcode上也有人总结出来：LeetCode Largest Rectangle in Histogram O(n) 解法详析， Maximal Rectangle 带权的分位方案 Weighted Quantile Sketch主要参考论文3.3节 在近似的分裂点查找算法中，一个步骤就是提出候选分裂点，通常情况下，一个特征的分位数使候选分裂点均匀地分布在数据集上，就像前文举的关于特征分位数的例子。 考虑 $\cal{D}_k = \lbrace (x_{1k},h_1), (x_{2k},h_2), (x_{3k},h_3), \cdot \cdot \cdot , (x_{nk},h_n)\rbrace$ 代表每个样本的第 $k$ 个特征和其对应的二阶梯度所组成的集合。那么我们现在就能用分位数来定义下面的这个排序函数 $r_k:\Bbb R \rightarrow [0,1]$$$r_k(z) = \frac 1 {\sum_{(x,h) \in \cal{D}_k}h} \sum_{(x,h)\in \cal{D}_k,x \lt z} h$$上式表示的就是该特征的值小于 $z$ 的样本所占总样本的比例。于是我们就能用下面这个不等式来寻找分裂候选点$\lbrace s_{k1},s_{k2},s_{k3}, \cdots, s_{kl} \rbrace$$$|r_k(s_{k,j}) - r_k(s_{k, j+1})| \lt \epsilon,\ s_{k1}=\underset{i}{min}\ x_{ik},s_{kl}=\underset{i}{max}\ x_{ik}$$上式中 $\epsilon$ 的作用：控制让相邻两个候选分裂点相差不超过某个值 $\epsilon$ ，那么 $1/\epsilon$ 的整数值就代表几分位，举例 $\epsilon=1/3$ ，那么就是三分位，即有 $3-1$ 个候选分裂点。数学上，从最小值开始，每次增加 $ϵ∗(\underset{i}\max x_{ik}−\underset{i}\min x_{ik})$ 作为分裂候选点。然后在这些分裂候选点中选择一个最大分数作为最后的分裂点，而且每个数据点的权重是 $h_i$ ，原因如下：$$\begin{align}Obj^{(t)} &amp;\approx \sum_{i = 1}^n [g_i f_t(x_i) + \frac 1 2 h_i f_t^2 (x_i)] + \Omega (f_t) \\&amp;=\sum_{i=1}^N\frac{1}{2}h_i\left(2\frac{g_i}{h_i}f_t({\bf x_i}) + f_t^2({\bf x_i})\right) + \Omega(f_t) \\&amp;=\sum_{i=1}^N \frac{1}{2}h_i\left(\frac{g_i^2}{h_i^2} +2\frac{g_i}{h_i}f_t({\bf x_i}) + f_t^2({\bf x_i})\right) + \Omega(f_t) - \frac{g_i^2}{2h_i} \\&amp;=\sum_{i=1}^N \frac{1}{2}{\color{green}h_i}\left( f_t({\bf x_i}) – ({\color{green}- \frac{g_i}{h_i}})\right)^2 + \Omega(f_t) - \frac{g_i^2}{2h_i} \\&amp;=\sum_{i=1}^N \frac{1}{2}{\color{green}h_i}\left( f_t({\bf x_i}) – ({\color{green}- \frac{g_i}{h_i}})\right)^2 + \Omega(f_t) - constant\end{align}$$说明：这部分论文原文推导有些错误，国外问答网站 stack exchange 给出很明确的答复， 上式可以视为标签为 $-\frac{g_i}{h_i}$ 且权重为 $h_i$ 的平方误差，此时视 $\frac{g_i^2}{2h_i}$ 常数 （因为是来自上一轮的梯度和二阶梯度）。 现在应该明白 Weighted Quantile Sketch 带权的分位方案的由来，下面举个例子： 即要切分为3个，总和为1.8，因此第1个在0.6处，第2个在1.2处。此图来自知乎weapon大神的《 GBDT算法原理与系统设计简介》 注意稀疏问题的分裂点查找 Sparsity-aware Split Finding主要参考论文3.4节 对于数据缺失数据、one-hot编码等造成的特征稀疏现象，作者在论文中提出可以处理稀疏特征的分裂算法，主要是对稀疏特征值缺失的样本学习出默认节点分裂方向： 默认miss value进右子树，对non-missing value的样本在左子树的统计值 $G_L$ 与 $H_L$，右子树为 $G-G_L$ 与$H−H_L$，其中包含miss的样本，统计这种方案（默认miss value进右子树）的分数。 默认miss value进左子树，对non-missing value的样本在右子树的统计值 $G_R$ 与 $H_R$，左子树为 $G-G_R$ 与$H−H_R$ ，其中包含miss的样本，统计这种方案（默认miss value进左子树）的分数。 选择分数（即增益）比较大的方案。 这样最后求出增益最大的特征值以及 miss value 的分裂方向，作者在论文中提出基于稀疏分裂算法： （修正：下文 “Input: d feature dimension” 这里 “d” 应该改为 “m”） 使用了该方法，相当于比传统方法多遍历了一次，但是它只在非缺失值的样本上进行迭代，因此其复杂度与非缺失值的样本成线性关系。在 Allstate-10k 数据集上，比传统方法快了50倍： 旨在并行学习的列块结构 Column Block for Parallel Learning主要参考论文4.1节 CSR vs CSC 稀疏矩阵的压缩存储形式，比较常见的其中两种：压缩的稀疏行（Compressed Sparse Row）和 压缩的稀疏列（Compressed Sparse Row） CSR结构包含非0数据块values，行偏移offsets，列下标indices。offsets数组大小为（总行数目+1），CSR 是对稠密矩阵的压缩，实际上直接访问稠密矩阵元素 $(i,j)$ 并不高效，毕竟损失部分信息，访问过程如下： 根据行 $i$ 得到偏移区间开始位置 offsets[i]与区间结束位置 offsets[i+1]-1，得到 $i$ 行数据块 values[offsets[i]..(offsets[i+1]-1)]， 与非0的列下表indices[offsets[i]..(offsets[i+1]-1)] 在列下标数据块中二分查找 $j$，找不到则返回0，否则找到下标值 $k$，返回 values[offsets[i]+k] 从访问单个元素来说，相比坐标系的存储结构，那么从 $O(1)$ 时间复杂度升到 $O(\log N)$, N 为该行非稀疏数据项个数。但是如果要遍历访问整行非0数据，则无需访问indices数组，时间复杂度反而更低，因为少了大量的稀疏为0的数据访问。 CSC 与 CSR 变量结构上并无差别，只是变量意义不同 values仍然为矩阵的非0数据块 offsets为列偏移，即特征id对应数组 indices为行下标，对应样本id数组 XBGoost使用CSC 主要用于对特征的全局预排序。预先将 CSR 数据转化为无序的 CSC 数据，遍历每个特征，并对每个特征 $i$ 进行排序：sort(&amp;values[offsets[i]], &amp;values[offsets[i+1]-1])。全局特征排序后，后期节点分裂可以复用全局排序信息，而不需要重新排序。 矩阵的存储形式，参考此文：稀疏矩阵存储格式总结+存储效率对比:COO,CSR,DIA,ELL,HYB 采取这种存储结构的好处 未完待续。。。。。 关注缓存的存取 Cache-aware Access使用Block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的。这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。 因此，对于exact greedy算法中, 使用缓存预取。具体来说，对每个线程分配一个连续的buffer，读取梯度信息并存入Buffer中（这样就实现了非连续到连续的转化），然后再统计梯度信息。该方式在训练样本数大的时候特别有用，见下图： 在近似算法中，对块的大小进行了合理的设置。定义Block的大小为Block中最多的样本数。设置合适的大小是很重要的，设置过大则容易导致命中率低，过小则容易导致并行化效率不高。经过实验，发现 $2^{16}$ 比较好，那么上文提到CSC存储结构的 indices 数组（存储的行下表）的元素占用的字节数就是 16/8 = 2 。 核外块的计算 Blocks for Out-of-core ComputationXGBoost 中提出 Out-of-core Computation优化，解决了在硬盘上读取数据耗时过长，吞吐量不足 多线程对数据分块压缩 Block Compression 存储在硬盘上，再将数据传输到内存，最后再用独立的线程解压缩，核心思想：将磁盘的读取消耗转换为解压缩所消耗的计算资源。 分布式数据库系统的常见设计：Block Sharding 将数据分片到多块硬盘上，每块硬盘分配一个预取线程，将数据fetche到in-memory buffer中。训练线程交替读取多块缓存的同时，计算任务也在运转，提升了硬盘总体的吞吐量。 注：这部分内容属于外存算法External_memory_algorithm XGBoost 对 GBDT 实现的不同之处这部分内容主要参考了知乎上的一个问答 机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？ - 知乎 根据他们的总结和我自己对论文的理解和补充。 传统GBDT以CART作为基分类器，xgboost支持多种基础分类器。比如，线性分类器，这个时候xgboost相当于带 L1 和 L2正则化项 的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 可以通过booster [default=gbtree] 设置参数，详细参照官网 gbtree: tree-based models gblinear: linear models DART: Dropouts meet Multiple Additive Regression Trees dropout 在深度学习里面也经常使用，需要注意的是无论深度学习还是机器学习：使用droput训练出来的模型，预测的时候要使dropout失效。 传统GBDT在优化时只用到一阶导数信息，xgboost则对损失函数函数进行了二阶泰勒展开，同时用到了一阶和二阶导数，这样相对会精确地代表损失函数的值。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导，详细参照官网API。 并行处理，相比GBM有了速度的飞跃 借助 OpenMP ，自动利用单机CPU的多核进行并行计算 支持GPU加速 支持分布式 剪枝 当新增分裂带来负增益时，GBM会停止分裂（贪心策略，非全局的剪枝） XGBoost一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝（事后，进行全局剪枝） xgboost在代价函数里加入了显示的正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和，防止过拟合，这也是xgboost优于传统GBDT的一个特性。正则化的两个部分，都是为了防止过拟合，剪枝是都有的，叶子结点输出L2平滑是新增的。 Built-in Cross-Validation 内置交叉验证 XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.This is unlike GBM where we have to run a grid-search and only a limited values can be tested. XGBoost允许在每一轮boosting迭代中使用交叉验证，这样可以方便地获得最优boosting迭代次数 GBM使用网格搜索，只能检测有限个值 continue on Existing Model 可以保存模型下次接着训练，方便在线学习 User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications.GBM implementation of sklearn also has this feature so they are even on this point. High Flexibility 可定制损失函数，只要这个损失函数2阶可导 XGBoost allow users to define custom optimization objectives and evaluation criteria.This adds a whole new dimension to the model and there is no limit to what we can do. 提供多语言接口 命令行（Command Line Interface， CLI） C++/Python（可以和scikit-learn结合）/R（可以和caret包结合）/Julia/JAVA和JVM语言（如Scala、 Hadoop平台等） xgboost工具支持并行，执行速度确实比其他Gradient Boosting实现快 模型性能：在结构化数据集上，在分类／回归/排序预测建模上表现突出，相比之下，神经网络尤其擅长非结构化的数据集（比如：图片，语音） 注意xgboost不同于随机森林中的并行粒度是：tree，xgboost与其他提升方法（比如GBDT）一样，也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。 我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 总体来说，这部分内容需要学习很多，特别是涉及到分布式地并发优化和资源调度算法，这就不仅仅是数学模型的问题了，还涉及到系统设计，程序运行性能的优化，本人实在是才疏学浅，这部分内容理解尚浅，进一步学习还需要其他论文和看XGBoost源码，有些优化的地方也不是作者首创，表示从附录的论文中得以学习集成到XGBoost中，真的是集万千之大作，作者不愧是上海交大ACM班出身。大神的访谈：https://cosx.org/2015/06/interview-of-tianqi/ 优化的角度马琳同学的回答 非常棒，真是让我感受到了：横看成岭侧成峰 高可用的xgboost由于xgboost发展平稳成熟，现在已经非常易用，下图来自官网 hello world来自官网，其他复杂的demo，参看github的demo目录 Python 12345678910import xgboost as xgb# read in datadtrain = xgb.DMatrix('demo/data/agaricus.txt.train')dtest = xgb.DMatrix('demo/data/agaricus.txt.test')# specify parameters via mapparam = &#123;'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' &#125;num_round = 2bst = xgb.train(param, dtrain, num_round)# make predictionpreds = bst.predict(dtest) 在jupter notebook中运行结果 树形提升器1234import xgboost as xgb# read in datadtrain = xgb.DMatrix('demo/data/agaricus.txt.train')dtest = xgb.DMatrix('demo/data/agaricus.txt.test') [18:22:42] 6513x127 matrix with 143286 entries loaded from demo/data/agaricus.txt.train [18:22:42] 1611x127 matrix with 35442 entries loaded from demo/data/agaricus.txt.test 1234# specify parameters via mapparam = &#123;'max_depth':3, 'eta':1, 'silent': 0, 'objective':'binary:logistic' &#125;num_round = 2bst = xgb.train(param, dtrain, num_round) [18:22:42] C:\Users\Administrator\Desktop\xgboost\src\tree\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3 [18:22:42] C:\Users\Administrator\Desktop\xgboost\src\tree\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3 1234# make predictionpreds = bst.predict(dtest)print(preds)print(bst.eval(dtest)) [0.10828121 0.85500014 0.10828121 ... 0.95467216 0.04156424 0.95467216] [0] eval-error:0.000000 DART提升器 Dropouts meet Multiple Additive Regression Trees123456789101112param = &#123;'booster': 'dart', 'max_depth': 4, 'eta': 0.001, 'objective': 'binary:logistic', 'silent': 0, 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.5, 'skip_drop': 0.0&#125;#Command Line Parameters: 提升的轮次数num_round = 2bst = xgb.train(param, dtrain, num_round) 1234[18:22:42] C:\Users\Administrator\Desktop\xgboost\src\tree\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=4[18:22:42] C:\Users\Administrator\Desktop\xgboost\src\gbm\gbtree.cc:494: drop 0 trees, weight = 1[18:22:42] C:\Users\Administrator\Desktop\xgboost\src\tree\updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=4[18:22:42] C:\Users\Administrator\Desktop\xgboost\src\gbm\gbtree.cc:494: drop 1 trees, weight = 0.999001 1234# make predictionpreds = bst.predict(dtest, ntree_limit=num_round)print(preds)print(bst.eval(dtest)) [0.4990105 0.5009742 0.4990105 ... 0.5009742 0.4990054 0.5009742] [0] eval-error:0.007449 参数详解官网，看懂参数的前提是把前文数学公式和理论看懂，这部分内容主要是对官网的翻译。 运行XGBoost之前，我们必须设置3种类型的参数：通用参数（general parameters），提升器参数（booster paramter），任务参数（task parameter）。 通用参数：与我们所使用的提升器（通常是树型提升器或者线性提升器）的提升算法相关。 提升器参数：取决于你所选择的哪种提升器 学习任务的参数：这些参数决定了学习的方案（learning scenario）。例如：在排名任务场景下，回归任务可能使用不同的参数。 命令行参数：与 XGBoost 的命令行接口（CLI）版本的行为相关。 Note Parameters in R package In R-package, you can use . (dot) to replace underscore(与underline同义) in the parameters, for example, you can use max.depth to indicate max_depth. The underscore parameters are also valid in R. General Parameters Parameters for Tree Booster Additional parameters for Dart Booster (booster=dart) Parameters for Linear Booster (booster=gblinear) Parameters for Tweedie Regression (objective=reg:tweedie) Learning Task Parameters Command Line Parameters 通用参数 general parameters booster [default=gbtree] 设定基础提升器的参数 Which booster to use. Can be gbtree, gblinear or dart; gbtree and dart use tree based models while gblinear uses linear functions. silent [default=0]: 设置成1则没有运行信息的输出，最好是设置为0. nthread [default to maximum number of threads available if not set]：线程数 disable_default_eval_metric [default=0] Flag to disable default metric. Set to &gt;0 to disable. ，使默认的模型评估器失效的标识 num_pbuffer [set automatically by XGBoost, no need to be set by user] Size of prediction buffer, normally set to number of training instances. The buffers are used to save the prediction results of last boosting step. num_feature [set automatically by XGBoost, no need to be set by user] Feature dimension used in boosting, set to maximum dimension of the feature 提升器参数 Booster parameters树提升器参数 Parameters for Tree Booster eta [default=0.3], range $[0, 1]$ shrinkage参数，用于更新叶子节点权重时，乘以该系数，避免步长过大。参数值越大，越可能无法收敛。把学习率 eta 设置的小一些，小学习率可以使得后面的学习更加仔细。 gamma [default=0 alias: min_split_loss] , range $[0, \infty]$ 功能与min_split_loss 一样，（alias是“别名，又名”的意思，联想linux命令：alias就非常容易理解，即给相应的命令起了新的名字，引用同一个程序，功能是一样的），损失函数减少的最小量。 max_depth [default=6], range $[0, \infty]$ 每颗树的最大深度，树高越深，越容易过拟合。 min_child_weight [default=1], range: $[0, \infty]$ 这个参数默认是 1，是每个叶子里面loss函数二阶导（ hessian）的和至少是多少，对正负样本不均衡时的 0-1 分类而言，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 max_delta_step [default=0] , range: $[0, \infty]$ Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update. 这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。这个参数一般用不到，但是你可以挖掘出来它更多的用处。 subsample [default=1], range: $[0, 1]$ 训练实例的抽样率，较低的值使得算法更加保守，防止过拟合，但是太小的值也会造成欠拟合。如果设置0.5，那就意味着随机树的生长之前，随机抽取训练数据的50%做样本。 colsample_bytree [default=1], range: $[0, 1]$ 在构建每棵树的时候，特征（这里说是列，因为样本是按行存储的，那么列就是相应的特征）的采样率，用的特征进行列采样. colsample_bytree 表示的是每次分割节点时，抽取特征的比例。 lambda [default=1, alias: reg_lambda] 作用于权重值的 L2 正则化项参数，参数越大，模型越不容易过拟合。 alpha [default=0, alias: reg_alpha] 作用于权重值的 L1 正则项参数，参数值越大，模型越不容易过拟合。 tree_method string [default=auto] 用来设定树的构建算法，欲知详情请看陈天奇论文中的引用资料： reference paper. The tree construction algorithm used in XGBoost. See description in the reference paper. 分布式和外存版本仅仅支持 tree_method=approx Distributed and external memory version only support tree_method=approx. 选项：auto, exact, approx, hist, gpu_exact, gpu_hist, auto Choices: auto,exact,approx,hist,gpu_exact,gpu_hist,auto auto: Use heuristic to choose the fastest method. 启发式地选择快速算法 ​ - For small to medium dataset, exact greedy (exact) will be used. 中小数据量采用精确的贪婪搜索算法（指代前文说的树的生长过程中，节点分裂算法，所以很好理解） ​ - For very large dataset, approximate algorithm (approx) will be chosen. 非常大的数据集，近似算法将被选用。 ​ - Because old behavior is always use exact greedy in single machine, user will get a message when approximate algorithm is chosen to notify this choice. 因为旧的行为总是使用精确的贪婪算法，所以在近似算法被选用的时候，用户会收到一个通知消息，告诉用户近似算法被选用。 exact: Exact greedy algorithm. 精确地贪婪算法 approx: Approximate greedy algorithm using quantile sketch and gradient histogram. 近似算法采用分位方案和梯度直方图方案。 hist: Fast histogram optimized approximate greedy algorithm. It uses some performance improvements such as bins caching. 优化过的近似贪婪算法的快速算法，这个快速算法采用一些性能改善（的策略），例如桶的缓存（这里桶指的是直方图算法中所用的特征数据划分成不同的桶，欲知详情，查看陈天奇论文以及论文的引用资料） gpu_exact: GPU implementation of exact algorithm. gpu_hist: GPU implementation of hist algorithm. sketch_eps [default=0.03], range: (0, 1) 全称：sketch epsilon 即 分位算法中的 $\epsilon$ 参数 Only used for tree_method=approx. 仅仅用于近似算法 This roughly translates into O(1 / sketch_eps) number of bins. Compared to directly select number of bins, this comes with theoretical guarantee with sketch accuracy. 大致理解为桶数的倒数值。与直接给出桶数相比，这个与带权分位草案（Weighted Quantitle Sketch）能够保证理论上一致 Usually user does not have to tune this. But consider setting to a lower number for more accurate enumeration of split candidates. 通常情况下，不需要用户调试这个参数，但是考虑到设置一个更低的值能够枚举更精确的分割候选点。 scale_pos_weight [default=1] 正标签的权重缩放值 Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances). 控制样本正负标签的平衡，对于标签不平衡的样本有用，一个经典的值是：训练样本中具有负标签的实例数量/训练样本中正标签的实例数量。（举例：-1:2000个 +1:8000个，那么训练过程中每个正标签实例权重只有负标签实例的25%） See Parameters Tuning for more discussion. Also, see Higgs Kaggle competition demo for examples: R, py1, py2, py3. updater [default=grow_colmaker,prune] 逗号分割的字符串定义树的生成器和剪枝，注意这些生成器已经模块化，只要指定名字即可。 A comma separated string defining the sequence of tree updaters to run, providing a modular way to construct and to modify the trees. This is an advanced parameter that is usually set automatically, depending on some other parameters. However, it could be also set explicitly by a user. The following updater plugins exist: grow_colmaker: non-distributed column-based construction of trees. 单机版本下的基于列数据生长树，这里distributed tree 是xgboost有两种策略：单机版non-distributed和distributed分布式版本，比如单机版用的是精确贪婪的方式寻找分割数据点，分布式版本在采用的是近似直方图算法） distcol: distributed tree construction with column-based data splitting mode. 用基于列数据的分割模式来构建一个树（即：生长一棵树），且树是按照分布式版本的算法构建的。 grow_histmaker: distributed tree construction with row-based data splitting based on global proposal of histogram counting. 基于全局数据的直方图统计信息，并按照行分割的方式地进行树的生长。 grow_local_histmaker: based on local histogram counting. 基于局部数据（当前节点，非整棵树）的直方图统计 grow_skmaker: uses the approximate sketching algorithm. 使用近似草案算法。 sync: synchronizes trees in all distributed nodes. 在分布式地所有节点中同步树（的信息） refresh: refreshes tree’s statistics and/or leaf values based on the current data. Note that no random subsampling of data rows is performed. 刷新树的统计信息或者基于当前数据的叶子节点的值，注意：没有进行数据行的随机子抽样。 prune: prunes the splits where loss &lt; min_split_loss (or $\gamma$). 在当前节点小于被定义的最小分割损失时，那么进行剪枝。 In a distributed setting, the implicit updater sequence value would be adjusted to grow_histmaker,prune.在分布式环境下，这个参数值被显示地调整为grow_histmaker,prune refresh_leaf [default=1] This is a parameter of the refresh updater plugin. When this flag is 1, tree leafs as well as tree nodes’ stats are updated. When it is 0, only node stats are updated. 用来标记是否刷新叶子节点信息的标识。当这个标志位为0时，只有节点的统计信息被更新。 process_type [default=default] A type of boosting process to run. Choices:default,update default: The normal boosting process which creates new trees. update: Starts from an existing model and only updates its trees. In each boosting iteration, a tree from the initial model is taken, a specified sequence of updater plugins is run for that tree, and a modified tree is added to the new model. The new model would have either the same or smaller number of trees, depending on the number of boosting iteratons performed. Currently, the following built-in updater plugins could be meaningfully used with this process type: refresh, prune. With process_type=update, one cannot use updater plugins that create new trees. grow_policy [default=depthwise] 树的生长策略，基于深度或者基于最高损失变化 Controls a way new nodes are added to the tree. Currently supported only if tree_method is set to hist. Choices:depthwise, lossguide depthwise: split at nodes closest to the root. 按照离根节点最近的节点进行分裂 lossguide: split at nodes with highest loss change. max_leaves [default=0] 叶子节点的最大数目，只有当参数grow_policy=lossguide`才相关（起作用） Maximum number of nodes to be added. Only relevant when grow_policy=lossguide is set. max_bin, [default=256] 桶的最大数目 Only used if tree_method is set to hist.只有参数 tree_method=hist 时，这个参数才被使用。 Maximum number of discrete bins to bucket continuous features. 用来控制将连续特征离散化为多个直方图的直方图数目。 Increasing this number improves the optimality of splits at the cost of higher computation time. 增加此值提高了拆分的最优性, 但是是以更多的计算时间为代价的。 predictor , [default=cpu_predictor] 设定预测器算法的参数 The type of predictor algorithm to use. Provides the same results but allows the use of GPU or CPU. cpu_predictor: Multicore CPU prediction algorithm. 多核cpu预测器算法 gpu_predictor: Prediction using GPU. Default when tree_method is gpu_exact or gpu_hist. GPU预测器算法，当参数 tree_method = gpu_exact or gpu_hist 时，预测器算法默认采用 gpu_predictor 。 Additional parameters for Dart Booster (booster=dart)此部分可参考：原始论文 和 DART介绍 Note 在测试集上预测的时候，必须通过参数 ntree_limits 要关闭掉dropout功能 Using predict() with DART booster If the booster object is DART type, predict() will perform dropouts, i.e. only some of the trees will be evaluated. This will produce incorrect results if data is not the training data. To obtain correct results on test sets, set ntree_limit to a nonzero value, e.g. 12&gt;preds = bst.predict(dtest, ntree_limit=num_round)&gt; sample_type [default=uniform] 设定抽样算法的类型 Type of sampling algorithm. uniform: dropped trees are selected uniformly. 所有的树被统一处理，指的是权重一样，同样的几率被选为辍学树（被选为辍学的树，即不参与训练的学习过程） weighted: dropped trees are selected in proportion to weight. 选择辍学树的时候是正比于权重。 normalize_type [default=tree] 归一化（又名：标准化）算法的的类型，这个地方是与深度学习中的dropout不太一样。 Type of normalization algorithm. tree: new trees have the same weight of each of dropped trees. 新树拥有跟每一颗辍学树一样的权重 Weight of new trees are 1 / (k + learning_rate). Dropped trees are scaled by a factor of k / (k + learning_rate). forest: new trees have the same weight of sum of dropped trees (forest).新树的权重等于所有辍学树的权重总和 Weight of new trees are 1 / (1 + learning_rate). Dropped trees are scaled by a factor of 1 / (1 + learning_rate). rate_drop [default=0.0], range: [0.0, 1.0] 辍学率，与深度学习中的一样意思 Dropout rate (a fraction of previous trees to drop during the dropout). one_drop [default=0] 设置是否在选择辍学的过程中，至少一棵树被选为辍学树。 When this flag is enabled, at least one tree is always dropped during the dropout (allows Binomial-plus-one or epsilon-dropout from the original DART paper). skip_drop [default=0.0], range: [0.0, 1.0] 在提升迭代的过程中，跳过辍学过程的概率，即不执行dropout功能的概率 Probability of skipping the dropout procedure during a boosting iteration. If a dropout is skipped, new trees are added in the same manner as gbtree. Note that non-zero skip_drop has higher priority than rate_drop or one_drop. 注意到非0值得skip_drop参数比rate_drop和one_drop参数拥有更高的优先级。 学习任务的参数 Learning Task ParametersSpecify the learning task and the corresponding learning objective. The objective options are below: objective[default=reg:linear] 这个参数定义需要被最小化的损失函数 reg:linear: linear regression reg:logistic: logistic regression binary:logistic: logistic regression for binary classification, output probability binary:logitraw: logistic regression for binary classification, output score before logistic transformation binary:hinge: hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities. 2分类的链式损失 gpu:reg:linear, gpu:reg:logistic, gpu:binary:logistic, gpu:binary:logitraw: versions of the corresponding objective functions evaluated on the GPU; note that like the GPU histogram algorithm, they can only be used when the entire training session uses the same dataset count:poisson –poisson regression for count data, output mean of poisson distribution max_delta_step is set to 0.7 by default in poisson regression (used to safeguard optimization) survival:cox: Cox regression for right censored survival time data (negative values are considered right censored). Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function h(t) = h0(t) * HR). 比例风险回归模型(proportional hazards model，简称Cox模型)” 这块不太懂 multi:softmax: set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes) 多分类输出one-hot向量 multi:softprob: same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata * nclass matrix. The result contains predicted probability of each data point belonging to each class. 多分类输出各个类的概率向量 rank:pairwise: Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized rank:ndcg: Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized rank:map: Use LambdaMART to perform list-wise ranking where Mean Average Precision (MAP) is maximized reg:gamma: gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed. reg:tweedie: Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed. base_score [default=0.5] The initial prediction score of all instances, global bias For sufficient number of iterations, changing this value will not have too much effect. eval_metric [default according to objective] 对于有效数据的度量方法 Evaluation metrics for validation data, a default metric will be assigned according to objective (rmse for regression, and error for classification, mean average precision for ranking) User can add multiple evaluation metrics. Python users: remember to pass the metrics in as list of parameters pairs instead of map, so that latter eval_metric won’t override previous one The choices are listed below: rmse: root mean square error 均方根误差 mae: mean absolute error 平均绝对误差 logloss: negative log-likelihood 负对数似然函数值 error: Binary classification error rate. It is calculated as #(wrong cases)/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances. 二分类错误率(阈值为0.5) error@t: a different than 0.5 binary classification threshold value could be specified by providing a numerical value through ‘t’指定2分类误差率的阈值t merror: Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases). 多分类错误率 mlogloss: Multiclass logloss. 多分类的负对数似然函数值 auc: Area under the curve 曲线下面积 aucpr: Area under the PR curve 准确率和召回率曲线下的面积 ndcg: Normalized Discounted Cumulative Gain map: Mean Average Precision 主集合的平均准确率(MAP)是每个主题的平均准确率的平均值 ndcg@n, map@n: ‘n’ can be assigned as an integer to cut off the top positions in the lists for evaluation. ndcg-, map-, ndcg@n-, map@n-: In XGBoost, NDCG and MAP will evaluate the score of a list without any positive samples as 1. By adding “-” in the evaluation metric XGBoost will evaluate these score as 0 to be consistent under some conditions. poisson-nloglik: negative log-likelihood for Poisson regression gamma-nloglik: negative log-likelihood for gamma regression cox-nloglik: negative partial log-likelihood for Cox proportional hazards regression gamma-deviance: residual deviance for gamma regression tweedie-nloglik: negative log-likelihood for Tweedie regression (at a specified value of the tweedie_variance_power parameter) seed [default=0] 随机数的种子 Random number seed. 设置它可以复现随机数据的结果，也可以用于调整参数 命令行参数 Command Line ParametersThe following parameters are only used in the console version of XGBoost num_round The number of rounds for boosting data The path of training data test:data The path of test data to do prediction save_period [default=0] The period to save the model. Setting save_period=10 means that for every 10 rounds XGBoost will save the model. Setting it to 0 means not saving any model during the training. task [default=train] options:train,pred,eval,dump train: training using data pred: making prediction for test:data eval: for evaluating statistics specified by eval[name]=filename dump: for dump the learned model into text format model_in [default=NULL] Path to input model, needed for test, eval, dump tasks. If it is specified in training, XGBoost will continue training from the input model. model_out [default=NULL] Path to output model after training finishes. If not specified, XGBoost will output files with such names as 0003.model where 0003 is number of boosting rounds. model_dir [default=models/] The output directory of the saved models during training fmap Feature map, used for dumping model dump_format [default=text] options:text, json Format of model dump file name_dump [default=dump.txt] Name of model dump file name_pred [default=pred.txt] Name of prediction file, used in pred mode pred_margin [default=0] Predict margin instead of transformed probabilityXGBoost GPU SupportXGBoost Python Package 调参调参主要参考 Complete Guide to Parameter Tuning in XGBoost (with codes in Python) ，有空再详细说明。 https://www.cnblogs.com/infaraway/p/7890558.html 引用 陈天奇的论文 XGBoost: A Scalable Tree Boosting System 陈天奇的演讲视频 XGBoost A Scalable Tree Boosting System June 02, 2016 演讲幻灯片 和 官网幻灯片 XGBoost 官网 XGBoost的贡献者之一的 演讲 机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？ - 知乎]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《统计学习方法总结》]]></title>
    <url>%2F2018%2F10%2F01%2Fsummary_of_LiHang_Statistical-learning-methods%2F</url>
    <content type="text"><![CDATA[这是个人学习完李航 《统计学习方法》 的总结笔记，之前是在纸质版书籍上做的笔记，接下来会陆续更新每一章总结，由于时间有限，将以PDF注解的方式梳理10个主要统计学习方法。那么注意勘误(errata)，首先将引入作者自己的总结，而后，我再对每个具体算法和关联性进行总结，逐渐形成系统性的理解，算法之间的分类，联系，优缺点对比，以及适用场景才是我们学习的重点。 总结 1 适用问题分类问题是从实例的特征向量到类标记的预测问题；标注问题是从观测序列到标记序列(或状态序列)的预测问题。可以认为分类问题是标注问题的特殊情况。 分类问题中可能的预测结果是二类或多类；而标注问题中可能的预测结果是所有的标记序列，其数目是指数级的。 感知机、k近邻法、朴素贝叶斯法、决策树是简单的分类方法，具有模型直观、方法简单、实现容易等特点； 逻辑斯谛回归与最大熵模型、支持向量机、提升方法是更复杂但更有效的分类方法，往往分类准确率更高； 隐马尔可夫模型、条件随机场是主要的标注方法。通常条件随机场的标注准确率更高。 2 模型分类问题与标注问题的预测模型都可以认为是表示从输入空间到输出空间的映射.它们可以写成条件概率分布 $P(Y|X)$ 或决策函数 $Y=f(X)$ 的形式。前者表示给定输入条件下输出的概率模型，后者表示输入到输出的非概率模型。有的模型只是其中一种，有的模型可以看成2者兼有。 朴素贝叶斯法、隐马尔可夫模型是概率模型；感知机、k近邻法、支持向量机、提升方法是非概率模型；而决策树、逻辑斯谛回归与最大熵模型、条件随机场既可以看作是概率模型，又可以看作是非概率模型。 直接学习条件概率分布 $P(Y | X)$ 或决策函数 $Y=f(X)$ 的方法为判别方法，对应的模型是判别模型：感知机、k近邻法、决策树、逻辑斯谛回归与最大熵模型、支持向量机、提升方法、条件随机场是判别方法。 首先学习联合概率分布 $P(X,Y)$，从而求得条件概率分布 $P(Y|X)$ 的方法是生成方法，对应的模型是生成模型：朴素贝叶斯法、隐马尔可夫模型是生成方法。 决策树是定义在一般的特征空间上的，可以含有连续变量或离散变量。感知机、支持向量机、k近邻法的特征空间是欧氏空间(更一般地，是希尔伯特空间)。提升方法的模型是弱分类器的线性组合，弱分类器的特征空间就是提升方法模型的特征空间。 感知机模型是线性模型；而逻辑斯谛回归与最大熵模型、条件随机场是对数线性模型；k近邻法、决策树、支持向量机(包含核函数)、提升方法使用的是非线性模型。 3 学习策略在二类分类的监督学习中，支持向量机、逻辑斯谛回归与最大熵模型、提升方法各自使用合页损失函数、逻辑斯谛损失函数、指数损失函数，分别写为 这3种损失函数都是0-1损失函数的上界，具有相似的形状。 从上图可以认为支持向量机、逻辑斯谛回归与最大熵模型、提升方法使用不同的代理损失函数(surrogateloas Punotion)表示分类的损失，定义经验风险或结构风险函数，实现二类分类学习任务。学习的策略是优化以下结构风险函数 第1项为经验风险(经验损失)，第2项为正则化项，L为损失函数，J(f)为模型的复杂度。 支持向量机用L2范数表示模型的复杂度。原始的逻辑斯谛回归与最大熵模型没有正则化项，可以给它们加上L2范数正则化项。提升方法没有显式的正则化项，通常通过早停止(early stopping)的方法达到正则化的效果。 概率模型的学习可以形式化为极大似然估计或贝叶斯估计的极大后验概率估计。学习的策略是极小化对数似然损失或极小化正则化的对数似然损失。极大后验概率估计时，正则化项是先验概率的负对数。 决策树学习的策略是正则化的极大似然估计，损失函数是对数似然损失，正则化项是决策树的复杂度。 逻辑斯谛回归与最大熵模型、条件随机场的学习策略既可以看成是极大似然估计(或正则化的极大似然估计)，又可以看成是极小化逻辑斯谛损失(或正则化的逻辑斯谛损失)。 朴素贝叶斯模型、隐马尔可夫模型的非监督学习也是极大似然估计或极大后验概率估计，但这时模型含有隐变量。 4 学习算法 朴素贝叶斯法与隐马尔可夫模型的监督学习，最优解即极大似然估计值，可以由概率计算公式直接计算。 感知机、逻辑斯谛回归与最大熵模型、条件随机场的学习利用梯度下降法、拟牛顿法等一般的无约束最优化问题的解法。 支持向量机学习，可以解凸二次规划的对偶问题。有序列最小最优化算法等方法。 决策树学习是基于启发式算法的典型例子。可以认为特征选择、生成、剪枝是启发式地进行正则化的极大似然估计。 提升方法利用学习的模型是加法模型、损失函数是指数损失函数的特点，启发式地从前向后逐步学习模型，以达到逼近优化目标函数的目的。 EM算法是一种迭代的求解含隐变量概率模型参数的方法，它的收敛性可以保证，但是不能保证收敛到全局最优。 支持向量机学习、逻辑斯谛回归与最大熵模型学习、条件随机场学习是凸优化问题，全局最优解保证存在。而其他学习问题则不是凸优化问题。 分章节总结数学基础不再重复，请参考我的数学笔记线性代数总结 和 MIT的概率论教材 《introduction to probability》 和 凸优化 第8章 提升方法之AdaBoost\BoostingTree\GBDT MaxEnt HMM CRF 第9章EM/GMM/F-MM/GEM SVM 第4章 朴素贝叶斯]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《统计学习方法》第9章 EM/GMM/F-MM/GEM]]></title>
    <url>%2F2018%2F10%2F01%2F9.EM_and_GEM_LiHang-Statistical-Learning-Methods%2F</url>
    <content type="text"><![CDATA[前言EM（期望最大）算法有很多的应用，最广泛的就是混合高斯模型、聚类、HMM等等，本质上就是一种优化算法，不断迭代，获得优值，与梯度下降、牛顿法、共轭梯度法都起到同一类的作用。 本文是对李航《统计学习方法》的第9章复习总结，主要内容如下 EM（期望最大）算法证明有跳跃性的地方全部事无巨细地写出来， 在 三硬币例子解析 这一节将会把这个例子跟公式一一对应起来 GMM（高斯混合模型）迭代公式证明 F函数的极大-极大算法（Maximization-Maximization-algorithm）和GEM 详细证明 当然大家也可以参考 Stanford 吴恩达主讲的 CS299 Machine Learning 的 EM课件 ，相比之下《统计学习方法》这本书在 Jensen‘s inequality（琴声不等式）讲的不够详细，其他都差不多，只是Q函数定义不同，这两种定义都很流行所以后文也会介绍区别。 正文9.1 EM算法的引入概率模型有时既含有观测变量（observable variable） ， 又含有隐变量（hidden variable）或潜在变量（latent variable） 。 如果概率模型的变量都是观测变量， 那么给定数据， 可以直接用极大似然估计法或贝叶斯估计法估计模型参数。 但是， 当模型含有隐变量时， 就不能简单地使用这些估计方法。 EM算法就是含有隐变量的概率模型参数的极大似然估计法， 或极大后验概率估计法。 我们仅讨论极大似然估计， 极大后验概率估计与其类似。 9.1.1 EM算法 这里， 随机变量 $y$ 是观测变量， 表示一次试验观测的结果是1或0； 随机变量 $z$ 是隐变量， 表示未观测到的掷硬币 $A$ 的结果； $\theta＝( \pi ,p， q)$ 是模型参数。 这一模型是以上数据的生成模型。 注意， 随机变量 $y$ 的数据可以观测， 随机变量 $z$ 的数据不可观测。$$\begin{align}P(y|\theta) &amp;= \sum\limits_{z}P(y,z|\theta)=\sum\limits_{z}\frac{P(z,\theta)}{P(\theta)}\cdot\frac{P(y,z,\theta)}{P(z, \theta)}=\sum\limits_{z}P(z|\theta)P(y|z,\theta) \\&amp;= P(z=1|\theta)P(y|z=1, \theta) + P(z=0|\theta)P(y|z=0, \theta)\\&amp;= \pi p^y(1-p)^{(1-y)} + (1 - \pi) q^y(1-q)^{(1-y)} \tag{9.1}\\&amp;= \begin{cases} \pi p + (1 - \pi) q, &amp;y=1\\ \pi (1-p) + (1-\pi)(1-q), &amp;y=0\end{cases}\end{align}$$将观测数据表示为 $Y＝(Y_1， Y_2,…,Y_n)^T$， 未观测数据表示为 $Z＝(Z_1,Z_2,…,Z_n)^T$， 则观测数据的似然函数为$$P(Y|\theta) = \sum\limits_{Z}P(Y,Z|\theta)=\sum\limits_{Z}P(Z|\theta)P(Y|Z,\theta) \tag{9.2}$$即：$$P(Y|\theta)= \prod_{j=1}^{n}\left\{\pi p^{y_j}(1-p)^{(1-y_j)} + (1 - \pi) q^{y_j}(1-q)^{(1-y_j)}\right\} \tag{9.3}$$考虑求模型参数 $\theta =(\pi, p, q) $ 的极大似然估计，即：$$\begin{align}\hat{\theta}&amp;=\mathop{\arg\max}_{\theta} \mathrm{log}P(Y|\theta) \\&amp;= \mathop{\arg\max}_{\theta}\log\prod_{j=1}^{n}P(Y|\theta) \Leftarrow\text{n次抛硬币试验都是独立} \\&amp;= \mathop{\arg\max}_{\theta}\sum\limits_{j=1}^{n}\log P(Y|\theta) \\&amp;= \mathop{\arg\max}_{\theta}\sum\limits_{j=1}^{n}\log\left\{\sum\limits_{Z}{P(Z|\theta)P(Y|Z,\theta)}\right\} \tag{9-3}\end{align}$$问题：这里为什么要取对数？ 取对数之后累积变为累和，求导更加方便（后面三硬币例子解析将会看到） 概率累积会出现数值非常小的情况，比如1e-30，由于计算机的精度是有限的，无法识别这一类数据，取对数之后，更易于计算机的识别(1e-30以10为底取对数后便得到-30)。 这个问题没有解析解，因为隐变量数据无法获得，只有通过迭代的方法求解。 EM算法就是可以用于求解这个问题的一种迭代算法。 一般地， 用 $Y$ 表示观测随机变量的数据， $Z$ 表示隐随机变量的数据。 $Y$ 和 $Z$ 连在一起称为完全数据（complete-data） ， 观测数据 $Y$ 又称为不完全数据（incomplete-data） 。 假设给定观测数据 $Y$， 其概率分布是 $P(Y|\theta)$， 其中是需要估计的模型参数， 那么不完全数据 $Y$ 的似然函数是 $P(Y|\theta)$， 对数似然函数 $L(\theta)＝\mathrm{log}P(Y|\theta)$ ； 假设 $Y$ 和 $Z$ 的联合概率分布是 $P(Y, Z|\theta)$， 那么完全数据的对数似然函数是 $\mathrm{log}P(Y, Z|\theta)$。 9.1.2 EM算法的导出 注：书上给出琴声不等式（$\ln\sum_j\lambda_jy_j\geq \sum_j\lambda_j\log y_j,\quad \lambda_j\ge 0,\sum_j\lambda_j=1$），自行维基百科一下了解详情。最后一步源自于 $Z$ 所有可能取值的概率和为1$$\mathrm{log}P(Y|\theta^{(i)})=\mathrm{log}P(Y|\theta^{(i)}) \cdot \sum\limits_{Z}P(Z|Y, \theta^{(i)})$$$$\begin{align}\theta^{(i+1)} &amp;= \mathop{\arg\max}_{\theta} \left\{ L(\theta^{(i)}) + \sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}\right\} \\&amp;= \mathop{\arg\max}_{\theta}\left\{ \mathrm{log}P(Y|\theta^{(i)})\sum\limits_{Z}P(Z|Y, \theta^{(i)}) + \sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})} \right\} \\\end{align}$$加号右边，利用对数函数的性质得到：$$\begin{align}&amp;\sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})} \\&amp;=\sum\limits_{Z}P(Z|Y,\theta^{(i)})\left\{\mathrm{log}[P(Y|Z,\theta)P(Z|\theta)] - \mathrm{log}[P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})]\right\} \\&amp;=\sum\limits_{Z}P(Z|Y,\theta^{(i)})\left\{\mathrm{log}[P(Y|Z,\theta)P(Z|\theta)] - \mathrm{log}P(Z|Y,\theta^{(i)})-\mathrm{log}P(Y|\theta^{(i)})\right\} \\&amp;= \sum\limits_{Z}P(Z|Y,\theta^{(i)})\mathrm{log}[P(Y|Z,\theta)P(Z|\theta)] - \sum\limits_{Z}P(Z|Y,\theta^{(i)})\mathrm{log}P(Z|Y,\theta^{(i)})-\sum\limits_{Z}P(Z|Y,\theta^{(i)})\mathrm{log}P(Y|\theta^{(i)}) \\\end{align}$$代入上式可得：$$\begin{align}\theta^{(i+1)} &amp;= \mathop{\arg\max}_{\theta} \left\{ \sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}[P(Y|Z,\theta)P(Z|\theta)]-\sum\limits_{Z}P(Z|Y,\theta^{(i)})\mathrm{log}P(Z|Y,\theta^{(i)}) \right\} \\\end{align}$$ 由于在迭代求第 $i+1$ 步时，$\theta^{(i)}$ 是已知的，那么由训练数据中可以求得 $P(Z|Y,\theta^{(i)})$ ，所以在 $\theta^{(i)}$ 值确定的情况下，$P(Z|Y,\theta^{(i)})$ 的值也是确定的而不是变量，那么对上式极大化等价求解对下面式子的极大化$$\begin{align}\theta^{(i+1)} &amp;= \mathop{\arg\max}_{\theta} \left\{ \sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}[P(Y|Z,\theta)P(Z|\theta)]\right\} \\&amp;= \mathop{\arg\max}_{\theta} \left\{ \sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}P(Y,Z|\theta)\right\} \\&amp;= \mathop{\arg\max}_{\theta}Q(\theta, \theta^{(i)}) \tag{9.17}\end{align}$$ Q函数 EM算法 EM算法解释 9.1.3 EM算法在非监督学习中的应用 9.2 EM算法的收敛性这一部分原书讲的比较详细，不画蛇添足，贴上来。 三硬币例子解析前文讲到抛硬币的例子，现在重新详细推导一下三硬币这个例子。 $j$ 是训练集中的数据编号，实际上书上这里求得是$$\begin{align}P(Z|y_j,\theta^{(i)}) = \cases{P(Z=1|y_j,\theta^{(i)})=\mu_{j}^{(i+1)} \\ P(Z=0|y_j,\theta^{(i)})=1- \mu_{j}^{(i+1)}}\end{align}$$前文已知Q函数：$$Q(\theta, \theta^{(i)})=\sum\limits_{Z}P(Z|Y, \theta^{(i)})\mathrm{log}P(Y,Z|\theta)$$ 第一步求期望即求Q函数，由本文开头的 9.1.1 EM算法 这一节的公式 (9-3) 和 Q函数得到，在多个样本情况下 Q 函数为：$$\begin{align}Q(\theta, \theta^{(i)}) &amp;= \sum\limits_{j=1}^{n}\sum\limits_{Z}P(Z|y_j, \theta^{(i)})\log P(y_j,Z|\theta)\\&amp;= \sum\limits_{j=1}^{n}\left\{ P(Z=1|y_j, \theta^{(i)})\mathrm{log}P(y_j,Z=1|\theta) + P(Z=0|y_j, \theta^{(i)})\mathrm{log}P(y_j,Z=0|\theta) \right\}\\&amp;= \sum\limits_{j=1}^{n}\left\{\mu_{j}^{(i+1)}log P(y_j,Z=1|\theta) + (1-\mu_{j}^{(i+1)})\mathrm{log}P(y_j,Z=0|\theta) \right\}\\&amp;= \sum\limits_{j=1}^{n}\left\{\mu_{j}^{(i+1)}\log [\pi p^{y_j}(1-p)^{1-y_j}]+(1-\mu_{j}^{(i+1)})\log [(1-\pi )q^{y_j}(1-q)^{1-y_j}] \right\}\\\end{align}$$ 第二步极大化Q函数$\begin{align}\theta^{(i+1)} = \mathop{\arg\max}_{\theta}Q(\theta, \theta^{(i)}) = \mathop{\arg\max}_{\theta} \left\{\sum\limits_{j=1}^{n} \sum\limits_{Z}P(Z|y_j, \theta^{(i)})\log P(y_j,Z|\theta)\right\}\end{align}$ 用微积分求解最大值，先求导数为0点（为了求导方便令对数的底数为e，即认为此处对数函数为自然对数）：$$\begin{aligned} \frac{\partial Q(\theta,\theta^{(i)})}{\partial \pi}&amp;=\sum_{j=1}^N\{\frac{\mu_{j}^{(i+1)}\ln [\pi p^{y_j}(1-p)^{1-y_j}]+(1-\mu_{j}^{(i+1)})\ln [(1-\pi )q^{y_j}(1-q)^{1-y_j}] }{\partial \pi}\}\\&amp;=\sum_{j=1}^N\{ \mu_{j}^{(i+1)}\frac{p^{y_j}(1-p)^{1-y_j}}{\pi p^{y_j}(1-p)^{1-y_j}}+(1-\mu_{j}^{(i+1)})\frac{-q^{y_j}(1-q)^{1-y_j}}{(1-\pi )q^{y_j}(1-q)^{1-y_j}} \}\\&amp;=\sum_{j=1}^N\{ \frac{\mu_{j}^{(i+1)}-\pi }{\pi (1-\pi)}\}\\&amp;=\frac{(\sum_{j=1}^N\mu_{j}^{(i+1)})-n\pi }{\pi (1-\pi)} \end{aligned}$$ $$\begin{aligned}\because \quad\frac{\partial Q(\theta,\theta^{(i)})}{\partial \pi}=0 &amp;\implies \pi =\frac 1n\sum_{j=1}^N\mu_{j}^{(i+1)}\\\therefore \quad \pi^{(i+1)}&amp;=\frac 1n\sum_{j=1}^N\mu_{j}^{(i+1)} \end{aligned}$$ $$\begin{aligned} \frac{\partial Q(\theta,\theta^{(i)})}{\partial p}&amp;=\sum_{j=1}^N\{\frac{\mu_{j}^{(i+1)}\ln [\pi p^{y_j}(1-p)^{1-y_j}]+(1-\mu_{j}^{(i+1)})\ln [(1-\pi )q^{y_j}(1-q)^{1-y_j}] }{\partial p}\}\\&amp;=\sum_{j=1}^N\{\mu_{j}^{(i+1)}\frac{\pi (y_jp^{y_j-1}(1-p)^{1-y_j}+p^{y_j}(-1)(1-y_j)(1-p)^{1-y_j-1})}{\pi p^{y_j}(1-p)^{1-y_j}}+0 \}\\&amp;=\sum_{j=1}^N\{ \frac{\mu_{j}^{(i+1)}(y_j-p) }{p(1-p)}\}\\&amp;=\frac{(\sum_{j=1}^N\mu_{j}^{(i+1)}y_j)-(p\sum_{j=1}^N\mu_{j}^{(i+1)}) }{p(1-p)} \end{aligned}$$ $$\begin{aligned}\because \quad \frac{\partial Q(\theta,\theta^{(i)})}{\partial p}=0 &amp;\implies p =\frac{\sum_{j=1}^N \mu^{(i+1)}_j y_j}{\sum_{j=1}^N\mu^{(i+1)}_j} \\\therefore \quad p^{(i+1)}&amp;=\frac{\sum_{j=1}^N\mu^{(i+1)}_j y_j}{\sum_{j=1}^N\mu^{(i+1)}_j} \\q^{(i+1)}&amp;=\frac{\sum_{j=1}^N(1-\mu^{(i+1)}_j)y_j}{\sum_{j=1}^N(1-\mu^{(i+1)}_j)}\end{aligned}$$可以参照书上的结果，一模一样： CS299 EM算法与《统计学习方法》的表述不同点 《统计学习方法》这部分术语源自于鼎鼎大名的ESL 全称：The Elements of Statistical Learning，这也是Stanford统计经典巨作。 Stanford 吴恩达主讲的 CS299 Machine Learning 的 EM课件 由本文的推导，易得 ESL 中的 $ Q_{ESL} = Q_{CS299}\frac{\log P(X,Z;\theta)}{Q_{CS299}} $ 9.3 EM算法在高斯混合模型学习中的应用EM算法的一个重要应用是高斯混合模型的参数估计。 高斯混合模型应用广泛， 在许多情况下， EM算法是学习高斯混合模型（Gaussian misture model） 的有效方法。 9.3.1 高斯混合模型 9.3.2 高斯混合模型参数估计的EM算法 注意：上面的极大化的求混合模型参数迭代公式的过程参考： 大牛JerryLead 的 （EM算法）The EM Algorithm 与K-means比较相同点：都是可用于聚类的算法；都需要指定K值。 不同点：GMM可以给出一个样本属于某类的概率是多少。 9.4 EM算法的推广EM算法还可以解释为F函数（F function） 的极大-极大算法（maximization maximization algorithm） ， 基于这个解释有若干变形与推广， 如广义期望极大（generalized expectation maximization，GEM） 算法。 注：原文引理(9.1)(9.2)的证明有坑需要注意，先看原文，后面列出详细过程 9.4.1 F函数的极大-极大算法 熵这块，不清楚的可以回顾一下我的另一篇总结：《机器学习中的信息论基础》 。 引理9.1需要更详细说明：$$L=E_{\tilde{p}}\log P(Y,Z|\theta) - E_{\tilde{p}}\log \tilde{P}(Z) + \lambda\left\{1-\sum\limits_{Z}\tilde{P}(Z)\right\}$$证明过程思路：拉格朗日求有约束的极大值。需要注意，由累加号和均值可以看出这里的 $Z$ 是指 $Z_i, i$ 这里是 $Z$ 的离散值的标号 ，因此需要重写公式 (9.35) 比较清楚：$$L=\sum\limits_{Z_i}{\tilde{P}(Z_i)}\log P(Y,Z_i|\theta) - \sum\limits_{Z_i}{\tilde{P}(Z_i)}\log \tilde{P}(Z_i)+\lambda\left\{1-\sum\limits_{Z_i}\tilde{P}(Z_i)\right\}$$所以这里其实是 $L$ 关于 $P(Z_i)$的求导（这里作者求导的时候把对数函数默认当做自然对数）：$$\begin{align}&amp;\frac{\partial{L}}{\partial{\tilde{P}(Z_i)}}=\log P(Y,Z_i|\theta)-\log \tilde{P}(Z_i)-1-\lambda \\&amp;\because\quad\frac{\partial{L}}{\partial{\tilde{P}(Z_i)}}=0\\ &amp;\therefore\quad \lambda=\log P(Y,Z_i|\theta)-\log \tilde{P}(Z_i)-1\end{align}$$上式两端同取对数：$$\begin{align}\lambda+1&amp;=\log P(Y,Z_i|\theta)-\log \tilde{P}(Z_i) \\ &amp;\Rightarrow e^{\lambda+1}=\frac{P(Y,Z_i|\theta)}{\tilde{P}(Z_i)} \\&amp;\Rightarrow\tilde{P}(Z_i)=\frac{P(Y,Z_i|\theta)}{e^{\lambda+1}} \tag{9-1}\end{align}$$由离散变量的概率和为1，得到：$$\begin{align}\sum\limits_{Z_i}e^{\lambda+1} &amp;= \frac{\sum\limits_{Z_i}P(Y,Z_i|\theta)}{\sum\limits_{Z_i}\tilde{P}(Z_i)} \Rightarrow\\e^{\lambda+1} &amp;= P(Y|\theta) \tag{9-2}\end{align}$$将 (9-2) 代入 (9-1)​ 式，得到$$\begin{align}\tilde{P}(Z_i)&amp;=\frac{P(Y,Z_i|\theta)}{P(Y|\theta)} \\&amp;=\frac{P(Y,Z_i,\theta)}{p(\theta)}\frac{P(\theta)}{P(Y,\theta)} \\&amp;= P(Z_i|Y,\theta)\end{align}$$这里前提条件是 $\theta$ 是固定情况下的推导过程，所以原文给上式标记出了 $\theta$ ，又因为每个 $Z_i$ 都符合这个式子，那么可重写上式：$$\tilde{P}_{\theta}(Z) = P(Z|Y,\theta)$$这样引理9.1证明完毕。 引理9.2如下 由公式 $(9.33)$ 和 $(9.34)$ :$$F(\tilde{P}, \theta)=E_{\tilde{p}}[\log P(Y,Z|\theta)] + H(\tilde{P}) \\\tilde{P}_{\theta}(Z) = P(Z|Y,\theta)$$得到：$$\begin{align}F(\tilde{P}, \theta)&amp;=\sum\limits_{Z}{\tilde{P}_{\theta}(Z)}\log P(Y,Z|\theta) - \sum\limits_{Z}{\tilde{P}_{\theta}(Z)}\log \tilde{P}_{\theta}(Z) \\&amp;=\sum\limits_{Z} P(Z|Y,\theta)\log P(Y,Z|\theta) - \sum\limits_{Z}P(Z|Y,\theta)\log P(Z|Y,\theta) \\&amp;=\sum\limits_{Z}P(Z|Y,\theta)[\log P(Y,Z|\theta) - \log P(Z|Y,\theta)] \\&amp;=\sum\limits_{Z}P(Z|Y,\theta)\log\frac{P(Y,Z|\theta)}{P(Z|Y,\theta)}\\&amp;=\sum\limits_{Z}P(Z|Y,\theta)\log\left\{\frac{P(Y,Z,\theta)}{p(\theta)}\frac{P(Y,\theta)}{P(Y,Z,\theta)}\right\}\\&amp;= \sum\limits_{Z}P(Z|Y,\theta)\log P(Y|\theta) \\&amp;= \log P(Y|\theta) \\ \end{align}$$引理9.2证明完毕 9.4.2 GEM算法 本章概要 引用 The Expectation Maximization Algorithm: A short tutorial - Sean Borman 李航《统计学习方法》 大牛JerryLead 的 （EM算法）The EM Algorithm 人人都懂EM算法 EM算法简述及简单示例（三硬币模型）]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《统计学习方法》第8章 提升方法之AdaBoost\BoostingTree\GBDT]]></title>
    <url>%2F2018%2F10%2F01%2F8.Booting-Methods_LiHang-Statistical-Learning-Methods%2F</url>
    <content type="text"><![CDATA[前言在深度学习火起来之前，提升方法（包括AdaBoost, GBDT, XGBoost）是 kaggle 等比赛中的利器，所以提升方法 （boosting） 是必备的知识点。李航《统计学习方法》第8章——提升方法主要内容：AdaBoost, Boosting Tree, GBDT(这一块原文不够详细，将补充一些)。写本文主要目的是复习（毕竟之前看纸质版做的笔记）， 对于证明比较跳跃和勘误的地方我都做了注解，以便初学者快速阅读理解不会卡住，另外本文拓展部分补充了：集成学习。另外李航这本书著作于2012年，陈天奇 的 XGBoost (eXtreme Gradient Boosting) 在2015年惊艳四方，本文暂时不叙述这个，将会另开一篇：XGBoost入门，包括微软的LightGBM，但是本文是 XGBoost 和 LightGBM 的基础，所以要先总结本文。初学者看到前言这么多名词不必畏惧，直接看正文，本人也是从本章正文学习，再拓展出这些的，初学直接看正文，一步一步往下顺。 正文提升（boosting） 方法是一种常用的统计学习方法， 应用广泛且 有效。 在分类问题中， 它通过改变训练样本的权重， 学习多个分类 器， 并将这些分类器进行线性组合， 提高分类的性能。 本章主要内容 提升方法的思路和代表性的提升算法AdaBoost； 通过训练误差分析探讨AdaBoost为什么能够提高学习精度； 并且从 前向分步加法模型的角度解释AdaBoost； 然后叙述提升方法更具体的 实例——提升树（boosting tree）和GBDT 。 8.1 提升方法AdaBoost算法8.1.1 提升方法的基本思路提升方法的思想对于一个复杂任务来说， 将多个专 家的判断进行适当的综合所得出的判断， 要比其中任何一个专家单独 的判断好。 实际上， 就是“三个臭皮匠顶个诸葛亮”的道理 历史背景历史上， Kearns和Valiant首先提出了“强可学习（strongly learnable） ”和“弱可学习（weakly learnable） ”的概念。 指出： 在概率近似正确（probably approximately correct， PAC） 学习的框架中， 一 个概念（一个类） ， 如果存在一个多项式的学习算法能够学习它， 并 且正确率很高， 那么就称这个概念是强可学习的； 一个概念， 如果存 在一个多项式的学习算法能够学习它， 学习的正确率仅比随机猜测略 好， 那么就称这个概念是弱可学习的。 非常有趣的是Schapire后来证 明强可学习与弱可学习是等价的， 也就是说， 在PAC学习的框架下， 一个概念是强可学习的充分必要条件是这个概念是弱可学习的。 这样一来， 问题便成为， 在学习中， 如果已经发现了“弱学习算 法”， 那么能否将它提升（boost） 为“强学习算法”。 大家知道， 发现 弱学习算法通常要比发现强学习算法容易得多。 那么如何具体实施提 升， 便成为开发提升方法时所要解决的问题。 关于提升方法的研究很 多， 有很多算法被提出。 最具代表性的是AdaBoost算法（AdaBoost algorithm） 。 对于分类问题而言， 给定一个训练样本集， 求比较粗糙的分类规 则（弱分类器） 要比求精确的分类规则（强分类器） 容易得多。提升方法就是从弱学习算法出发， 反复学习， 得到一系列弱分类器（又称 为基本分类器） ， 然后组合这些弱分类器， 构成一个强分类器。 大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分 布） ， 针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。 提升方法的核心问题和思想对提升方法来说， 有两个问题需要回答： 一是在每一轮如 何改变训练数据的权值或概率分布； 二是如何将弱分类器组合成一个强分类器。 关于第1个问题， AdaBoost的做法是提高那些被前一轮弱分类器错误分类样本的权值， 而降低那些被正确分类样本的权值。 这样一来， 那些没有得到正确分类的数据， 由于其权值的加大而受到后一轮的弱分类器的更大关注。 于是， 分类问题被一系列的弱分类器“分而治之”。 至于第2个问题， 即弱分类器的组合， AdaBoost采取加权多数表决的方法。 具体地， 加大分类误差率小的弱分类器的权值， 使其在表决中起较大的作用， 减小分类误差率大的弱分类器的权值， 使其在表决中起较小的作用。 AdaBoost的巧妙之处就在于它将这些想法自然且有效地实现在一 种算法里。 8.1.2 AdaBoost算法 8.1.3 AdaBoost的例子 8.2 AdaBoost算法的训练误差分析 8.3 AdaBoost算法的解释 8.3.1 前向分步算法 8.3.2 前向分步算法与AdaBoost 8.4 提升树 8.4.1 提升树模型 8.4.2 提升树算法 8.4.3 梯度提升Gradient Boosting有前面的例子非常容易理解这部分内容，不再赘述， GBDT如果看懂前文，那么理解GBDT就简单很多。有一篇博客，清晰易懂，总结很不错，推荐看一下： 梯度提升树(GBDT)原理小结 ，里面的这有一个提示：关于多分类问题：每轮都在拟合概率向量 [类别1概率，类别2概率…,类别k的概率] 的伪残差。 以下摘录自：大名鼎鼎的 Stanford 统计教材 《The Elements of Statistical Learning》。根据《李航统计学习方法》的提升方法这一章的参考附录正是此书，对比之下，异曲同工，这里不再赘述，快速总结一下。 仔细体会这句话：它和梯度下降十分相似，不同在于GBDT 就是在函数空间的“梯度下降”，以前学习ML和DL的梯度下降是在参数空间的梯度下降。 在梯度下降中，不断减去 $\frac{\partial{f(x)}}{\partial{\theta}}$，希望求得 $min_{\theta}f(x)$ ; 同理梯度提升中不断减去$\frac{\partial{L(y,f(x))}}{f(x)}$，希望求得 $min_{f(x)}L(y, f(x))$ 。这里引用 https://wepon.me 的介绍： 注意：《统计学习方法》这一章介绍的都是最原始的提升树算法，实际上有进一步改进的调整： 步进参数 $\nu$ ，专业名词为shrinkage，又称收缩，类似梯度下降算法的学习率，这里为什么称为步进参数，因为前文提到：提升方法是加法模型的前向分布算法，提升树也属于提升方法中一种（基学习器是决策树）。 例如：运用在GBDT中 $f_m(x)=f_{m-1}(x)+\nu\cdot \sum\limits_{j=1}^{J}\gamma_{jm}I(x\in R_{jm})$ 正则化手段：subsampling 子抽样（包括：样本子抽样和特征的子抽样），提高泛化能力。 例如：在随机森林中，每棵树的训练样本都是对原始训练集有放回的子抽样，好处如下： 如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，依然没有解决决策树过拟合问题。随机抽样是为了保证不同决策树之间的多样性，从而提高模型的泛化能力。使得随机森林不容易陷入过拟合，并且具有较好的抗噪能力（比如：对缺省值不敏感）。 而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是”求同”。如果是无放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是”有偏的”，从而影响最终的投票结果。为了保证最终结果的可靠性，同时又要保证模型的泛化能力，需要每一颗树既要“求同“ 又要 ”存异”。 深入理解请看GDBT的论文：Greedy Function Approximation：A Gradient Boosting Machine 拓展—集成学习实际上，提升方法（boosting） 属于集成学习（ensemble learning） 的一种，集成模型还有2类比较出名：bagging 方法（Bagging 由来：Bootstrap aggregating）和 Stacking方法，随机森林属于第二类。由于随机森林不像提升树不用等待上一轮（上一棵树）结果，各棵树都可以独立求解（包括独立进行子抽样），即可以在树这个粒度下并发进行运算求解，在大规模数据下并发性能良好，随机森林比较简单。后来 XGBoost 和 lightGBM 的良好地实现了算法模型GBDT，虽然依然不是在树粒度层面的并发，但是运行时间和效果在kaggle、KDD Cup等比赛中惊艳四方，将会在日后另一篇中总结xgboost。 进一步学习的经典资料 wiki关于集成模型的介绍 https://en.wikipedia.org/wiki/Ensemble_learning 周志华老师：《Ensemble Methods Foundations and Algorithms》 用 Stanford 的统计教材 The Elements of Statistical Learning 进一步学习与补充 第9章 Additive Models, Trees, and Related Methods 第10章 Boosting and Additive Trees 第14 章 Random Forests 第15 章 Ensemble learning 知乎探讨：为什么说bagging是减少variance，而boosting是减少bias? - 知乎 https://www.zhihu.com/question/26760839 为什么在实际的kaggle比赛中，GBDT和Random Forest效果非常好以下来自马超博士的回答 - 知乎 这是一个非常好，也非常值得思考的问题。换一个方式来问这个问题：为什么基于 tree-ensemble 的机器学习方法，在实际的 kaggle 比赛中效果非常好？ 通常，解释一个机器学习模型的表现是一件很复杂事情，而这篇文章尽可能用最直观的方式来解释这一问题。 我主要从三个方面来回答楼主这个问题。 理论模型 （站在 vc-dimension 的角度） 实际数据 系统的实现 （主要基于 xgboost） 通常决定一个机器学习模型能不能取得好的效果，以上三个方面的因素缺一不可。 站在理论模型的角度统计机器学习里经典的 vc-dimension 理论告诉我们：一个机器学习模型想要取得好的效果，这个模型需要满足以下两个条件： 模型在我们的训练数据上的表现要不错，也就是 trainning error 要足够小。 模型的 vc-dimension 要低。换句话说，就是模型的自由度不能太大，以防overfit. 当然，这是我用大白话描述出来的，真正的 vc-dimension 理论需要经过复杂的数学推导，推出 vc-bound. vc-dimension 理论其实是从另一个角度刻画了一个我们所熟知的概念，那就是 bias variance trade-off. 好，现在开始让我们想象一个机器学习任务。对于这个任务，一定会有一个 “上帝函数” 可以完美的拟合所有数据（包括训练数据，以及未知的测试数据）。很可惜，这个函数我们肯定是不知道的 （不然就不需要机器学习了）。我们只可能选择一个 “假想函数” 来 逼近 这个 “上帝函数”，我们通常把这个 “假想函数” 叫做 hypothesis. 在这些 hypothesis 里，我们可以选择 svm, 也可以选择 logistic regression. 可以选择单棵决策树，也可以选择 tree-ensemble (gbdt, random forest). 现在的问题就是，为什么 tree-ensemble 在实际中的效果很好呢？ 区别就在于 “模型的可控性”。 先说结论，tree-ensemble 这样的模型的可控性是好的，而像 LR 这样的模型的可控性是不够好的（或者说，可控性是没有 tree-ensemble 好的）。为什么会这样？别急，听我慢慢道来。 我们之前说，当我们选择一个 hypothsis 后，就需要在训练数据上进行训练，从而逼近我们的 “上帝函数”。我们都知道，对于 LR 这样的模型。如果 underfit，我们可以通过加 feature，或者通过高次的特征转换来使得我们的模型在训练数据上取得足够高的正确率。而对于 tree-enseble 来说，我们解决这一问题的方法是通过训练更多的 “弱弱” 的 tree. 所以，这两类模型都可以把 training error 做的足够低，也就是说模型的表达能力都是足够的。但是这样就完事了吗？没有，我们还需要让我们的模型的 vc-dimension 低一些。而这里，重点来了。在 tree-ensemble 模型中，通过加 tree 的方式，对于模型的 vc-dimension 的改变是比较小的。而在 LR 中，初始的维数设定，或者说特征的高次转换对于 vc-dimension 的影响都是更大的。换句话说，tree-ensemble 总是用一些 “弱弱” 的树联合起来去逼近 “上帝函数”，一次一小步，总能拟合的比较好。而对于 LR 这样的模型，我们很难去猜到这个“上帝函数”到底长什么样子（到底是2次函数还是3次函数？上帝函数如果是介于2次和3次之间怎么办呢？）。所以，一不小心我们设定的多项式维数高了，模型就 “刹不住车了”。俗话说的好，步子大了，总会扯着蛋。这也就是我们之前说的，tree-ensemble 模型的可控性更好，也即更不容易 overfit. 站在数据的角度除了理论模型之外, 实际的数据也对我们的算法最终能取得好的效果息息相关。kaggle 比赛选择的都是真实世界中的问题。所以数据多多少少都是有噪音的。而基于树的算法通常抗噪能力更强。比如在树模型中，我们很容易对缺失值进行处理。除此之外，基于树的模型对于 categorical feature 也更加友好。 除了数据噪音之外，feature 的多样性也是 tree-ensemble 模型能够取得更好效果的原因之一。通常在一个kaggle任务中，我们可能有年龄特征，收入特征，性别特征等等从不同 channel 获得的特征。而特征的多样性也正是为什么工业界很少去使用 svm 的一个重要原因之一，因为 svm 本质上是属于一个几何模型，这个模型需要去定义 instance 之间的 kernel 或者 similarity （对于linear svm 来说，这个similarity 就是内积）。这其实和我们在之前说过的问题是相似的，我们无法预先设定一个很好的similarity。这样的数学模型使得 svm 更适合去处理 “同性质”的特征，例如图像特征提取中的 lbp 。而从不同 channel 中来的 feature 则更适合 tree-based model, 这些模型对数据的 distributation 通常并不敏感。 站在系统实现的角度除了有合适的模型和数据，一个良好的机器学习系统实现往往也是算法最终能否取得好的效果的关键。一个好的机器学习系统实现应该具备以下特征： 正确高效的实现某种模型。我真的见过有些机器学习的库实现某种算法是错误的。而高效的实现意味着可以快速验证不同的模型和参数。 系统具有灵活、深度的定制功能。 系统简单易用。 系统具有可扩展性, 可以从容处理更大的数据。 到目前为止，xgboost 是我发现的唯一一个能够很好的满足上述所有要求的 machine learning package. 在此感谢青年才俊 陈天奇。 在效率方面，xgboost 高效的 c++ 实现能够通常能够比其它机器学习库更快的完成训练任务。在灵活性方面，xgboost 可以深度定制每一个子分类器，并且可以灵活的选择 loss function（logistic，linear，softmax 等等）。除此之外，xgboost还提供了一系列在机器学习比赛中十分有用的功能，例如 early-stop， cv 等等在易用性方面，xgboost 提供了各种语言的封装，使得不同语言的用户都可以使用这个优秀的系统。最后，在可扩展性方面，xgboost 提供了分布式训练（底层采用 rabit 接口），并且其分布式版本可以跑在各种平台之上，例如 mpi, yarn, spark 等等。 有了这么多优秀的特性，自然这个系统会吸引更多的人去使用它来参加 kaggle 比赛。 综上所述，理论模型，实际的数据，良好的系统实现，都是使得 tree-ensemble 在实际的 kaggle 比赛中“屡战屡胜”的原因。 用一句话与大家共勉：算法学习要学习算法之间的联系与区别，优缺点和适用场合，这样能做到融会贯通。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《统计学习方法》第4章 NaiveBayes]]></title>
    <url>%2F2018%2F10%2F01%2F4.Naive-Bayes_LiHang-Statistical-Learning-Methods%2F</url>
    <content type="text"><![CDATA[前言写本文章主要目的是复习（毕竟之前看纸质版做的笔记）， 对于证明比较跳跃和勘误的地方我都做了注解，以便初学者和以后复习地时候快速阅读理解不会卡住。 朴素贝叶斯法 4.1 朴素贝叶斯法的学习与分类4.1.1 基本方法 4.1.2 后验概率最大化的含义 4.2 朴素贝叶斯法的参数估计4.2.1 极大似然估计 4.2.2 学习与分类算法 例子 4.2.3 贝叶斯估计 本章概要 习题]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>统计学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[summary of learning Deep Learning Specialization]]></title>
    <url>%2F2018%2F06%2F28%2Fsummary_of_learning_of_Deep_Learning_Specializatio_on_Coursera%2F</url>
    <content type="text"><![CDATA[EnglishThis is my summary of learning Deep Learning Specialization on Coursera, which consists of 5 courses as following: 1st course: Neural Networks and Deep Learning 2nd course: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization 3rd course: Structuring Machine Learning Projects 4th course: Convolutional Neural Networks 5th course: Sequence Models And, here are my summaries of them: 1st course: summary_of_neural-networks-deep-learning 2nd course: summary_of_Improving-Deep-Neural-Networks 3rd course: summary_of_Structuring-Machine-Learning-Projects 4th course: summary_of_convolutional-neural-networks 5th course: summary_of_nlp-sequence-models I spent about 45 days in finishing this Deep learning Specialization and the personal lecture notes, summaries and assignments, but as the saying goes, “gain new knowledge by reviewing the old”. Therefore, I will stick at learning more about Deep Learning and renew the content of this specilization. if you need more details about this Deep Learning Specilization in English, please refer deeplearning.ai or the specialization on Coursera. Tip: if you are familiar with Chinese, you can read the content as following. 中文本文是我个人对吴恩达的深度学习专项课程的学习总结，此文有5个子课程，总结如下： 1st course: summary_of_neural-networks-deep-learning 2nd course: summary_of_Improving-Deep-Neural-Networks 3rd course: summary_of_Structuring-Machine-Learning-Projects 4th course: summary_of_convolutional-neural-networks 5th course: summary_of_nlp-sequence-models 此专项课程的中文目录如下: 如果需要详细视频内容和课程ppt，请参考网易云课堂：吴恩达给你的人工智能第一课， 但是网易并没有提供完成作业的平台，完成作业还需要到 Coursera。]]></content>
      <categories>
        <category>English, 中文</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Trigger word]]></title>
    <url>%2F2018%2F06%2F06%2FTrigger_word_detection-v1%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 3rd week and the copyright belongs to deeplearning.ai. Trigger Word DetectionWelcome to the final programming assignment of this specialization! In this week’s videos, you learned about applying deep learning to speech recognition. In this assignment, you will construct a speech dataset and implement an algorithm for trigger word detection (sometimes also called keyword detection, or wakeword detection). Trigger word detection is the technology that allows devices like Amazon Alexa, Google Home, Apple Siri, and Baidu DuerOS to wake up upon hearing a certain word. For this exercise, our trigger word will be “Activate.” Every time it hears you say “activate,” it will make a “chiming” sound. By the end of this assignment, you will be able to record a clip of yourself talking, and have the algorithm trigger a chime when it detects you saying “activate.” After completing this assignment, perhaps you can also extend it to run on your laptop so that every time you say “activate” it starts up your favorite app, or turns on a network connected lamp in your house, or triggers some other event? In this assignment you will learn to: Structure a speech recognition project Synthesize and process audio recordings to create train/dev datasets Train a trigger word detection model and make predictions Lets get started! Run the following cell to load the package you are going to use. 12345678910import numpy as npfrom pydub import AudioSegmentimport randomimport sysimport ioimport osimport globimport IPythonfrom td_utils import *%matplotlib inline 1 - Data synthesis: Creating a speech datasetLet’s start by building a dataset for your trigger word detection algorithm. A speech dataset should ideally be as close as possible to the application you will want to run it on. In this case, you’d like to detect the word “activate” in working environments (library, home, offices, open-spaces …). You thus need to create recordings with a mix of positive words (“activate”) and negative words (random words other than activate) on different background sounds. Let’s see how you can create such a dataset. 1.1 - Listening to the dataOne of your friends is helping you out on this project, and they’ve gone to libraries, cafes, restaurants, homes and offices all around the region to record background noises, as well as snippets of audio of people saying positive/negative words. This dataset includes people speaking in a variety of accents. In the raw_data directory, you can find a subset of the raw audio files of the positive words, negative words, and background noise. You will use these audio files to synthesize a dataset to train the model. The “activate” directory contains positive examples of people saying the word “activate”. The “negatives” directory contains negative examples of people saying random words other than “activate”. There is one word per audio recording. The “backgrounds” directory contains 10 second clips of background noise in different environments. Run the cells below to listen to some examples. 1IPython.display.Audio("./raw_data/activates/1.wav") Your browser does not support the audio element. 1IPython.display.Audio("./raw_data/negatives/4.wav") Your browser does not support the audio element. 1IPython.display.Audio("./raw_data/backgrounds/1.wav") Your browser does not support the audio element. You will use these three type of recordings (positives/negatives/backgrounds) to create a labelled dataset. 1.2 - From audio recordings to spectrogramsWhat really is an audio recording? A microphone records little variations in air pressure over time, and it is these little variations in air pressure that your ear also perceives as sound. You can think of an audio recording is a long list of numbers measuring the little air pressure changes detected by the microphone. We will use audio sampled at 44100 Hz (or 44100 Hertz). This means the microphone gives us 44100 numbers per second. Thus, a 10 second audio clip is represented by 441000 numbers (= $10 \times 44100$). It is quite difficult to figure out from this “raw” representation of audio whether the word “activate” was said. In order to help your sequence model more easily learn to detect triggerwords, we will compute a spectrogram of the audio. The spectrogram tells us how much different frequencies are present in an audio clip at a moment in time. (If you’ve ever taken an advanced class on signal processing or on Fourier transforms, a spectrogram is computed by sliding a window over the raw audio signal, and calculates the most active frequencies in each window using a Fourier transform. If you don’t understand the previous sentence, don’t worry about it.) Lets see an example. 1IPython.display.Audio("audio_examples/example_train.wav") Your browser does not support the audio element. 1x = graph_spectrogram("audio_examples/example_train.wav") The graph above represents how active each frequency is (y axis) over a number of time-steps (x axis). Figure 1: Spectrogram of an audio recording, where the color shows the degree to which different frequencies are present (loud) in the audio at different points in time. Green squares means a certain frequency is more active or more present in the audio clip (louder); blue squares denote less active frequencies. The dimension of the output spectrogram depends upon the hyperparameters of the spectrogram software and the length of the input. In this notebook, we will be working with 10 second audio clips as the “standard length” for our training examples. The number of timesteps of the spectrogram will be 5511. You’ll see later that the spectrogram will be the input $x$ into the network, and so $T_x = 5511$. 123_, data = wavfile.read("audio_examples/example_train.wav")print("Time steps in audio recording before spectrogram", data[:,0].shape)print("Time steps in input after spectrogram", x.shape) Time steps in audio recording before spectrogram (441000,) Time steps in input after spectrogram (101, 5511) Now, you can define: 12Tx = 5511 # The number of time steps input to the model from the spectrogramn_freq = 101 # Number of frequencies input to the model at each time step of the spectrogram Note that even with 10 seconds being our default training example length, 10 seconds of time can be discretized to different numbers of value. You’ve seen 441000 (raw audio) and 5511 (spectrogram). In the former case, each step represents $10/441000 \approx 0.000023$ seconds. In the second case, each step represents $10/5511 \approx 0.0018$ seconds. For the 10sec of audio, the key values you will see in this assignment are: $441000$ (raw audio) $5511 = T_x$ (spectrogram output, and dimension of input to the neural network). $10000$ (used by the pydub module to synthesize audio) $1375 = T_y$ (the number of steps in the output of the GRU you’ll build). Note that each of these representations correspond to exactly 10 seconds of time. It’s just that they are discretizing them to different degrees. All of these are hyperparameters and can be changed (except the 441000, which is a function of the microphone). We have chosen values that are within the standard ranges uses for speech systems. Consider the $T_y = 1375$ number above. This means that for the output of the model, we discretize the 10s into 1375 time-intervals (each one of length $10/1375 \approx 0.0072$s) and try to predict for each of these intervals whether someone recently finished saying “activate.” Consider also the 10000 number above. This corresponds to discretizing the 10sec clip into 10/10000 = 0.001 second itervals. 0.001 seconds is also called 1 millisecond, or 1ms. So when we say we are discretizing according to 1ms intervals, it means we are using 10,000 steps. 1Ty = 1375 # The number of time steps in the output of our model 1.3 - Generating a single training exampleBecause speech data is hard to acquire and label, you will synthesize your training data using the audio clips of activates, negatives, and backgrounds. It is quite slow to record lots of 10 second audio clips with random “activates” in it. Instead, it is easier to record lots of positives and negative words, and record background noise separately (or download background noise from free online sources). To synthesize a single training example, you will: Pick a random 10 second background audio clip Randomly insert 0-4 audio clips of “activate” into this 10sec clip Randomly insert 0-2 audio clips of negative words into this 10sec clip Because you had synthesized the word “activate” into the background clip, you know exactly when in the 10sec clip the “activate” makes its appearance. You’ll see later that this makes it easier to generate the labels $y^{\langle t \rangle}$ as well. You will use the pydub package to manipulate audio. Pydub converts raw audio files into lists of Pydub data structures (it is not important to know the details here). Pydub uses 1ms as the discretization interval (1ms is 1 millisecond = 1/1000 seconds) which is why a 10sec clip is always represented using 10,000 steps. 123456# Load audio segments using pydub activates, negatives, backgrounds = load_raw_audio()print("background len: " + str(len(backgrounds[0]))) # Should be 10,000, since it is a 10 sec clipprint("activate[0] len: " + str(len(activates[0]))) # Maybe around 1000, since an "activate" audio clip is usually around 1 sec (but varies a lot)print("activate[1] len: " + str(len(activates[1]))) # Different "activate" clips can have different lengths background len: 10000 activate[0] len: 916 activate[1] len: 1579 Overlaying positive/negative words on the background: Given a 10sec background clip and a short audio clip (positive or negative word), you need to be able to “add” or “insert” the word’s short audio clip onto the background. To ensure audio segments inserted onto the background do not overlap, you will keep track of the times of previously inserted audio clips. You will be inserting multiple clips of positive/negative words onto the background, and you don’t want to insert an “activate” or a random word somewhere that overlaps with another clip you had previously added. For clarity, when you insert a 1sec “activate” onto a 10sec clip of cafe noise, you end up with a 10sec clip that sounds like someone sayng “activate” in a cafe, with “activate” superimposed on the background cafe noise. You do not end up with an 11 sec clip. You’ll see later how pydub allows you to do this. Creating the labels at the same time you overlay: Recall also that the labels $y^{\langle t \rangle}$ represent whether or not someone has just finished saying “activate.” Given a background clip, we can initialize $y^{\langle t \rangle}=0$ for all $t$, since the clip doesn’t contain any “activates.” When you insert or overlay an “activate” clip, you will also update labels for $y^{\langle t \rangle}$, so that 50 steps of the output now have target label 1. You will train a GRU to detect when someone has finished saying “activate”. For example, suppose the synthesized “activate” clip ends at the 5sec mark in the 10sec audio—exactly halfway into the clip. Recall that $T_y = 1375$, so timestep $687 = $ int(1375*0.5) corresponds to the moment at 5sec into the audio. So, you will set $y^{\langle 688 \rangle} = 1$. Further, you would quite satisfied if the GRU detects “activate” anywhere within a short time-internal after this moment, so we actually set 50 consecutive values of the label $y^{\langle t \rangle}$ to 1. Specifically, we have $y^{\langle 688 \rangle} = y^{\langle 689 \rangle} = \cdots = y^{\langle 737 \rangle} = 1$. This is another reason for synthesizing the training data: It’s relatively straightforward to generate these labels $y^{\langle t \rangle}$ as described above. In contrast, if you have 10sec of audio recorded on a microphone, it’s quite time consuming for a person to listen to it and mark manually exactly when “activate” finished. Here’s a figure illustrating the labels $y^{\langle t \rangle}$, for a clip which we have inserted “activate”, “innocent”, activate”, “baby.” Note that the positive labels “1” are associated only with the positive words. Figure 2 To implement the training set synthesis process, you will use the following helper functions. All of these function will use a 1ms discretization interval, so the 10sec of audio is alwsys discretized into 10,000 steps. get_random_time_segment(segment_ms) gets a random time segment in our background audio is_overlapping(segment_time, existing_segments) checks if a time segment overlaps with existing segments insert_audio_clip(background, audio_clip, existing_times) inserts an audio segment at a random time in our background audio using get_random_time_segment and is_overlapping insert_ones(y, segment_end_ms) inserts 1’s into our label vector y after the word “activate” The function get_random_time_segment(segment_ms) returns a random time segment onto which we can insert an audio clip of duration segment_ms. Read through the code to make sure you understand what it is doing. 123456789101112131415def get_random_time_segment(segment_ms): """ Gets a random time segment of duration segment_ms in a 10,000 ms audio clip. Arguments: segment_ms -- the duration of the audio clip in ms ("ms" stands for "milliseconds") Returns: segment_time -- a tuple of (segment_start, segment_end) in ms """ segment_start = np.random.randint(low=0, high=10000-segment_ms) # Make sure segment doesn't run past the 10sec background segment_end = segment_start + segment_ms - 1 return (segment_start, segment_end) Next, suppose you have inserted audio clips at segments (1000,1800) and (3400,4500). I.e., the first segment starts at step 1000, and ends at step 1800. Now, if we are considering inserting a new audio clip at (3000,3600) does this overlap with one of the previously inserted segments? In this case, (3000,3600) and (3400,4500) overlap, so we should decide against inserting a clip here. For the purpose of this function, define (100,200) and (200,250) to be overlapping, since they overlap at timestep 200. However, (100,199) and (200,250) are non-overlapping. Exercise: Implement is_overlapping(segment_time, existing_segments) to check if a new time segment overlaps with any of the previous segments. You will need to carry out 2 steps: Create a “False” flag, that you will later set to “True” if you find that there is an overlap. Loop over the previous_segments’ start and end times. Compare these times to the segment’s start and end times. If there is an overlap, set the flag defined in (1) as True. You can use:123for ....: if ... &lt;= ... and ... &gt;= ...: ... Hint: There is overlap if the segment starts before the previous segment ends, and the segment ends after the previous segment starts. 12345678910111213141516171819202122232425262728# GRADED FUNCTION: is_overlappingdef is_overlapping(segment_time, previous_segments): """ Checks if the time of a segment overlaps with the times of existing segments. Arguments: segment_time -- a tuple of (segment_start, segment_end) for the new segment previous_segments -- a list of tuples of (segment_start, segment_end) for the existing segments Returns: True if the time segment overlaps with any of the existing segments, False otherwise """ segment_start, segment_end = segment_time ### START CODE HERE ### (≈ 4 line) # Step 1: Initialize overlap as a "False" flag. (≈ 1 line) overlap = False; # Step 2: loop over the previous_segments start and end times. # Compare start/end times and set the flag to True if there is an overlap (≈ 3 lines) for previous_start, previous_end in previous_segments: if segment_end &gt;= previous_start and segment_start &lt;= previous_end: overlap = True; ### END CODE HERE ### return overlap 1234overlap1 = is_overlapping((950, 1430), [(2000, 2550), (260, 949)])overlap2 = is_overlapping((2305, 2950), [(824, 1532), (1900, 2305), (3424, 3656)])print("Overlap 1 = ", overlap1)print("Overlap 2 = ", overlap2) Overlap 1 = False Overlap 2 = True Expected Output: Overlap 1 False Overlap 2 True Now, lets use the previous helper functions to insert a new audio clip onto the 10sec background at a random time, but making sure that any newly inserted segment doesn’t overlap with the previous segments. Exercise: Implement insert_audio_clip() to overlay an audio clip onto the background 10sec clip. You will need to carry out 4 steps: Get a random time segment of the right duration in ms. Make sure that the time segment does not overlap with any of the previous time segments. If it is overlapping, then go back to step 1 and pick a new time segment. Add the new time segment to the list of existing time segments, so as to keep track of all the segments you’ve inserted. Overlay the audio clip over the background using pydub. We have implemented this for you. 12345678910111213141516171819202122232425262728293031323334353637# GRADED FUNCTION: insert_audio_clipdef insert_audio_clip(background, audio_clip, previous_segments): """ Insert a new audio segment over the background noise at a random time step, ensuring that the audio segment does not overlap with existing segments. Arguments: background -- a 10 second background audio recording. audio_clip -- the audio clip to be inserted/overlaid. previous_segments -- times where audio segments have already been placed Returns: new_background -- the updated background audio """ # Get the duration of the audio clip in ms segment_ms = len(audio_clip) ### START CODE HERE ### # Step 1: Use one of the helper functions to pick a random time segment onto which to insert # the new audio clip. (≈ 1 line) segment_time = get_random_time_segment(segment_ms); # Step 2: Check if the new segment_time overlaps with one of the previous_segments. If so, keep # picking new segment_time at random until it doesn't overlap. (≈ 2 lines) while is_overlapping(segment_time, previous_segments): segment_time = get_random_time_segment(segment_ms); # Step 3: Add the new segment_time to the list of previous_segments (≈ 1 line) previous_segments.append(segment_time); ### END CODE HERE ### # Step 4: Superpose audio segment and background new_background = background.overlay(audio_clip, position = segment_time[0]) return new_background, segment_time 12345np.random.seed(5)audio_clip, segment_time = insert_audio_clip(backgrounds[0], activates[0], [(3790, 4400)])audio_clip.export("insert_test.wav", format="wav")print("Segment Time: ", segment_time)IPython.display.Audio("insert_test.wav") Segment Time: (2254, 3169) Your browser does not support the audio element. Expected Output Segment Time (2254, 3169) 12# Expected audioIPython.display.Audio("audio_examples/insert_reference.wav") Your browser does not support the audio element. Finally, implement code to update the labels $y^{\langle t \rangle}$, assuming you just inserted an “activate.” In the code below, y is a (1,1375) dimensional vector, since $T_y = 1375$. If the “activate” ended at time step $t$, then set $y^{\langle t+1 \rangle} = 1$ as well as for up to 49 additional consecutive values. However, make sure you don’t run off the end of the array and try to update y[0][1375], since the valid indices are y[0][0] through y[0][1374] because $T_y = 1375$. So if “activate” ends at step 1370, you would get only y[0][1371] = y[0][1372] = y[0][1373] = y[0][1374] = 1 Exercise: Implement insert_ones(). You can use a for loop. (If you are an expert in python’s slice operations, feel free also to use slicing to vectorize this.) If a segment ends at segment_end_ms (using a 10000 step discretization), to convert it to the indexing for the outputs $y$ (using a $1375$ step discretization), we will use this formula:1segment_end_y = int(segment_end_ms * Ty / 10000.0) 12345678910111213141516171819202122232425262728# GRADED FUNCTION: insert_onesdef insert_ones(y, segment_end_ms): """ Update the label vector y. The labels of the 50 output steps strictly after the end of the segment should be set to 1. By strictly we mean that the label of segment_end_y should be 0 while, the 50 followinf labels should be ones. Arguments: y -- numpy array of shape (1, Ty), the labels of the training example segment_end_ms -- the end time of the segment in ms Returns: y -- updated labels """ # duration of the background (in terms of spectrogram time-steps) segment_end_y = int(segment_end_ms * Ty / 10000.0) # Add 1 to the correct index in the background label (y) ### START CODE HERE ### (≈ 3 lines) for i in range(segment_end_y + 1, segment_end_y + 51): if i &lt; Ty: y[0, i] = 1 ### END CODE HERE ### return y 123arr1 = insert_ones(np.zeros((1, Ty)), 9700)plt.plot(insert_ones(arr1, 4251)[0,:])print("sanity checks:", arr1[0][1333], arr1[0][634], arr1[0][635]) sanity checks: 0.0 1.0 0.0 Expected Output sanity checks: 0.0 1.0 0.0 Finally, you can use insert_audio_clip and insert_ones to create a new training example.Exercise: Implement create_training_example(). You will need to carry out the following steps:1. Initialize the label vector $y$ as a numpy array of zeros and shape $(1, T_y)$.2. Initialize the set of existing segments to an empty list.3. Randomly select 0 to 4 “activate” audio clips, and insert them onto the 10sec clip. Also insert labels at the correct position in the label vector $y$.4. Randomly select 0 to 2 negative audio clips, and insert them into the 10sec clip.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# GRADED FUNCTION: create_training_exampledef create_training_example(background, activates, negatives): """ Creates a training example with a given background, activates, and negatives. Arguments: background -- a 10 second background audio recording activates -- a list of audio segments of the word "activate" negatives -- a list of audio segments of random words that are not "activate" Returns: x -- the spectrogram of the training example y -- the label at each time step of the spectrogram """ # Set the random seed np.random.seed(18) # Make background quieter background = background - 20 ### START CODE HERE ### # Step 1: Initialize y (label vector) of zeros (≈ 1 line) y = np.zeros((1, Ty)); # Step 2: Initialize segment times as empty list (≈ 1 line) previous_segments = []; ### END CODE HERE ### # Select 0-4 random "activate" audio clips from the entire list of "activates" recordings number_of_activates = np.random.randint(0, 5) random_indices = np.random.randint(len(activates), size=number_of_activates) random_activates = [activates[i] for i in random_indices] ### START CODE HERE ### (≈ 3 lines) # Step 3: Loop over randomly selected "activate" clips and insert in background for random_activate in random_activates: # Insert the audio clip on the background background, segment_time = insert_audio_clip(background, random_activate, previous_segments); # Retrieve segment_start and segment_end from segment_time segment_start, segment_end = segment_time; # Insert labels in "y" y = insert_ones(y, segment_end); ### END CODE HERE ### # Select 0-2 random negatives audio recordings from the entire list of "negatives" recordings number_of_negatives = np.random.randint(0, 3) random_indices = np.random.randint(len(negatives), size=number_of_negatives) random_negatives = [negatives[i] for i in random_indices] ### START CODE HERE ### (≈ 2 lines) # Step 4: Loop over randomly selected negative clips and insert in background for random_negative in random_negatives: # Insert the audio clip on the background background, _ = insert_audio_clip(background, random_negative, previous_segments); ### END CODE HERE ### # Standardize the volume of the audio clip background = match_target_amplitude(background, -20.0) # Export new training example file_handle = background.export("train" + ".wav", format="wav") print("File (train.wav) was saved in your directory.") # Get and plot spectrogram of the new recording (background with superposition of positive and negatives) x = graph_spectrogram("train.wav") return x, y1x, y = create_training_example(backgrounds[0], activates, negatives) File (train.wav) was saved in your directory.Expected OutputNow you can listen to the training example you created and compare it to the spectrogram generated above.1IPython.display.Audio("train.wav") Your browser does not support the audio element.Expected Output1IPython.display.Audio("audio_examples/train_reference.wav") Your browser does not support the audio element.Finally, you can plot the associated labels for the generated training example.1plt.plot(y[0]) [&lt;matplotlib.lines.Line2D at 0x7efcb81a8da0&gt;]Expected Output## 1.4 - Full training setYou’ve now implemented the code needed to generate a single training example. We used this process to generate a large training set. To save time, we’ve already generated a set of training examples.123# Load preprocessed training examplesX = np.load("./XY_train/X.npy")Y = np.load("./XY_train/Y.npy")## 1.5 - Development setTo test our model, we recorded a development set of 25 examples. While our training data is synthesized, we want to create a development set using the same distribution as the real inputs. Thus, we recorded 25 10-second audio clips of people saying “activate” and other random words, and labeled them by hand. This follows the principle described in Course 3 that we should create the dev set to be as similar as possible to the test set distribution; that’s why our dev set uses real rather than synthesized audio.123# Load preprocessed dev set examplesX_dev = np.load("./XY_dev/X_dev.npy")Y_dev = np.load("./XY_dev/Y_dev.npy")# 2 - ModelNow that you’ve built a dataset, lets write and train a trigger word detection model!The model will use 1-D convolutional layers, GRU layers, and dense layers. Let’s load the packages that will allow you to use these layers in Keras. This might take a minute to load.12345from keras.callbacks import ModelCheckpointfrom keras.models import Model, load_model, Sequentialfrom keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1Dfrom keras.layers import GRU, Bidirectional, BatchNormalization, Reshapefrom keras.optimizers import Adam Using TensorFlow backend.## 2.1 - Build the modelHere is the architecture we will use. Take some time to look over the model and see if it makes sense. Figure 3 One key step of this model is the 1D convolutional step (near the bottom of Figure 3). It inputs the 5511 step spectrogram, and outputs a 1375 step output, which is then further processed by multiple layers to get the final $T_y = 1375$ step output. This layer plays a role similar to the 2D convolutions you saw in Course 4, of extracting low-level features and then possibly generating an output of a smaller dimension.Computationally, the 1-D conv layer also helps speed up the model because now the GRU has to process only 1375 timesteps rather than 5511 timesteps. The two GRU layers read the sequence of inputs from left to right, then ultimately uses a dense+sigmoid layer to make a prediction for $y^{\langle t \rangle}$. Because $y$ is binary valued (0 or 1), we use a sigmoid output at the last layer to estimate the chance of the output being 1, corresponding to the user having just said “activate.”Note that we use a uni-directional RNN rather than a bi-directional RNN. This is really important for trigger word detection, since we want to be able to detect the trigger word almost immediately after it is said. If we used a bi-directional RNN, we would have to wait for the whole 10sec of audio to be recorded before we could tell if “activate” was said in the first second of the audio clip.Implementing the model can be done in four steps:Step 1: CONV layer. Use Conv1D() to implement this, with 196 filters,a filter size of 15 (kernel_size=15), and stride of 4. [See documentation.]Step 2: First GRU layer. To generate the GRU layer, use:1X = GRU(units = 128, return_sequences = True)(X)Setting return_sequences=True ensures that all the GRU’s hidden states are fed to the next layer. Remember to follow this with Dropout and BatchNorm layers.Step 3: Second GRU layer. This is similar to the previous GRU layer (remember to use return_sequences=True), but has an extra dropout layer.Step 4: Create a time-distributed dense layer as follows:1X = TimeDistributed(Dense(1, activation = "sigmoid"))(X)This creates a dense layer followed by a sigmoid, so that the parameters used for the dense layer are the same for every time step. [See documentation.]Exercise: Implement model(), the architecture is presented in Figure 3.123456789101112131415161718192021222324252627282930313233343536373839404142# GRADED FUNCTION: modeldef model(input_shape): """ Function creating the model's graph in Keras. Argument: input_shape -- shape of the model's input data (using Keras conventions) Returns: model -- Keras model instance """ X_input = Input(shape = input_shape) ### START CODE HERE ### # Step 1: CONV layer (≈4 lines) X = Conv1D(196, 15, strides = 4)(X_input); # CONV1D X = BatchNormalization()(X); # Batch normalization X = Activation('relu')(X); # ReLu activation X = Dropout(0.8)(X); # dropout (use 0.8) # Step 2: First GRU Layer (≈4 lines) X = GRU(128, return_sequences = True)(X); # GRU (use 128 units and return the sequences) X = Dropout(0.8)(X); # dropout (use 0.8) X = BatchNormalization()(X); # Batch normalization # Step 3: Second GRU Layer (≈4 lines) X = GRU(128, return_sequences = True)(X); # GRU (use 128 units and return the sequences) X = Dropout(0.8)(X); # dropout (use 0.8) X = BatchNormalization()(X); # Batch normalization X = Dropout(0.8)(X); # dropout (use 0.8) # Step 4: Time-distributed dense layer (≈1 line) X = TimeDistributed(Dense(1, activation = "sigmoid"))(X) # time distributed (sigmoid) ### END CODE HERE ### model = Model(inputs = X_input, outputs = X) return model1model = model(input_shape = (Tx, n_freq))Let’s print the model summary to keep track of the shapes.1model.summary() _ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 5511, 101) 0 _ conv1d_1 (Conv1D) (None, 1375, 196) 297136 _ batch_normalization_1 (Batch (None, 1375, 196) 784 _ activation_1 (Activation) (None, 1375, 196) 0 _ dropout_1 (Dropout) (None, 1375, 196) 0 _ gru_1 (GRU) (None, 1375, 128) 124800 _ dropout_2 (Dropout) (None, 1375, 128) 0 _ batch_normalization_2 (Batch (None, 1375, 128) 512 _ gru_2 (GRU) (None, 1375, 128) 98688 _ dropout_3 (Dropout) (None, 1375, 128) 0 _ batch_normalization_3 (Batch (None, 1375, 128) 512 _ dropout_4 (Dropout) (None, 1375, 128) 0 _ time_distributed_1 (TimeDist (None, 1375, 1) 129 ================================================================= Total params: 522,561 Trainable params: 521,657 Non-trainable params: 904 _Expected Output: Total params 522,561 Trainable params 521,657 Non-trainable params 904 The output of the network is of shape (None, 1375, 1) while the input is (None, 5511, 101). The Conv1D has reduced the number of steps from 5511 at spectrogram to 1375. 2.2 - Fit the modelTrigger word detection takes a long time to train. To save time, we’ve already trained a model for about 3 hours on a GPU using the architecture you built above, and a large training set of about 4000 examples. Let’s load the model. 1model = load_model('./models/tr_model.h5') You can train the model further, using the Adam optimizer and binary cross entropy loss, as follows. This will run quickly because we are training just for one epoch and with a small training set of 26 examples. 12opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)model.compile(loss='binary_crossentropy', optimizer=opt, metrics=["accuracy"]) 1model.fit(X, Y, batch_size = 5, epochs=1) Epoch 1/1 26/26 [==============================] - 23s - loss: 0.0727 - acc: 0.9806 &lt;keras.callbacks.History at 0x7efc4e3727f0&gt; 2.3 - Test the modelFinally, let’s see how your model performs on the dev set. 12loss, acc = model.evaluate(X_dev, Y_dev)print("Dev set accuracy = ", acc) 25/25 [==============================] - 4s Dev set accuracy = 0.946036338806 This looks pretty good! However, accuracy isn’t a great metric for this task, since the labels are heavily skewed to 0’s, so a neural network that just outputs 0’s would get slightly over 90% accuracy. We could define more useful metrics such as F1 score or Precision/Recall. But let’s not bother with that here, and instead just empirically see how the model does. 3 - Making PredictionsNow that you have built a working model for trigger word detection, let’s use it to make predictions. This code snippet runs audio (saved in a wav file) through the network. 1234567891011121314def detect_triggerword(filename): plt.subplot(2, 1, 1) x = graph_spectrogram(filename) # the spectogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model x = x.swapaxes(0,1) x = np.expand_dims(x, axis=0) predictions = model.predict(x) plt.subplot(2, 1, 2) plt.plot(predictions[0,:,0]) plt.ylabel('probability') plt.show() return predictions Once you’ve estimated the probability of having detected the word “activate” at each output step, you can trigger a “chiming” sound to play when the probability is above a certain threshold. Further, $y^{\langle t \rangle}$ might be near 1 for many values in a row after “activate” is said, yet we want to chime only once. So we will insert a chime sound at most once every 75 output steps. This will help prevent us from inserting two chimes for a single instance of “activate”. (This plays a role similar to non-max suppression from computer vision.) 12345678910111213141516171819chime_file = "audio_examples/chime.wav"def chime_on_activate(filename, predictions, threshold): audio_clip = AudioSegment.from_wav(filename) chime = AudioSegment.from_wav(chime_file) Ty = predictions.shape[1] # Step 1: Initialize the number of consecutive output steps to 0 consecutive_timesteps = 0 # Step 2: Loop over the output steps in the y for i in range(Ty): # Step 3: Increment consecutive output steps consecutive_timesteps += 1 # Step 4: If prediction is higher than the threshold and more than 75 consecutive output steps have passed if predictions[0,i,0] &gt; threshold and consecutive_timesteps &gt; 75: # Step 5: Superpose audio and background using pydub audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds)*1000) # Step 6: Reset consecutive output steps to 0 consecutive_timesteps = 0 audio_clip.export("chime_output.wav", format='wav') 3.3 - Test on dev examplesLet’s explore how our model performs on two unseen audio clips from the development set. Lets first listen to the two dev set clips. 1IPython.display.Audio("./raw_data/dev/1.wav") Your browser does not support the audio element. 1IPython.display.Audio("./raw_data/dev/2.wav") Your browser does not support the audio element. Now lets run the model on these audio clips and see if it adds a chime after “activate”! 1234filename = "./raw_data/dev/1.wav"prediction = detect_triggerword(filename)chime_on_activate(filename, prediction, 0.5)IPython.display.Audio("./chime_output.wav") Your browser does not support the audio element. 1234filename = "./raw_data/dev/2.wav"prediction = detect_triggerword(filename)chime_on_activate(filename, prediction, 0.5)IPython.display.Audio("./chime_output.wav") Your browser does not support the audio element. CongratulationsYou’ve come to the end of this assignment! Here’s what you should remember: Data synthesis is an effective way to create a large training set for speech problems, specifically trigger word detection. Using a spectrogram and optionally a 1D conv layer is a common pre-processing step prior to passing audio data to an RNN, GRU or LSTM. An end-to-end deep learning approach can be used to built a very effective trigger word detection system. Congratulations on finishing the fimal assignment! Thank you for sticking with us through the end and for all the hard work you’ve put into learning deep learning. We hope you have enjoyed the course! 4 - Try your own example! (OPTIONAL/UNGRADED)In this optional and ungraded portion of this notebook, you can try your model on your own audio clips! Record a 10 second audio clip of you saying the word “activate” and other random words, and upload it to the Coursera hub as myaudio.wav. Be sure to upload the audio as a wav file. If your audio is recorded in a different format (such as mp3) there is free software that you can find online for converting it to wav. If your audio recording is not 10 seconds, the code below will either trim or pad it as needed to make it 10 seconds. 12345678910# Preprocess the audio to the correct formatdef preprocess_audio(filename): # Trim or pad audio segment to 10000ms padding = AudioSegment.silent(duration=10000) segment = AudioSegment.from_wav(filename)[:10000] segment = padding.overlay(segment) # Set frame rate to 44100 segment = segment.set_frame_rate(44100) # Export as wav segment.export(filename, format='wav') Once you’ve uploaded your audio file to Coursera, put the path to your file in the variable below. 1your_filename = "audio_examples/my_audio.wav" 12preprocess_audio(your_filename)IPython.display.Audio(your_filename) # listen to the audio you uploaded Your browser does not support the audio element. Finally, use the model to predict when you say activate in the 10 second audio clip, and trigger a chime. If beeps are not being added appropriately, try to adjust the chime_threshold. 1234chime_threshold = 0.5prediction = detect_triggerword(your_filename)chime_on_activate(your_filename, prediction, chime_threshold)IPython.display.Audio("./chime_output.wav") Your browser does not support the audio element.]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[summary of nlp sequence models]]></title>
    <url>%2F2018%2F06%2F06%2Fsummary_of_nlp-sequence-models%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal summary after studying the course, nlp sequence models, which belongs to Deep Learning Specialization. and the copyright belongs to deeplearning.ai. My personal note$1_{st}$ week : Building a Recurrent Neural Network Step by Step 01_why-sequence-models 02_notation 03_recurrent-neural-network-model 04_backpropagation-through-time 05_different-types-of-rnns 06_language-model-and-sequence-generation 07_sampling-novel-sequences 08_vanishing-gradients-with-rnns 09_gated-recurrent-unit-gru 10_long-short-term-memory-lstm 11_bidirectional-rnn 12_deep-rnns $2_{nd}$ week : natural language processing word embeddings 01_introduction-to-word-embeddings 01_word-representation 02_using-word-embeddings 03_properties-of-word-embeddings 04_embedding-matrix 02_learning-word-embeddings-word2vec-glove 01_learning-word-embeddings 02_word2vec 03_negative-sampling 04_glove-word-vectors 03_applications-using-word-embeddings 01_sentiment-classification 02_debiasing-word-embeddings $3_{rd}$ week : sequence models attention mechanism 01_various-sequence-to-sequence-architectures 01_basic-models 02_picking-the-most-likely-sentence 03_beam-search 04_refinements-to-beam-search 05_error-analysis-in-beam-search 06_bleu-score-optional 07_attention-model-intuition 08_attention-model 02_speech-recognition-audio-data 01_speech-recognition 02_trigger-word-detection conclusion of Deep Learning Specialization and thank-you My personal programming assignments$1_{st}$ week: Building a Recurrent Neural Network Step by Step Dinosaurus Island Character level language model final Improvise a Jazz Solo with an LSTM Network $2_{nd}$ week: Word Vector Representation Emojify $3_{rd}$ week: machine translation Trigger word]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural machine translation with attention]]></title>
    <url>%2F2018%2F06%2F05%2FNeural%2Bmachine%2Btranslation%2Bwith%2Battention%2B-%2Bv4%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 3rd week and the copyright belongs to deeplearning.ai. Neural Machine TranslationWelcome to your first programming assignment for this week! You will build a Neural Machine Translation (NMT) model to translate human readable dates (“25th of June, 2009”) into machine readable dates (“2009-06-25”). You will do this using an attention model, one of the most sophisticated sequence to sequence models. This notebook was produced together with NVIDIA’s Deep Learning Institute. Let’s load all the packages you will need for this assignment. 123456789101112131415from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiplyfrom keras.layers import RepeatVector, Dense, Activation, Lambdafrom keras.optimizers import Adamfrom keras.utils import to_categoricalfrom keras.models import load_model, Modelimport keras.backend as Kimport numpy as npfrom faker import Fakerimport randomfrom tqdm import tqdmfrom babel.dates import format_datefrom nmt_utils import *import matplotlib.pyplot as plt%matplotlib inline Using TensorFlow backend. 1 - Translating human readable dates into machine readable datesThe model you will build here could be used to translate from one language to another, such as translating from English to Hindi. However, language translation requires massive datasets and usually takes days of training on GPUs. To give you a place to experiment with these models even without using massive datasets, we will instead use a simpler “date translation” task. The network will input a date written in a variety of possible formats (e.g. “the 29th of August 1958”, “03/30/1968”, “24 JUNE 1987”) and translate them into standardized, machine readable dates (e.g. “1958-08-29”, “1968-03-30”, “1987-06-24”). We will have the network learn to output dates in the common machine-readable format YYYY-MM-DD. 1.1 - DatasetWe will train the model on a dataset of 10000 human readable dates and their equivalent, standardized, machine readable dates. Let’s run the following cells to load the dataset and print some examples. 12m = 10000dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m) 100%|██████████| 10000/10000 [00:01&lt;00:00, 8435.76it/s] 1dataset[:10] [(&apos;9 may 1998&apos;, &apos;1998-05-09&apos;), (&apos;10.09.70&apos;, &apos;1970-09-10&apos;), (&apos;4/28/90&apos;, &apos;1990-04-28&apos;), (&apos;thursday january 26 1995&apos;, &apos;1995-01-26&apos;), (&apos;monday march 7 1983&apos;, &apos;1983-03-07&apos;), (&apos;sunday may 22 1988&apos;, &apos;1988-05-22&apos;), (&apos;tuesday july 8 2008&apos;, &apos;2008-07-08&apos;), (&apos;08 sep 1999&apos;, &apos;1999-09-08&apos;), (&apos;1 jan 1981&apos;, &apos;1981-01-01&apos;), (&apos;monday may 22 1995&apos;, &apos;1995-05-22&apos;)] You’ve loaded: dataset: a list of tuples of (human readable date, machine readable date) human_vocab: a python dictionary mapping all characters used in the human readable dates to an integer-valued index machine_vocab: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. These indices are not necessarily consistent with human_vocab. inv_machine_vocab: the inverse dictionary of machine_vocab, mapping from indices back to characters. Let’s preprocess the data and map the raw text data into the index values. We will also use Tx=30 (which we assume is the maximum length of the human readable date; if we get a longer input, we would have to truncate it) and Ty=10 (since “YYYY-MM-DD” is 10 characters long). 12345678Tx = 30Ty = 10X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)print("X.shape:", X.shape)print("Y.shape:", Y.shape)print("Xoh.shape:", Xoh.shape)print("Yoh.shape:", Yoh.shape) X.shape: (10000, 30) Y.shape: (10000, 10) Xoh.shape: (10000, 30, 37) Yoh.shape: (10000, 10, 11) You now have: X: a processed version of the human readable dates in the training set, where each character is replaced by an index mapped to the character via human_vocab. Each date is further padded to $T_x$ values with a special character (&lt; pad &gt;). X.shape = (m, Tx) Y: a processed version of the machine readable dates in the training set, where each character is replaced by the index it is mapped to in machine_vocab. You should have Y.shape = (m, Ty). Xoh: one-hot version of X, the “1” entry’s index is mapped to the character thanks to human_vocab. Xoh.shape = (m, Tx, len(human_vocab)) Yoh: one-hot version of Y, the “1” entry’s index is mapped to the character thanks to machine_vocab. Yoh.shape = (m, Tx, len(machine_vocab)). Here, len(machine_vocab) = 11 since there are 11 characters (‘-‘ as well as 0-9). Lets also look at some examples of preprocessed training examples. Feel free to play with index in the cell below to navigate the dataset and see how source/target dates are preprocessed. 123456789index = 0print("Source date:", dataset[index][0])print("Target date:", dataset[index][1])print()print("Source after preprocessing (indices):", X[index])print("Target after preprocessing (indices):", Y[index])print()print("Source after preprocessing (one-hot):", Xoh[index])print("Target after preprocessing (one-hot):", Yoh[index]) Source date: 9 may 1998 Target date: 1998-05-09 Source after preprocessing (indices): [12 0 24 13 34 0 4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36] Target after preprocessing (indices): [ 2 10 10 9 0 1 6 0 1 10] Source after preprocessing (one-hot): [[ 0. 0. 0. ..., 0. 0. 0.] [ 1. 0. 0. ..., 0. 0. 0.] [ 0. 0. 0. ..., 0. 0. 0.] ..., [ 0. 0. 0. ..., 0. 0. 1.] [ 0. 0. 0. ..., 0. 0. 1.] [ 0. 0. 0. ..., 0. 0. 1.]] Target after preprocessing (one-hot): [[ 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [ 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [ 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] 2 - Neural machine translation with attentionIf you had to translate a book’s paragraph from French to English, you would not read the whole paragraph, then close the book and translate. Even during the translation process, you would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English you are writing down. The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. 2.1 - Attention mechanismIn this part, you will implement the attention mechanism presented in the lecture videos. Here is a figure to remind you how the model works. The diagram on the left shows the attention model. The diagram on the right shows what one “Attention” step does to calculate the attention variables $\alpha^{\langle t, t’ \rangle}$, which are used to compute the context variable $context^{\langle t \rangle}$ for each timestep in the output ($t=1, \ldots, T_y$). Figure 1: Neural machine translation with attentionHere are some properties of the model that you may notice:1. There are two separate LSTMs in this model (see diagram on the left). Because the one at the bottom of the picture is a Bi-directional LSTM and comes before the attention mechanism, we will call it pre-attention Bi-LSTM. The LSTM at the top of the diagram comes after the attention mechanism, so we will call it the post-attention LSTM. The pre-attention Bi-LSTM goes through $T_x$ time steps; the post-attention LSTM goes through $T_y$ time steps.2. The post-attention LSTM passes $s^{\langle t \rangle}, c^{\langle t \rangle}$ from one time step to the next. In the lecture videos, we were using only a basic RNN for the post-activation sequence model, so the state captured by the RNN output activations $s^{\langle t\rangle}$. But since we are using an LSTM here, the LSTM has both the output activation $s^{\langle t\rangle}$ and the hidden cell state $c^{\langle t\rangle}$. However, unlike previous text generation examples (such as Dinosaurus in week 1), in this model the post-activation LSTM at time $t$ does will not take the specific generated $y^{\langle t-1 \rangle}$ as input; it only takes $s^{\langle t\rangle}$ and $c^{\langle t\rangle}$ as input. We have designed the model this way, because (unlike language generation where adjacent characters are highly correlated) there isn’t as strong a dependency between the previous character and the next character in a YYYY-MM-DD date.3. We use $a^{\langle t \rangle} = [\overrightarrow{a}^{\langle t \rangle}; \overleftarrow{a}^{\langle t \rangle}]$ to represent the concatenation of the activations of both the forward-direction and backward-directions of the pre-attention Bi-LSTM.4. The diagram on the right uses a RepeatVector node to copy $s^{\langle t-1 \rangle}$’s value $T_x$ times, and then Concatenation to concatenate $s^{\langle t-1 \rangle}$ and $a^{\langle t \rangle}$ to compute $e^{\langle t, t’}$, which is then passed through a softmax to compute $\alpha^{\langle t, t’ \rangle}$. We’ll explain how to use RepeatVector and Concatenation in Keras below.Lets implement this model. You will start by implementing two functions: one_step_attention() and model().1) one_step_attention(): At step $t$, given all the hidden states of the Bi-LSTM ($[a^{},a^{}, …, a^{&lt;T_x&gt;}]$) and the previous hidden state of the second LSTM ($s^{}$), one_step_attention() will compute the attention weights ($[\alpha^{&lt;t,1&gt;},\alpha^{&lt;t,2&gt;}, …, \alpha^{&lt;t,T_x&gt;}]$) and output the context vector (see Figure 1 (right) for details): $$context^{} = \sum_{t' = 0}^{T_x} \alpha^{}\tag{1}$$ Note that we are denoting the attention in this notebook $context^{\langle t \rangle}$. In the lecture videos, the context was denoted $c^{\langle t \rangle}$, but here we are calling it $context^{\langle t \rangle}$ to avoid confusion with the (post-attention) LSTM’s internal memory cell variable, which is sometimes also denoted $c^{\langle t \rangle}$.2) model(): Implements the entire model. It first runs the input through a Bi-LSTM to get back $[a^{},a^{}, …, a^{&lt;T_x&gt;}]$. Then, it calls one_step_attention() $T_y$ times (for loop). At each iteration of this loop, it gives the computed context vector $c^{}$ to the second LSTM, and runs the output of the LSTM through a dense layer with softmax activation to generate a prediction $\hat{y}^{}$.Exercise: Implement one_step_attention(). The function model() will call the layers in one_step_attention() $T_y$ using a for-loop, and it is important that all $T_y$ copies have the same weights. I.e., it should not re-initiaiize the weights every time. In other words, all $T_y$ steps should have shared weights. Here’s how you can implement layers with shareable weights in Keras:1. Define the layer objects (as global variables for examples).2. Call these objects when propagating the input.We have defined the layers you need as global variables. Please run the following cells to create them. Please check the Keras documentation to make sure you understand what these layers are: RepeatVector(), Concatenate(), Dense(), Activation(), Dot().1234567# Defined shared layers as global variablesrepeator = RepeatVector(Tx)concatenator = Concatenate(axis=-1)densor1 = Dense(10, activation = "tanh")densor2 = Dense(1, activation = "relu")activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebookdotor = Dot(axes = 1)Now you can use these layers to implement one_step_attention(). In order to propagate a Keras tensor object X through one of these layers, use layer(X) (or layer([X,Y]) if it requires multiple inputs.), e.g. densor(X) will propagate X through the Dense(1) layer defined above.12345678910111213141516171819202122232425262728293031# GRADED FUNCTION: one_step_attentiondef one_step_attention(a, s_prev): """ Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights "alphas" and the hidden states "a" of the Bi-LSTM. Arguments: a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a) s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s) Returns: context -- context vector, input of the next (post-attetion) LSTM cell """ ### START CODE HERE ### # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states "a" (≈ 1 line) s_prev = repeator(s_prev); # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line) concat = concatenator([a, s_prev]); # Use densor1 to propagate concat through a small fully-connected neural network to compute the "intermediate energies" variable e. (≈1 lines) e = densor1(concat); # Use densor2 to propagate e through a small fully-connected neural network to compute the "energies" variable energies. (≈1 lines) energies = densor2(e); # Use "activator" on "energies" to compute the attention weights "alphas" (≈ 1 line) alphas = activator(energies); # Use dotor together with "alphas" and "a" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line) context = dotor([alphas, a]); ### END CODE HERE ### return contextYou will be able to check the expected output of one_step_attention() after you’ve coded the model() function.Exercise: Implement model() as explained in figure 2 and the text above. Again, we have defined global layers that will share weights to be used in model().1234n_a = 32n_s = 64post_activation_LSTM_cell = LSTM(n_s, return_state = True)output_layer = Dense(len(machine_vocab), activation=softmax)Now you can use these layers $T_y$ times in a for loop to generate the outputs, and their parameters will not be reinitialized. You will have to carry out the following steps:1. Propagate the input into a Bidirectional LSTM2. Iterate for $t = 0, \dots, T_y-1$: 1. Call one_step_attention() on $[\alpha^{&lt;t,1&gt;},\alpha^{&lt;t,2&gt;}, …, \alpha^{&lt;t,T_x&gt;}]$ and $s^{}$ to get the context vector $context^{}$. 2. Give $context^{}$ to the post-attention LSTM cell. Remember pass in the previous hidden-state $s^{\langle t-1\rangle}$ and cell-states $c^{\langle t-1\rangle}$ of this LSTM using initial_state= [previous hidden state, previous cell state]. Get back the new hidden state $s^{}$ and the new cell state $c^{}$. 3. Apply a softmax layer to $s^{}$, get the output. 4. Save the output by adding it to the list of outputs.3. Create your Keras model instance, it should have three inputs (“inputs”, $s^{}$ and $c^{}$) and output the list of “outputs”.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# GRADED FUNCTION: modeldef model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size): """ Arguments: Tx -- length of the input sequence Ty -- length of the output sequence n_a -- hidden state size of the Bi-LSTM n_s -- hidden state size of the post-attention LSTM human_vocab_size -- size of the python dictionary "human_vocab" machine_vocab_size -- size of the python dictionary "machine_vocab" Returns: model -- Keras model instance """ # Define the inputs of your model with a shape (Tx,) # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,) X = Input(shape=(Tx, human_vocab_size)) s0 = Input(shape=(n_s,), name='s0') c0 = Input(shape=(n_s,), name='c0') s = s0 c = c0 # Initialize empty list of outputs outputs = [] ### START CODE HERE ### # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line) a = Bidirectional(LSTM(n_a, return_sequences = True))(X); # Step 2: Iterate for Ty steps for t in range(Ty): # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line) context = one_step_attention(a, s); # Step 2.B: Apply the post-attention LSTM cell to the "context" vector. # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line) s, _, c = post_activation_LSTM_cell(context, initial_state= [s, c]); # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line) out = output_layer(s); # Step 2.D: Append "out" to the "outputs" list (≈ 1 line) outputs.append(out); # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line) model = Model(inputs = [X, s0, c0], outputs = outputs); ### END CODE HERE ### return modelRun the following cell to create your model.1model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))Let’s get a summary of the model to check if it matches the expected output.1model.summary() ____ Layer (type) Output Shape Param # Connected to ==================================================================================================== input_1 (InputLayer) (None, 30, 37) 0 ____ s0 (InputLayer) (None, 64) 0 ____ bidirectional_1 (Bidirectional) (None, 30, 64) 17920 input_1[0][0] ____ repeat_vector_1 (RepeatVector) (None, 30, 64) 0 s0[0][0] lstm_1[0][0] lstm_1[1][0] lstm_1[2][0] lstm_1[3][0] lstm_1[4][0] lstm_1[5][0] lstm_1[6][0] lstm_1[7][0] lstm_1[8][0] ____ concatenate_1 (Concatenate) (None, 30, 128) 0 bidirectional_1[0][0] repeat_vector_1[0][0] bidirectional_1[0][0] repeat_vector_1[1][0] bidirectional_1[0][0] repeat_vector_1[2][0] bidirectional_1[0][0] repeat_vector_1[3][0] bidirectional_1[0][0] repeat_vector_1[4][0] bidirectional_1[0][0] repeat_vector_1[5][0] bidirectional_1[0][0] repeat_vector_1[6][0] bidirectional_1[0][0] repeat_vector_1[7][0] bidirectional_1[0][0] repeat_vector_1[8][0] bidirectional_1[0][0] repeat_vector_1[9][0] ____ dense_1 (Dense) (None, 30, 10) 1290 concatenate_1[0][0] concatenate_1[1][0] concatenate_1[2][0] concatenate_1[3][0] concatenate_1[4][0] concatenate_1[5][0] concatenate_1[6][0] concatenate_1[7][0] concatenate_1[8][0] concatenate_1[9][0] ____ dense_2 (Dense) (None, 30, 1) 11 dense_1[0][0] dense_1[1][0] dense_1[2][0] dense_1[3][0] dense_1[4][0] dense_1[5][0] dense_1[6][0] dense_1[7][0] dense_1[8][0] dense_1[9][0] ____ attention_weights (Activation) (None, 30, 1) 0 dense_2[0][0] dense_2[1][0] dense_2[2][0] dense_2[3][0] dense_2[4][0] dense_2[5][0] dense_2[6][0] dense_2[7][0] dense_2[8][0] dense_2[9][0] ____ dot_1 (Dot) (None, 1, 64) 0 attention_weights[0][0] bidirectional_1[0][0] attention_weights[1][0] bidirectional_1[0][0] attention_weights[2][0] bidirectional_1[0][0] attention_weights[3][0] bidirectional_1[0][0] attention_weights[4][0] bidirectional_1[0][0] attention_weights[5][0] bidirectional_1[0][0] attention_weights[6][0] bidirectional_1[0][0] attention_weights[7][0] bidirectional_1[0][0] attention_weights[8][0] bidirectional_1[0][0] attention_weights[9][0] bidirectional_1[0][0] ____ c0 (InputLayer) (None, 64) 0 ____ lstm_1 (LSTM) [(None, 64), (None, 6 33024 dot_1[0][0] s0[0][0] c0[0][0] dot_1[1][0] lstm_1[0][0] lstm_1[0][2] dot_1[2][0] lstm_1[1][0] lstm_1[1][2] dot_1[3][0] lstm_1[2][0] lstm_1[2][2] dot_1[4][0] lstm_1[3][0] lstm_1[3][2] dot_1[5][0] lstm_1[4][0] lstm_1[4][2] dot_1[6][0] lstm_1[5][0] lstm_1[5][2] dot_1[7][0] lstm_1[6][0] lstm_1[6][2] dot_1[8][0] lstm_1[7][0] lstm_1[7][2] dot_1[9][0] lstm_1[8][0] lstm_1[8][2] ____ dense_3 (Dense) (None, 11) 715 lstm_1[0][0] lstm_1[1][0] lstm_1[2][0] lstm_1[3][0] lstm_1[4][0] lstm_1[5][0] lstm_1[6][0] lstm_1[7][0] lstm_1[8][0] lstm_1[9][0] ==================================================================================================== Total params: 52,960 Trainable params: 52,960 Non-trainable params: 0 ____Expected Output:Here is the summary you should see Total params: 52,960 Trainable params: 52,960 Non-trainable params: 0 bidirectional_1’s output shape (None, 30, 64) repeat_vector_1’s output shape (None, 30, 64) concatenate_1’s output shape (None, 30, 128) attention_weights’s output shape (None, 30, 1) dot_1’s output shape (None, 1, 64) dense_3’s output shape (None, 11) As usual, after creating your model in Keras, you need to compile it and define what loss, optimizer and metrics your are want to use. Compile your model using categorical_crossentropy loss, a custom Adam optimizer (learning rate = 0.005, $\beta_1 = 0.9$, $\beta_2 = 0.999$, decay = 0.01) and [&#39;accuracy&#39;] metrics: 1234### START CODE HERE ### (≈2 lines)opt = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, decay=0.01);model.compile(loss = 'categorical_crossentropy',optimizer=opt, metrics = ['accuracy']);### END CODE HERE ### The last step is to define all your inputs and outputs to fit the model: You already have X of shape $(m = 10000, T_x = 30)$ containing the training examples. You need to create s0 and c0 to initialize your post_activation_LSTM_cell with 0s. Given the model() you coded, you need the “outputs” to be a list of 11 elements of shape (m, T_y). So that: outputs[i][0], ..., outputs[i][Ty] represent the true labels (characters) corresponding to the $i^{th}$ training example (X[i]). More generally, outputs[i][j] is the true label of the $j^{th}$ character in the $i^{th}$ training example. 123s0 = np.zeros((m, n_s))c0 = np.zeros((m, n_s))outputs = list(Yoh.swapaxes(0,1)) Let’s now fit the model and run it for one epoch. 1model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100) Epoch 1/1 10000/10000 [==============================] - 31s - loss: 22.1424 - dense_3_loss_1: 2.3001 - dense_3_loss_2: 2.2528 - dense_3_loss_3: 2.3607 - dense_3_loss_4: 2.5894 - dense_3_loss_5: 1.6743 - dense_3_loss_6: 1.9239 - dense_3_loss_7: 2.6330 - dense_3_loss_8: 1.5383 - dense_3_loss_9: 2.0970 - dense_3_loss_10: 2.7730 - dense_3_acc_1: 0.0035 - dense_3_acc_2: 0.0309 - dense_3_acc_3: 0.0000e+00 - dense_3_acc_4: 0.0045 - dense_3_acc_5: 0.9581 - dense_3_acc_6: 0.0000e+00 - dense_3_acc_7: 0.0027 - dense_3_acc_8: 0.9599 - dense_3_acc_9: 0.0051 - dense_3_acc_10: 0.0088 &lt;keras.callbacks.History at 0x7f8fbd556f60&gt; While training you can see the loss as well as the accuracy on each of the 10 positions of the output. The table below gives you an example of what the accuracies could be if the batch had 2 examples: Thus, dense_2_acc_8: 0.89 means that you are predicting the 7th character of the output correctly 89% of the time in the current batch of data. We have run this model for longer, and saved the weights. Run the next cell to load our weights. (By training a model for several minutes, you should be able to obtain a model of similar accuracy, but loading our model will save you time.) 1model.load_weights('models/model.h5') You can now see the results on new examples. 1234567891011EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']for example in EXAMPLES: source = string_to_int(example, Tx, human_vocab) source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1) prediction = model.predict([source, s0, c0]) prediction = np.argmax(prediction, axis = -1) output = [inv_machine_vocab[int(i)] for i in prediction] print("source:", example) print("output:", ''.join(output)) source: 3 May 1979 output: 1979-05-03 source: 5 April 09 output: 2009-05-05 source: 21th of August 2016 output: 2016-08-21 source: Tue 10 Jul 2007 output: 2007-07-10 source: Saturday May 9 2018 output: 2018-05-09 source: March 3 2001 output: 2001-03-03 source: March 3rd 2001 output: 2001-03-03 source: 1 March 2001 output: 2001-03-01 You can also change these examples to test with your own examples. The next part will give you a better sense on what the attention mechanism is doing–i.e., what part of the input the network is paying attention to when generating a particular output character. 3 - Visualizing Attention (Optional / Ungraded)Since the problem has a fixed output length of 10, it is also possible to carry out this task using 10 different softmax units to generate the 10 characters of the output. But one advantage of the attention model is that each part of the output (say the month) knows it needs to depend only on a small part of the input (the characters in the input giving the month). We can visualize what part of the output is looking at what part of the input. Consider the task of translating “Saturday 9 May 2018” to “2018-05-09”. If we visualize the computed $\alpha^{\langle t, t’ \rangle}$ we get this: Figure 8: Full Attention Map Notice how the output ignores the “Saturday” portion of the input. None of the output timesteps are paying much attention to that portion of the input. We see also that 9 has been translated as 09 and May has been correctly translated into 05, with the output paying attention to the parts of the input it needs to to make the translation. The year mostly requires it to pay attention to the input’s “18” in order to generate “2018.” 3.1 - Getting the activations from the networkLets now visualize the attention values in your network. We’ll propagate an example through the network, then visualize the values of $\alpha^{\langle t, t’ \rangle}$. To figure out where the attention values are located, let’s start by printing a summary of the model . 1model.summary() ____________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ==================================================================================================== input_1 (InputLayer) (None, 30, 37) 0 ____________________________________________________________________________________________________ s0 (InputLayer) (None, 64) 0 ____________________________________________________________________________________________________ bidirectional_1 (Bidirectional) (None, 30, 64) 17920 input_1[0][0] ____________________________________________________________________________________________________ repeat_vector_1 (RepeatVector) (None, 30, 64) 0 s0[0][0] lstm_1[0][0] lstm_1[1][0] lstm_1[2][0] lstm_1[3][0] lstm_1[4][0] lstm_1[5][0] lstm_1[6][0] lstm_1[7][0] lstm_1[8][0] ____________________________________________________________________________________________________ concatenate_1 (Concatenate) (None, 30, 128) 0 bidirectional_1[0][0] repeat_vector_1[0][0] bidirectional_1[0][0] repeat_vector_1[1][0] bidirectional_1[0][0] repeat_vector_1[2][0] bidirectional_1[0][0] repeat_vector_1[3][0] bidirectional_1[0][0] repeat_vector_1[4][0] bidirectional_1[0][0] repeat_vector_1[5][0] bidirectional_1[0][0] repeat_vector_1[6][0] bidirectional_1[0][0] repeat_vector_1[7][0] bidirectional_1[0][0] repeat_vector_1[8][0] bidirectional_1[0][0] repeat_vector_1[9][0] ____________________________________________________________________________________________________ dense_1 (Dense) (None, 30, 10) 1290 concatenate_1[0][0] concatenate_1[1][0] concatenate_1[2][0] concatenate_1[3][0] concatenate_1[4][0] concatenate_1[5][0] concatenate_1[6][0] concatenate_1[7][0] concatenate_1[8][0] concatenate_1[9][0] ____________________________________________________________________________________________________ dense_2 (Dense) (None, 30, 1) 11 dense_1[0][0] dense_1[1][0] dense_1[2][0] dense_1[3][0] dense_1[4][0] dense_1[5][0] dense_1[6][0] dense_1[7][0] dense_1[8][0] dense_1[9][0] ____________________________________________________________________________________________________ attention_weights (Activation) (None, 30, 1) 0 dense_2[0][0] dense_2[1][0] dense_2[2][0] dense_2[3][0] dense_2[4][0] dense_2[5][0] dense_2[6][0] dense_2[7][0] dense_2[8][0] dense_2[9][0] ____________________________________________________________________________________________________ dot_1 (Dot) (None, 1, 64) 0 attention_weights[0][0] bidirectional_1[0][0] attention_weights[1][0] bidirectional_1[0][0] attention_weights[2][0] bidirectional_1[0][0] attention_weights[3][0] bidirectional_1[0][0] attention_weights[4][0] bidirectional_1[0][0] attention_weights[5][0] bidirectional_1[0][0] attention_weights[6][0] bidirectional_1[0][0] attention_weights[7][0] bidirectional_1[0][0] attention_weights[8][0] bidirectional_1[0][0] attention_weights[9][0] bidirectional_1[0][0] ____________________________________________________________________________________________________ c0 (InputLayer) (None, 64) 0 ____________________________________________________________________________________________________ lstm_1 (LSTM) [(None, 64), (None, 6 33024 dot_1[0][0] s0[0][0] c0[0][0] dot_1[1][0] lstm_1[0][0] lstm_1[0][2] dot_1[2][0] lstm_1[1][0] lstm_1[1][2] dot_1[3][0] lstm_1[2][0] lstm_1[2][2] dot_1[4][0] lstm_1[3][0] lstm_1[3][2] dot_1[5][0] lstm_1[4][0] lstm_1[4][2] dot_1[6][0] lstm_1[5][0] lstm_1[5][2] dot_1[7][0] lstm_1[6][0] lstm_1[6][2] dot_1[8][0] lstm_1[7][0] lstm_1[7][2] dot_1[9][0] lstm_1[8][0] lstm_1[8][2] ____________________________________________________________________________________________________ dense_3 (Dense) (None, 11) 715 lstm_1[0][0] lstm_1[1][0] lstm_1[2][0] lstm_1[3][0] lstm_1[4][0] lstm_1[5][0] lstm_1[6][0] lstm_1[7][0] lstm_1[8][0] lstm_1[9][0] ==================================================================================================== Total params: 52,960 Trainable params: 52,960 Non-trainable params: 0 ____________________________________________________________________________________________________ Navigate through the output of model.summary() above. You can see that the layer named attention_weights outputs the alphas of shape (m, 30, 1) before dot_2 computes the context vector for every time step $t = 0, \ldots, T_y-1$. Lets get the activations from this layer. The function attention_map() pulls out the attention values from your model and plots them. 1attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, "Tuesday 09 Oct 1993", num = 7, n_s = 64) &lt;matplotlib.figure.Figure at 0x7f8fcf43c748&gt; On the generated plot you can observe the values of the attention weights for each character of the predicted output. Examine this plot and check that where the network is paying attention makes sense to you. In the date translation application, you will observe that most of the time attention helps predict the year, and hasn’t much impact on predicting the day/month. Congratulations!You have come to the end of this assignment Here’s what you should remember from this notebook: Machine translation models can be used to map from one sequence to another. They are useful not just for translating human languages (like French-&gt;English) but also for tasks like date format translation. An attention mechanism allows a network to focus on the most relevant parts of the input when producing a specific part of the output. A network using an attention mechanism can translate from inputs of length $T_x$ to outputs of length $T_y$, where $T_x$ and $T_y$ can be different. You can visualize attention weights $\alpha^{\langle t,t’ \rangle}$ to see what the network is paying attention to while generating each output. Congratulations on finishing this assignment! You are now able to implement an attention model and use it to learn complex mappings from one sequence to another.]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Operations on word vectors]]></title>
    <url>%2F2018%2F06%2F03%2FOperations%2Bon%2Bword%2Bvectors%2B-%2Bv2%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 2nd week and the copyright belongs to deeplearning.ai. Operations on word vectorsWelcome to your first assignment of this week! Because word embeddings are very computionally expensive to train, most ML practitioners will load a pre-trained set of embeddings. After this assignment you will be able to: Load pre-trained word vectors, and measure similarity using cosine similarity Use word embeddings to solve word analogy problems such as Man is to Woman as King is to __. Modify word embeddings to reduce their gender bias Let’s get started! Run the following cell to load the packages you will need. 12import numpy as npfrom w2v_utils import * Using TensorFlow backend. Next, lets load the word vectors. For this assignment, we will use 50-dimensional GloVe vectors to represent words. Run the following cell to load the word_to_vec_map. 1words, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt') You’ve loaded: words: set of words in the vocabulary. word_to_vec_map: dictionary mapping words to their GloVe vector representation. You’ve seen that one-hot vectors do not do a good job cpaturing what words are similar. GloVe vectors provide much more useful information about the meaning of individual words. Lets now see how you can use GloVe vectors to decide how similar two words are. 1 - Cosine similarityTo measure how similar two words are, we need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows: $$\text{CosineSimilarity(u, v)} = \frac {u . v} {||u||_2 ||v||_2} = cos(\theta) \tag{1}$$ where $u.v$ is the dot product (or inner product) of two vectors, $||u||_2$ is the norm (or length) of the vector $u$, and $\theta$ is the angle between $u$ and $v$. This similarity depends on the angle between $u$ and $v$. If $u$ and $v$ are very similar, their cosine similarity will be close to 1; if they are dissimilar, the cosine similarity will take a smaller value. Figure 1: The cosine of the angle between two vectors is a measure of how similar they are Exercise: Implement the function cosine_similarity() to evaluate similarity between word vectors. Reminder: The norm of $u$ is defined as $ ||u||_2 = \sqrt{\sum_{i=1}^{n} u_i^2}$ 1234567891011121314151617181920212223242526272829# GRADED FUNCTION: cosine_similaritydef cosine_similarity(u, v): """ Cosine similarity reflects the degree of similariy between u and v Arguments: u -- a word vector of shape (n,) v -- a word vector of shape (n,) Returns: cosine_similarity -- the cosine similarity between u and v defined by the formula above. """ distance = 0.0 ### START CODE HERE ### # Compute the dot product between u and v (≈1 line) dot = np.dot(u, v); # Compute the L2 norm of u (≈1 line) norm_u = np.linalg.norm(u); # Compute the L2 norm of v (≈1 line) norm_v = np.linalg.norm(v); # Compute the cosine similarity defined by formula (1) (≈1 line) cosine_similarity = dot / norm_u / norm_v; ### END CODE HERE ### return cosine_similarity 123456789101112father = word_to_vec_map["father"]mother = word_to_vec_map["mother"]ball = word_to_vec_map["ball"]crocodile = word_to_vec_map["crocodile"]france = word_to_vec_map["france"]italy = word_to_vec_map["italy"]paris = word_to_vec_map["paris"]rome = word_to_vec_map["rome"]print("cosine_similarity(father, mother) = ", cosine_similarity(father, mother))print("cosine_similarity(ball, crocodile) = ",cosine_similarity(ball, crocodile))print("cosine_similarity(france - paris, rome - italy) = ",cosine_similarity(france - paris, rome - italy)) cosine_similarity(father, mother) = 0.890903844289 cosine_similarity(ball, crocodile) = 0.274392462614 cosine_similarity(france - paris, rome - italy) = -0.675147930817 Expected Output: cosine_similarity(father, mother) = 0.890903844289 cosine_similarity(ball, crocodile) = 0.274392462614 cosine_similarity(france - paris, rome - italy) = -0.675147930817 After you get the correct expected output, please feel free to modify the inputs and measure the cosine similarity between other pairs of words! Playing around the cosine similarity of other inputs will give you a better sense of how word vectors behave. 2 - Word analogy taskIn the word analogy task, we complete the sentence “a is to b as c is to ____“. An example is ‘man is to woman as king is to queen‘ . In detail, we are trying to find a word d, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner: $e_b - e_a \approx e_d - e_c$. We will measure the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity. Exercise: Complete the code below to be able to perform word analogies! 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# GRADED FUNCTION: complete_analogydef complete_analogy(word_a, word_b, word_c, word_to_vec_map): """ Performs the word analogy task as explained above: a is to b as c is to ____. Arguments: word_a -- a word, string word_b -- a word, string word_c -- a word, string word_to_vec_map -- dictionary that maps words to their corresponding vectors. Returns: best_word -- the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity """ # convert words to lower case word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower() ### START CODE HERE ### # Get the word embeddings v_a, v_b and v_c (≈1-3 lines) e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]; ### END CODE HERE ### words = word_to_vec_map.keys() max_cosine_sim = -100 # Initialize max_cosine_sim to a large negative number best_word = None # Initialize best_word with None, it will help keep track of the word to output # loop over the whole word vector set for w in words: # to avoid best_word being one of the input words, pass on them. if w in [word_a, word_b, word_c] : continue ### START CODE HERE ### # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c) (≈1 line) cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c); # If the cosine_sim is more than the max_cosine_sim seen so far, # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines) if cosine_sim &gt; max_cosine_sim: max_cosine_sim = cosine_sim; best_word = w; ### END CODE HERE ### return best_word Run the cell below to test your code, this may take 1-2 minutes. 123triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]for triad in triads_to_try: print ('&#123;&#125; -&gt; &#123;&#125; :: &#123;&#125; -&gt; &#123;&#125;'.format( *triad, complete_analogy(*triad,word_to_vec_map))) italy -&gt; italian :: spain -&gt; spanish india -&gt; delhi :: japan -&gt; tokyo man -&gt; woman :: boy -&gt; girl small -&gt; smaller :: large -&gt; larger Expected Output: italy -&gt; italian :: spain -&gt; spanish india -&gt; delhi :: japan -&gt; tokyo man -&gt; woman :: boy -&gt; girl small -&gt; smaller :: large -&gt; larger Once you get the correct expected output, please feel free to modify the input cells above to test your own analogies. Try to find some other analogy pairs that do work, but also find some where the algorithm doesn’t give the right answer: For example, you can try small-&gt;smaller as big-&gt;?. Congratulations!You’ve come to the end of this assignment. Here are the main points you should remember: Cosine similarity a good way to compare similarity between pairs of word vectors. (Though L2 distance works too.) For NLP applications, using a pre-trained set of word vectors from the internet is often a good way to get started. Even though you have finished the graded portions, we recommend you take a look too at the rest of this notebook. Congratulations on finishing the graded portions of this notebook! 3 - Debiasing word vectors (OPTIONAL/UNGRADED)In the following exercise, you will examine gender biases that can be reflected in a word embedding, and explore algorithms for reducing the bias. In addition to learning about the topic of debiasing, this exercise will also help hone your intuition about what word vectors are doing. This section involves a bit of linear algebra, though you can probably complete it even without being expert in linear algebra, and we encourage you to give it a shot. This portion of the notebook is optional and is not graded. Lets first see how the GloVe word embeddings relate to gender. You will first compute a vector $g = e_{woman}-e_{man}$, where $e_{woman}$ represents the word vector corresponding to the word woman, and $e_{man}$ corresponds to the word vector corresponding to the word man. The resulting vector $g$ roughly encodes the concept of “gender”. (You might get a more accurate representation if you compute $g_1 = e_{mother}-e_{father}$, $g_2 = e_{girl}-e_{boy}$, etc. and average over them. But just using $e_{woman}-e_{man}$ will give good enough results for now.) 12g = word_to_vec_map['woman'] - word_to_vec_map['man']print(g) [-0.087144 0.2182 -0.40986 -0.03922 -0.1032 0.94165 -0.06042 0.32988 0.46144 -0.35962 0.31102 -0.86824 0.96006 0.01073 0.24337 0.08193 -1.02722 -0.21122 0.695044 -0.00222 0.29106 0.5053 -0.099454 0.40445 0.30181 0.1355 -0.0606 -0.07131 -0.19245 -0.06115 -0.3204 0.07165 -0.13337 -0.25068714 -0.14293 -0.224957 -0.149 0.048882 0.12191 -0.27362 -0.165476 -0.20426 0.54376 -0.271425 -0.10245 -0.32108 0.2516 -0.33455 -0.04371 0.01258 ] Now, you will consider the cosine similarity of different words with $g$. Consider what a positive value of similarity means vs a negative cosine similarity. 1234567print ('List of names and their similarities with constructed vector:')# girls and boys namename_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']for w in name_list: print (w, cosine_similarity(word_to_vec_map[w], g)) List of names and their similarities with constructed vector: john -0.23163356146 marie 0.315597935396 sophie 0.318687898594 ronaldo -0.312447968503 priya 0.17632041839 rahul -0.169154710392 danielle 0.243932992163 reza -0.079304296722 katy 0.283106865957 yasmin 0.233138577679 As you can see, female first names tend to have a positive cosine similarity with our constructed vector $g$, while male first names tend to have a negative cosine similarity. This is not suprising, and the result seems acceptable. But let’s try with some other words. 12345print('Other words and their similarities:')word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', 'technology', 'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']for w in word_list: print (w, cosine_similarity(word_to_vec_map[w], g)) Other words and their similarities: lipstick 0.276919162564 guns -0.18884855679 science -0.0608290654093 arts 0.00818931238588 literature 0.0647250443346 warrior -0.209201646411 doctor 0.118952894109 tree -0.0708939917548 receptionist 0.330779417506 technology -0.131937324476 fashion 0.0356389462577 teacher 0.179209234318 engineer -0.0803928049452 pilot 0.00107644989919 computer -0.103303588739 singer 0.185005181365 Do you notice anything surprising? It is astonishing how these results reflect certain unhealthy gender stereotypes. For example, “computer” is closer to “man” while “literature” is closer to “woman”. Ouch! We’ll see below how to reduce the bias of these vectors, using an algorithm due to Boliukbasi et al., 2016. Note that some word pairs such as “actor”/“actress” or “grandmother”/“grandfather” should remain gender specific, while other words such as “receptionist” or “technology” should be neutralized, i.e. not be gender-related. You will have to treat these two type of words differently when debiasing. 3.1 - Neutralize bias for non-gender specific wordsThe figure below should help you visualize what neutralizing does. If you’re using a 50-dimensional word embedding, the 50 dimensional space can be split into two parts: The bias-direction $g$, and the remaining 49 dimensions, which we’ll call $g_{\perp}$. In linear algebra, we say that the 49 dimensional $g_{\perp}$ is perpendicular (or “othogonal”) to $g$, meaning it is at 90 degrees to $g$. The neutralization step takes a vector such as $e_{receptionist}$ and zeros out the component in the direction of $g$, giving us $e_{receptionist}^{debiased}$. Even though $g_{\perp}$ is 49 dimensional, given the limitations of what we can draw on a screen, we illustrate it using a 1 dimensional axis below. Figure 2: The word vector for “receptionist” represented before and after applying the neutralize operation. Exercise: Implement neutralize() to remove the bias of words such as “receptionist” or “scientist”. Given an input embedding $e$, you can use the following formulas to compute $e^{debiased}$: $$e^{bias_component} = \frac{e \cdot g}{||g||_2^2} * g\tag{2}$$$$e^{debiased} = e - e^{bias_component}\tag{3}$$ If you are an expert in linear algebra, you may recognize $e^{bias_component}$ as the projection of $e$ onto the direction $g$. If you’re not an expert in linear algebra, don’t worry about this. 123456789101112131415161718192021222324252627def neutralize(word, g, word_to_vec_map): """ Removes the bias of "word" by projecting it on the space orthogonal to the bias axis. This function ensures that gender neutral words are zero in the gender subspace. Arguments: word -- string indicating the word to debias g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender) word_to_vec_map -- dictionary mapping words to their corresponding vectors. Returns: e_debiased -- neutralized word vector representation of the input "word" """ ### START CODE HERE ### # Select word vector representation of "word". Use word_to_vec_map. (≈ 1 line) e = word_to_vec_map[word]; # Compute e_biascomponent using the formula give above. (≈ 1 line) e_biascomponent = np.dot(e, g) / np.dot(g, g) * g; # Neutralize e by substracting e_biascomponent from it # e_debiased should be equal to its orthogonal projection. (≈ 1 line) e_debiased = e - e_biascomponent; ### END CODE HERE ### return e_debiased 12345e = "receptionist"print("cosine similarity between " + e + " and g, before neutralizing: ", cosine_similarity(word_to_vec_map["receptionist"], g))e_debiased = neutralize("receptionist", g, word_to_vec_map)print("cosine similarity between " + e + " and g, after neutralizing: ", cosine_similarity(e_debiased, g)) cosine similarity between receptionist and g, before neutralizing: 0.330779417506 cosine similarity between receptionist and g, after neutralizing: -5.60374039375e-17 Expected Output: The second result is essentially 0, up to numerical roundof (on the order of $10^{-17}$). cosine similarity between receptionist and g, before neutralizing: : 0.330779417506 cosine similarity between receptionist and g, after neutralizing: : -3.26732746085e-17 3.2 - Equalization algorithm for gender-specific wordsNext, lets see how debiasing can also be applied to word pairs such as “actress” and “actor.” Equalization is applied to pairs of words that you might want to have differ only through the gender property. As a concrete example, suppose that “actress” is closer to “babysit” than “actor.” By applying neutralizing to “babysit” we can reduce the gender-stereotype associated with babysitting. But this still does not guarantee that “actor” and “actress” are equidistant from “babysit.” The equalization algorithm takes care of this. The key idea behind equalization is to make sure that a particular pair of words are equi-distant from the 49-dimensional $g_\perp$. The equalization step also ensures that the two equalized steps are now the same distance from $e_{receptionist}^{debiased}$, or from any other work that has been neutralized. In pictures, this is how equalization works: The derivation of the linear algebra to do this is a bit more complex. (See Bolukbasi et al., 2016 for details.) But the key equations are: $$ \mu = \frac{e_{w1} + e_{w2}}{2}\tag{4}$$ $$ \mu_{B} = \frac {\mu \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis}\tag{5}$$ $$\mu_{\perp} = \mu - \mu_{B} \tag{6}$$ $$ e_{w1B} = \frac {e_{w1} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} \text{bias_axis}\tag{7}$$$$ e_{w2B} = \frac {e_{w2} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} \text{bias_axis}\tag{8}$$ $$e_{w1B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w1B}} - \mu_B} {|(e_{w1} - \mu_{\perp}) - \mu_B)|} \tag{9}$$ $$e_{w2B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w2B}} - \mu_B} {|(e_{w2} - \mu_{\perp}) - \mu_B)|} \tag{10}$$ $$e_1 = e_{w1B}^{corrected} + \mu_{\perp} \tag{11}$$$$e_2 = e_{w2B}^{corrected} + \mu_{\perp} \tag{12}$$ Exercise: Implement the function below. Use the equations above to get the final equalized version of the pair of words. Good luck! 1234567891011121314151617181920212223242526272829303132333435363738394041def equalize(pair, bias_axis, word_to_vec_map): """ Debias gender specific words by following the equalize method described in the figure above. Arguments: pair -- pair of strings of gender specific words to debias, e.g. ("actress", "actor") bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender word_to_vec_map -- dictionary mapping words to their corresponding vectors Returns e_1 -- word vector corresponding to the first word e_2 -- word vector corresponding to the second word """ ### START CODE HERE ### # Step 1: Select word vector representation of "word". Use word_to_vec_map. (≈ 2 lines) w1, w2 = pair; e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]; # Step 2: Compute the mean of e_w1 and e_w2 (≈ 1 line) mu = (e_w1 + e_w2) / 2; # Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines) mu_B = np.dot(mu, bias_axis) / np.dot(bias_axis, bias_axis) * bias_axis; mu_orth = mu - mu_B; # Step 4: Use equations (7) and (8) to compute e_w1B and e_w2B (≈2 lines) e_w1B = np.dot(e_w1, bias_axis) / np.dot(bias_axis, bias_axis) * bias_axis; e_w2B = np.dot(e_w2, bias_axis) / np.dot(bias_axis, bias_axis) * bias_axis; # Step 5: Adjust the Bias part of e_w1B and e_w2B using the formulas (9) and (10) given above (≈2 lines) corrected_e_w1B = np.sqrt(np.absolute(1 - np.linalg.norm(mu_orth) ** 2)) * (e_w1B - mu_B) / np.linalg.norm(e_w1 - mu_orth - mu_B); corrected_e_w2B = np.sqrt(np.absolute(1 - np.linalg.norm(mu_orth) ** 2)) * (e_w2B - mu_B) / np.linalg.norm(e_w2 - mu_orth - mu_B); # Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines) e1 = corrected_e_w1B + mu_orth; e2 = corrected_e_w2B + mu_orth; ### END CODE HERE ### return e1, e2 12345678print("cosine similarities before equalizing:")print("cosine_similarity(word_to_vec_map[\"man\"], gender) = ", cosine_similarity(word_to_vec_map["man"], g))print("cosine_similarity(word_to_vec_map[\"woman\"], gender) = ", cosine_similarity(word_to_vec_map["woman"], g))print()e1, e2 = equalize(("man", "woman"), g, word_to_vec_map)print("cosine similarities after equalizing:")print("cosine_similarity(e1, gender) = ", cosine_similarity(e1, g))print("cosine_similarity(e2, gender) = ", cosine_similarity(e2, g)) cosine similarities before equalizing: cosine_similarity(word_to_vec_map[&quot;man&quot;], gender) = -0.117110957653 cosine_similarity(word_to_vec_map[&quot;woman&quot;], gender) = 0.356666188463 cosine similarities after equalizing: cosine_similarity(e1, gender) = -0.700436428931 cosine_similarity(e2, gender) = 0.700436428931 Expected Output: cosine similarities before equalizing: cosine_similarity(word_to_vec_map[“man”], gender) = -0.117110957653 cosine_similarity(word_to_vec_map[“woman”], gender) = 0.356666188463 cosine similarities after equalizing: cosine_similarity(u1, gender) = -0.700436428931 cosine_similarity(u2, gender) = 0.700436428931 Please feel free to play with the input words in the cell above, to apply equalization to other pairs of words. These debiasing algorithms are very helpful for reducing bias, but are not perfect and do not eliminate all traces of bias. For example, one weakness of this implementation was that the bias direction $g$ was defined using only the pair of words _woman_ and _man_. As discussed earlier, if $g$ were defined by computing $g_1 = e_{woman} - e_{man}$; $g_2 = e_{mother} - e_{father}$; $g_3 = e_{girl} - e_{boy}$; and so on and averaging over them, you would obtain a better estimate of the “gender” dimension in the 50 dimensional word embedding space. Feel free to play with such variants as well. CongratulationsYou have come to the end of this notebook, and have seen a lot of the ways that word vectors can be used as well as modified. Congratulations on finishing this notebook! References: The debiasing algorithm is from Bolukbasi et al., 2016, Man is to Computer Programmer as Woman is toHomemaker? Debiasing Word Embeddings The GloVe word embeddings were due to Jeffrey Pennington, Richard Socher, and Christopher D. Manning. (https://nlp.stanford.edu/projects/glove/)]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sequence models attention mechanism]]></title>
    <url>%2F2018%2F06%2F03%2F03_sequence-models-attention-mechanism%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal lecture note after studying the course nlp sequence models at the 3rd week and the copyright belongs to deeplearning.ai.## 01_various-sequence-to-sequence-architectures 01_basic-modelsHello, and welcome to this final week of this course, as well as to the final week of this sequence of five courses in the deep learning specialization. You’re nearly at the finish line. In this week, you hear about sequence-to-sequence models, which are useful for everything from machine translation to speech recognition. Let’s start with the basic models and then later this week you, hear about beam search, the attention model, and we’ll wrap up the discussion of models for audio data, like speech. Let’s get started. Let’s say you want to input a French sentence like Jane visite l’Afrique en septembre, and you want to translate it to the English sentence, Jane is visiting Africa in September. As usual, let’s use x through x, in this case , to represent the words in the input sequence, and we’ll use y through y to represent the words in the output sequence. So, how can you train a new network to input the sequence x and output the sequence y? Well, here’s something you could do, and the ideas I’m about to present are mainly from these two papers due to Sutskever, Oriol Vinyals, and Quoc Le, and that one by Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwen, and Yoshua Bengio. First, let’s have a network, which we’re going to call the encoder network be built as a RNN, and this could be a GRU and LSTM, feed in the input French words one word at a time. And after ingesting the input sequence, the RNN then offers a vector that represents the input sentence. After that, you can build a decoder network which I’m going to draw here, which takes as input the encoding output by the encoder network shown in black on the left, and then can be trained to output the translation one word at a time until eventually it outputs say, the end of sequence or end the sentence token upon which the decoder stops and as usual we could take the generated tokens and feed them to the next [inaudible] in the sequence like we ‘re doing before when synthesizing text using the language model. One of the most remarkable recent results in deep learning is that this model works, given enough pairs of French and English sentences. If you train the model to input a French sentence and output the corresponding English translation, this will actually work decently well. And this model simply uses an encoder network, whose job it is to find an encoding of the input French sentence and then use a decoder network to then generate the corresponding English translation. An architecture very similar to this also works for image captioning so given an image like the one shown here, maybe wanted to be captioned automatically as a cat sitting on a chair. So how do you train a new network to input an image and output a caption like that phrase up there? Here’s what you can do. From the earlier course on ConvNet you’ve seen how you can input an image into a convolutional network, maybe a pre-trained AlexNet, and have that learn an encoding or learn a set of features of the input image. So, this is actually the AlexNet architecture and if we get rid of this final Softmax unit, the pre-trained AlexNet can give you a 4096-dimensional feature vector of which to represent this picture of a cat. And so this pre-trained network can be the encoder network for the image and you now have a 4096-dimensional vector that represents the image. You can then take this and feed it to an RNN, whose job it is to generate the caption one word at a time. So similar to what we saw with machine translation translating from French to English, you can now input a feature vector describing the input and then have it generate an output sequence or output set of words one word at a time. And this actually works pretty well for image captioning, especially if the caption you want to generate is not too long. As far as I know, this type of model was first proposed by Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille, although it turns out there were multiple groups coming up with very similar models independently and at about the same time. So two other groups that had done very similar work at about the same time and I think independently of Mao et al were Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan, as well as Andrej Karpathy and Fei-Fei Yi. So, you’ve now seen how a basic sequence-to-sequence model works, or how a basic image-to-sequence or image captioning model works, but there are some differences between how you would run a model like this, so generating a sequence compared to how you were synthesizing novel text using a language model. One of the key differences is, you don’t want a randomly chosen translation, you maybe want the most likely translation, or you don’t want a randomly chosen caption, maybe not, but you might want the best caption and most likely caption. So let’s see in the next video how you go about generating that. 02_picking-the-most-likely-sentenceThere are some similarities between the sequence to sequence machine translation model and the language models that you have worked within the first week of this course, but there are some significant differences as well. Let’s take a look. So, you can think of machine translation as building a conditional language model. Here’s what I mean, in language modeling, this was the network we had built in the first week. And this model allows you to estimate the probability of a sentence. That’s what a language model does. And you can also use this to generate novel sentences, and sometimes when you are writing x1 and x2 here, where in this example, x2 would be equal to y1 or equal to y and one is just a feedback. But x1, x2, and so on were not important. So just to clean this up for this slide, I’m going to just cross these off. X1 could be the vector of all zeros and x2, x3 are just the previous output you are generating. So that was the language model. The machine translation model looks as follows, and I am going to use a couple different colors, green and purple, to denote respectively the coded network in green and the decoded network in purple. And you notice that the decoded network looks pretty much identical to the language model that we had up there. So what the machine translation model is, is very similar to the language model, except that instead of always starting along with the vector of all zeros, it instead has an encoded network that figures out some representation for the input sentence, and it takes that input sentence and starts off the decoded network with representation of the input sentence rather than with the representation of all zeros. So, that’s why I call this a conditional language model, and instead of modeling the probability of any sentence, it is now modeling the probability of, say, the output English translation, conditions on some input French sentence. So in other words, you’re trying to estimate the probability of an English translation. Like, what’s the chance that the translation is “Jane is visiting Africa in September,” but conditions on the input French censors like, “Jane visite I’Afrique en septembre.” So, this is really the probability of an English sentence conditions on an input French sentence which is why it is a conditional language model. Now, if you want to apply this model to actually translate a sentence from French into English, given this input French sentence, the model might tell you what is the probability of difference in corresponding English translations. So, x is the French sentence, “Jane visite l’Afrique en septembre.” And, this now tells you what is the probability of different English translations of that French input. And, what you do not want is to sample outputs at random. If you sample words from this distribution, p of y given x, maybe one time you get a pretty good translation, “Jane is visiting Africa in September.” But, maybe another time you get a different translation, “Jane is going to be visiting Africa in September. “ Which sounds a little awkward but is not a terrible translation, just not the best one. And sometimes, just by chance, you get, say, others: “In September, Jane will visit Africa.” And maybe, just by chance, sometimes you sample a really bad translation: “Her African friend welcomed Jane in September.” So, when you’re using this model for machine translation, you’re not trying to sample at random from this distribution. Instead, what you would like is to find the English sentence, y, that maximizes that conditional probability. So in developing a machine translation system, one of the things you need to do is come up with an algorithm that can actually find the value of y that maximizes this term over here. The most common algorithm for doing this is called beam search, and it’s something you’ll see in the next video. But, before moving on to describe beam search, you might wonder, why not just use greedy search? So, what is greedy search? Well, greedy search is an algorithm from computer science which says to generate the first word just pick whatever is the most likely first word according to your conditional language model. Going to your machine translation model and then after having picked the first word, you then pick whatever is the second word that seems most likely, then pick the third word that seems most likely. This algorithm is called greedy search. And, what you would really like is to pick the entire sequence of words, $y^{}, y^{}$ , up to $y^{&lt;T_y&gt;}$, that’s there, that maximizes the joint probability of that whole thing. And it turns out that the greedy approach, where you just pick the best first word, and then, after having picked the best first word, try to pick the best second word, and then, after that, try to pick the best third word, that approach doesn’t really work. To demonstrate that, let’s consider the following two translations. The first one is a better translation, so hopefully, in our machine translation model, it will say that p of y given x is higher for the first sentence. It’s just a better, more succinct translation of the French input. The second one is not a bad translation, it’s just more verbose, it has more unnecessary words. But, if the algorithm has picked “Jane is” as the first two words, because “going” is a more common English word, probably the chance of “Jane is going,” given the French input, this might actually be higher than the chance of “Jane is visiting,” given the French sentence. So, it’s quite possible that if you just pick the third word based on whatever maximizes the probability of just the first three words, you end up choosing option number two. But, this ultimately ends up resulting in a less optimal sentence, in a less good sentence as measured by this model for p of y given x. I know this was may be a slightly hand-wavey argument, but, this is an example of a broader phenomenon, where if you want to find the sequence of words, y1, y2, all the way up to the final word that together maximize the probability, it’s not always optimal to just pick one word at a time. And, of course, the total number of combinations of words in the English sentence is exponentially larger. So, if you have just 10,000 words in a dictionary and if you’re contemplating translations that are up to ten words long, then there are 10000 to the tenth possible sentences that are ten words long. Picking words from the vocabulary size, the dictionary size of 10000 words. So, this is just a huge space of possible sentences, and it’s impossible to rate them all, which is why the most common thing to do is use an approximate search out of them. And, what an approximate search algorithm does, is it will try, it won’t always succeed, but it will to pick the sentence, y, that maximizes that conditional probability. And, even though it’s not guaranteed to find the value of y that maximizes this, it usually does a good enough job. So, to summarize, in this video, you saw how machine translation can be posed as a conditional language modeling problem. But one major difference between this and the earlier language modeling problems is rather than wanting to generate a sentence at random, you may want to try to find the most likely English sentence, most likely English translation. But the set of all English sentences of a certain length is too large to exhaustively enumerate. So, we have to resort to a search algorithm. So, with that, let’s go onto the next video where you’ll learn about beam search algorithm. 03_beam-searchIn this video, you learn about the beam search algorithm. In the last video, you remember how for machine translation given an input French sentence, you don’t want to output a random English translation, you want to output the best and the most likely English translation. The same is also true for speech recognition where given an input audio clip, you don’t want to output a random text transcript of that audio, you want to output the best, maybe the most likely, text transcript. Beam search is the most widely used algorithm to do this. And in this video, you see how to get beam search to work for yourself. Let’s just try Beam Search using our running example of the French sentence, “Jane, visite l’Afrique en Septembre”. Hopefully being translated into, “Jane, visits Africa in September”. The first thing Beam search has to do is try to pick the first words of the English translation, that’s going to operate. So here I’ve listed, say, 10,000 words into vocabulary. And to simplify the problem a bit, I’m going to ignore capitalization. So I’m just listing all the words in lower case. So, in the first step of Beam Search, I use this network fragment with the coalition in green and decoalition in purple, to try to evaluate what is the probability of that for a square. So, what’s the probability of the first output y, given the input sentence x gives the French input. So, whereas greedy search will pick only the one most likely words and move on, Beam Search instead can consider multiple alternatives. So, the Beam Search algorithm has a parameter called B, which is called the beam width and for this example I’m going to set the beam width to be with the three. And what this means is Beam search will cause that not just one possibility but consider three at the time. So in particular, let’s say evaluating this probability over different choices the first words, it finds that the choices in, Jane and September are the most likely three possibilities for the first words in the English outputs. Then Beam search will stowaway in computer memory that it wants to try all of three of these words, and if the beam width parameter were said differently, the beam width parameter was 10, then we keep track of not just three but of the ten, most likely possible choices for the first word. So, to be clear in order to perform this first step of Beam search, what you need to do is run the input French sentence through this encoder network and then this first step will then decode the network, this is a softmax output overall 10,000 possibilities. Then you would take those 10,000 possible outputs and keep in memory which were the top three. Let’s go into the second step of Beam search. Having picked in, Jane and September as the three most likely choice of the first word, what Beam search will do now, is for each of these three choices consider what should be the second word, so after “in” maybe a second word is “a” or maybe as Aaron, I’m just listing words from the vocabulary, from the dictionary or somewhere down the list will be September, somewhere down the list there’s visit and then all the way to z and then the last word is zulu. So, to evaluate the probability of second word, it will use this new network fragments where is coder in green and for the decoder portion when trying to decide what comes after in. Remember the decoder first outputs, y hat one. So, I’m going to set to this y hat one to the word “in” as it goes back in. So there’s the word “in” because it decided for now. That’s because It trying to figure out that the first word was “in”, what is the second word, and then this will output I guess y hat two. And so by hard wiring y hat one here, really the inputs here to be the first words “in” this time were fragment can be used to evaluate whether it’s the probability of the second word given the input french sentence and that the first words of the translation has been the word “in”. Now notice that what we also need help out in this second step would be assertions to find the pair of the first and second words that is most likely it’s not just a second where is most likely that the pair of the first and second whereas the most likely and by the rules of conditional probability. This can be expressed as P of the first words times P of probability of the second words. Which you are getting from this network fragment and so if for each of the three words you’ve chosen “in”, “Jane,” and “September” you save away this probability then you can multiply them by this second probabilities to get the probability of the first and second words. So now you’ve seen how if the first word was “in” how you can evaluate the probability of the second word. Now at first it was “Jane” you do the same thing. The sentence could be “Jane a”,” Jane Aaron”, and so on down to “Jane is”, “Jane visits” and so on. And you will use this in your network fragments let me draw this in as well where here you will hardwire, Y hat One to be Jane. And so with the First word y one hat’s hard wired as Jane than just the network fragments can tell you what’s the probability of the second words to me. And given that the first word is “Jane”. And then same as above you can multiply with P of Y1 to get the probability of Y1 and Y2 for each of these 10,000 different possible choices for the second word. And then finally do the same thing for September although words from a down to Zulu and use this network fragment. That just goes in as well to see if the first word was September. What was the most likely options for the second words. So for this second step of beam search because we’re continuing to use a beam width of three and because there are 10,000 words in the vocabulary you’d end up considering three times 10000 or thirty thousand possibilities because there are 10,000 here, 10,000 here, 10,000 here as the beam width times the number of words in the vocabulary and what you do is you evaluate all of these 30000 options according to the probably the first and second words and then pick the top three. So with a cut down, these 30,000 possibilities down to three again down the beam width rounded again so let’s say that 30,000 choices, the most likely were in September and say Jane is, and Jane visits sorry this bit messy but those are the most likely three out of the 30,000 choices then that’s what Beam’s search would memorize away and take on to the next step being surge. So notice one thing if beam search decides that the most likely choices are the first and second words are in September, or Jane is, or Jane visits. Then what that means is that it is now rejecting September as a candidate for the first words of the output English translation so we’re now down to two possibilities for the first words but we still have a beam width of three keeping track of three choices for pairs of Y1, Y2 before going onto the third step of beam search. Just want to notice that because of beam width is equal to three, every step you instantiate three copies of the network to evaluate these partial sentence fragments and the output. And it’s because of beam width is equal to three that you have three copies of the network with different choices for the first words, but these three copies of the network can be very efficiently used to evaluate all 30,000 options for the second word. So just don’t instantiate 30,000 copies of the network or three copies of the network to very quickly evaluate all 10,000 possible outputs at that softmax output say for Y2. Let’s just quickly illustrate one more step of beam search. So said that the most likely choices for first two words were in September, Jane is, and Jane visits and for each of these pairs of words which we should have saved the way in computer memory the probability of Y1 and Y2 given the input X given the French sentence X. So similar to before, we now want to consider what is the third word. So in September a? In September Aaron? All the way down to is in September Zulu and to evaluate possible choices for the third word, you use this network fragments where you Hardwire the first word here to be in the second word to be September. And so this network fragment allows you to evaluate what’s the probability of the third word given the input French sentence X and given that the first two words are in September and English output. And then you do the same thing for the second fragment. So like so. And same thing for Jane visits and so beam search will then once again pick the top three possibilities may be that things in September. Jane is a likely outcome or Jane is visiting is likely or maybe Jane visits Africa is likely for that first three words and then it keeps going and then you go onto the fourth step of beam search hat one more word and on it goes. And the outcome of this process hopefully will be that adding one word at a time that Beam search will decide that. Jane visits Africa in September will be terminated by the end of sentence symbol using that system is quite common. They’ll find that this is a likely output English sentence and you’ll see more details of this yourself. In this week’s exercise as well where you get to play with beam search yourself. So with a beam of three being searched considers three possibilities at a time. Notice that if the beam width was said to be equal to one, say cause there’s only one, then this essentially becomes the greedy search algorithm which we had discussed in the last video but by considering multiple possibilities say three or ten or some other number at the same time beam search will usually find a much better output sentence than greedy search. You’ve now seen how Beam Search works but it turns out there’s some additional tips and tricks refinements that help you to make beam search work even better. Let’s go onto the next video to take a look. 04_refinements-to-beam-searchIn the last video, you saw the basic beam search algorithm. In this video, you’ll learn some little changes that make it work even better. Length normalization is a small change to the beam search algorithm that can help you get much better results. Here’s what it is. Beam search is maximizing this probability. And this product here is just expressing the observation that P(y1) up to y(Ty), given x, can be expressed as P(y1) given x times P(y2), given x and y1 times dot dot dot, up to I guess p of y Ty given x and y1 up to y t1-1. Maybe this notation is a bit more scary and more intimidating than it needs to be, but this is that probabilities that you see previously. Now, if you’re implementing these, these probabilities are all numbers less than 1. Often they’re much less than 1. And multiplying a lot of numbers less than 1 will result in a tiny, tiny, tiny number, which can result in numerical underflow. Meaning that it’s too small for the floating part representation in your computer to store accurately. So in practice, instead of maximizing this product, we will take logs. And if you insert a log there, then log of a product becomes a sum of a log, and maximizing this sum of log probabilities should give you the same results in terms of selecting the most likely sentence y. So by taking logs, you end up with a more numerically stable algorithm that is less prone to rounding errors, numerical rounding errors, or to really numerical underflow. And because the log function, that’s the logarithmic function, this is strictly monotonically increasing function, maximizing P(y). And because the logarithmic function, here’s the log function, is a strictly monotonically increasing function, we know that maximizing log P(y) given x should give you the same result as maximizing P(y) given x. As in the same value of y that maximizes this should also maximize that. So in most implementations, you keep track of the sum of logs of the probabilities rather than the protocol of probabilities. Now, there’s one other change to this objective function that makes the machine translation algorithm work even better. Which is that, if you referred to this original objective up here, if you have a very long sentence, the probability of that sentence is going to be low, because you’re multiplying as many terms here. Lots of numbers are less than 1 to estimate the probability of that sentence. And so if you multiply all the numbers that are less than 1 together, you just tend to end up with a smaller probability. And so this objective function has an undesirable effect, that maybe it unnaturally tends to prefer very short translations. It tends to prefer very short outputs. Because the probability of a short sentence is determined just by multiplying fewer of these numbers are less than 1. And so the product would just be not quite as small. And by the way, the same thing is true for this. The log of our probability is always less than or equal to 1. You’re actually in this range of the log. So the more terms you have together, the more negative this thing becomes. So there’s one other change to the algorithm that makes it work better, which is instead of using this as the objective you’re trying to maximize, one thing you could do is normalize this by the number of words in your translation. And so this takes the average of the log of the probability of each word. And this significantly reduces the penalty for outputting longer translations. And in practice, as a heuristic instead of dividing by Ty, by the number of words in the output sentence, sometimes you use a softer approach. We have Ty to the power of alpha, where maybe alpha is equal to 0.7. So if alpha was equal to 1, then yeah, completely normalizing by length. If alpha was equal to 0, then, well, Ty to the 0 would be 1, then you’re just not normalizing at all. And this is somewhat in between full normalization, and no normalization, and alpha’s another hyper parameter you have within that you can tune to try to get the best results. And have to admit, using alpha this way, this is a heuristic or this is a hack. There isn’t a great theoretical justification for it, but people have found this works well. People have found that it works well in practice, so many groups will do this. And you can try out different values of alpha and see which one gives you the best result. So just to wrap up how you run beam search, as you run beam search you see a lot of sentences with length equal 1, a lot of sentences with length equal 2, a lot of sentences with length equals 3. And so on, and maybe you run beam search for 30 steps and you consider output sentences up to length 30, let’s say. And so with beam with a 3, you will be keeping track of the top three possibilities for each of these possible sentence lengths, 1, 2, 3, 4 and so on, up to 30. Then, you would look at all of the output sentences and score them against this score. And so you can take your top sentences and just compute this objective function onto sentences that you have seen through the beam search process. And then finally, of all of these sentences that you validate this way, you pick the one that achieves the highest value on this normalized log probability objective. Sometimes it’s called a normalized log likelihood objective. And then that would be the final translation, your outputs. So that’s how you implement beam search, and you get to play this yourself in this week’s problem exercise. Finally, a few implementational details, how do you choose the beam width B? The larger B is, the more possibilities you’re considering, and does the better the sentence you probably find. But the larger B is, the more computationally expensive your algorithm is, because you’re also keeping a lot more possibilities around. All right, so finally, let’s just wrap up with some thoughts on how to choose the beam width B. So here are the pros and cons of setting B to be very large versus very small. If the beam width is very large, then you consider a lot of possibilities, and so you tend to get a better result because you are consuming a lot of different options, but it will be slower. And the memory requirements will also grow, will also be compositionally slower. Whereas if you use a very small beam width, then you get a worse result because you’re just keeping less possibilities in mind as the algorithm is running. But you get a result faster and the memory requirements will also be lower. So in the previous video, we used in our running example a beam width of three, so we’re keeping three possibilities in mind. In practice, that is on the small side. In production systems, it’s not uncommon to see a beam width maybe around 10, and I think beam width of 100 would be considered very large for a production system, depending on the application. But for research systems where people want to squeeze out every last drop of performance in order to publish the paper with the best possible result. It’s not uncommon to see people use beam widths of 1,000 or 3,000, but this is very application, that’s why it’s a domain dependent. So I would say try other variety of values of B as you work through your application. But when B gets very large, there is often diminishing returns. So for many applications, I would expect to see a huge gain as you go from a beam widht of 1, which is very greedy search, to 3, to maybe 10. But the gains as you go from 1,000 to 3,000 in beam width might not be as big. And for those of you that have taken maybe a lot of computer science courses before, if you’re familiar with computer science search algorithms like BFS, Breadth First Search, or DFS, Depth First Search. The way to think about beam search is that, unlike those other algorithms which you have learned about in a computer science algorithms course, and don’t worry about it if you’ve not heard of these algorithms. But if you’ve heard of Breadth First Search and Depth First Search then unlike those algorithms, which are exact search algorithms. Beam search runs much faster but does not guarantee to find the exact maximum for this argmax that you would like to find. If you haven’t heard of breadth first search or depth first search, don’t worry about it, it’s not important for our purposes. But if you have, this is how beam search relates to those algorithms. So that’s it for beam search, which is a widely used algorithm in many production systems, or in many commercial systems. Now, in the circles in the sequence of courses of deep learning, we talked a lot about error analysis. It turns out, one of the most useful tools I’ve found is to be able to do error analysis on beam search. So you sometimes wonder, should I increase my beam width? Is my beam width working well enough? And there’s some simple things you can compute to give you guidance on whether you need to work on improving your search algorithm. Let’s talk about that in the next video. 05_error-analysis-in-beam-searchIn the third course of this sequence of five courses, you saw how error analysis can help you focus your time on doing the most useful work for your project. Now, beam search is an approximate search algorithm, also called a heuristic search algorithm. And so it doesn’t always output the most likely sentence. It’s only keeping track of B equals 3 or 10 or 100 top possibilities. So what if beam search makes a mistake? In this video, you’ll learn how error analysis interacts with beam search and how you can figure out whether it is the beam search algorithm that’s causing problems and worth spending time on. Or whether it might be your RNN model that is causing problems and worth spending time on. Let’s take a look at how to do error analysis with beam search. Let’s use this example of Jane visite l’Afrique en septembre. So let’s say that in your machine translation dev set, your development set, the human provided this translation and Jane visits Africa in September, and I’m going to call this y. So it is a pretty good translation written by a human. Then let’s say that when you run beam search on your learned RNN model and your learned translation model, it ends up with this translation, which we will call y-hat, Jane visited Africa last September, which is a much worse translation of the French sentence. It actually changes the meaning, so it’s not a good translation. Now, your model has two main components. There is a neural network model, the sequence to sequence model. We shall just call this your RNN model. It’s really an encoder and a decoder. And you have your beam search algorithm, which you’re running with some beam width b. And wouldn’t it be nice if you could attribute this error, this not very good translation, to one of these two components? Was it the RNN or really the neural network that is more to blame, or is it the beam search algorithm, that is more to blame? And what you saw in the third course of the sequence is that it’s always tempting to collect more training data that never hurts. So in similar way, it’s always tempting to increase the beam width that never hurts or pretty much never hurts. But just as getting more training data by itself might not get you to the level of performance you want. In the same way, increasing the beam width by itself might not get you to where you want to go. But how do you decide whether or not improving the search algorithm is a good use of your time? So just how you can break the problem down and figure out what’s actually a good use of your time. Now, the RNN, the neural network, what was called RNN really means the encoder and the decoder. It computes P(y given x). So for example, for a sentence, Jane visits Africa in September, you plug in Jane visits Africa. Again, I’m ignoring upper versus lowercase now, right, and so on. And this computes P(y given x). So it turns out that the most useful thing for you to do at this point is to compute using this model to compute P(y given x) as well as to compute P(y-hat given x) using your RNN model. And then to see which of these two is bigger. So it’s possible that the left side is bigger than the right hand side. It’s also possible that P(y*) is less than P(y-hat) actually, or less than or equal to, right? Depending on which of these two cases hold true, you’d be able to more clearly ascribe this particular error, this particular bad translation to one of the RNN or the beam search algorithm being had greater fault. So let’s take out the logic behind this. Here are the two sentences from the previous slide. And remember, we’re going to compute P(y given x) and P(y-hat given x) and see which of these two is bigger. So there are going to be two cases. In case 1, P(y given x) as output by the RNN model is greater than P(y-hat given x). What does this mean? Well, the beam search algorithm chose y-hat, right? The way you got y-hat was you had an RNN that was computing P(y given x). And beam search’s job was to try to find a value of y that gives that arg max. But in this case, y actually attains a higher value for P(y given x) than the y-hat. So what this allows you to conclude is beam search is failing to actually give you the value of y that maximizes P(y given x) because the one job that beam search had was to find the value of y that makes this really big. But it chose y-hat, the y actually gets a much bigger value. So in this case, you could conclude that beam search is at fault. Now, how about the other case? In case 2, P(y given x) is less than or equal to P(y-hat given x), right? And then either this or this has gotta be true. So either case 1 or case 2 has to hold true. What do you conclude under case 2? Well, in our example, y is a better translation than y-hat. But according to the RNN, P(y) is less than P(y-hat), so saying that y is a less likely output than y-hat. So in this case, it seems that the RNN model is at fault and it might be worth spending more time working on the RNN. There’s some subtleties here pertaining to length normalizations that I’m glossing over. There’s some subtleties pertaining to length normalizations that I’m glossing over. And if you are using some sort of length normalization, instead of evaluating these probabilities, you should be evaluating the optimization objective that takes into account length normalization. But ignoring that complication for now, in this case, what this tells you is that even though y is a better translation, the RNN ascribed y in lower probability than the inferior translation. So in this case, I will say the RNN model is at fault. So the error analysis process looks as follows. You go through the development set and find the mistakes that the algorithm made in the development set. And so in this example, let’s say that P(y given x) was 2 x 10 to the -10, whereas, P(y-hat given x) was 1 x 10 to the -10. Using the logic from the previous slide, in this case, we see that beam search actually chose y-hat, which has a lower probability than y. So I will say beam search is at fault. So I’ll abbreviate that B. And then you go through a second mistake or second bad output by the algorithm, look at these probabilities. And maybe for the second example, you think the model is at fault. I’m going to abbreviate the RNN model with R. And you go through more examples. And sometimes the beam search is at fault, sometimes the model is at fault, and so on. And through this process, you can then carry out error analysis to figure out what fraction of errors are due to beam search versus the RNN model. And with an error analysis process like this, for every example in your dev sets, where the algorithm gives a much worse output than the human translation, you can try to ascribe the error to either the search algorithm or to the objective function, or to the RNN model that generates the objective function that beam search is supposed to be maximizing. And through this, you can try to figure out which of these two components is responsible for more errors. And only if you find that beam search is responsible for a lot of errors, then maybe is we’re working hard to increase the beam width. Whereas in contrast, if you find that the RNN model is at fault, then you could do a deeper layer of analysis to try to figure out if you want to add regularization, or get more training data, or try a different network architecture, or something else. And so a lot of the techniques that you saw in the third course in the sequence will be applicable there. So that’s it for error analysis using beam search. I found this particular error analysis process very useful whenever you have an approximate optimization algorithm, such as beam search that is working to optimize some sort of objective, some sort of cost function that is output by a learning algorithm, such as a sequence-to-sequence model or a sequence-to-sequence RNN that we’ve been discussing in these lectures. So with that, I hope that you’ll be more efficient at making these types of models work well for your applications. 06_bleu-score-optionalOne of the challenges of machine translation is that, given a French sentence, there could be multiple English translations that are equally good translations of that French sentence. So how do you evaluate a machine translation system if there are multiple equally good answers, unlike, say, image recognition where there’s one right answer? You just measure accuracy. If there are multiple great answers, how do you measure accuracy? The way this is done conventionally is through something called the BLEU score. So, in this optional video, I want to share with you, I want to give you a sense of how the BLEU score works. Let’s say you are given a French sentence Le chat est sur le tapis. And you are given a reference, human generated translation of this, which is the the cat is on the mat. But there are multiple, pretty good translations of this. So a different human, different person might translate it as there is a cat on the mat. And both of these are actually just perfectly fine translations of the French sentence. What the BLEU score does is given a machine generated translation, it allows you to automatically compute a score that measures how good is that machine translation. And the intuition is so long as the machine generated translation is pretty close to any of the references provided by humans, then it will get a high BLEU score. BLEU, by the way, stands for bilingual evaluation, Understudy. So in the theater world, an understudy is someone that learns the role of a more senior actor so they can take over the role of the more senior actor, if necessary. And motivation for BLEU is that, whereas you could ask human evaluators to evaluate the machine translation system, the BLEU score is an understudy, could be a substitute for having humans evaluate every output of a machine translation system. So the BLEU score was due to Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. This paper has been incredibly influential, and is, actually, quite a readable paper. So I encourage you to take a look if you have time. So, the intuition behind the BLEU score is we’re going to look at the machine generated output and see if the types of words it generates appear in at least one of the human generated references. And so these human generated references would be provided as part of the depth set or as part of the test set. Now, let’s look at a somewhat extreme example. Let’s say that the machine translation system abbreviating machine translation is MT. So the machine translation, or the MT output, is the the the the the the the. So this is clearly a pretty terrible translation. So one way to measure how good the machine translation output is, is to look at each the words in the output and see if it appears in the references. And so, this would be called a precision of the machine translation output. And in this case, there are seven words in the machine translation output. And every one of these 7 words appears in either Reference 1 or Reference 2, right? So the word the appears in both references. So each of these words looks like a pretty good word to include. So this will have a precision of 7 over 7. It looks like it was a great precision. So this is why the basic precision measure of what fraction of the words in the MT output also appear in the references. This is not a particularly useful measure, because it seems to imply that this MT output has very high precision. So instead, what we’re going to use is a modified precision measure in which we will give each word credit only up to the maximum number of times it appears in the reference sentences. So in Reference 1, the word, the, appears twice. In Reference 2, the word, the, appears just once. So 2 is bigger than 1, and so we’re going to say that the word, the, gets credit up to twice. So, with a modified precision, we will say that, it gets a score of 2 out of 7, because out of 7 words, we’ll give it a 2 credits for appearing. So here, the denominator is the count of the number of times the word, the, appears of 7 words in total. And the numerator is the count of the number of times the word, the, appears. We clip this count, we take a max, or we clip this count, at 2. So this gives us the modified precision measure. Now, so far, we’ve been looking at words in isolation. In the BLEU score, you don’t want to just look at isolated words. You maybe want to look at pairs of words as well. Let’s define a portion of the BLEU score on bigrams. And bigrams just means pairs of words appearing next to each other. So now, let’s see how we could use bigrams to define the BLEU score. And this will just be a portion of the final BLEU score. And we’ll take unigrams, or single words, as well as bigrams, which means pairs of words into account as well as maybe even longer sequences of words, such as trigrams, which means three words pairing together. So, let’s continue our example from before. We have to same Reference 1 and Reference 2. But now let’s say the machine translation or the MT System has a slightly better output. The cat the cat on the mat. Still not a great translation, but maybe better than the last one. So here, the possible bigrams are, well there’s the cat, but ignore case. And then there’s cat the, that’s another bigram. And then there’s the cat again, but I’ve already had that, so let’s skip that. And then cat on is the next one. And then on the, and the mat. So these are the bigrams in the machine translation output. And so let’s count up, How many times each of these bigrams appear. The cat appears twice, cat the appears once, and the others all appear just once. And then finally, let’s define the clipped count, so count, and then subscript clip. And to define that, let’s take this column of numbers, but give our algorithm credit only up to the maximum number of times that that bigram appears in either Reference 1 or Reference 2. So the cat appears a maximum of once in either of the references. So I’m going to clip that count to 1. Cat the, well, it doesn’t appear in Reference 1 or Reference 2, so I clip that to 0. Cat on, yep, that appears once. We give it credit for once. On the appears once, give that credit for once, and the mat appears once. So these are the clipped counts. We’re taking all the counts and clipping them, really reducing them to be no more than the number of times that bigram appears in at least one of the references. And then, finally, our modified bigram precision will be the sum of the count clipped. So that’s 1, 2, 3, 4 divided by the total number of bigrams. That’s 2, 3, 4, 5, 6, so 4 out of 6 or two-thirds is the modified precision on bigrams. So let’s just formalize this a little bit further. With what we had developed on unigrams, we defined this modified precision computed on unigrams as P subscript 1. The P stands for precision and the subscript 1 here means that we’re referring to unigrams. But that is defined as sum over the unigrams. So that just means sum over the words that appear in the machine translation output. So this is called y hat of count clip, Of that unigram. Divided by sum of our unigrams in the machine translation output of count, number of counts of that unigram, right? And so this is what we had gotten I guess is 2 out of 7, 2 slides back. So the 1 here refers to unigram, meaning we’re looking at single words in isolation. You can also define Pn as the n-gram version, Instead of unigram, for n-gram. So this would be sum over the n-grams in the machine translation output of count clip of that n-gram divided by sum over n-grams of the count of that n-gram. And so these precisions, or these modified precision scores, measured on unigrams or on bigrams, which we did on a previous slide, or on trigrams, which are triples of words, or even higher values of n for other n-grams. This allows you to measure the degree to which the machine translation output is similar or maybe overlaps with the references. And one thing that you could probably convince yourself of is if the MT output is exactly the same as either Reference 1 or Reference 2, then all of these values P1, and P2 and so on, they’ll all be equal to 1.0. So to get a modified precision of 1.0, you just have to be exactly equal to one of the references. And sometimes it’s possible to achieve this even if you aren’t exactly the same as any of the references. But you kind of combine them in a way that hopefully still results in a good translation. Finally, Finally, let’s put this together to form the final BLEU score. So P subscript n is the BLEU score computed on n-grams only. Also the modified precision computed on n-grams only. And by convention to compute one number, you compute P1, P2, P3 and P4, and combine them together using the following formula. It’s going to be the average, so sum from n = 1 to 4 of Pn and divide that by 4. So basically taking the average. By convention the BLEU score is defined as, e to the this, then exponentiations, and linear operate, exponentiation is strictly monotonically increasing operation and then we actually adjust this with one more factor called the, BP penalty. So BP, Stands for brevity penalty. The details maybe aren’t super important. But to just give you a sense, it turns out that if you output very short translations, it’s easier to get high precision. Because probably most of the words you output appear in the references. But we don’t want translations that are very short. So the BP, or the brevity penalty, is an adjustment factor that penalizes translation systems that output translations that are too short. So the formula for the brevity penalty is the following. $${P_n}{\rm{ = }}\frac{{\sum\limits_{n - gram \in \widehat y} {Coun{t_{clip}}(n - gram)} }}{{\sum\limits_{n - gram \in \widehat y} {Count(n - gram)} }}$$ $$BP \exp(\dfrac{1}{4}\sum_{n=1}^{4}P_{n})$$ $$BP = \left\{ \begin{array}{l} 1, if{\kern 1pt} {\kern 1pt} MT\_length > reference\_length{\kern 1pt} {\kern 1pt} \\ \exp (1 - MT\_length/reference\_length), otherwise \end{array} \right.$$ It’s equal to 1 if your machine translation system actually outputs things that are longer than the human generated reference outputs. And otherwise is some formula like that that overall penalizes shorter translations. So, in the details you can find in this paper. So, once again, earlier in this set of courses, you saw the importance of having a single real number evaluation metric. Because it allows you to try out two ideas, see which one achieves a higher score, and then try to stick with the one that achieved the higher score. So the reason the BLEU score was revolutionary for machine translation was because this gave a pretty good, by no means perfect, but pretty good single real number evaluation metric. And so that accelerated the progress of the entire field of machine translation. I hope this video gave you a sense of how the BLEU score works. In practice, few people would implement a BLEU score from scratch. There are open source implementations that you can download and just use to evaluate your own system. But today, BLEU score is used to evaluate many systems that generate text, such as machine translation systems, as well as the example I showed briefly earlier of image captioning systems where you would have a system, have a neural network generated image caption. And then use the BLEU score to see how much that overlaps with maybe a reference caption or multiple reference captions that were generated by people. So the BLEU score is a useful single real number evaluation metric to use whenever you want your algorithm to generate a piece of text. And you want to see whether it has similar meaning as a reference piece of text generated by humans. This is not used for speech recognition, because in speech recognition, there’s usually one ground truth. And you just use other measures to see if you got the speech transcription on pretty much, exactly word for word correct. But for things like image captioning, and multiple captions for a picture, it could be about equally good or for machine translations. There are multiple translations, but equally good. The BLEU score gives you a way to evaluate that automatically and therefore speed up your development. So with that, I hope you have a sense of how the BLEU score works. 07_attention-model-intuitionFor most of this week, you’ve been using a Encoder-Decoder architecture for machine translation. Where one RNN reads in a sentence and then different one outputs a sentence. There’s a modification to this called the Attention Model, that makes all this work much better. The attention algorithm, the attention idea has been one of the most influential ideas in deep learning. Let’s take a look at how that works. Get a very long French sentence like this. What we are asking this green encoder in your network to do is, to read in the whole sentence and then memorize the whole sentences and store it in the activations conveyed here. Then for the purple network, the decoder network till then generate the English translation. Jane went to Africa last September and enjoyed the culture and met many wonderful people; she came back raving about how wonderful her trip was, and is tempting me to go too. Now, the way a human translator would translate this sentence is not to first read the whole French sentence and then memorize the whole thing and then regurgitate an English sentence from scratch. Instead, what the human translator would do is read the first part of it, maybe generate part of the translation. Look at the second part, generate a few more words, look at a few more words, generate a few more words and so on. You kind of work part by part through the sentence, because it’s just really difficult to memorize the whole long sentence like that. What you see for the Encoder-Decoder architecture above is that, it works quite well for short sentences, so we might achieve a relatively high Bleu score, but for very long sentences, maybe longer than 30 or 40 words, the performance comes down. The Bleu score might look like this as the sentence that varies and short sentences are just hard to translate, hard to get all the words, right? Long sentences, it doesn’t do well on because it’s just difficult to get in your network to memorize a super long sentence. In this and the next video, you’ll see the Attention Model which translates maybe a bit more like humans might, looking at part of the sentence at a time and with an Attention Model, machine translation systems performance can look like this, because by working one part of the sentence at a time, you don’t see this huge dip which is really measuring the ability of a neural network to memorize a long sentence which maybe isn’t what we most badly need a neural network to do. In this video, I want to just give you some intuition about how attention works and then we’ll flesh out the details in the next video. The Attention Model was due to Dimitri, Bahdanau, Camcrun Cho, Yoshe Bengio and even though it was obviously developed for machine translation, it spread to many other application areas as well. This is really a very influential, I think very seminal paper in the deep learning literature. Let’s illustrate this with a short sentence, even though these ideas were maybe developed more for long sentences, but it’ll be easier to illustrate these ideas with a simpler example. We have our usual sentence, Jane visite l’Afrique en Septembre. Let’s say that we use a R and N, and in this case, I’m going to use a bidirectional R and N, in order to compute some set of features for each of the input words and you have to understand it, bidirectional RNN with outputs Y1 to Y3 and so on up to Y5 but we’re not doing a word for word translation, let me get rid of the Y’s on top. But using a bidirectional R and N, what we’ve done is for each other words, really for each of the five positions into sentence, you can compute a very rich set of features about the words in the sentence and maybe surrounding words in every position. Now, let’s go ahead and generate the English translation. We’re going to use another RNN to generate the English translations. Here’s my RNN note as usual and instead of using A to denote the activation, in order to avoid confusion with the activations down here, I’m just going to use a different notation, I’m going to use S to denote the hidden state in this RNN up here, so instead of writing A1 I’m going to right S1 and so we hope in this model that the first word it generates will be Jane, to generate Jane visits Africa in September. Now, the question is, when you’re trying to generate this first word, this output, what part of the input French sentence should you be looking at? Seems like you should be looking primarily at this first word, maybe a few other words close by, but you don’t need to be looking way at the end of the sentence. What the Attention Model would be computing is a set of attention weights and we’re going to use $\alpha^{&lt;1, 1&gt;}$ to denote when you’re generating the first words, how much should you be paying attention to this first piece of information here. And then we’ll also come up with a second that’s called Attention Weight, $\alpha^{&lt;1, 2&gt;}$ which tells us what we’re trying to compute the first work of Jane, how much attention we’re paying to this second work from the inputs and so on and the $\alpha^{&lt;1, 3&gt;}$ and so on, and together this will tell us what is exactly the context from denoter C that we should be paying attention to, and that is input to this RNN unit to then try to generate the first words. That’s one step of the R and N, we will flesh out all these details in the next video. For the second step of this R and N, we’re going to have a new hidden state S two and we’re going to have a new set of the attention weights. We’re going to have $\alpha^{&lt;2, 1&gt;}$ to tell us when we generate in the second word. I guess this will be visits maybe that being the ground trip label. How much should we paying attention to the first word in the french input and also, $\alpha^{&lt;2, 2&gt;}$ and so on. How much should we paying attention the word visite, how much should we pay attention to the free and so on. And of course, the first word we generate in Jane is also an input to this, and then we have some context that we’re paying attention to and the second step, there’s also an input and that together will generate the second word and that leads us to the third step, S three, where this is an input and we have some new context C that depends on the various $\alpha^{&lt;3, t&gt;}$ for the different time sets, that tells us how much should we be paying attention to the different words from the input French sentence and so on. So, some things I haven’t specified yet, but that will go further into detail in the next video of this, how exactly this context defines and the goal of the context is for the third word is really should capture that maybe we should be looking around this part of the sentence. The formula you use to do that will defer to the next video as well as how do you compute these attention weights. And you see in the next video that $\alpha^{&lt;3, t&gt;}$, which is, when you’re trying to generate the third word, I guess this would be the Africa, just getting the right output. The amounts that this RNN step should be paying attention to the French word that time T, that depends on the activations of the bidirectional RNN at time T, I guess it depends on the fourth activations and the, backward activations at time T and it will depend on the state from the previous steps, it will depend on S two, and these things together will influence, how much you pay attention to a specific word in the input French sentence. But we’ll flesh out all these details in the next video. But the key intuition to take away is that this way the RNN marches forward generating one word at a time, until eventually it generates maybe the EOS and at every step, there are these attention weighs. $\alpha^{&lt;t, t’&gt;}$ that tells it, when you’re trying to generate the T, English word, how much should you be paying attention to the T prime French words.And this allows it on every time step to look only maybe within a local window of the French sentence to pay attention to, when generating a specific English word. I hope this video conveys some intuition about Attention Model and that we now have a rough sense of, maybe how the algorithm works. Let’s go to the next video to flesh out the details of the Attention Model. 08_attention-modelIn the last video, you saw how the attention model allows a neural network to pay attention to only part of an input sentence while it’s generating a translation, much like a human translator might. Let’s now formalize that intuition into the exact details of how you would implement an attention model. So same as in the previous video, let’s assume you have an input sentence and you use a bidirectional RNN, or bidirectional GRU, or bidirectional LSTM to compute features on every word. In practice, GRUs and LSTMs are often used for this, with maybe LSTMs be more common. And so for the forward occurrence, you have a forward occurrence first time step. Activation backward occurrence, first time step. Activation forward occurrence, second time step. Activation backward and so on. For all of them in just a forward fifth time step a backwards fifth time step. We had a zero here technically we can also have I guess a backwards sixth as a factor of all zero, actually that’s a factor of all zeroes. And then to simplify the notation going forwards at every time step, even though you have the features computed from the forward occurrence and from the backward occurrence in the bidirectional RNN. I’m just going to use $a^{t’}$ to represent both of these concatenated together, $a^{&lt;t^{&lt;\prime&gt;}&gt;}=({\overrightarrow a^{&lt;t^{\prime}&gt;}},{\overleftarrow a^{&lt;t^{\prime}&gt;}})$. So a of t is going to be a feature vector for time step t. Although to be consistent with notation, we’re using second, I’m going to call this $t^\prime$. Actually, I’m going to use $t^{\prime}$ to index into the words in the French sentence. Next, we have our forward only, so it’s a single direction RNN with state s to generate the translation. And so the first time step, it should generate $y^{}$ and just will have as input some context C. And if you want to index it with time I guess you could write a $C^{}$ but sometimes I just right C without the superscript one. And this will depend on the attention parameters so $\alpha^{&lt;1,1&gt;}$, $\alpha^{&lt;1,2&gt;}$ and so on tells us how much attention. And so these alpha parameters tells us how much the context would depend on the features we’re getting or the activations we’re getting from the different time steps. And so the way we define the context is actually be a way to some of the features from the different time steps weighted by these attention weights. So more formally the attention weights will satisfy this that they are all be non-negative, so it will be a zero positive and they’ll sum to one. We’ll see later how to make sure this is true. And we will have the context or the context at time one often drop that superscript that’s going to be sum over $t^{\prime}$, all the values of $t^{\prime}$ of this weighted sum of these activations $c^{} = \sum\alpha^{&lt;1, t^{\prime}&gt;}a^{&lt;t^{\prime}&gt;}$. So this term, $\alpha^{&lt;1, t^{\prime}&gt;}$, here are the attention weights and this term, $a^{&lt;t^{\prime}&gt;}$, here comes from here $a^{&lt;t^{\prime}&gt;}=({\overrightarrow a^{&lt;t^{\prime}&gt;}},{\overleftarrow a ^{&lt;t^{\prime}&gt;}})$. So $\alpha^{&lt;t, t^{\prime}&gt;}$ is the amount of attention that’s $y^t$ should pay to $a^{t^{\prime}}$. So in other words, when you’re generating the t of the output words, how much you should be paying attention to the $t^{\prime}$th input to word. So that’s one step of generating the output and then at the next time step, you generate the second output and is again done some of where now you have a new set of attention weights on they to find a new way to sum. That generates a new context. This, $y^{}$, is also input and that allows you to generate the second word. Only now just this way to sum becomes the context of the second time step is $c^{} = \sum\alpha^{&lt;2, t^{\prime}&gt;}a^{&lt;t^{\prime}&gt;}$. So using these context vectors. $c^{}$ right there back, $c^{}$, and so on. This network uo here, which circled in purple color, here looks like a pretty standard RNN sequence with the context vectors as output and we can just generate the translation one word at a time. We have also define how to compute the context vectors in terms of these attention ways and those features of the input sentence. So the only remaining thing to do is to define how to actually compute these attention weights. Let’s do that on the next slide. So just to recap, $\alpha^{&lt;t, t^{\prime}&gt;}$ is the amount of attention you should paid to $a^{&lt;t^{\prime}&gt;}$ when you’re trying to generate the $t^{th}$ words in the output translation. So let me just write down the formula and we talk of how this works. This is formula you could use the compute $\alpha^{&lt;t, t^{\prime}&gt;}$ which is going to compute these terms $e^{&lt;t, t^{\prime}&gt;}$ and then use essentially a softmax to make sure that these weights sum to one if you sum over $t^{\prime}$. So for every fix value of t, these things, ${\alpha^{}} =\frac{{\exp({e^{}})}}{{\sum\limits_{t^{\prime} = 1}^{{T_x}} {\exp({e^{}})}}}$ , sum to one if you’re summing over $t^{\prime}$. And using this softmax prioritization, just ensures this properly sums to one. Now how do we compute these factors e. Well, one way to do so is to use a small neural network as follows. So $s^{}$ was the neural network state from the previous time step. So here is the network we have.If you’re trying to generate $y^t$ then $s^{}$ was the hidden state from the previous step that just fell into $s^t$ and that’s one input to very small neural network. Usually, one hidden layer in neural network because you need to compute these a lot. And then $a^{&lt;t^{\prime}&gt;}$ the features from time step $t^{\prime}$ is the other inputs. And the intuition is, if you want to decide how much attention to pay to the activation of $t^{\prime}$. Well, the things that seems like it should depend the most on is what is your own hidden state activation from the previous time step. You don’t have the current state activation yet because of context feeds into this so you haven’t computed that. But look at whatever you’re hidden stages of this RNN generating the upper translation and then for each of the positions, each of the words look at their features. So it seems pretty natural that $\alpha^{&lt;t, t^{\prime}&gt;}$ and $e^{&lt;t, t^{\prime}&gt;}$ should depend on these two quantities. But we don’t know what the function is. So one thing you could do is just train a very small neural network to learn whatever this function should be. And trust that back propagation trust gradient descent to learn the right function. And it turns out that if you implemented this whole model and train it with gradient descent, the whole thing actually works. This little neural network does a pretty decent job telling you how much attention $y^t$ should pay to $a^{&lt;t^{\prime}&gt;}$and this formula ${\alpha^{}} =\frac{{\exp({e^{}})}}{{\sum\limits_{t^{\prime} = 1}^{{T_x}} {\exp({e^{}})}}}$ makes sure that the attention weights sum to one and then as you chug along generating one word at a time, this neural network actually pays attention to the right parts of the input sentence that learns all this automatically using gradient descent.Now, one downside to this algorithm is that it does take quadratic time or quadratic cost to run this algorithm. If you have $T_x$ words in the input and $T_y$ words in the output then the total number of these attention parameters are going to be $T_x$ times $T_y$. And so this algorithm runs in quadratic cost. Although in machine translation applications where neither input nor output sentences is usually that long maybe quadratic cost is actually acceptable. Although, there is some research work on trying to reduce costs as well. Now, so far up in describing the attention idea in the context of machine translation. Without going too much into detail this idea has been applied to other problems as well. So just image captioning. So in the image capturing problem the task is to look at the picture and write a caption for that picture. So in this paper set to the bottom by Kevin Chu, Jimmy Barr, Ryan Kiros, Kelvin Shaw, Aaron Korver, Russell Zarkutnov, Virta Zemo, and Andrew Benjo they also showed that you could have a very similar architecture. Look at the picture and pay attention only to parts of the picture at a time while you’re writing a caption for a picture. So if you’re interested, then I encourage you to take a look at that paper as well. And you get to play with all this and more in the programming exercise.Whereas machine translation is a very complicated problem in the prior exercise you get to implement and play of the attention while you yourself for the date normalization problem. So the problem inputting a date like this. This actually has a date of the Apollo Moon landing and normalizing it into standard formats or a date like this and having a neural network a sequence, sequence model normalize it to this format. This by the way is the birthday of William Shakespeare. Also it’s believed to be. And what you see in prior exercises as you can train a neural network to input dates in any of these formats and have it use an attention model to generate a normalized format for these dates. One other thing that sometimes fun to do is to look at the visualizations of the attention weights. So here’s a machine translation example and here were plotted in different colors. the magnitude of the different attention weights. I don’t want to spend too much time on this but you find that the corresponding input and output words you find that the attention weights will tend to be high. Thus, suggesting that when it’s generating a specific word in output is, usually paying attention to the correct words in the input and all this including learning where to pay attention when was all learned using propagation with an attention model. So that’s it for the attention model really one of the most powerful ideas in deep learning. I hope you enjoy implementing and playing with these ideas yourself later in this week’s programming exercises. 02_speech-recognition-audio-data01_speech-recognitionOne of the most exciting developments were sequence-to-sequence models has been the rise of very accurate speech recognition. We’re nearing the end of the course, we want to take just a couple of videos to give you a sense of how these sequence-to-sequence models are applied to audio data, such as the speech. So, what is the speech recognition problem? You’re given an audio clip, x, and your job is to automatically find a text transcript, y. So, an audio clip, if you plot it looks like this, the horizontal axis here is time, and what a microphone does is it really measures minuscule changes in air pressure, and the way you’re hearing my voice right now is that your ear is detecting little changes in air pressure, probably generated either by your speakers or by a headset. And some audio clips like this plots with the air pressure against time. And, if this audio clip is of me saying, “the quick brown fox”, then hopefully, a speech recognition algorithm can input that audio clip and output that transcript. And because even the human ear doesn’t process raw wave forms, but the human ear has physical structures that measures the amounts of intensity of different frequencies, there is, a common pre-processing step for audio data is to run your raw audio clip and generate a spectrogram. So, this is the plots where the horizontal axis is time, and the vertical axis is frequencies, and intensity of different colors shows the amount of energy. So, how loud is the sound at different frequencies? At different times? And so, these types of spectrograms, or you might also hear people talk about false blank outputs, is often commonly applied pre-processing step before audio is pass into in the running algorithm. And the human ear does a computation pretty similar to this pre-processing step. So, one of the most exciting trends in speech recognition is that, once upon a time, speech recognition systems used to be built using phonemes and this where, I want to say hand-engineered basic units of cells. So, the quick brown fox represented as phonemes. I’m going to simplify a bit, let say, “The” has a “de” and “e” sound and Quick, has a “ku” and “wu”, “ik”, “k” sound, and linguist used to write off these basic units of sound, and try the Greek language down to these basic units of sound. So, brown, this aren’t the official phonemes which are written with more complicated notation, but linguists use to hypothesize that writing down audio in terms of these basic units of sound called phonemes would be the best way to do speech recognition. But with end-to-end deep learning, we’re finding that phonemes representations are no longer necessary. But instead, you can built systems that input an audio clip and directly output a transcript without needing to use hand-engineered representations like these. One of the things that made this possible was going to much larger data sets. So, academic data sets on speech recognition might be as a 300 hours, and in academia, 3000 hour data sets of transcribed audio would be considered reasonable size, so lot of research has been done, a lot of research papers that are written on data sets there are several thousand voice. But, the best commercial systems are now trains on over 10,000 hours and sometimes over a 100,000 hours of audio. And, it’s really moving to a much larger audio data sets, transcribe audio data sets were both x and y, together with deep learning algorithm, that has driven a lot of progress is speech recognition. So, how do you build a speech recognition system? In the last video, we’re talking about the attention model. So, one thing you could do is actually do that, where on the horizontal axis, you take in different time frames of the audio input, and then you have an attention model try to output the transcript like, “the quick brown fox”, or what it was said. One other method that seems to work well is to use the CTC cost for speech recognition. CTC stands for Connection is Temporal Classification and is due to Alex Graves, Santiago Fernandes, Faustino Gomez, and Jürgen Schmidhuber. So, here’s the idea. Let’s say the audio clip was someone saying, “the quick brown fox”. We’re going to use a new network structured like this with an equal number of input x’s and output y’s, and I have drawn a simple of what uni-directional for the RNN for this, but in practice, this will usually be a bidirectional LSTM and bidirectional GRU and usually, a deeper model. But notice that the number of time steps here is very large and in speech recognition, usually the number of input time steps is much bigger than the number of output time steps. So, for example, if you have 10 seconds of audio and your features come at a 100 hertz so 100 samples per second, then a 10 second audio clip would end up with a thousand inputs. Right, so it’s 100 hertz times 10 seconds, and so with a thousand inputs. But your output might not have a thousand alphabets, might not have a thousand characters. So, what do you do? The CTC cost function allows the RNN to generate an output like this ttt, there’s a special character called the blank character, which we’re going to write as an underscore here, h_eee___, and then maybe a space, we’re going to write like this, so that a space and then _ qqq. And, this is considered a correct output for the first parts of the space, quick with the Q, and the basic rule for the CTC cost function is to collapse repeated characters not separated by “blank”. So, to be clear, I’m using this underscore to denote a special blank character and that’s different than the space character. So, there is a space here between the and quick, so I should output a space. But, by collapsing repeated characters, not separated by blank, it actually collapse the sequence into t, h, e, and then space, and q, and this allows your network to have a thousand outputs by repeating characters allow the times. So, inserting a bunch of blank characters and still ends up with a much shorter output text transcript. So, this phrase here “the quick brown fox” including spaces actually has 19 characters, and if somehow, the newer network is forced upwards of a thousand characters by allowing the network to insert blanks and repeated characters and can still represent this 19 character upwards with this 1000 outputs of values of Y. So, this paper by Alex Grace, as well as by those deep speech recognition system, which I was involved in, used this idea to build effective Speech recognition systems. So, I hope that gives you a rough sense of how speech recognition models work. Attention like models work and CTC models work and present two different options of how to go about building these systems. Now, today, building effective where production skills speech recognition system is a pretty significant effort and requires a very large data set. But, what I like to do in the next video is share you, how you can build a trigger word detection system, where keyword detection system which is actually much easier and can be done with even a smaller or more reasonable amount of data. So, let’s talk about that in the next video. 02_trigger-word-detectionyou’ve now learned so much about deep learning and sequence models that we can actually describe a trigger word system quite simply just on one slide as you see in this video but when the rise of speech recognition have been more and more devices you can wake up with your voice and those are sometimes called trigger word detection systems so let’s see how you can build a trigger word system. Examples of triggering systems include Amazon echo which is broken out with that word Alexa. The Baidu DuerOs part devices woken up with face xiaodunihao. Apple Siri working out with hey Siri and Google home woken up with Ok Google. So stands the trigger word detection that if you have say an Amazon echo in your living room, you can walk the living room and just say: “Alexa what time is it” and have it wake up. It’ll be triggered by the words of Alexa and answer your voice query. So if you can build a trigger word detection system maybe you can make your computer do something by telling it computer activate. One of my friends also works on turning on an offer particular lamp using a trigger word kind of as a fun project but what I want to show you is how you can build a trigger word detection system. Now the trigger word detection literature is still evolving so there actually isn’t a single universally agreed on algorithm for trigger word detection yet the literature on trigger word detection algorithm is still evolving so there isn’t wide consensus yet on what’s the best algorithm for trigger word detection so I’m just going to show you one example of an algorithm you can use. now you’ve seen our ends like this and what we really do is take an audio clip maybe compute spectrogram features and that generates features $x^{} x^{} x^{}$ or audio features $x^{} x^{} x^{}$ that you pass through an RNN and so all that remains to be done is to define the target labels Y so if this point in the audio clip is when someone just finished saying the trigger word such as “Alexa”, “nihaobaidu” or “hey Siri” or “Okay Google” then in the training sets you can set the target labels to be zero for everything before that point and right after that to set the target label of one and then if a little bit later on you know the trigger word was set again and the trigger word said at this point then you can again set the target label to be one right after that now this type of labeling scheme for an RNN you know could work actually this won’t actually work reasonably well. One slight disadvantage of this is it creates a very imbalanced training set so if a lot more zeros than ones. So one other thing you could do that it’s getting a little bit of a hack but could make them all the little bit easy to train is instead of setting only a single time step to output one you can actually make an output a few ones for several times or for a fixed period of time before reverting back to zero so and that slightly evens out the ratio of ones to zeros but this is a little bit of a hack. But if this is when in the audio clipper trigger where the set then right after that you can set the target label to one and if this is the trigger words said again, then right after that just when you want the RNN to output one so you get to play more of this as well in the programming exercise but so I think you should feel quite proud of yourself we’ve learned enough about the learning that it just takes one picture at one slide to this to describe something as complicated as trigger word detection and based on this I hope you’d be able to implement something that works and allows you to detect trigger words but you see more of this in the program exercise. So that’s it for trigger words and I hope you feel quite proud of yourself for how much you’ve learned about deep learning that you can now describe trigger words in just one slide in a few minutes and that you’ve been hopeful II implemented and get it to work maybe even make it do something fun in your house that I’m like turn on or turn off um you could do something like a computer when you’re when someone else says they trigger words on this is the last technical video of this course and to wrap up in this course on sequence models you learned about rnns including both gr use and LS TMS and then in the second week you learned a lot about word embeddings and how they learn representations of words and then in this week you learned about the attention model as well as how to use it to process audio data and I hope you have fun implementing all of these ideas in this beast program sighs let’s go on to the last video. conclusion-and-thank-youcongratulations on making it this far I just wanna wrap up and leave you with a few final thoughts we’ve been on quite a journey together but if you’ve taken the whole specialization then you’ve learned about new networks and deep learning how to improve deep neural networks of the structure machine learning projects convolutional neural networks and then in this most recent course sequence models and I know you work really hard and I also hope you feel very proud of yourself for your hard work and for how much you’ve done.so I want to leave you one maybe important thought which is that I think deep learning is a superpower with deep learning algorithms you can make a computer see you can have a computer synthesize novel art or synthesized music or you can have a computer translate from one language to another maybe have it locally radiology image and render a medical diagnosis or build pieces of a car that can drive itself and if that isn’t a superpower I don’t know what is and as we wrap up this sequence of courses as we wrap up this specialization I hope that you will find ways to use these ideas to further your career to pursue your dreams but perhaps most important to do whatever you think is the best work you can do our humanity the world today has challenges but with the power of a on power of deep learning I think we can make it a much better place and now that you have this superpower I hope you will use it to go out there and make life better for yourself but also for other people and of course I also hope you feel very proud of your accomplishments in the power far you’ve come and of all that you’ve learned and when you complete this sequence of causes you should also share it on social media like Twitter or Facebook and let your friends know. and finally the very last thing I want to say to you is congratulations on Nikolas I hope you feel great about your accomplishments but also I want to thank you very much I know that you have a busy life but despite that spends a lot of time watching these videos and maybe spent a long time also working on the quizzes and the programming exercises I hope you enjoyed it and you got a lot out of the process but I’m also very grateful for all your time you spend and for all your hard work you put into learning these materials so thank you very much.]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Emojify]]></title>
    <url>%2F2018%2F06%2F03%2FEmojify%2B-%2Bv2%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 2nd week and the copyright belongs to deeplearning.ai. Emojify!Welcome to the second assignment of Week 2. You are going to use word vector representations to build an Emojifier. Have you ever wanted to make your text messages more expressive? Your emojifier app will help you do that. So rather than writing “Congratulations on the promotion! Lets get coffee and talk. Love you!” the emojifier can automatically turn this into “Congratulations on the promotion! 👍 Lets get coffee and talk. ☕️ Love you! ❤️” You will implement a model which inputs a sentence (such as “Let’s go see the baseball game tonight!”) and finds the most appropriate emoji to be used with this sentence (⚾️). In many emoji interfaces, you need to remember that ❤️ is the “heart” symbol rather than the “love” symbol. But using word vectors, you’ll see that even if your training set explicitly relates only a few words to a particular emoji, your algorithm will be able to generalize and associate words in the test set to the same emoji even if those words don’t even appear in the training set. This allows you to build an accurate classifier mapping from sentences to emojis, even using a small training set. In this exercise, you’ll start with a baseline model (Emojifier-V1) using word embeddings, then build a more sophisticated model (Emojifier-V2) that further incorporates an LSTM. Lets get started! Run the following cell to load the package you are going to use. 123456import numpy as npfrom emo_utils import *import emojiimport matplotlib.pyplot as plt%matplotlib inline 1 - Baseline model: Emojifier-V11.1 - Dataset EMOJISETLet’s start by building a simple baseline classifier. You have a tiny dataset (X, Y) where: X contains 127 sentences (strings) Y contains a integer label between 0 and 4 corresponding to an emoji for each sentence Figure 1: EMOJISET - a classification problem with 5 classes. A few examples of sentences are given here. Let’s load the dataset using the code below. We split the dataset between training (127 examples) and testing (56 examples). 12X_train, Y_train = read_csv('data/train_emoji.csv')X_test, Y_test = read_csv('data/tesss.csv') 1maxLen = len(max(X_train, key=len).split()) Run the following cell to print sentences from X_train and corresponding labels from Y_train. Change index to see different examples. Because of the font the iPython notebook uses, the heart emoji may be colored black rather than red. 12index = 1print(X_train[index], label_to_emoji(Y_train[index])) I am proud of your achievements 😄 1.2 - Overview of the Emojifier-V1In this part, you are going to implement a baseline model called “Emojifier-v1”. Figure 2: Baseline model (Emojifier-V1). The input of the model is a string corresponding to a sentence (e.g. “I love you). In the code, the output will be a probability vector of shape (1,5), that you then pass in an argmax layer to extract the index of the most likely emoji output. To get our labels into a format suitable for training a softmax classifier, lets convert $Y$ from its current shape current shape $(m, 1)$ into a “one-hot representation” $(m, 5)$, where each row is a one-hot vector giving the label of one example, You can do so using this next code snipper. Here, Y_oh stands for “Y-one-hot” in the variable names Y_oh_train and Y_oh_test: 12Y_oh_train = convert_to_one_hot(Y_train, C = 5)Y_oh_test = convert_to_one_hot(Y_test, C = 5) Let’s see what convert_to_one_hot() did. Feel free to change index to print out different values. 12index = 50print(Y_train[index], "is converted into one hot", Y_oh_train[index]) 0 is converted into one hot [ 1. 0. 0. 0. 0.] All the data is now ready to be fed into the Emojify-V1 model. Let’s implement the model! 1.3 - Implementing Emojifier-V1As shown in Figure (2), the first step is to convert an input sentence into the word vector representation, which then get averaged together. Similar to the previous exercise, we will use pretrained 50-dimensional GloVe embeddings. Run the following cell to load the word_to_vec_map, which contains all the vector representations. 1word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt') You’ve loaded: word_to_index: dictionary mapping from words to their indices in the vocabulary (400,001 words, with the valid indices ranging from 0 to 400,000) index_to_word: dictionary mapping from indices to their corresponding words in the vocabulary word_to_vec_map: dictionary mapping words to their GloVe vector representation. Run the following cell to check if it works. 1234word = "cucumber"index = 289846print("the index of", word, "in the vocabulary is", word_to_index[word])print("the", str(index) + "th word in the vocabulary is", index_to_word[index]) the index of cucumber in the vocabulary is 113317 the 289846th word in the vocabulary is potatos Exercise: Implement sentence_to_avg(). You will need to carry out two steps: Convert every sentence to lower-case, then split the sentence into a list of words. X.lower() and X.split() might be useful. For each word in the sentence, access its GloVe representation. Then, average all these values. 123456789101112131415161718192021222324252627282930# GRADED FUNCTION: sentence_to_avgdef sentence_to_avg(sentence, word_to_vec_map): """ Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word and averages its value into a single vector encoding the meaning of the sentence. Arguments: sentence -- string, one training example from X word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation Returns: avg -- average vector encoding information about the sentence, numpy-array of shape (50,) """ ### START CODE HERE ### # Step 1: Split sentence into list of lower case words (≈ 1 line) words = sentence.lower().split(); # Initialize the average word vector, should have the same shape as your word vectors. avg = np.zeros((word_to_vec_map[words[0]].shape)); # Step 2: average the word vectors. You can loop over the words in the list "words". for w in words: avg += word_to_vec_map[w]; avg = avg / len(words); ### END CODE HERE ### return avg 12avg = sentence_to_avg("Morrocan couscous is my favorite dish", word_to_vec_map)print("avg = ", avg) avg = [-0.008005 0.56370833 -0.50427333 0.258865 0.55131103 0.03104983 -0.21013718 0.16893933 -0.09590267 0.141784 -0.15708967 0.18525867 0.6495785 0.38371117 0.21102167 0.11301667 0.02613967 0.26037767 0.05820667 -0.01578167 -0.12078833 -0.02471267 0.4128455 0.5152061 0.38756167 -0.898661 -0.535145 0.33501167 0.68806933 -0.2156265 1.797155 0.10476933 -0.36775333 0.750785 0.10282583 0.348925 -0.27262833 0.66768 -0.10706167 -0.283635 0.59580117 0.28747333 -0.3366635 0.23393817 0.34349183 0.178405 0.1166155 -0.076433 0.1445417 0.09808667] Expected Output: avg= [-0.008005 0.56370833 -0.50427333 0.258865 0.55131103 0.03104983 -0.21013718 0.16893933 -0.09590267 0.141784 -0.15708967 0.18525867 0.6495785 0.38371117 0.21102167 0.11301667 0.02613967 0.26037767 0.05820667 -0.01578167 -0.12078833 -0.02471267 0.4128455 0.5152061 0.38756167 -0.898661 -0.535145 0.33501167 0.68806933 -0.2156265 1.797155 0.10476933 -0.36775333 0.750785 0.10282583 0.348925 -0.27262833 0.66768 -0.10706167 -0.283635 0.59580117 0.28747333 -0.3366635 0.23393817 0.34349183 0.178405 0.1166155 -0.076433 0.1445417 0.09808667] ModelYou now have all the pieces to finish implementing the model() function. After using sentence_to_avg() you need to pass the average through forward propagation, compute the cost, and then backpropagate to update the softmax’s parameters. Exercise: Implement the model() function described in Figure (2). Assuming here that $Yoh$ (“Y one hot”) is the one-hot encoding of the output labels, the equations you need to implement in the forward pass and to compute the cross-entropy cost are:$$ z^{(i)} = W . avg^{(i)} + b$$$$ a^{(i)} = softmax(z^{(i)})$$$$ \mathcal{L}^{(i)} = - \sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)$$ It is possible to come up with a more efficient vectorized implementation. But since we are using a for-loop to convert the sentences one at a time into the avg^{(i)} representation anyway, let’s not bother this time. We provided you a function softmax(). 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# GRADED FUNCTION: modeldef model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400): """ Model to train word vector representations in numpy. Arguments: X -- input data, numpy array of sentences as strings, of shape (m, 1) Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1) word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation learning_rate -- learning_rate for the stochastic gradient descent algorithm num_iterations -- number of iterations Returns: pred -- vector of predictions, numpy-array of shape (m, 1) W -- weight matrix of the softmax layer, of shape (n_y, n_h) b -- bias of the softmax layer, of shape (n_y,) """ np.random.seed(1) # Define number of training examples m = Y.shape[0] # number of training examples n_y = 5 # number of classes n_h = 50 # dimensions of the GloVe vectors # Initialize parameters using Xavier initialization W = np.random.randn(n_y, n_h) / np.sqrt(n_h) b = np.zeros((n_y,)) # Convert Y to Y_onehot with n_y classes Y_oh = convert_to_one_hot(Y, C = n_y) # Optimization loop for t in range(num_iterations): # Loop over the number of iterations for i in range(m): # Loop over the training examples ### START CODE HERE ### (≈ 4 lines of code) # Average the word vectors of the words from the i'th training example avg = sentence_to_avg(X[i], word_to_vec_map); # Forward propagate the avg through the softmax layer z = np.dot(W, avg) + b; a = softmax(z); # Compute cost using the i'th training label's one hot representation and "A" (the output of the softmax) cost = np.sum(-Y_oh[i] * np.log(a)); ### END CODE HERE ### # Compute gradients dz = a - Y_oh[i] dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h)) db = dz # Update parameters with Stochastic Gradient Descent W = W - learning_rate * dW b = b - learning_rate * db if t % 100 == 0: print("Epoch: " + str(t) + " --- cost = " + str(cost)) pred = predict(X, Y, W, b, word_to_vec_map) return pred, W, b 123456789101112131415161718192021print(X_train.shape)print(Y_train.shape)print(np.eye(5)[Y_train.reshape(-1)].shape)print(X_train[0])print(type(X_train))Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])print(Y.shape)X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear', 'Lets go party and drinks','Congrats on the new job','Congratulations', 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you', 'You totally deserve this prize', 'Let us go play football', 'Are you down for football this afternoon', 'Work hard play harder', 'It is suprising how people can be dumb sometimes', 'I am very disappointed','It is the best day in my life', 'I think I will end up alone','My life is so boring','Good job', 'Great so awesome'])print(X.shape)print(np.eye(5)[Y_train.reshape(-1)].shape)print(type(X_train)) (132,) (132,) (132, 5) never talk to me again &lt;class &apos;numpy.ndarray&apos;&gt; (20,) (20,) (132, 5) &lt;class &apos;numpy.ndarray&apos;&gt; Run the next cell to train your model and learn the softmax parameters (W,b). 12pred, W, b = model(X_train, Y_train, word_to_vec_map)print(pred) Epoch: 0 --- cost = 1.95204988128 Accuracy: 0.348484848485 Epoch: 100 --- cost = 0.0797181872601 Accuracy: 0.931818181818 Epoch: 200 --- cost = 0.0445636924368 Accuracy: 0.954545454545 Epoch: 300 --- cost = 0.0343226737879 Accuracy: 0.969696969697 [[ 3.] [ 2.] [ 3.] [ 0.] [ 4.] [ 0.] [ 3.] [ 2.] [ 3.] [ 1.] [ 3.] [ 3.] [ 1.] [ 3.] [ 2.] [ 3.] [ 2.] [ 3.] [ 1.] [ 2.] [ 3.] [ 0.] [ 2.] [ 2.] [ 2.] [ 1.] [ 4.] [ 3.] [ 3.] [ 4.] [ 0.] [ 3.] [ 4.] [ 2.] [ 0.] [ 3.] [ 2.] [ 2.] [ 3.] [ 4.] [ 2.] [ 2.] [ 0.] [ 2.] [ 3.] [ 0.] [ 3.] [ 2.] [ 4.] [ 3.] [ 0.] [ 3.] [ 3.] [ 3.] [ 4.] [ 2.] [ 1.] [ 1.] [ 1.] [ 2.] [ 3.] [ 1.] [ 0.] [ 0.] [ 0.] [ 3.] [ 4.] [ 4.] [ 2.] [ 2.] [ 1.] [ 2.] [ 0.] [ 3.] [ 2.] [ 2.] [ 0.] [ 3.] [ 3.] [ 1.] [ 2.] [ 1.] [ 2.] [ 2.] [ 4.] [ 3.] [ 3.] [ 2.] [ 4.] [ 0.] [ 0.] [ 3.] [ 3.] [ 3.] [ 3.] [ 2.] [ 0.] [ 1.] [ 2.] [ 3.] [ 0.] [ 2.] [ 2.] [ 2.] [ 3.] [ 2.] [ 2.] [ 2.] [ 4.] [ 1.] [ 1.] [ 3.] [ 3.] [ 4.] [ 1.] [ 2.] [ 1.] [ 1.] [ 3.] [ 1.] [ 0.] [ 4.] [ 0.] [ 3.] [ 3.] [ 4.] [ 4.] [ 1.] [ 4.] [ 3.] [ 0.] [ 2.]] Expected Output (on a subset of iterations): Epoch: 0 cost = 1.95204988128 Accuracy: 0.348484848485 Epoch: 100 cost = 0.0797181872601 Accuracy: 0.931818181818 Epoch: 200 cost = 0.0445636924368 Accuracy: 0.954545454545 Epoch: 300 cost = 0.0343226737879 Accuracy: 0.969696969697 Great! Your model has pretty high accuracy on the training set. Lets now see how it does on the test set. 1.4 - Examining test set performance1234print("Training set:")pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)print('Test set:')pred_test = predict(X_test, Y_test, W, b, word_to_vec_map) Training set: Accuracy: 0.977272727273 Test set: Accuracy: 0.857142857143 Expected Output: Train set accuracy 97.7 Test set accuracy 85.7 Random guessing would have had 20% accuracy given that there are 5 classes. This is pretty good performance after training on only 127 examples. In the training set, the algorithm saw the sentence “I love you“ with the label ❤️. You can check however that the word “adore” does not appear in the training set. Nonetheless, lets see what happens if you write “I adore you.” 12345X_my_sentences = np.array(["i adore you", "i love you", "funny lol", "lets play with a ball", "food is ready", "not feeling happy"])Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)print_predictions(X_my_sentences, pred) Accuracy: 0.833333333333 i adore you ❤️ i love you ❤️ funny lol 😄 lets play with a ball ⚾ food is ready 🍴 not feeling happy 😄 Amazing! Because adore has a similar embedding as love, the algorithm has generalized correctly even to a word it has never seen before. Words such as heart, dear, beloved or adore have embedding vectors similar to love, and so might work too—feel free to modify the inputs above and try out a variety of input sentences. How well does it work? Note though that it doesn’t get “not feeling happy” correct. This algorithm ignores word ordering, so is not good at understanding phrases like “not happy.” Printing the confusion matrix can also help understand which classes are more difficult for your model. A confusion matrix shows how often an example whose label is one class (“actual” class) is mislabeled by the algorithm with a different class (“predicted” class). 1234print(Y_test.shape)print(' '+ label_to_emoji(0)+ ' ' + label_to_emoji(1) + ' ' + label_to_emoji(2)+ ' ' + label_to_emoji(3)+' ' + label_to_emoji(4))print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))plot_confusion_matrix(Y_test, pred_test) (56,) ❤️ ⚾ 😄 😞 🍴 Predicted 0.0 1.0 2.0 3.0 4.0 All Actual 0 6 0 0 1 0 7 1 0 8 0 0 0 8 2 2 0 16 0 0 18 3 1 1 2 12 0 16 4 0 0 1 0 6 7 All 9 9 19 13 6 56 What you should remember from this part: Even with a 127 training examples, you can get a reasonably good model for Emojifying. This is due to the generalization power word vectors gives you. Emojify-V1 will perform poorly on sentences such as “This movie is not good and not enjoyable” because it doesn’t understand combinations of words–it just averages all the words’ embedding vectors together, without paying attention to the ordering of words. You will build a better algorithm in the next part. 2 - Emojifier-V2: Using LSTMs in Keras:Let’s build an LSTM model that takes as input word sequences. This model will be able to take word ordering into account. Emojifier-V2 will continue to use pre-trained word embeddings to represent words, but will feed them into an LSTM, whose job it is to predict the most appropriate emoji. Run the following cell to load the Keras packages. 12345678import numpy as npnp.random.seed(0)from keras.models import Modelfrom keras.layers import Dense, Input, Dropout, LSTM, Activationfrom keras.layers.embeddings import Embeddingfrom keras.preprocessing import sequencefrom keras.initializers import glorot_uniformnp.random.seed(1) Using TensorFlow backend. 2.1 - Overview of the modelHere is the Emojifier-v2 you will implement: Figure 3: Emojifier-V2. A 2-layer LSTM sequence classifier. 2.2 Keras and mini-batchingIn this exercise, we want to train Keras using mini-batches. However, most deep learning frameworks require that all sequences in the same mini-batch have the same length. This is what allows vectorization to work: If you had a 3-word sentence and a 4-word sentence, then the computations needed for them are different (one takes 3 steps of an LSTM, one takes 4 steps) so it’s just not possible to do them both at the same time. The common solution to this is to use padding. Specifically, set a maximum sequence length, and pad all sequences to the same length. For example, of the maximum sequence length is 20, we could pad every sentence with “0”s so that each input sentence is of length 20. Thus, a sentence “i love you” would be represented as $(e_{i}, e_{love}, e_{you}, \vec{0}, \vec{0}, \ldots, \vec{0})$. In this example, any sentences longer than 20 words would have to be truncated. One simple way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set. 2.3 - The Embedding layerIn Keras, the embedding matrix is represented as a “layer”, and maps positive integers (indices corresponding to words) into dense vectors of fixed size (the embedding vectors). It can be trained or initialized with a pretrained embedding. In this part, you will learn how to create an Embedding() layer in Keras, initialize it with the GloVe 50-dimensional vectors loaded earlier in the notebook. Because our training set is quite small, we will not update the word embeddings but will instead leave their values fixed. But in the code below, we’ll show you how Keras allows you to either train or leave fixed this layer. The Embedding() layer takes an integer matrix of size (batch size, max input length) as input. This corresponds to sentences converted into lists of indices (integers), as shown in the figure below. Figure 4: Embedding layer. This example shows the propagation of two examples through the embedding layer. Both have been zero-padded to a length of max_len=5. The final dimension of the representation is (2,max_len,50) because the word embeddings we are using are 50 dimensional. The largest integer (i.e. word index) in the input should be no larger than the vocabulary size. The layer outputs an array of shape (batch size, max input length, dimension of word vectors). The first step is to convert all your training sentences into lists of indices, and then zero-pad all these lists so that their length is the length of the longest sentence. Exercise: Implement the function below to convert X (array of sentences as strings) into an array of indices corresponding to words in the sentences. The output shape should be such that it can be given to Embedding() (described in Figure 4). 12345678910111213141516171819202122232425262728293031323334353637383940# GRADED FUNCTION: sentences_to_indicesdef sentences_to_indices(X, word_to_index, max_len): """ Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences. The output shape should be such that it can be given to `Embedding()` (described in Figure 4). Arguments: X -- array of sentences (strings), of shape (m, 1) word_to_index -- a dictionary containing the each word mapped to its index max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. Returns: X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len) """ m = X.shape[0] # number of training examples ### START CODE HERE ### # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line) X_indices = np.zeros((m, max_len)); for i in range(m): # loop over training examples # Convert the ith training sentence in lower case and split is into words. You should get a list of words. sentence_words = X[i].lower().split(); # Initialize j to 0 j = 0 # Loop over the words of sentence_words for w in sentence_words: # Set the (i,j)th entry of X_indices to the index of the correct word. X_indices[i, j] = word_to_index[w]; # Increment j to j + 1 j = j + 1; ### END CODE HERE ### return X_indices Run the following cell to check what sentences_to_indices() does, and check your results. 1234X1 = np.array(["funny lol", "lets play baseball", "food is ready for you"])X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)print("X1 =", X1)print("X1_indices =", X1_indices) X1 = [&apos;funny lol&apos; &apos;lets play baseball&apos; &apos;food is ready for you&apos;] X1_indices = [[ 155345. 225122. 0. 0. 0.] [ 220930. 286375. 69714. 0. 0.] [ 151204. 192973. 302254. 151349. 394475.]] Expected Output: X1 = [‘funny lol’ ‘lets play football’ ‘food is ready for you’] X1_indices = [[ 155345. 225122. 0. 0. 0.] [ 220930. 286375. 151266. 0. 0.] [ 151204. 192973. 302254. 151349. 394475.]] Let’s build the Embedding() layer in Keras, using pre-trained word vectors. After this layer is built, you will pass the output of sentences_to_indices() to it as an input, and the Embedding() layer will return the word embeddings for a sentence. Exercise: Implement pretrained_embedding_layer(). You will need to carry out the following steps: Initialize the embedding matrix as a numpy array of zeroes with the correct shape. Fill in the embedding matrix with all the word embeddings extracted from word_to_vec_map. Define Keras embedding layer. Use Embedding(). Be sure to make this layer non-trainable, by setting trainable = False when calling Embedding(). If you were to set trainable = True, then it will allow the optimization algorithm to modify the values of the word embeddings. Set the embedding weights to be equal to the embedding matrix 123456789101112131415161718192021222324252627282930313233343536# GRADED FUNCTION: pretrained_embedding_layerdef pretrained_embedding_layer(word_to_vec_map, word_to_index): """ Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors. Arguments: word_to_vec_map -- dictionary mapping words to their GloVe vector representation. word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words) Returns: embedding_layer -- pretrained layer Keras instance """ vocab_len = len(word_to_index) + 1 # adding 1 to fit Keras embedding (requirement) emb_dim = word_to_vec_map["cucumber"].shape[0] # define dimensionality of your GloVe word vectors (= 50) ### START CODE HERE ### # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim) emb_matrix = np.zeros((vocab_len, emb_dim)); # Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary for word, index in word_to_index.items(): emb_matrix[index, :] = word_to_vec_map[word]; # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. embedding_layer = Embedding(vocab_len, emb_dim, trainable = False); ### END CODE HERE ### # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None". embedding_layer.build((None,)) # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained. embedding_layer.set_weights([emb_matrix]) return embedding_layer 12embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)print("weights[0][1][3] =", embedding_layer.get_weights()[0][1][3]) weights[0][1][3] = -0.3403 Expected Output: weights[0][1][3] = -0.3403 2.3 Building the Emojifier-V2Lets now build the Emojifier-V2 model. You will do so using the embedding layer you have built, and feed its output to an LSTM network. Figure 3: Emojifier-v2. A 2-layer LSTM sequence classifier. Exercise: Implement Emojify_V2(), which builds a Keras graph of the architecture shown in Figure 3. The model takes as input an array of sentences of shape (m, max_len, ) defined by input_shape. It should output a softmax probability vector of shape (m, C = 5). You may need Input(shape = ..., dtype = &#39;...&#39;), LSTM(), Dropout(), Dense(), and Activation(). 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# GRADED FUNCTION: Emojify_V2def Emojify_V2(input_shape, word_to_vec_map, word_to_index): """ Function creating the Emojify-v2 model's graph. Arguments: input_shape -- shape of the input, usually (max_len,) word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words) Returns: model -- a model instance in Keras """ ### START CODE HERE ### # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices). sentence_indices = Input(shape = input_shape, dtype = 'int32'); # Create the embedding layer pretrained with GloVe Vectors (≈1 line) embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index); # Propagate sentence_indices through your embedding layer, you get back the embeddings embeddings = embedding_layer(sentence_indices); # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state # Be careful, the returned output should be a batch of sequences. X = LSTM(128, return_sequences = True)(embeddings); # Add dropout with a probability of 0.5 X = Dropout(0.5)(X); # Propagate X trough another LSTM layer with 128-dimensional hidden state # Be careful, the returned output should be a single hidden state, not a batch of sequences. X = LSTM(128, return_sequences = False)(X); # Add dropout with a probability of 0.5 X = Dropout(0.5)(X); # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors. X = Dense(5, activation = 'softmax')(X); # Add a softmax activation X = Activation('softmax')(X); # Create Model instance which converts sentence_indices into X. model = Model(inputs = sentence_indices, outputs = X); ### END CODE HERE ### return model Run the following cell to create your model and check its summary. Because all sentences in the dataset are less than 10 words, we chose max_len = 10. You should see your architecture, it uses “20,223,927” parameters, of which 20,000,050 (the word embeddings) are non-trainable, and the remaining 223,877 are. Because our vocabulary size has 400,001 words (with valid indices from 0 to 400,000) there are 400,001*50 = 20,000,050 non-trainable parameters. 12model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 10) 0 _________________________________________________________________ embedding_2 (Embedding) (None, 10, 50) 20000050 _________________________________________________________________ lstm_1 (LSTM) (None, 10, 128) 91648 _________________________________________________________________ dropout_1 (Dropout) (None, 10, 128) 0 _________________________________________________________________ lstm_2 (LSTM) (None, 128) 131584 _________________________________________________________________ dropout_2 (Dropout) (None, 128) 0 _________________________________________________________________ dense_1 (Dense) (None, 5) 645 _________________________________________________________________ activation_1 (Activation) (None, 5) 0 ================================================================= Total params: 20,223,927 Trainable params: 223,877 Non-trainable params: 20,000,050 _________________________________________________________________ As usual, after creating your model in Keras, you need to compile it and define what loss, optimizer and metrics your are want to use. Compile your model using categorical_crossentropy loss, adam optimizer and [&#39;accuracy&#39;] metrics: 1model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) It’s time to train your model. Your Emojifier-V2 model takes as input an array of shape (m, max_len) and outputs probability vectors of shape (m, number of classes). We thus have to convert X_train (array of sentences as strings) to X_train_indices (array of sentences as list of word indices), and Y_train (labels as indices) to Y_train_oh (labels as one-hot vectors). 12X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)Y_train_oh = convert_to_one_hot(Y_train, C = 5) Fit the Keras model on X_train_indices and Y_train_oh. We will use epochs = 50 and batch_size = 32. 1model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True) Epoch 1/50 132/132 [==============================] - 0s - loss: 1.6086 - acc: 0.1818 Epoch 2/50 132/132 [==============================] - 0s - loss: 1.5867 - acc: 0.3409 Epoch 3/50 132/132 [==============================] - 0s - loss: 1.5721 - acc: 0.2652 Epoch 4/50 132/132 [==============================] - 0s - loss: 1.5540 - acc: 0.3485 Epoch 5/50 132/132 [==============================] - 0s - loss: 1.5413 - acc: 0.3030 Epoch 6/50 132/132 [==============================] - 0s - loss: 1.5195 - acc: 0.3712 Epoch 7/50 132/132 [==============================] - 0s - loss: 1.5275 - acc: 0.3258 Epoch 8/50 132/132 [==============================] - 0s - loss: 1.4633 - acc: 0.4545 Epoch 9/50 132/132 [==============================] - 0s - loss: 1.4320 - acc: 0.4924 Epoch 10/50 132/132 [==============================] - 0s - loss: 1.3712 - acc: 0.6136 Epoch 11/50 132/132 [==============================] - 0s - loss: 1.3441 - acc: 0.6136 Epoch 12/50 132/132 [==============================] - 0s - loss: 1.2784 - acc: 0.6894 Epoch 13/50 132/132 [==============================] - 0s - loss: 1.2723 - acc: 0.6364 Epoch 14/50 132/132 [==============================] - 0s - loss: 1.2651 - acc: 0.6667 Epoch 15/50 132/132 [==============================] - 0s - loss: 1.2106 - acc: 0.6970 Epoch 16/50 132/132 [==============================] - 0s - loss: 1.2334 - acc: 0.7197 Epoch 17/50 132/132 [==============================] - 0s - loss: 1.2150 - acc: 0.7045 Epoch 18/50 132/132 [==============================] - 0s - loss: 1.1613 - acc: 0.7803 Epoch 19/50 132/132 [==============================] - 0s - loss: 1.1587 - acc: 0.7576 Epoch 20/50 132/132 [==============================] - 0s - loss: 1.1129 - acc: 0.8182 Epoch 21/50 132/132 [==============================] - 0s - loss: 1.1016 - acc: 0.8030 Epoch 22/50 132/132 [==============================] - 0s - loss: 1.1939 - acc: 0.6970 Epoch 23/50 132/132 [==============================] - 0s - loss: 1.2618 - acc: 0.6288 Epoch 24/50 132/132 [==============================] - 0s - loss: 1.2123 - acc: 0.6818 Epoch 25/50 132/132 [==============================] - 0s - loss: 1.1606 - acc: 0.7652 Epoch 26/50 132/132 [==============================] - 0s - loss: 1.1066 - acc: 0.8030 Epoch 27/50 132/132 [==============================] - 0s - loss: 1.1312 - acc: 0.7727 Epoch 28/50 132/132 [==============================] - 0s - loss: 1.1400 - acc: 0.7652 Epoch 29/50 132/132 [==============================] - 0s - loss: 1.1107 - acc: 0.8030 Epoch 30/50 132/132 [==============================] - 0s - loss: 1.0676 - acc: 0.8485 Epoch 31/50 132/132 [==============================] - 0s - loss: 1.0660 - acc: 0.8258 Epoch 32/50 132/132 [==============================] - 0s - loss: 1.0450 - acc: 0.8712 Epoch 33/50 132/132 [==============================] - 0s - loss: 1.0246 - acc: 0.8939 Epoch 34/50 132/132 [==============================] - 0s - loss: 1.0163 - acc: 0.8939 Epoch 35/50 132/132 [==============================] - 0s - loss: 1.0080 - acc: 0.9015 Epoch 36/50 132/132 [==============================] - 0s - loss: 1.0144 - acc: 0.9015 Epoch 37/50 132/132 [==============================] - 0s - loss: 1.0861 - acc: 0.8106 Epoch 38/50 132/132 [==============================] - 0s - loss: 1.0484 - acc: 0.8561 Epoch 39/50 132/132 [==============================] - 0s - loss: 1.1126 - acc: 0.7955 Epoch 40/50 132/132 [==============================] - 0s - loss: 1.0712 - acc: 0.8561 Epoch 41/50 132/132 [==============================] - 0s - loss: 1.0277 - acc: 0.8864 Epoch 42/50 132/132 [==============================] - 0s - loss: 1.0459 - acc: 0.8561 Epoch 43/50 132/132 [==============================] - 0s - loss: 1.0214 - acc: 0.8864 Epoch 44/50 132/132 [==============================] - 0s - loss: 1.0012 - acc: 0.9091 Epoch 45/50 132/132 [==============================] - 0s - loss: 0.9877 - acc: 0.9242 Epoch 46/50 132/132 [==============================] - 0s - loss: 0.9827 - acc: 0.9167 Epoch 47/50 132/132 [==============================] - 0s - loss: 0.9835 - acc: 0.9167 Epoch 48/50 132/132 [==============================] - 0s - loss: 0.9817 - acc: 0.9242 Epoch 49/50 132/132 [==============================] - 0s - loss: 0.9894 - acc: 0.9167 Epoch 50/50 132/132 [==============================] - 0s - loss: 0.9780 - acc: 0.9318 &lt;keras.callbacks.History at 0x7f49ffd55e48&gt; Your model should perform close to 100% accuracy on the training set. The exact accuracy you get may be a little different. Run the following cell to evaluate your model on the test set. 12345X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)Y_test_oh = convert_to_one_hot(Y_test, C = 5)loss, acc = model.evaluate(X_test_indices, Y_test_oh)print()print("Test accuracy = ", acc) 32/56 [================&gt;.............] - ETA: 0s Test accuracy = 0.839285714286 You should get a test accuracy between 80% and 95%. Run the cell below to see the mislabelled examples. 12345678910# This code allows you to see the mislabelled examplesC = 5y_test_oh = np.eye(C)[Y_test.reshape(-1)]X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)pred = model.predict(X_test_indices)for i in range(len(X_test)): x = X_test_indices num = np.argmax(pred[i]) if(num != Y_test[i]): print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip()) Expected emoji:😄 prediction: she got me a nice present ❤️ Expected emoji:😞 prediction: work is hard 😄 Expected emoji:😞 prediction: This girl is messing with me ❤️ Expected emoji:😞 prediction: work is horrible 😄 Expected emoji:😄 prediction: you brighten my day ❤️ Expected emoji:😞 prediction: she is a bully 😄 Expected emoji:😞 prediction: My life is so boring ❤️ Expected emoji:😄 prediction: will you be my valentine 😞 Expected emoji:😄 prediction: What you did was awesome 😞 Now you can try it on your own example. Write your own sentence below. 1234# Change the sentence below to see your prediction. Make sure all the words are in the Glove embeddings. x_test = np.array(['not feeling happy'])X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)print(x_test[0] +' '+ label_to_emoji(np.argmax(model.predict(X_test_indices)))) not feeling happy 😄 Previously, Emojify-V1 model did not correctly label “not feeling happy,” but our implementation of Emojiy-V2 got it right. (Keras’ outputs are slightly random each time, so you may not have obtained the same result.) The current model still isn’t very robust at understanding negation (like “not happy”) because the training set is small and so doesn’t have a lot of examples of negation. But if the training set were larger, the LSTM model would be much better than the Emojify-V1 model at understanding such complex sentences. Congratulations!You have completed this notebook! ❤️❤️❤️ What you should remember: If you have an NLP task where the training set is small, using word embeddings can help your algorithm significantly. Word embeddings allow your model to work on words in the test set that may not even have appeared in your training set. Training sequence models in Keras (and in most other deep learning frameworks) requires a few important details: To use mini-batches, the sequences need to be padded so that all the examples in a mini-batch have the same length. An Embedding() layer can be initialized with pretrained values. These values can be either fixed or trained further on your dataset. If however your labeled dataset is small, it’s usually not worth trying to train a large pre-trained set of embeddings. LSTM() has a flag called return_sequences to decide if you would like to return every hidden states or only the last one. You can use Dropout() right after LSTM() to regularize your network. Congratulations on finishing this assignment and building an Emojifier. We hope you’re happy with what you’ve accomplished in this notebook! 😀😀😀😀😀😀AcknowledgmentsThanks to Alison Darcy and the Woebot team for their advice on the creation of this assignment. Woebot is a chatbot friend that is ready to speak with you 24/7. As part of Woebot’s technology, it uses word embeddings to understand the emotions of what you say. You can play with it by going to http://woebot.io]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Building a Recurrent Neural Network Step by Step]]></title>
    <url>%2F2018%2F06%2F02%2FBuilding%2Ba%2BRecurrent%2BNeural%2BNetwork%2B-%2BStep%2Bby%2BStep%2B-%2Bv3%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 1st week and the copyright belongs to deeplearning.ai. Building your Recurrent Neural Network - Step by StepWelcome to Course 5’s first assignment! In this assignment, you will implement your first Recurrent Neural Network in numpy. Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have “memory”. They can read inputs $x^{\langle t \rangle}$ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a uni-directional RNN to take information from the past to process later inputs. A bidirection RNN can take context from both the past and the future. Notation: Superscript $[l]$ denotes an object associated with the $l^{th}$ layer. Example: $a^{[4]}$ is the $4^{th}$ layer activation. $W^{[5]}$ and $b^{[5]}$ are the $5^{th}$ layer parameters. Superscript $(i)$ denotes an object associated with the $i^{th}$ example. Example: $x^{(i)}$ is the $i^{th}$ training example input. Superscript $\langle t \rangle$ denotes an object at the $t^{th}$ time-step. Example: $x^{\langle t \rangle}$ is the input x at the $t^{th}$ time-step. $x^{(i)\langle t \rangle}$ is the input at the $t^{th}$ timestep of example $i$. Lowerscript $i$ denotes the $i^{th}$ entry of a vector. Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the activations in layer $l$. We assume that you are already familiar with numpy and/or have completed the previous courses of the specialization. Let’s get started! Let’s first import all the packages that you will need during this assignment. 12import numpy as npfrom rnn_utils import * 1 - Forward propagation for the basic Recurrent Neural NetworkLater this week, you will generate music using an RNN. The basic RNN that you will implement has the structure below. In this example, $T_x = T_y$. Figure 1: Basic RNN model Here’s how you can implement an RNN: Steps: Implement the calculations needed for one time-step of the RNN. Implement a loop over $T_x$ time-steps in order to process all the inputs, one at a time. Let’s go! 1.1 - RNN cellA Recurrent neural network can be seen as the repetition of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell. Figure 2: Basic RNN cell. Takes as input $x^{\langle t \rangle}$ (current input) and $a^{\langle t - 1\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\langle t \rangle}$ which is given to the next RNN cell and also used to predict $y^{\langle t \rangle}$ Exercise: Implement the RNN-cell described in Figure (2). Instructions: Compute the hidden state with tanh activation: $a^{\langle t \rangle} = \tanh(W_{aa} a^{\langle t-1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)$. Using your new hidden state $a^{\langle t \rangle}$, compute the prediction $\hat{y}^{\langle t \rangle} = softmax(W_{ya} a^{\langle t \rangle} + b_y)$. We provided you a function: softmax. Store $(a^{\langle t \rangle}, a^{\langle t-1 \rangle}, x^{\langle t \rangle}, parameters)$ in cache Return $a^{\langle t \rangle}$ , $y^{\langle t \rangle}$ and cache We will vectorize over $m$ examples. Thus, $x^{\langle t \rangle}$ will have dimension $(n_x,m)$, and $a^{\langle t \rangle}$ will have dimension $(n_a,m)$. 123456789101112131415161718192021222324252627282930313233343536373839# GRADED FUNCTION: rnn_cell_forwarddef rnn_cell_forward(xt, a_prev, parameters): """ Implements a single forward step of the RNN-cell as described in Figure (2) Arguments: xt -- your input data at timestep "t", numpy array of shape (n_x, m). a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m) parameters -- python dictionary containing: Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x) Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a) Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a) ba -- Bias, numpy array of shape (n_a, 1) by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1) Returns: a_next -- next hidden state, of shape (n_a, m) yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m) cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters) """ # Retrieve parameters from "parameters" Wax = parameters["Wax"] Waa = parameters["Waa"] Wya = parameters["Wya"] ba = parameters["ba"] by = parameters["by"] ### START CODE HERE ### (≈2 lines) # compute next activation state using the formula given above a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba); # compute output of the current cell using the formula given above yt_pred = softmax(np.dot(Wya, a_next) + by); ### END CODE HERE ### # store values you need for backward propagation in cache cache = (a_next, a_prev, xt, parameters) return a_next, yt_pred, cache 123456789101112131415np.random.seed(1)xt = np.random.randn(3,10)a_prev = np.random.randn(5,10)Waa = np.random.randn(5,5)Wax = np.random.randn(5,3)Wya = np.random.randn(2,5)ba = np.random.randn(5,1)by = np.random.randn(2,1)parameters = &#123;"Waa": Waa, "Wax": Wax, "Wya": Wya, "ba": ba, "by": by&#125;a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)print("a_next[4] = ", a_next[4])print("a_next.shape = ", a_next.shape)print("yt_pred[1] =", yt_pred[1])print("yt_pred.shape = ", yt_pred.shape) a_next[4] = [ 0.59584544 0.18141802 0.61311866 0.99808218 0.85016201 0.99980978 -0.18887155 0.99815551 0.6531151 0.82872037] a_next.shape = (5, 10) yt_pred[1] = [ 0.9888161 0.01682021 0.21140899 0.36817467 0.98988387 0.88945212 0.36920224 0.9966312 0.9982559 0.17746526] yt_pred.shape = (2, 10) Expected Output: a_next[4]: [ 0.59584544 0.18141802 0.61311866 0.99808218 0.85016201 0.99980978 -0.18887155 0.99815551 0.6531151 0.82872037] a_next.shape: (5, 10) yt[1]: [ 0.9888161 0.01682021 0.21140899 0.36817467 0.98988387 0.88945212 0.36920224 0.9966312 0.9982559 0.17746526] yt.shape: (2, 10) 1.2 - RNN forward passYou can see an RNN as the repetition of the cell you’ve just built. If your input sequence of data is carried over 10 time steps, then you will copy the RNN cell 10 times. Each cell takes as input the hidden state from the previous cell ($a^{\langle t-1 \rangle}$) and the current time-step’s input data ($x^{\langle t \rangle}$). It outputs a hidden state ($a^{\langle t \rangle}$) and a prediction ($y^{\langle t \rangle}$) for this time-step. Figure 3: Basic RNN. The input sequence $x = (x^{\langle 1 \rangle}, x^{\langle 2 \rangle}, …, x^{\langle T_x \rangle})$ is carried over $T_x$ time steps. The network outputs $y = (y^{\langle 1 \rangle}, y^{\langle 2 \rangle}, …, y^{\langle T_x \rangle})$. Exercise: Code the forward propagation of the RNN described in Figure (3). Instructions: Create a vector of zeros ($a$) that will store all the hidden states computed by the RNN. Initialize the “next” hidden state as $a_0$ (initial hidden state). Start looping over each time step, your incremental index is $t$ : Update the “next” hidden state and the cache by running rnn_cell_forward Store the “next” hidden state in $a$ ($t^{th}$ position) Store the prediction in y Add the cache to the list of caches Return $a$, $y$ and caches 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# GRADED FUNCTION: rnn_forwarddef rnn_forward(x, a0, parameters): """ Implement the forward propagation of the recurrent neural network described in Figure (3). Arguments: x -- Input data for every time-step, of shape (n_x, m, T_x). a0 -- Initial hidden state, of shape (n_a, m) parameters -- python dictionary containing: Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a) Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x) Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a) ba -- Bias numpy array of shape (n_a, 1) by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1) Returns: a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x) y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x) caches -- tuple of values needed for the backward pass, contains (list of caches, x) """ # Initialize "caches" which will contain the list of all caches caches = [] # Retrieve dimensions from shapes of x and Wy n_x, m, T_x = x.shape n_y, n_a = parameters["Wya"].shape ### START CODE HERE ### # initialize "a" and "y" with zeros (≈2 lines) a = np.zeros((n_a, m, T_x)); y_pred = np.zeros((n_y, m, T_x)); # Initialize a_next (≈1 line) a_next = a0; # loop over all time-steps for t in range(T_x): # Update next hidden state, compute the prediction, get the cache (≈1 line) a_next, yt_pred, cache = rnn_cell_forward(x[:, :, t], a_next, parameters); # Save the value of the new "next" hidden state in a (≈1 line) a[:, :, t] = a_next; # Save the value of the prediction in y (≈1 line) y_pred[:, :, t] = yt_pred; # Append "cache" to "caches" (≈1 line) caches.append(cache); ### END CODE HERE ### # store values needed for backward propagation in cache caches = (caches, x) return a, y_pred, caches 1234567891011121314151617np.random.seed(1)x = np.random.randn(3,10,4)a0 = np.random.randn(5,10)Waa = np.random.randn(5,5)Wax = np.random.randn(5,3)Wya = np.random.randn(2,5)ba = np.random.randn(5,1)by = np.random.randn(2,1)parameters = &#123;"Waa": Waa, "Wax": Wax, "Wya": Wya, "ba": ba, "by": by&#125;a, y_pred, caches = rnn_forward(x, a0, parameters)print("a[4][1] = ", a[4][1])print("a.shape = ", a.shape)print("y_pred[1][3] =", y_pred[1][3])print("y_pred.shape = ", y_pred.shape)print("caches[1][1][3] =", caches[1][1][3])print("len(caches) = ", len(caches)) a[4][1] = [-0.99999375 0.77911235 -0.99861469 -0.99833267] a.shape = (5, 10, 4) y_pred[1][3] = [ 0.79560373 0.86224861 0.11118257 0.81515947] y_pred.shape = (2, 10, 4) caches[1][1][3] = [-1.1425182 -0.34934272 -0.20889423 0.58662319] len(caches) = 2 Expected Output: a[4][1]: [-0.99999375 0.77911235 -0.99861469 -0.99833267] a.shape: (5, 10, 4) y[1][3]: [ 0.79560373 0.86224861 0.11118257 0.81515947] y.shape: (2, 10, 4) cache[1][1][3]: [-1.1425182 -0.34934272 -0.20889423 0.58662319] len(cache): 2 Congratulations! You’ve successfully built the forward propagation of a recurrent neural network from scratch. This will work well enough for some applications, but it suffers from vanishing gradient problems. So it works best when each output $y^{\langle t \rangle}$ can be estimated using mainly “local” context (meaning information from inputs $x^{\langle t’ \rangle}$ where $t’$ is not too far from $t$). In the next part, you will build a more complex LSTM model, which is better at addressing vanishing gradients. The LSTM will be better able to remember a piece of information and keep it saved for many timesteps. 2 - Long Short-Term Memory (LSTM) networkThis following figure shows the operations of an LSTM-cell. Figure 4: LSTM-cell. This tracks and updates a “cell state” or memory variable $c^{\langle t \rangle}$ at every time-step, which can be different from $a^{\langle t \rangle}$. Similar to the RNN example above, you will start by implementing the LSTM cell for a single time-step. Then you can iteratively call it from inside a for-loop to have it process an input with $T_x$ time-steps. About the gates- Forget gateFor the sake of this illustration, lets assume we are reading words in a piece of text, and want use an LSTM to keep track of grammatical structures, such as whether the subject is singular or plural. If the subject changes from a singular word to a plural word, we need to find a way to get rid of our previously stored memory value of the singular/plural state. In an LSTM, the forget gate lets us do this: $$\Gamma_f^{\langle t \rangle} = \sigma(W_f[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_f)\tag{1} $$ Here, $W_f$ are weights that govern the forget gate’s behavior. We concatenate $[a^{\langle t-1 \rangle}, x^{\langle t \rangle}]$ and multiply by $W_f$. The equation above results in a vector $\Gamma_f^{\langle t \rangle}$ with values between 0 and 1. This forget gate vector will be multiplied element-wise by the previous cell state $c^{\langle t-1 \rangle}$. So if one of the values of $\Gamma_f^{\langle t \rangle}$ is 0 (or close to 0) then it means that the LSTM should remove that piece of information (e.g. the singular subject) in the corresponding component of $c^{\langle t-1 \rangle}$. If one of the values is 1, then it will keep the information. - Update gateOnce we forget that the subject being discussed is singular, we need to find a way to update it to reflect that the new subject is now plural. Here is the formulat for the update gate: $$\Gamma_u^{\langle t \rangle} = \sigma(W_u[a^{\langle t-1 \rangle}, x^{\{t\}}] + b_u)\tag{2} $$ Similar to the forget gate, here $\Gamma_u^{\langle t \rangle}$ is again a vector of values between 0 and 1. This will be multiplied element-wise with $\tilde{c}^{\langle t \rangle}$, in order to compute $c^{\langle t \rangle}$. - Updating the cellTo update the new subject we need to create a new vector of numbers that we can add to our previous cell state. The equation we use is: $$ \tilde{c}^{\langle t \rangle} = \tanh(W_c[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_c)\tag{3} $$ Finally, the new cell state is: $$ c^{\langle t \rangle} = \Gamma_f^{\langle t \rangle}* c^{\langle t-1 \rangle} + \Gamma_u^{\langle t \rangle} *\tilde{c}^{\langle t \rangle} \tag{4} $$ - Output gateTo decide which outputs we will use, we will use the following two formulas: $$ \Gamma_o^{\langle t \rangle}= \sigma(W_o[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_o)\tag{5}$$$$ a^{\langle t \rangle} = \Gamma_o^{\langle t \rangle}* \tanh(c^{\langle t \rangle})\tag{6} $$ Where in equation 5 you decide what to output using a sigmoid function and in equation 6 you multiply that by the $\tanh$ of the previous state. 2.1 - LSTM cellExercise: Implement the LSTM cell described in the Figure (3). Instructions: Concatenate $a^{\langle t-1 \rangle}$ and $x^{\langle t \rangle}$ in a single matrix: $concat = \begin{bmatrix} a^{\langle t-1 \rangle} \\ x^{\langle t \rangle} \end{bmatrix}$ Compute all the formulas 1-6. You can use sigmoid() (provided) and np.tanh(). Compute the prediction $y^{\langle t \rangle}$. You can use softmax() (provided). 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# GRADED FUNCTION: lstm_cell_forwarddef lstm_cell_forward(xt, a_prev, c_prev, parameters): """ Implement a single forward step of the LSTM-cell as described in Figure (4) Arguments: xt -- your input data at timestep "t", numpy array of shape (n_x, m). a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m) c_prev -- Memory state at timestep "t-1", numpy array of shape (n_a, m) parameters -- python dictionary containing: Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x) bf -- Bias of the forget gate, numpy array of shape (n_a, 1) Wi -- Weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x) bi -- Bias of the save gate, numpy array of shape (n_a, 1) Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x) bc -- Bias of the first "tanh", numpy array of shape (n_a, 1) Wo -- Weight matrix of the focus gate, numpy array of shape (n_a, n_a + n_x) bo -- Bias of the focus gate, numpy array of shape (n_a, 1) Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a) by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1) Returns: a_next -- next hidden state, of shape (n_a, m) c_next -- next memory state, of shape (n_a, m) yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m) cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters) Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilda), c stands for the memory value """ # Retrieve parameters from "parameters" Wf = parameters["Wf"] bf = parameters["bf"] Wi = parameters["Wi"] bi = parameters["bi"] Wc = parameters["Wc"] bc = parameters["bc"] Wo = parameters["Wo"] bo = parameters["bo"] Wy = parameters["Wy"] by = parameters["by"] # Retrieve dimensions from shapes of xt and Wy n_x, m = xt.shape n_y, n_a = Wy.shape ### START CODE HERE ### # Concatenate a_prev and xt (≈3 lines) concat = np.zeros((n_a + n_x, m)); concat[: n_a, :] = a_prev; concat[n_a :, :] = xt; # Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines) ft = sigmoid(np.dot(Wf, concat) + bf); it = sigmoid(np.dot(Wi, concat) + bi); cct = np.tanh(np.dot(Wc, concat) + bc); c_next = ft * c_prev + it * cct; ot = sigmoid(np.dot(Wo, concat) + bo); a_next = ot * np.tanh(c_next); # Compute prediction of the LSTM cell (≈1 line) yt_pred = softmax(np.dot(Wy, a_next) + by); ### END CODE HERE ### # store values needed for backward propagation in cache cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) return a_next, c_next, yt_pred, cache 1234567891011121314151617181920212223242526np.random.seed(1)xt = np.random.randn(3,10)a_prev = np.random.randn(5,10)c_prev = np.random.randn(5,10)Wf = np.random.randn(5, 5+3)bf = np.random.randn(5,1)Wi = np.random.randn(5, 5+3)bi = np.random.randn(5,1)Wo = np.random.randn(5, 5+3)bo = np.random.randn(5,1)Wc = np.random.randn(5, 5+3)bc = np.random.randn(5,1)Wy = np.random.randn(2,5)by = np.random.randn(2,1)parameters = &#123;"Wf": Wf, "Wi": Wi, "Wo": Wo, "Wc": Wc, "Wy": Wy, "bf": bf, "bi": bi, "bo": bo, "bc": bc, "by": by&#125;a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)print("a_next[4] = ", a_next[4])print("a_next.shape = ", c_next.shape)print("c_next[2] = ", c_next[2])print("c_next.shape = ", c_next.shape)print("yt[1] =", yt[1])print("yt.shape = ", yt.shape)print("cache[1][3] =", cache[1][3])print("len(cache) = ", len(cache)) a_next[4] = [-0.66408471 0.0036921 0.02088357 0.22834167 -0.85575339 0.00138482 0.76566531 0.34631421 -0.00215674 0.43827275] a_next.shape = (5, 10) c_next[2] = [ 0.63267805 1.00570849 0.35504474 0.20690913 -1.64566718 0.11832942 0.76449811 -0.0981561 -0.74348425 -0.26810932] c_next.shape = (5, 10) yt[1] = [ 0.79913913 0.15986619 0.22412122 0.15606108 0.97057211 0.31146381 0.00943007 0.12666353 0.39380172 0.07828381] yt.shape = (2, 10) cache[1][3] = [-0.16263996 1.03729328 0.72938082 -0.54101719 0.02752074 -0.30821874 0.07651101 -1.03752894 1.41219977 -0.37647422] len(cache) = 10 Expected Output : a_next[4]: [-0.66408471 0.0036921 0.02088357 0.22834167 -0.85575339 0.00138482 0.76566531 0.34631421 -0.00215674 0.43827275] a_next.shape: (5, 10) c_next[2]: [ 0.63267805 1.00570849 0.35504474 0.20690913 -1.64566718 0.11832942 0.76449811 -0.0981561 -0.74348425 -0.26810932] c_next.shape: (5, 10) yt[1]: [ 0.79913913 0.15986619 0.22412122 0.15606108 0.97057211 0.31146381 0.00943007 0.12666353 0.39380172 0.07828381] yt.shape: (2, 10) cache[1][3]: [-0.16263996 1.03729328 0.72938082 -0.54101719 0.02752074 -0.30821874 0.07651101 -1.03752894 1.41219977 -0.37647422] len(cache): 10 2.2 - Forward pass for LSTMNow that you have implemented one step of an LSTM, you can now iterate this over this using a for-loop to process a sequence of $T_x$ inputs. Figure 4: LSTM over multiple time-steps. Exercise: Implement lstm_forward() to run an LSTM over $T_x$ time-steps. Note: $c^{\langle 0 \rangle}$ is initialized with zeros. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# GRADED FUNCTION: lstm_forwarddef lstm_forward(x, a0, parameters): """ Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3). Arguments: x -- Input data for every time-step, of shape (n_x, m, T_x). a0 -- Initial hidden state, of shape (n_a, m) parameters -- python dictionary containing: Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x) bf -- Bias of the forget gate, numpy array of shape (n_a, 1) Wi -- Weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x) bi -- Bias of the save gate, numpy array of shape (n_a, 1) Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x) bc -- Bias of the first "tanh", numpy array of shape (n_a, 1) Wo -- Weight matrix of the focus gate, numpy array of shape (n_a, n_a + n_x) bo -- Bias of the focus gate, numpy array of shape (n_a, 1) Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a) by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1) Returns: a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x) y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x) caches -- tuple of values needed for the backward pass, contains (list of all the caches, x) """ # Initialize "caches", which will track the list of all the caches caches = [] ### START CODE HERE ### # Retrieve dimensions from shapes of xt and Wy (≈2 lines) n_x, m, T_x = x.shape; n_y, n_a = parameters['Wy'].shape; # initialize "a", "c" and "y" with zeros (≈3 lines) a = np.zeros((n_a, m, T_x)); c = np.zeros((n_a, m, T_x)); y = np.zeros((n_y, m, T_x)); # Initialize a_next and c_next (≈2 lines) a_next = a0; c_next = np.zeros((n_a, m)); # loop over all time-steps for t in range(T_x): # Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line) a_next, c_next, yt_pred, cache = lstm_cell_forward(x[:, :, t], a_next, c_next, parameters); # Save the value of the new "next" hidden state in a (≈1 line) a[:, :, t] = a_next; # Save the value of the prediction in y (≈1 line) y[:, :, t] = yt_pred; # Save the value of the next cell state (≈1 line) c[:, :, t] = c_next; # Append the cache into caches (≈1 line) caches.append(cache); ### END CODE HERE ### # store values needed for backward propagation in cache caches = (caches, x) return a, y, c, caches 123456789101112131415161718192021222324np.random.seed(1)x = np.random.randn(3,10,7)a0 = np.random.randn(5,10)Wf = np.random.randn(5, 5+3)bf = np.random.randn(5,1)Wi = np.random.randn(5, 5+3)bi = np.random.randn(5,1)Wo = np.random.randn(5, 5+3)bo = np.random.randn(5,1)Wc = np.random.randn(5, 5+3)bc = np.random.randn(5,1)Wy = np.random.randn(2,5)by = np.random.randn(2,1)parameters = &#123;"Wf": Wf, "Wi": Wi, "Wo": Wo, "Wc": Wc, "Wy": Wy, "bf": bf, "bi": bi, "bo": bo, "bc": bc, "by": by&#125;a, y, c, caches = lstm_forward(x, a0, parameters)print("a[4][3][6] = ", a[4][3][6])print("a.shape = ", a.shape)print("y[1][4][3] =", y[1][4][3])print("y.shape = ", y.shape)print("caches[1][1[1]] =", caches[1][1][1])print("c[1][2][1]", c[1][2][1])print("len(caches) = ", len(caches)) a[4][3][6] = 0.172117767533 a.shape = (5, 10, 7) y[1][4][3] = 0.95087346185 y.shape = (2, 10, 7) caches[1][1[1]] = [ 0.82797464 0.23009474 0.76201118 -0.22232814 -0.20075807 0.18656139 0.41005165] c[1][2][1] -0.855544916718 len(caches) = 2 Expected Output: a[4][3][6] = 0.172117767533 a.shape = (5, 10, 7) y[1][4][3] = 0.95087346185 y.shape = (2, 10, 7) caches[1][1][1] = [ 0.82797464 0.23009474 0.76201118 -0.22232814 -0.20075807 0.18656139 0.41005165] c[1][2][1] = -0.855544916718 len(caches) = 2 Congratulations! You have now implemented the forward passes for the basic RNN and the LSTM. When using a deep learning framework, implementing the forward pass is sufficient to build systems that achieve great performance. The rest of this notebook is optional, and will not be graded. 3 - Backpropagation in recurrent neural networks (OPTIONAL / UNGRADED)In modern deep learning frameworks, you only have to implement the forward pass, and the framework takes care of the backward pass, so most deep learning engineers do not need to bother with the details of the backward pass. If however you are an expert in calculus and want to see the details of backprop in RNNs, you can work through this optional portion of the notebook. When in an earlier course you implemented a simple (fully connected) neural network, you used backpropagation to compute the derivatives with respect to the cost to update the parameters. Similarly, in recurrent neural networks you can to calculate the derivatives with respect to the cost in order to update the parameters. The backprop equations are quite complicated and we did not derive them in lecture. However, we will briefly present them below. 3.1 - Basic RNN backward passWe will start by computing the backward pass for the basic RNN-cell. Figure 5: RNN-cell’s backward pass. Just like in a fully-connected neural network, the derivative of the cost function $J$ backpropagates through the RNN by following the chain-rule from calculas. The chain-rule is also used to calculate $(\frac{\partial J}{\partial W_{ax}},\frac{\partial J}{\partial W_{aa}},\frac{\partial J}{\partial b})$ to update the parameters $(W_{ax}, W_{aa}, b_a)$. Deriving the one step backward functions:To compute the rnn_cell_backward you need to compute the following equations. It is a good exercise to derive them by hand. The derivative of $\tanh$ is $1-\tanh(x)^2$. You can find the complete proof here. Note that: $ \text{sech}(x)^2 = 1 - \tanh(x)^2$ Similarly for $\frac{ \partial a^{\langle t \rangle} } {\partial W_{ax}}, \frac{ \partial a^{\langle t \rangle} } {\partial W_{aa}}, \frac{ \partial a^{\langle t \rangle} } {\partial b}$, the derivative of $\tanh(u)$ is $(1-\tanh(u)^2)du$. The final two equations also follow same rule and are derived using the $\tanh$ derivative. Note that the arrangement is done in a way to get the same dimensions to match. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def rnn_cell_backward(da_next, cache): """ Implements the backward pass for the RNN-cell (single time-step). Arguments: da_next -- Gradient of loss with respect to next hidden state cache -- python dictionary containing useful values (output of rnn_step_forward()) Returns: gradients -- python dictionary containing: dx -- Gradients of input data, of shape (n_x, m) da_prev -- Gradients of previous hidden state, of shape (n_a, m) dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x) dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a) dba -- Gradients of bias vector, of shape (n_a, 1) """ # Retrieve values from cache (a_next, a_prev, xt, parameters) = cache # Retrieve values from parameters Wax = parameters["Wax"] Waa = parameters["Waa"] Wya = parameters["Wya"] ba = parameters["ba"] by = parameters["by"] ### START CODE HERE ### # compute the gradient of tanh with respect to a_next (≈1 line) dtanh = (1 - a_next * a_next) * da_next; # compute the gradient of the loss with respect to Wax (≈2 lines) dWax = np.dot(dtanh, xt.T); dxt = np.dot(Wax.T, dtanh); # compute the gradient with respect to Waa (≈2 lines) dWaa = np.dot(dtanh, a_prev.T); da_prev = np.dot(Waa.T, dtanh); # compute the gradient with respect to b (≈1 line) dba = np.sum(dtanh, keepdims = True, axis = -1); ### END CODE HERE ### # Store the gradients in a python dictionary gradients = &#123;"dxt": dxt, "da_prev": da_prev, "dWax": dWax, "dWaa": dWaa, "dba": dba&#125; return gradients 123456789101112131415161718192021222324np.random.seed(1)xt = np.random.randn(3,10)a_prev = np.random.randn(5,10)Wax = np.random.randn(5,3)Waa = np.random.randn(5,5)Wya = np.random.randn(2,5)b = np.random.randn(5,1)by = np.random.randn(2,1)parameters = &#123;"Wax": Wax, "Waa": Waa, "Wya": Wya, "ba": ba, "by": by&#125;a_next, yt, cache = rnn_cell_forward(xt, a_prev, parameters)da_next = np.random.randn(5,10)gradients = rnn_cell_backward(da_next, cache)print("gradients[\"dxt\"][1][2] =", gradients["dxt"][1][2])print("gradients[\"dxt\"].shape =", gradients["dxt"].shape)print("gradients[\"da_prev\"][2][3] =", gradients["da_prev"][2][3])print("gradients[\"da_prev\"].shape =", gradients["da_prev"].shape)print("gradients[\"dWax\"][3][1] =", gradients["dWax"][3][1])print("gradients[\"dWax\"].shape =", gradients["dWax"].shape)print("gradients[\"dWaa\"][1][2] =", gradients["dWaa"][1][2])print("gradients[\"dWaa\"].shape =", gradients["dWaa"].shape)print("gradients[\"dba\"][4] =", gradients["dba"][4])print("gradients[\"dba\"].shape =", gradients["dba"].shape) gradients[&quot;dxt&quot;][1][2] = -0.460564103059 gradients[&quot;dxt&quot;].shape = (3, 10) gradients[&quot;da_prev&quot;][2][3] = 0.0842968653807 gradients[&quot;da_prev&quot;].shape = (5, 10) gradients[&quot;dWax&quot;][3][1] = 0.393081873922 gradients[&quot;dWax&quot;].shape = (5, 3) gradients[&quot;dWaa&quot;][1][2] = -0.28483955787 gradients[&quot;dWaa&quot;].shape = (5, 5) gradients[&quot;dba&quot;][4] = [ 0.80517166] gradients[&quot;dba&quot;].shape = (5, 1) Expected Output: gradients[“dxt”][1][2] = -0.460564103059 gradients[“dxt”].shape = (3, 10) gradients[“da_prev”][2][3] = 0.0842968653807 gradients[“da_prev”].shape = (5, 10) gradients[“dWax”][3][1] = 0.393081873922 gradients[“dWax”].shape = (5, 3) gradients[“dWaa”][1][2] = -0.28483955787 gradients[“dWaa”].shape = (5, 5) gradients[“dba”][4] = [ 0.80517166] gradients[“dba”].shape = (5, 1) Backward pass through the RNNComputing the gradients of the cost with respect to $a^{\langle t \rangle}$ at every time-step $t$ is useful because it is what helps the gradient backpropagate to the previous RNN-cell. To do so, you need to iterate through all the time steps starting at the end, and at each step, you increment the overall $db_a$, $dW_{aa}$, $dW_{ax}$ and you store $dx$. Instructions: Implement the rnn_backward function. Initialize the return variables with zeros first and then loop through all the time steps while calling the rnn_cell_backward at each time timestep, update the other variables accordingly. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354def rnn_backward(da, caches): """ Implement the backward pass for a RNN over an entire sequence of input data. Arguments: da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x) caches -- tuple containing information from the forward pass (rnn_forward) Returns: gradients -- python dictionary containing: dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x) da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m) dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x) dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a) dba -- Gradient w.r.t the bias, of shape (n_a, 1) """ ### START CODE HERE ### # Retrieve values from the first cache (t=1) of caches (≈2 lines) caches, x = caches; a1, a0, x1, parameters = caches[0]; # Retrieve dimensions from da's and x1's shapes (≈2 lines) n_a, m, T_x = da.shape; n_x, m = x1.shape; # initialize the gradients with the right sizes (≈6 lines) dx = np.zeros((n_x, m, T_x)); dWax = np.zeros((parameters['Wax'].shape)); dWaa = np.zeros((parameters['Waa'].shape)); dba = np.zeros((parameters['ba'].shape)); da0 = np.zeros(a0.shape); da_prevt = np.zeros((n_a, m)); # Loop through all the time steps for t in reversed(range(T_x)): # Compute gradients at time step t. Choose wisely the "da_next" and the "cache" to use in the backward propagation step. (≈1 line) gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t]); # Retrieve derivatives from gradients (≈ 1 line) dxt, da_prevt, dWaxt, dWaat, dbat = gradients['dxt'], gradients['da_prev'], gradients['dWax'], gradients['dWaa'], gradients['dba']; # Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines) dWax += dWaxt; dWaa += dWaat; dba += dbat; dx[:, :, t] = dxt; # Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) da0 = da_prevt; ### END CODE HERE ### # Store the gradients in a python dictionary gradients = &#123;"dx": dx, "da0": da0, "dWax": dWax, "dWaa": dWaa,"dba": dba&#125; return gradients 1234567891011121314151617181920212223np.random.seed(1)x = np.random.randn(3,10,4)a0 = np.random.randn(5,10)Wax = np.random.randn(5,3)Waa = np.random.randn(5,5)Wya = np.random.randn(2,5)ba = np.random.randn(5,1)by = np.random.randn(2,1)parameters = &#123;"Wax": Wax, "Waa": Waa, "Wya": Wya, "ba": ba, "by": by&#125;a, y, caches = rnn_forward(x, a0, parameters)da = np.random.randn(5, 10, 4)gradients = rnn_backward(da, caches)print("gradients[\"dx\"][1][2] =", gradients["dx"][1][2])print("gradients[\"dx\"].shape =", gradients["dx"].shape)print("gradients[\"da0\"][2][3] =", gradients["da0"][2][3])print("gradients[\"da0\"].shape =", gradients["da0"].shape)print("gradients[\"dWax\"][3][1] =", gradients["dWax"][3][1])print("gradients[\"dWax\"].shape =", gradients["dWax"].shape)print("gradients[\"dWaa\"][1][2] =", gradients["dWaa"][1][2])print("gradients[\"dWaa\"].shape =", gradients["dWaa"].shape)print("gradients[\"dba\"][4] =", gradients["dba"][4])print("gradients[\"dba\"].shape =", gradients["dba"].shape) gradients[&quot;dx&quot;][1][2] = [-2.07101689 -0.59255627 0.02466855 0.01483317] gradients[&quot;dx&quot;].shape = (3, 10, 4) gradients[&quot;da0&quot;][2][3] = -0.314942375127 gradients[&quot;da0&quot;].shape = (5, 10) gradients[&quot;dWax&quot;][3][1] = 11.2641044965 gradients[&quot;dWax&quot;].shape = (5, 3) gradients[&quot;dWaa&quot;][1][2] = 2.30333312658 gradients[&quot;dWaa&quot;].shape = (5, 5) gradients[&quot;dba&quot;][4] = [-0.74747722] gradients[&quot;dba&quot;].shape = (5, 1) Expected Output: gradients[“dx”][1][2] = [-2.07101689 -0.59255627 0.02466855 0.01483317] gradients[“dx”].shape = (3, 10, 4) gradients[“da0”][2][3] = -0.314942375127 gradients[“da0”].shape = (5, 10) gradients[“dWax”][3][1] = 11.2641044965 gradients[“dWax”].shape = (5, 3) gradients[“dWaa”][1][2] = 2.30333312658 gradients[“dWaa”].shape = (5, 5) gradients[“dba”][4] = [-0.74747722] gradients[“dba”].shape = (5, 1) 3.2 - LSTM backward pass3.2.1 One Step backwardThe LSTM backward pass is slighltly more complicated than the forward one. We have provided you with all the equations for the LSTM backward pass below. (If you enjoy calculus exercises feel free to try deriving these from scratch yourself.) 3.2.2 gate derivatives $$d \Gamma_o^{\langle t \rangle} = da_{next}*\tanh(c_{next}) * \Gamma_o^{\langle t \rangle}*(1-\Gamma_o^{\langle t \rangle})\tag{7}$$ $$d\tilde c^{\langle t \rangle} = dc_{next}*\Gamma_u^{\langle t \rangle}+ \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * i_t * da_{next} * \tilde c^{\langle t \rangle} * (1-\tanh(\tilde c)^2) \tag{8}$$ $$d\Gamma_u^{\langle t \rangle} = dc_{next}*\tilde c^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * \tilde c^{\langle t \rangle} * da_{next}*\Gamma_u^{\langle t \rangle}*(1-\Gamma_u^{\langle t \rangle})\tag{9}$$ $$d\Gamma_f^{\langle t \rangle} = dc_{next}*\tilde c_{prev} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * c_{prev} * da_{next}*\Gamma_f^{\langle t \rangle}*(1-\Gamma_f^{\langle t \rangle})\tag{10}$$ 3.2.3 parameter derivatives $$ dW_f = d\Gamma_f^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{11} $$ $$ dW_u = d\Gamma_u^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{12} $$ $$ dW_c = d\tilde c^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{13} $$ $$ dW_o = d\Gamma_o^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{14}$$ To calculate $db_f, db_u, db_c, db_o$ you just need to sum across the horizontal (axis= 1) axis on $d\Gamma_f^{\langle t \rangle}, d\Gamma_u^{\langle t \rangle}, d\tilde c^{\langle t \rangle}, d\Gamma_o^{\langle t \rangle}$ respectively. Note that you should have the `keep_dims = True` option. Finally, you will compute the derivative with respect to the previous hidden state, previous memory state, and input. $$ da_{prev} = W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c^{\langle t \rangle} + W_o^T * d\Gamma_o^{\langle t \rangle} \tag{15}$$ Here, the weights for equations 13 are the first n_a, (i.e. $W_f = W_f[:n_a,:]$ etc...) $$ dc_{prev} = dc_{next}\Gamma_f^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} * (1- \tanh(c_{next})^2)*\Gamma_f^{\langle t \rangle}*da_{next} \tag{16}$$ $$ dx^{\langle t \rangle} = W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c_t + W_o^T * d\Gamma_o^{\langle t \rangle}\tag{17} $$ where the weights for equation 15 are from n_a to the end, (i.e. $W_f = W_f[n_a:,:]$ etc...) **Exercise:** Implement `lstm_cell_backward` by implementing equations $7-17$ below. Good luck! :) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364def lstm_cell_backward(da_next, dc_next, cache): """ Implement the backward pass for the LSTM-cell (single time-step). Arguments: da_next -- Gradients of next hidden state, of shape (n_a, m) dc_next -- Gradients of next cell state, of shape (n_a, m) cache -- cache storing information from the forward pass Returns: gradients -- python dictionary containing: dxt -- Gradient of input data at time-step t, of shape (n_x, m) da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m) dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x) dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x) dWi -- Gradient w.r.t. the weight matrix of the input gate, numpy array of shape (n_a, n_a + n_x) dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x) dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x) dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1) dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1) dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1) dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1) """ # Retrieve information from "cache" (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache ### START CODE HERE ### n_a, m = a_next.shape; n_x, m = xt.shape; # Compute gates related derivatives, you can find their values can be found by looking carefully at equations (7) to (10) (≈4 lines) dot = da_next * np.tanh(c_next) * ot * (1 - ot); dcct = (dc_next * it + ot * (1 - np.tanh(c_next) ** 2) * it * da_next) * (1 - cct ** 2); dit = (dc_next * cct + ot * (1 - np.tanh(c_next) ** 2) * cct * da_next) * it * (1 - it); dft = (dc_next * c_prev + ot * (1 - np.tanh(c_next) ** 2) * c_prev * da_next) * ft * (1 - ft); ## Code equations (7) to (10) (≈4 lines) ##dit = None ##dft = None ##dot = None ##dcct = None ## # Compute parameters related derivatives. Use equations (11)-(14) (≈8 lines) concat = np.concatenate((a_prev, xt), axis = 0).T; dWf = np.dot(dft, concat); dWi = np.dot(dit, concat); dWc = np.dot(dcct, concat); dWo = np.dot(dot, concat); dbf = np.sum(dft, keepdims = True, axis = -1); dbi = np.sum(dit, keepdims = True, axis = -1); dbc = np.sum(dcct, keepdims = True, axis = -1); dbo = np.sum(dot, keepdims = True, axis = -1); # Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (15)-(17). (≈3 lines) da_prev = np.dot(parameters['Wf'][:, : n_a].T, dft) + np.dot(parameters['Wi'][:, : n_a].T, dit) + np.dot(parameters['Wc'][:, : n_a].T, dcct) + np.dot(parameters['Wo'][:, : n_a].T, dot); dc_prev = dc_next * ft + ot * (1 - np.tanh(c_next) ** 2) * ft * da_next; dxt = np.dot(parameters['Wf'][:, n_a :].T, dft) + np.dot(parameters['Wi'][:, n_a :].T, dit) + np.dot(parameters['Wc'][:, n_a :].T, dcct) + np.dot(parameters['Wo'][:, n_a :].T, dot); ### END CODE HERE ### # Save gradients in dictionary gradients = &#123;"dxt": dxt, "da_prev": da_prev, "dc_prev": dc_prev, "dWf": dWf,"dbf": dbf, "dWi": dWi,"dbi": dbi, "dWc": dWc,"dbc": dbc, "dWo": dWo,"dbo": dbo&#125; return gradients 1234567891011121314151617181920212223242526272829303132333435363738394041424344np.random.seed(1)xt = np.random.randn(3,10)a_prev = np.random.randn(5,10)c_prev = np.random.randn(5,10)Wf = np.random.randn(5, 5+3)bf = np.random.randn(5,1)Wi = np.random.randn(5, 5+3)bi = np.random.randn(5,1)Wo = np.random.randn(5, 5+3)bo = np.random.randn(5,1)Wc = np.random.randn(5, 5+3)bc = np.random.randn(5,1)Wy = np.random.randn(2,5)by = np.random.randn(2,1)parameters = &#123;"Wf": Wf, "Wi": Wi, "Wo": Wo, "Wc": Wc, "Wy": Wy, "bf": bf, "bi": bi, "bo": bo, "bc": bc, "by": by&#125;a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)da_next = np.random.randn(5,10)dc_next = np.random.randn(5,10)gradients = lstm_cell_backward(da_next, dc_next, cache)print("gradients[\"dxt\"][1][2] =", gradients["dxt"][1][2])print("gradients[\"dxt\"].shape =", gradients["dxt"].shape)print("gradients[\"da_prev\"][2][3] =", gradients["da_prev"][2][3])print("gradients[\"da_prev\"].shape =", gradients["da_prev"].shape)print("gradients[\"dc_prev\"][2][3] =", gradients["dc_prev"][2][3])print("gradients[\"dc_prev\"].shape =", gradients["dc_prev"].shape)print("gradients[\"dWf\"][3][1] =", gradients["dWf"][3][1])print("gradients[\"dWf\"].shape =", gradients["dWf"].shape)print("gradients[\"dWi\"][1][2] =", gradients["dWi"][1][2])print("gradients[\"dWi\"].shape =", gradients["dWi"].shape)print("gradients[\"dWc\"][3][1] =", gradients["dWc"][3][1])print("gradients[\"dWc\"].shape =", gradients["dWc"].shape)print("gradients[\"dWo\"][1][2] =", gradients["dWo"][1][2])print("gradients[\"dWo\"].shape =", gradients["dWo"].shape)print("gradients[\"dbf\"][4] =", gradients["dbf"][4])print("gradients[\"dbf\"].shape =", gradients["dbf"].shape)print("gradients[\"dbi\"][4] =", gradients["dbi"][4])print("gradients[\"dbi\"].shape =", gradients["dbi"].shape)print("gradients[\"dbc\"][4] =", gradients["dbc"][4])print("gradients[\"dbc\"].shape =", gradients["dbc"].shape)print("gradients[\"dbo\"][4] =", gradients["dbo"][4])print("gradients[\"dbo\"].shape =", gradients["dbo"].shape) gradients[&quot;dxt&quot;][1][2] = 3.23055911511 gradients[&quot;dxt&quot;].shape = (3, 10) gradients[&quot;da_prev&quot;][2][3] = -0.0639621419711 gradients[&quot;da_prev&quot;].shape = (5, 10) gradients[&quot;dc_prev&quot;][2][3] = 0.797522038797 gradients[&quot;dc_prev&quot;].shape = (5, 10) gradients[&quot;dWf&quot;][3][1] = -0.147954838164 gradients[&quot;dWf&quot;].shape = (5, 8) gradients[&quot;dWi&quot;][1][2] = 1.05749805523 gradients[&quot;dWi&quot;].shape = (5, 8) gradients[&quot;dWc&quot;][3][1] = 2.30456216369 gradients[&quot;dWc&quot;].shape = (5, 8) gradients[&quot;dWo&quot;][1][2] = 0.331311595289 gradients[&quot;dWo&quot;].shape = (5, 8) gradients[&quot;dbf&quot;][4] = [ 0.18864637] gradients[&quot;dbf&quot;].shape = (5, 1) gradients[&quot;dbi&quot;][4] = [-0.40142491] gradients[&quot;dbi&quot;].shape = (5, 1) gradients[&quot;dbc&quot;][4] = [ 0.25587763] gradients[&quot;dbc&quot;].shape = (5, 1) gradients[&quot;dbo&quot;][4] = [ 0.13893342] gradients[&quot;dbo&quot;].shape = (5, 1) Expected Output: gradients[“dxt”][1][2] = 3.23055911511 gradients[“dxt”].shape = (3, 10) gradients[“da_prev”][2][3] = -0.0639621419711 gradients[“da_prev”].shape = (5, 10) gradients[“dc_prev”][2][3] = 0.797522038797 gradients[“dc_prev”].shape = (5, 10) gradients[“dWf”][3][1] = -0.147954838164 gradients[“dWf”].shape = (5, 8) gradients[“dWi”][1][2] = 1.05749805523 gradients[“dWi”].shape = (5, 8) gradients[“dWc”][3][1] = 2.30456216369 gradients[“dWc”].shape = (5, 8) gradients[“dWo”][1][2] = 0.331311595289 gradients[“dWo”].shape = (5, 8) gradients[“dbf”][4] = [ 0.18864637] gradients[“dbf”].shape = (5, 1) gradients[“dbi”][4] = [-0.40142491] gradients[“dbi”].shape = (5, 1) gradients[“dbc”][4] = [ 0.25587763] gradients[“dbc”].shape = (5, 1) gradients[“dbo”][4] = [ 0.13893342] gradients[“dbo”].shape = (5, 1) 3.3 Backward pass through the LSTM RNNThis part is very similar to the rnn_backward function you implemented above. You will first create variables of the same dimension as your return variables. You will then iterate over all the time steps starting from the end and call the one step function you implemented for LSTM at each iteration. You will then update the parameters by summing them individually. Finally return a dictionary with the new gradients. Instructions: Implement the lstm_backward function. Create a for loop starting from $T_x$ and going backward. For each step call lstm_cell_backward and update the your old gradients by adding the new gradients to them. Note that dxt is not updated but is stored. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172def lstm_backward(da, caches): """ Implement the backward pass for the RNN with LSTM-cell (over a whole sequence). Arguments: da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x) dc -- Gradients w.r.t the memory states, numpy-array of shape (n_a, m, T_x) caches -- cache storing information from the forward pass (lstm_forward) Returns: gradients -- python dictionary containing: dx -- Gradient of inputs, of shape (n_x, m, T_x) da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m) dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x) dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x) dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x) dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x) dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1) dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1) dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1) dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1) """ # Retrieve values from the first cache (t=1) of caches. (caches, x) = caches (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0] ### START CODE HERE ### # Retrieve dimensions from da's and x1's shapes (≈2 lines) n_a, m, T_x = da.shape n_x, m = x1.shape # initialize the gradients with the right sizes (≈12 lines) dx = np.zeros([n_x, m, T_x]) da0 = np.zeros([n_a, m]) da_prevt = np.zeros([n_a, m]) dc_prevt = np.zeros([n_a, m]) dWf = np.zeros([n_a, n_a + n_x]) dWi = np.zeros([n_a, n_a + n_x]) dWc = np.zeros([n_a, n_a + n_x]) dWo = np.zeros([n_a, n_a + n_x]) dbf = np.zeros([n_a, 1]) dbi = np.zeros([n_a, 1]) dbc = np.zeros([n_a, 1]) dbo = np.zeros([n_a, 1]) # loop back over the whole sequence for t in reversed(range(T_x)): # Compute all gradients using lstm_cell_backward gradients = lstm_cell_backward(da[:,:,t],dc_prevt,caches[t]) # da_prevt, dc_prevt = gradients['da_prev'], gradients["dc_prev"] # Store or add the gradient to the parameters' previous step's gradient dx[:,:,t] = gradients['dxt'] dWf = dWf+gradients['dWf'] dWi = dWi+gradients['dWi'] dWc = dWc+gradients['dWc'] dWo = dWo+gradients['dWo'] dbf = dbf+gradients['dbf'] dbi = dbi+gradients['dbi'] dbc = dbc+gradients['dbc'] dbo = dbo+gradients['dbo'] # Set the first activation's gradient to the backpropagated gradient da_prev. da0 = gradients['da_prev'] ### END CODE HERE ### # Store the gradients in a python dictionary gradients = &#123;"dx": dx, "da0": da0, "dWf": dWf,"dbf": dbf, "dWi": dWi,"dbi": dbi, "dWc": dWc,"dbc": dbc, "dWo": dWo,"dbo": dbo&#125; return gradients 123456789101112131415161718192021222324252627282930313233343536373839np.random.seed(1)x = np.random.randn(3,10,7)a0 = np.random.randn(5,10)Wf = np.random.randn(5, 5+3)bf = np.random.randn(5,1)Wi = np.random.randn(5, 5+3)bi = np.random.randn(5,1)Wo = np.random.randn(5, 5+3)bo = np.random.randn(5,1)Wc = np.random.randn(5, 5+3)bc = np.random.randn(5,1)parameters = &#123;"Wf": Wf, "Wi": Wi, "Wo": Wo, "Wc": Wc, "Wy": Wy, "bf": bf, "bi": bi, "bo": bo, "bc": bc, "by": by&#125;a, y, c, caches = lstm_forward(x, a0, parameters)da = np.random.randn(5, 10, 4)gradients = lstm_backward(da, caches)print("gradients[\"dx\"][1][2] =", gradients["dx"][1][2])print("gradients[\"dx\"].shape =", gradients["dx"].shape)print("gradients[\"da0\"][2][3] =", gradients["da0"][2][3])print("gradients[\"da0\"].shape =", gradients["da0"].shape)print("gradients[\"dWf\"][3][1] =", gradients["dWf"][3][1])print("gradients[\"dWf\"].shape =", gradients["dWf"].shape)print("gradients[\"dWi\"][1][2] =", gradients["dWi"][1][2])print("gradients[\"dWi\"].shape =", gradients["dWi"].shape)print("gradients[\"dWc\"][3][1] =", gradients["dWc"][3][1])print("gradients[\"dWc\"].shape =", gradients["dWc"].shape)print("gradients[\"dWo\"][1][2] =", gradients["dWo"][1][2])print("gradients[\"dWo\"].shape =", gradients["dWo"].shape)print("gradients[\"dbf\"][4] =", gradients["dbf"][4])print("gradients[\"dbf\"].shape =", gradients["dbf"].shape)print("gradients[\"dbi\"][4] =", gradients["dbi"][4])print("gradients[\"dbi\"].shape =", gradients["dbi"].shape)print("gradients[\"dbc\"][4] =", gradients["dbc"][4])print("gradients[\"dbc\"].shape =", gradients["dbc"].shape)print("gradients[\"dbo\"][4] =", gradients["dbo"][4])print("gradients[\"dbo\"].shape =", gradients["dbo"].shape) gradients[&quot;dx&quot;][1][2] = [-0.00173313 0.08287442 -0.30545663 -0.43281115] gradients[&quot;dx&quot;].shape = (3, 10, 4) gradients[&quot;da0&quot;][2][3] = -0.095911501954 gradients[&quot;da0&quot;].shape = (5, 10) gradients[&quot;dWf&quot;][3][1] = -0.0698198561274 gradients[&quot;dWf&quot;].shape = (5, 8) gradients[&quot;dWi&quot;][1][2] = 0.102371820249 gradients[&quot;dWi&quot;].shape = (5, 8) gradients[&quot;dWc&quot;][3][1] = -0.0624983794927 gradients[&quot;dWc&quot;].shape = (5, 8) gradients[&quot;dWo&quot;][1][2] = 0.0484389131444 gradients[&quot;dWo&quot;].shape = (5, 8) gradients[&quot;dbf&quot;][4] = [-0.0565788] gradients[&quot;dbf&quot;].shape = (5, 1) gradients[&quot;dbi&quot;][4] = [-0.15399065] gradients[&quot;dbi&quot;].shape = (5, 1) gradients[&quot;dbc&quot;][4] = [-0.29691142] gradients[&quot;dbc&quot;].shape = (5, 1) gradients[&quot;dbo&quot;][4] = [-0.29798344] gradients[&quot;dbo&quot;].shape = (5, 1) Expected Output: gradients[“dx”][1][2] = [-0.00173313 0.08287442 -0.30545663 -0.43281115] gradients[“dx”].shape = (3, 10, 4) gradients[“da0”][2][3] = -0.095911501954 gradients[“da0”].shape = (5, 10) gradients[“dWf”][3][1] = -0.0698198561274 gradients[“dWf”].shape = (5, 8) gradients[“dWi”][1][2] = 0.102371820249 gradients[“dWi”].shape = (5, 8) gradients[“dWc”][3][1] = -0.0624983794927 gradients[“dWc”].shape = (5, 8) gradients[“dWo”][1][2] = 0.0484389131444 gradients[“dWo”].shape = (5, 8) gradients[“dbf”][4] = [-0.0565788] gradients[“dbf”].shape = (5, 1) gradients[“dbi”][4] = [-0.06997391] gradients[“dbi”].shape = (5, 1) gradients[“dbc”][4] = [-0.27441821] gradients[“dbc”].shape = (5, 1) gradients[“dbo”][4] = [ 0.16532821] gradients[“dbo”].shape = (5, 1) Congratulations !Congratulations on completing this assignment. You now understand how recurrent neural networks work! Lets go on to the next exercise, where you’ll use an RNN to build a character-level language model.]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Improvise a Jazz Solo with an LSTM Network]]></title>
    <url>%2F2018%2F06%2F02%2FImprovise%2Ba%2BJazz%2BSolo%2Bwith%2Ban%2BLSTM%2BNetwork%2B-%2Bv3%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 1st week and the copyright belongs to deeplearning.ai. Improvise a Jazz Solo with an LSTM NetworkWelcome to your final programming assignment of this week! In this notebook, you will implement a model that uses an LSTM to generate music. You will even be able to listen to your own music at the end of the assignment. You will learn to: Apply an LSTM to music generation. Generate your own jazz music with deep learning. Please run the following cell to load all the packages required in this assignment. This may take a few minutes. 12345678910111213141516from __future__ import print_functionimport IPythonimport sysfrom music21 import *import numpy as npfrom grammar import *from qa import *from preprocess import * from music_utils import *from data_utils import *from keras.models import load_model, Modelfrom keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVectorfrom keras.initializers import glorot_uniformfrom keras.utils import to_categoricalfrom keras.optimizers import Adamfrom keras import backend as K Using TensorFlow backend. 1 - Problem statementYou would like to create a jazz music piece specially for a friend’s birthday. However, you don’t know any instruments or music composition. Fortunately, you know deep learning and will solve this problem using an LSTM netwok. You will train a network to generate novel jazz solos in a style representative of a body of performed work. 1.1 - DatasetYou will train your algorithm on a corpus of Jazz music. Run the cell below to listen to a snippet of the audio from the training set: 1IPython.display.Audio('./data/30s_seq.mp3') &lt;audio controls=&quot;controls&quot; &gt; &lt;source src=&quot;http://pwmpcnhis.bkt.clouddn.com/gitpage/deeplearning.ai/nlp-sequence-models/jupter/week1/Jazz_improvisation_with_LSTM/data/30s_seq.mp3&quot; type=&quot;audio/mpeg&quot; /&gt; Your browser does not support the audio element. &lt;/audio&gt; We have taken care of the preprocessing of the musical data to render it in terms of musical “values.” You can informally think of each “value” as a note, which comprises a pitch and a duration. For example, if you press down a specific piano key for 0.5 seconds, then you have just played a note. In music theory, a “value” is actually more complicated than this–specifically, it also captures the information needed to play multiple notes at the same time. For example, when playing a music piece, you might press down two piano keys at the same time (playng multiple notes at the same time generates what’s called a “chord”). But we don’t need to worry about the details of music theory for this assignment. For the purpose of this assignment, all you need to know is that we will obtain a dataset of values, and will learn an RNN model to generate sequences of values. Our music generation system will use 78 unique values. Run the following code to load the raw music data and preprocess it into values. This might take a few minutes. 123456X, Y, n_values, indices_values = load_music_utils()print('shape of X:', X.shape)print('number of training examples:', X.shape[0])print('Tx (length of sequence):', X.shape[1])print('total # of unique values:', n_values)print('Shape of Y:', Y.shape) shape of X: (60, 30, 78) number of training examples: 60 Tx (length of sequence): 30 total # of unique values: 78 Shape of Y: (30, 60, 78) You have just loaded the following: X: This is an (m, $T_x$, 78) dimensional array. We have m training examples, each of which is a snippet of $T_x =30$ musical values. At each time step, the input is one of 78 different possible values, represented as a one-hot vector. Thus for example, X[i,t,:] is a one-hot vector representating the value of the i-th example at time t. Y: This is essentially the same as X, but shifted one step to the left (to the past). Similar to the dinosaurus assignment, we’re interested in the network using the previous values to predict the next value, so our sequence model will try to predict $y^{\langle t \rangle}$ given $x^{\langle 1\rangle}, \ldots, x^{\langle t \rangle}$. However, the data in Y is reordered to be dimension $(T_y, m, 78)$, where $T_y = T_x$. This format makes it more convenient to feed to the LSTM later. n_values: The number of unique values in this dataset. This should be 78. indices_values: python dictionary mapping from 0-77 to musical values. 1.2 - Overview of our modelHere is the architecture of the model we will use. This is similar to the Dinosaurus model you had used in the previous notebook, except that in you will be implementing it in Keras. The architecture is as follows: We will be training the model on random snippets of 30 values taken from a much longer piece of music. Thus, we won’t bother to set the first input $x^{\langle 1 \rangle} = \vec{0}$, which we had done previously to denote the start of a dinosaur name, since now most of these snippets of audio start somewhere in the middle of a piece of music. We are setting each of the snippts to have the same length $T_x = 30$ to make vectorization easier. 2 - Building the modelIn this part you will build and train a model that will learn musical patterns. To do so, you will need to build a model that takes in X of shape $(m, T_x, 78)$ and Y of shape $(T_y, m, 78)$. We will use an LSTM with 64 dimensional hidden states. Lets set n_a = 64. 1n_a = 64 Here’s how you can create a Keras model with multiple inputs and outputs. If you’re building an RNN where even at test time entire input sequence $x^{\langle 1 \rangle}, x^{\langle 2 \rangle}, \ldots, x^{\langle T_x \rangle}$ were given in advance, for example if the inputs were words and the output was a label, then Keras has simple built-in functions to build the model. However, for sequence generation, at test time we don’t know all the values of $x^{\langle t\rangle}$ in advance; instead we generate them one at a time using $x^{\langle t\rangle} = y^{\langle t-1 \rangle}$. So the code will be a bit more complicated, and you’ll need to implement your own for-loop to iterate over the different time steps. The function djmodel() will call the LSTM layer $T_x$ times using a for-loop, and it is important that all $T_x$ copies have the same weights. I.e., it should not re-initiaiize the weights every time—the $T_x$ steps should have shared weights. The key steps for implementing layers with shareable weights in Keras are: Define the layer objects (we will use global variables for this). Call these objects when propagating the input. We have defined the layers objects you need as global variables. Please run the next cell to create them. Please check the Keras documentation to make sure you understand what these layers are: Reshape(), LSTM(), Dense(). 123reshapor = Reshape((1, 78)) # Used in Step 2.B of djmodel(), belowLSTM_cell = LSTM(n_a, return_state = True) # Used in Step 2.Cdensor = Dense(n_values, activation='softmax') # Used in Step 2.D Each of reshapor, LSTM_cell and densor are now layer objects, and you can use them to implement djmodel(). In order to propagate a Keras tensor object X through one of these layers, use layer_object(X) (or layer_object([X,Y]) if it requires multiple inputs.). For example, reshapor(X) will propagate X through the Reshape((1,78)) layer defined above. Exercise: Implement djmodel(). You will need to carry out 2 steps: Create an empty list “outputs” to save the outputs of the LSTM Cell at every time step. Loop for $t \in 1, \ldots, T_x$: A. Select the “t”th time-step vector from X. The shape of this selection should be (78,). To do so, create a custom Lambda layer in Keras by using this line of code: 123456789 x = Lambda(lambda x: X[:,t,:])(X)``` Look over the Keras documentation to figure out what this does. It is creating a "temporary" or "unnamed" function (that's what Lambda functions are) that extracts out the appropriate one-hot vector, and making this function a Keras `Layer` object to apply to `X`. B. Reshape x to be (1,78). You may find the `reshapor()` layer (defined below) helpful. C. Run x through one step of LSTM_cell. Remember to initialize the LSTM_cell with the previous step's hidden state $a$ and cell state $c$. Use the following formatting:```pythona, _, c = LSTM_cell(input_x, initial_state=[previous hidden state, previous cell state]) D. Propagate the LSTM’s output activation value through a dense+softmax layer using densor. E. Append the predicted value to the list of “outputs” 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# GRADED FUNCTION: djmodeldef djmodel(Tx, n_a, n_values): """ Implement the model Arguments: Tx -- length of the sequence in a corpus n_a -- the number of activations used in our model n_values -- number of unique values in the music data Returns: model -- a keras model with the """ # Define the input of your model with a shape X = Input(shape=(Tx, n_values)) # Define s0, initial hidden state for the decoder LSTM a0 = Input(shape=(n_a,), name='a0') c0 = Input(shape=(n_a,), name='c0') a = a0 c = c0 ### START CODE HERE ### # Step 1: Create empty list to append the outputs while you iterate (≈1 line) outputs = []; # Step 2: Loop for t in range(Tx): # Step 2.A: select the "t"th time step vector from X. x = Lambda(lambda x: X[:,t,:])(X); # Step 2.B: Use reshapor to reshape x to be (1, n_values) (≈1 line) x = reshapor(x); # Step 2.C: Perform one step of the LSTM_cell a, _, c = LSTM_cell(x, initial_state=[a, c]); # Step 2.D: Apply densor to the hidden state output of LSTM_Cell out = densor(a); # Step 2.E: add the output to "outputs" p = outputs.append(out); # Step 3: Create model instance model = Model(input=[X, a0, c0], outputs = outputs); ### END CODE HERE ### return model Run the following cell to define your model. We will use Tx=30, n_a=64 (the dimension of the LSTM activations), and n_values=78. This cell may take a few seconds to run. 1model = djmodel(Tx = 30 , n_a = 64, n_values = 78) /opt/conda/lib/python3.6/site-packages/ipykernel/__main__.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[&lt;tf.Tenso..., inputs=[&lt;tf.Tenso...)` You now need to compile your model to be trained. We will Adam and a categorical cross-entropy loss. 123opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) Finally, lets initialize a0 and c0 for the LSTM’s initial state to be zero. 123m = 60a0 = np.zeros((m, n_a))c0 = np.zeros((m, n_a)) Lets now fit the model! We will turn Y to a list before doing so, since the cost function expects Y to be provided in this format (one list item per time-step). So list(Y) is a list with 30 items, where each of the list items is of shape (60,78). Lets train for 100 epochs. This will take a few minutes. 1model.fit([X, a0, c0], list(Y), epochs=100) Epoch 1/100 60/60 [==============================] - 5s - loss: 125.8264 - dense_1_loss_1: 4.3545 - dense_1_loss_2: 4.3464 - dense_1_loss_3: 4.3425 - dense_1_loss_4: 4.3442 - dense_1_loss_5: 4.3421 - dense_1_loss_6: 4.3446 - dense_1_loss_7: 4.3401 - dense_1_loss_8: 4.3457 - dense_1_loss_9: 4.3314 - dense_1_loss_10: 4.3323 - dense_1_loss_11: 4.3423 - dense_1_loss_12: 4.3389 - dense_1_loss_13: 4.3364 - dense_1_loss_14: 4.3380 - dense_1_loss_15: 4.3371 - dense_1_loss_16: 4.3311 - dense_1_loss_17: 4.3417 - dense_1_loss_18: 4.3396 - dense_1_loss_19: 4.3346 - dense_1_loss_20: 4.3342 - dense_1_loss_21: 4.3366 - dense_1_loss_22: 4.3406 - dense_1_loss_23: 4.3338 - dense_1_loss_24: 4.3317 - dense_1_loss_25: 4.3376 - dense_1_loss_26: 4.3340 - dense_1_loss_27: 4.3329 - dense_1_loss_28: 4.3416 - dense_1_loss_29: 4.3399 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0500 - dense_1_acc_3: 0.0500 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0333 - dense_1_acc_7: 0.0500 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.1000 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0667 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0667 - dense_1_acc_15: 0.0667 - dense_1_acc_16: 0.0500 - dense_1_acc_17: 0.0500 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.1000 - dense_1_acc_20: 0.0667 - dense_1_acc_21: 0.0500 - dense_1_acc_22: 0.0667 - dense_1_acc_23: 0.1167 - dense_1_acc_24: 0.1000 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.1000 - dense_1_acc_27: 0.0500 - dense_1_acc_28: 0.0500 - dense_1_acc_29: 0.0833 - dense_1_acc_30: 0.0000e+00 Epoch 2/100 60/60 [==============================] - 0s - loss: 122.6142 - dense_1_loss_1: 4.3317 - dense_1_loss_2: 4.2991 - dense_1_loss_3: 4.2729 - dense_1_loss_4: 4.2763 - dense_1_loss_5: 4.2523 - dense_1_loss_6: 4.2653 - dense_1_loss_7: 4.2464 - dense_1_loss_8: 4.2352 - dense_1_loss_9: 4.2288 - dense_1_loss_10: 4.2197 - dense_1_loss_11: 4.2248 - dense_1_loss_12: 4.2489 - dense_1_loss_13: 4.2078 - dense_1_loss_14: 4.2074 - dense_1_loss_15: 4.2073 - dense_1_loss_16: 4.1991 - dense_1_loss_17: 4.2009 - dense_1_loss_18: 4.2387 - dense_1_loss_19: 4.1921 - dense_1_loss_20: 4.2132 - dense_1_loss_21: 4.2112 - dense_1_loss_22: 4.1933 - dense_1_loss_23: 4.1941 - dense_1_loss_24: 4.2164 - dense_1_loss_25: 4.2240 - dense_1_loss_26: 4.1728 - dense_1_loss_27: 4.2027 - dense_1_loss_28: 4.2063 - dense_1_loss_29: 4.2258 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.1333 - dense_1_acc_3: 0.1500 - dense_1_acc_4: 0.1667 - dense_1_acc_5: 0.2000 - dense_1_acc_6: 0.1167 - dense_1_acc_7: 0.1667 - dense_1_acc_8: 0.1167 - dense_1_acc_9: 0.1833 - dense_1_acc_10: 0.1667 - dense_1_acc_11: 0.2000 - dense_1_acc_12: 0.0667 - dense_1_acc_13: 0.1333 - dense_1_acc_14: 0.1333 - dense_1_acc_15: 0.1167 - dense_1_acc_16: 0.1833 - dense_1_acc_17: 0.2000 - dense_1_acc_18: 0.0667 - dense_1_acc_19: 0.1333 - dense_1_acc_20: 0.1667 - dense_1_acc_21: 0.1333 - dense_1_acc_22: 0.1000 - dense_1_acc_23: 0.1167 - dense_1_acc_24: 0.1333 - dense_1_acc_25: 0.1167 - dense_1_acc_26: 0.1833 - dense_1_acc_27: 0.1000 - dense_1_acc_28: 0.1833 - dense_1_acc_29: 0.0833 - dense_1_acc_30: 0.0000e+00 Epoch 3/100 60/60 [==============================] - 0s - loss: 116.8061 - dense_1_loss_1: 4.3093 - dense_1_loss_2: 4.2449 - dense_1_loss_3: 4.1836 - dense_1_loss_4: 4.1745 - dense_1_loss_5: 4.1156 - dense_1_loss_6: 4.1481 - dense_1_loss_7: 4.0958 - dense_1_loss_8: 4.0446 - dense_1_loss_9: 3.9897 - dense_1_loss_10: 3.8988 - dense_1_loss_11: 3.8989 - dense_1_loss_12: 4.1165 - dense_1_loss_13: 3.8994 - dense_1_loss_14: 3.8898 - dense_1_loss_15: 3.9828 - dense_1_loss_16: 3.9182 - dense_1_loss_17: 3.8867 - dense_1_loss_18: 4.2104 - dense_1_loss_19: 3.8670 - dense_1_loss_20: 4.0711 - dense_1_loss_21: 4.0630 - dense_1_loss_22: 3.9217 - dense_1_loss_23: 3.9589 - dense_1_loss_24: 4.0469 - dense_1_loss_25: 4.0823 - dense_1_loss_26: 3.7266 - dense_1_loss_27: 3.9689 - dense_1_loss_28: 3.9623 - dense_1_loss_29: 4.1299 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.1500 - dense_1_acc_3: 0.2000 - dense_1_acc_4: 0.1333 - dense_1_acc_5: 0.1833 - dense_1_acc_6: 0.1000 - dense_1_acc_7: 0.1167 - dense_1_acc_8: 0.0833 - dense_1_acc_9: 0.1167 - dense_1_acc_10: 0.1167 - dense_1_acc_11: 0.0833 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.1000 - dense_1_acc_14: 0.1000 - dense_1_acc_15: 0.0500 - dense_1_acc_16: 0.0833 - dense_1_acc_17: 0.1000 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.1000 - dense_1_acc_20: 0.0667 - dense_1_acc_21: 0.0667 - dense_1_acc_22: 0.0500 - dense_1_acc_23: 0.0833 - dense_1_acc_24: 0.0833 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.1167 - dense_1_acc_27: 0.0500 - dense_1_acc_28: 0.0667 - dense_1_acc_29: 0.0333 - dense_1_acc_30: 0.0000e+00 Epoch 4/100 60/60 [==============================] - 0s - loss: 112.2963 - dense_1_loss_1: 4.2889 - dense_1_loss_2: 4.1981 - dense_1_loss_3: 4.0962 - dense_1_loss_4: 4.0810 - dense_1_loss_5: 3.9790 - dense_1_loss_6: 4.0129 - dense_1_loss_7: 3.9439 - dense_1_loss_8: 3.7697 - dense_1_loss_9: 3.8046 - dense_1_loss_10: 3.6386 - dense_1_loss_11: 3.7236 - dense_1_loss_12: 3.9783 - dense_1_loss_13: 3.7060 - dense_1_loss_14: 3.7075 - dense_1_loss_15: 3.7358 - dense_1_loss_16: 3.7286 - dense_1_loss_17: 3.8079 - dense_1_loss_18: 3.9018 - dense_1_loss_19: 3.6729 - dense_1_loss_20: 3.9865 - dense_1_loss_21: 3.9529 - dense_1_loss_22: 3.8378 - dense_1_loss_23: 3.7695 - dense_1_loss_24: 3.7576 - dense_1_loss_25: 3.9597 - dense_1_loss_26: 3.6666 - dense_1_loss_27: 3.6978 - dense_1_loss_28: 3.8733 - dense_1_loss_29: 4.0193 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.1500 - dense_1_acc_3: 0.2167 - dense_1_acc_4: 0.1833 - dense_1_acc_5: 0.2667 - dense_1_acc_6: 0.1333 - dense_1_acc_7: 0.1667 - dense_1_acc_8: 0.1833 - dense_1_acc_9: 0.1667 - dense_1_acc_10: 0.1667 - dense_1_acc_11: 0.1667 - dense_1_acc_12: 0.1000 - dense_1_acc_13: 0.1500 - dense_1_acc_14: 0.2167 - dense_1_acc_15: 0.1000 - dense_1_acc_16: 0.1167 - dense_1_acc_17: 0.1000 - dense_1_acc_18: 0.1000 - dense_1_acc_19: 0.1500 - dense_1_acc_20: 0.0833 - dense_1_acc_21: 0.0667 - dense_1_acc_22: 0.1167 - dense_1_acc_23: 0.0833 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.1000 - dense_1_acc_26: 0.1000 - dense_1_acc_27: 0.0833 - dense_1_acc_28: 0.1167 - dense_1_acc_29: 0.0667 - dense_1_acc_30: 0.0000e+00 Epoch 5/100 60/60 [==============================] - 0s - loss: 110.0390 - dense_1_loss_1: 4.2729 - dense_1_loss_2: 4.1581 - dense_1_loss_3: 4.0292 - dense_1_loss_4: 4.0164 - dense_1_loss_5: 3.8981 - dense_1_loss_6: 3.9318 - dense_1_loss_7: 3.8775 - dense_1_loss_8: 3.6710 - dense_1_loss_9: 3.7225 - dense_1_loss_10: 3.5653 - dense_1_loss_11: 3.6287 - dense_1_loss_12: 3.8595 - dense_1_loss_13: 3.6459 - dense_1_loss_14: 3.6176 - dense_1_loss_15: 3.7001 - dense_1_loss_16: 3.6384 - dense_1_loss_17: 3.7419 - dense_1_loss_18: 3.7274 - dense_1_loss_19: 3.6644 - dense_1_loss_20: 3.8134 - dense_1_loss_21: 3.8085 - dense_1_loss_22: 3.7113 - dense_1_loss_23: 3.6167 - dense_1_loss_24: 3.6441 - dense_1_loss_25: 3.9445 - dense_1_loss_26: 3.7134 - dense_1_loss_27: 3.6405 - dense_1_loss_28: 3.8265 - dense_1_loss_29: 3.9533 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.1333 - dense_1_acc_3: 0.2167 - dense_1_acc_4: 0.2000 - dense_1_acc_5: 0.1333 - dense_1_acc_6: 0.0333 - dense_1_acc_7: 0.1000 - dense_1_acc_8: 0.1500 - dense_1_acc_9: 0.0833 - dense_1_acc_10: 0.1000 - dense_1_acc_11: 0.1000 - dense_1_acc_12: 0.0667 - dense_1_acc_13: 0.1000 - dense_1_acc_14: 0.1500 - dense_1_acc_15: 0.0833 - dense_1_acc_16: 0.0500 - dense_1_acc_17: 0.0500 - dense_1_acc_18: 0.0833 - dense_1_acc_19: 0.0333 - dense_1_acc_20: 0.0500 - dense_1_acc_21: 0.0833 - dense_1_acc_22: 0.0833 - dense_1_acc_23: 0.1667 - dense_1_acc_24: 0.0500 - dense_1_acc_25: 0.0500 - dense_1_acc_26: 0.0500 - dense_1_acc_27: 0.0833 - dense_1_acc_28: 0.0167 - dense_1_acc_29: 0.0167 - dense_1_acc_30: 0.0000e+00 Epoch 6/100 60/60 [==============================] - 0s - loss: 106.1460 - dense_1_loss_1: 4.2571 - dense_1_loss_2: 4.1230 - dense_1_loss_3: 3.9604 - dense_1_loss_4: 3.9405 - dense_1_loss_5: 3.8132 - dense_1_loss_6: 3.8401 - dense_1_loss_7: 3.7750 - dense_1_loss_8: 3.5455 - dense_1_loss_9: 3.5752 - dense_1_loss_10: 3.4639 - dense_1_loss_11: 3.5982 - dense_1_loss_12: 3.7733 - dense_1_loss_13: 3.5049 - dense_1_loss_14: 3.4641 - dense_1_loss_15: 3.5221 - dense_1_loss_16: 3.5189 - dense_1_loss_17: 3.5414 - dense_1_loss_18: 3.5307 - dense_1_loss_19: 3.5341 - dense_1_loss_20: 3.6316 - dense_1_loss_21: 3.6324 - dense_1_loss_22: 3.5577 - dense_1_loss_23: 3.5073 - dense_1_loss_24: 3.5296 - dense_1_loss_25: 3.8212 - dense_1_loss_26: 3.4278 - dense_1_loss_27: 3.4614 - dense_1_loss_28: 3.5999 - dense_1_loss_29: 3.6956 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.1667 - dense_1_acc_3: 0.2000 - dense_1_acc_4: 0.2000 - dense_1_acc_5: 0.2500 - dense_1_acc_6: 0.0833 - dense_1_acc_7: 0.0833 - dense_1_acc_8: 0.1667 - dense_1_acc_9: 0.1000 - dense_1_acc_10: 0.2000 - dense_1_acc_11: 0.1333 - dense_1_acc_12: 0.1000 - dense_1_acc_13: 0.1833 - dense_1_acc_14: 0.2167 - dense_1_acc_15: 0.1167 - dense_1_acc_16: 0.1167 - dense_1_acc_17: 0.1333 - dense_1_acc_18: 0.1667 - dense_1_acc_19: 0.1833 - dense_1_acc_20: 0.1167 - dense_1_acc_21: 0.1500 - dense_1_acc_22: 0.1500 - dense_1_acc_23: 0.1833 - dense_1_acc_24: 0.1167 - dense_1_acc_25: 0.0667 - dense_1_acc_26: 0.2000 - dense_1_acc_27: 0.1000 - dense_1_acc_28: 0.1500 - dense_1_acc_29: 0.0667 - dense_1_acc_30: 0.0000e+00 Epoch 7/100 60/60 [==============================] - 0s - loss: 102.2579 - dense_1_loss_1: 4.2413 - dense_1_loss_2: 4.0875 - dense_1_loss_3: 3.8934 - dense_1_loss_4: 3.8654 - dense_1_loss_5: 3.7056 - dense_1_loss_6: 3.7368 - dense_1_loss_7: 3.6732 - dense_1_loss_8: 3.4290 - dense_1_loss_9: 3.4259 - dense_1_loss_10: 3.3381 - dense_1_loss_11: 3.4889 - dense_1_loss_12: 3.6443 - dense_1_loss_13: 3.3488 - dense_1_loss_14: 3.3007 - dense_1_loss_15: 3.3981 - dense_1_loss_16: 3.3846 - dense_1_loss_17: 3.3449 - dense_1_loss_18: 3.3858 - dense_1_loss_19: 3.4057 - dense_1_loss_20: 3.4521 - dense_1_loss_21: 3.4389 - dense_1_loss_22: 3.3936 - dense_1_loss_23: 3.4140 - dense_1_loss_24: 3.3620 - dense_1_loss_25: 3.6902 - dense_1_loss_26: 3.2316 - dense_1_loss_27: 3.3343 - dense_1_loss_28: 3.3640 - dense_1_loss_29: 3.4791 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.1333 - dense_1_acc_3: 0.2167 - dense_1_acc_4: 0.1833 - dense_1_acc_5: 0.2667 - dense_1_acc_6: 0.1667 - dense_1_acc_7: 0.1333 - dense_1_acc_8: 0.2333 - dense_1_acc_9: 0.1667 - dense_1_acc_10: 0.2000 - dense_1_acc_11: 0.1833 - dense_1_acc_12: 0.1333 - dense_1_acc_13: 0.1667 - dense_1_acc_14: 0.2667 - dense_1_acc_15: 0.1667 - dense_1_acc_16: 0.1500 - dense_1_acc_17: 0.1833 - dense_1_acc_18: 0.1167 - dense_1_acc_19: 0.1333 - dense_1_acc_20: 0.1833 - dense_1_acc_21: 0.1167 - dense_1_acc_22: 0.1333 - dense_1_acc_23: 0.1333 - dense_1_acc_24: 0.1500 - dense_1_acc_25: 0.0667 - dense_1_acc_26: 0.2167 - dense_1_acc_27: 0.1000 - dense_1_acc_28: 0.1500 - dense_1_acc_29: 0.1833 - dense_1_acc_30: 0.0000e+00 Epoch 8/100 60/60 [==============================] - 0s - loss: 98.0187 - dense_1_loss_1: 4.2277 - dense_1_loss_2: 4.0477 - dense_1_loss_3: 3.8258 - dense_1_loss_4: 3.7791 - dense_1_loss_5: 3.6089 - dense_1_loss_6: 3.6218 - dense_1_loss_7: 3.5396 - dense_1_loss_8: 3.2991 - dense_1_loss_9: 3.2584 - dense_1_loss_10: 3.1349 - dense_1_loss_11: 3.2992 - dense_1_loss_12: 3.4534 - dense_1_loss_13: 3.1133 - dense_1_loss_14: 3.0906 - dense_1_loss_15: 3.2273 - dense_1_loss_16: 3.2308 - dense_1_loss_17: 3.1062 - dense_1_loss_18: 3.2503 - dense_1_loss_19: 3.2314 - dense_1_loss_20: 3.2470 - dense_1_loss_21: 3.2910 - dense_1_loss_22: 3.2553 - dense_1_loss_23: 3.2761 - dense_1_loss_24: 3.2245 - dense_1_loss_25: 3.5042 - dense_1_loss_26: 3.0631 - dense_1_loss_27: 3.2291 - dense_1_loss_28: 3.2519 - dense_1_loss_29: 3.3307 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.1167 - dense_1_acc_3: 0.1667 - dense_1_acc_4: 0.1667 - dense_1_acc_5: 0.2667 - dense_1_acc_6: 0.1833 - dense_1_acc_7: 0.1500 - dense_1_acc_8: 0.2667 - dense_1_acc_9: 0.1667 - dense_1_acc_10: 0.2333 - dense_1_acc_11: 0.1667 - dense_1_acc_12: 0.1167 - dense_1_acc_13: 0.2833 - dense_1_acc_14: 0.2333 - dense_1_acc_15: 0.1500 - dense_1_acc_16: 0.2000 - dense_1_acc_17: 0.2167 - dense_1_acc_18: 0.1333 - dense_1_acc_19: 0.1667 - dense_1_acc_20: 0.2833 - dense_1_acc_21: 0.1667 - dense_1_acc_22: 0.1500 - dense_1_acc_23: 0.1500 - dense_1_acc_24: 0.1333 - dense_1_acc_25: 0.1167 - dense_1_acc_26: 0.2500 - dense_1_acc_27: 0.1000 - dense_1_acc_28: 0.1500 - dense_1_acc_29: 0.1667 - dense_1_acc_30: 0.0000e+00 Epoch 9/100 60/60 [==============================] - 0s - loss: 93.9753 - dense_1_loss_1: 4.2159 - dense_1_loss_2: 4.0105 - dense_1_loss_3: 3.7472 - dense_1_loss_4: 3.6921 - dense_1_loss_5: 3.4942 - dense_1_loss_6: 3.4897 - dense_1_loss_7: 3.4181 - dense_1_loss_8: 3.1503 - dense_1_loss_9: 3.1051 - dense_1_loss_10: 2.9563 - dense_1_loss_11: 3.1541 - dense_1_loss_12: 3.2926 - dense_1_loss_13: 2.9499 - dense_1_loss_14: 2.9662 - dense_1_loss_15: 3.0675 - dense_1_loss_16: 3.1146 - dense_1_loss_17: 2.9696 - dense_1_loss_18: 3.1479 - dense_1_loss_19: 3.0151 - dense_1_loss_20: 3.0469 - dense_1_loss_21: 3.0955 - dense_1_loss_22: 3.0573 - dense_1_loss_23: 3.1520 - dense_1_loss_24: 3.0335 - dense_1_loss_25: 3.3512 - dense_1_loss_26: 2.8350 - dense_1_loss_27: 3.1168 - dense_1_loss_28: 3.1114 - dense_1_loss_29: 3.2186 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.1167 - dense_1_acc_3: 0.2000 - dense_1_acc_4: 0.1500 - dense_1_acc_5: 0.2500 - dense_1_acc_6: 0.1833 - dense_1_acc_7: 0.1500 - dense_1_acc_8: 0.1833 - dense_1_acc_9: 0.2667 - dense_1_acc_10: 0.2500 - dense_1_acc_11: 0.1667 - dense_1_acc_12: 0.1500 - dense_1_acc_13: 0.3333 - dense_1_acc_14: 0.2333 - dense_1_acc_15: 0.2000 - dense_1_acc_16: 0.2333 - dense_1_acc_17: 0.3000 - dense_1_acc_18: 0.1333 - dense_1_acc_19: 0.2000 - dense_1_acc_20: 0.3333 - dense_1_acc_21: 0.1833 - dense_1_acc_22: 0.1500 - dense_1_acc_23: 0.2000 - dense_1_acc_24: 0.2000 - dense_1_acc_25: 0.1167 - dense_1_acc_26: 0.2667 - dense_1_acc_27: 0.1667 - dense_1_acc_28: 0.1500 - dense_1_acc_29: 0.2000 - dense_1_acc_30: 0.0000e+00 Epoch 10/100 60/60 [==============================] - 0s - loss: 89.7720 - dense_1_loss_1: 4.2048 - dense_1_loss_2: 3.9711 - dense_1_loss_3: 3.6677 - dense_1_loss_4: 3.6035 - dense_1_loss_5: 3.3800 - dense_1_loss_6: 3.3506 - dense_1_loss_7: 3.2899 - dense_1_loss_8: 3.0100 - dense_1_loss_9: 2.9501 - dense_1_loss_10: 2.7743 - dense_1_loss_11: 3.0100 - dense_1_loss_12: 3.0628 - dense_1_loss_13: 2.8252 - dense_1_loss_14: 2.8456 - dense_1_loss_15: 2.9193 - dense_1_loss_16: 2.9354 - dense_1_loss_17: 2.7749 - dense_1_loss_18: 3.0148 - dense_1_loss_19: 2.8805 - dense_1_loss_20: 2.8963 - dense_1_loss_21: 2.9775 - dense_1_loss_22: 2.8919 - dense_1_loss_23: 2.9468 - dense_1_loss_24: 2.8604 - dense_1_loss_25: 3.1973 - dense_1_loss_26: 2.6616 - dense_1_loss_27: 2.9519 - dense_1_loss_28: 2.9121 - dense_1_loss_29: 3.0059 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.0667 - dense_1_acc_3: 0.2000 - dense_1_acc_4: 0.1667 - dense_1_acc_5: 0.2000 - dense_1_acc_6: 0.2000 - dense_1_acc_7: 0.2167 - dense_1_acc_8: 0.1833 - dense_1_acc_9: 0.3000 - dense_1_acc_10: 0.2500 - dense_1_acc_11: 0.1667 - dense_1_acc_12: 0.1500 - dense_1_acc_13: 0.2833 - dense_1_acc_14: 0.2500 - dense_1_acc_15: 0.2500 - dense_1_acc_16: 0.2667 - dense_1_acc_17: 0.3000 - dense_1_acc_18: 0.1000 - dense_1_acc_19: 0.2167 - dense_1_acc_20: 0.2833 - dense_1_acc_21: 0.2167 - dense_1_acc_22: 0.1500 - dense_1_acc_23: 0.2333 - dense_1_acc_24: 0.1667 - dense_1_acc_25: 0.1167 - dense_1_acc_26: 0.2833 - dense_1_acc_27: 0.1667 - dense_1_acc_28: 0.2167 - dense_1_acc_29: 0.2167 - dense_1_acc_30: 0.0000e+00 Epoch 11/100 60/60 [==============================] - 0s - loss: 85.6615 - dense_1_loss_1: 4.1942 - dense_1_loss_2: 3.9323 - dense_1_loss_3: 3.5914 - dense_1_loss_4: 3.5058 - dense_1_loss_5: 3.2599 - dense_1_loss_6: 3.2069 - dense_1_loss_7: 3.1536 - dense_1_loss_8: 2.8428 - dense_1_loss_9: 2.8446 - dense_1_loss_10: 2.6600 - dense_1_loss_11: 2.8793 - dense_1_loss_12: 2.8746 - dense_1_loss_13: 2.6513 - dense_1_loss_14: 2.6880 - dense_1_loss_15: 2.7775 - dense_1_loss_16: 2.8001 - dense_1_loss_17: 2.6575 - dense_1_loss_18: 2.8262 - dense_1_loss_19: 2.6729 - dense_1_loss_20: 2.7437 - dense_1_loss_21: 2.7738 - dense_1_loss_22: 2.7370 - dense_1_loss_23: 2.8320 - dense_1_loss_24: 2.6954 - dense_1_loss_25: 2.9728 - dense_1_loss_26: 2.5801 - dense_1_loss_27: 2.7190 - dense_1_loss_28: 2.7862 - dense_1_loss_29: 2.8024 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.0667 - dense_1_acc_3: 0.2000 - dense_1_acc_4: 0.2167 - dense_1_acc_5: 0.2833 - dense_1_acc_6: 0.2167 - dense_1_acc_7: 0.2333 - dense_1_acc_8: 0.2833 - dense_1_acc_9: 0.2667 - dense_1_acc_10: 0.3167 - dense_1_acc_11: 0.1333 - dense_1_acc_12: 0.2000 - dense_1_acc_13: 0.3333 - dense_1_acc_14: 0.2333 - dense_1_acc_15: 0.2333 - dense_1_acc_16: 0.2500 - dense_1_acc_17: 0.2333 - dense_1_acc_18: 0.1500 - dense_1_acc_19: 0.2500 - dense_1_acc_20: 0.2833 - dense_1_acc_21: 0.2333 - dense_1_acc_22: 0.2000 - dense_1_acc_23: 0.2333 - dense_1_acc_24: 0.2000 - dense_1_acc_25: 0.1667 - dense_1_acc_26: 0.3333 - dense_1_acc_27: 0.2000 - dense_1_acc_28: 0.2167 - dense_1_acc_29: 0.3500 - dense_1_acc_30: 0.0000e+00 Epoch 12/100 60/60 [==============================] - 0s - loss: 81.9096 - dense_1_loss_1: 4.1837 - dense_1_loss_2: 3.8924 - dense_1_loss_3: 3.5047 - dense_1_loss_4: 3.4058 - dense_1_loss_5: 3.1285 - dense_1_loss_6: 3.0528 - dense_1_loss_7: 3.0213 - dense_1_loss_8: 2.6764 - dense_1_loss_9: 2.6832 - dense_1_loss_10: 2.5371 - dense_1_loss_11: 2.7424 - dense_1_loss_12: 2.7007 - dense_1_loss_13: 2.5169 - dense_1_loss_14: 2.5984 - dense_1_loss_15: 2.5748 - dense_1_loss_16: 2.6452 - dense_1_loss_17: 2.5546 - dense_1_loss_18: 2.6831 - dense_1_loss_19: 2.6039 - dense_1_loss_20: 2.6078 - dense_1_loss_21: 2.6546 - dense_1_loss_22: 2.5963 - dense_1_loss_23: 2.6691 - dense_1_loss_24: 2.6460 - dense_1_loss_25: 2.8278 - dense_1_loss_26: 2.3809 - dense_1_loss_27: 2.6169 - dense_1_loss_28: 2.5561 - dense_1_loss_29: 2.6480 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.1167 - dense_1_acc_3: 0.2167 - dense_1_acc_4: 0.2167 - dense_1_acc_5: 0.3000 - dense_1_acc_6: 0.2333 - dense_1_acc_7: 0.2833 - dense_1_acc_8: 0.3167 - dense_1_acc_9: 0.3167 - dense_1_acc_10: 0.3000 - dense_1_acc_11: 0.1667 - dense_1_acc_12: 0.2500 - dense_1_acc_13: 0.4000 - dense_1_acc_14: 0.2833 - dense_1_acc_15: 0.2500 - dense_1_acc_16: 0.2500 - dense_1_acc_17: 0.2500 - dense_1_acc_18: 0.1500 - dense_1_acc_19: 0.2833 - dense_1_acc_20: 0.3167 - dense_1_acc_21: 0.2667 - dense_1_acc_22: 0.2333 - dense_1_acc_23: 0.2333 - dense_1_acc_24: 0.2500 - dense_1_acc_25: 0.1333 - dense_1_acc_26: 0.3833 - dense_1_acc_27: 0.3000 - dense_1_acc_28: 0.3167 - dense_1_acc_29: 0.3167 - dense_1_acc_30: 0.0000e+00 Epoch 13/100 60/60 [==============================] - 0s - loss: 77.9424 - dense_1_loss_1: 4.1726 - dense_1_loss_2: 3.8520 - dense_1_loss_3: 3.4239 - dense_1_loss_4: 3.3049 - dense_1_loss_5: 3.0094 - dense_1_loss_6: 2.9040 - dense_1_loss_7: 2.8879 - dense_1_loss_8: 2.5388 - dense_1_loss_9: 2.5642 - dense_1_loss_10: 2.3932 - dense_1_loss_11: 2.5736 - dense_1_loss_12: 2.5587 - dense_1_loss_13: 2.3320 - dense_1_loss_14: 2.4560 - dense_1_loss_15: 2.4168 - dense_1_loss_16: 2.5107 - dense_1_loss_17: 2.3550 - dense_1_loss_18: 2.4863 - dense_1_loss_19: 2.4692 - dense_1_loss_20: 2.4468 - dense_1_loss_21: 2.5056 - dense_1_loss_22: 2.4056 - dense_1_loss_23: 2.4519 - dense_1_loss_24: 2.6144 - dense_1_loss_25: 2.6999 - dense_1_loss_26: 2.1690 - dense_1_loss_27: 2.4230 - dense_1_loss_28: 2.4840 - dense_1_loss_29: 2.5331 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.1167 - dense_1_acc_3: 0.2500 - dense_1_acc_4: 0.2000 - dense_1_acc_5: 0.3167 - dense_1_acc_6: 0.2167 - dense_1_acc_7: 0.3000 - dense_1_acc_8: 0.3500 - dense_1_acc_9: 0.3333 - dense_1_acc_10: 0.3333 - dense_1_acc_11: 0.2833 - dense_1_acc_12: 0.2500 - dense_1_acc_13: 0.3667 - dense_1_acc_14: 0.2500 - dense_1_acc_15: 0.2667 - dense_1_acc_16: 0.2333 - dense_1_acc_17: 0.3167 - dense_1_acc_18: 0.1833 - dense_1_acc_19: 0.2667 - dense_1_acc_20: 0.3333 - dense_1_acc_21: 0.3000 - dense_1_acc_22: 0.3000 - dense_1_acc_23: 0.2833 - dense_1_acc_24: 0.2500 - dense_1_acc_25: 0.1833 - dense_1_acc_26: 0.4167 - dense_1_acc_27: 0.2500 - dense_1_acc_28: 0.2167 - dense_1_acc_29: 0.3167 - dense_1_acc_30: 0.0000e+00 Epoch 14/100 60/60 [==============================] - 0s - loss: 74.5680 - dense_1_loss_1: 4.1639 - dense_1_loss_2: 3.8115 - dense_1_loss_3: 3.3439 - dense_1_loss_4: 3.1987 - dense_1_loss_5: 2.8879 - dense_1_loss_6: 2.7570 - dense_1_loss_7: 2.7657 - dense_1_loss_8: 2.4219 - dense_1_loss_9: 2.4471 - dense_1_loss_10: 2.2721 - dense_1_loss_11: 2.4152 - dense_1_loss_12: 2.4041 - dense_1_loss_13: 2.1848 - dense_1_loss_14: 2.3034 - dense_1_loss_15: 2.2661 - dense_1_loss_16: 2.3730 - dense_1_loss_17: 2.2420 - dense_1_loss_18: 2.3084 - dense_1_loss_19: 2.3039 - dense_1_loss_20: 2.3927 - dense_1_loss_21: 2.3191 - dense_1_loss_22: 2.2784 - dense_1_loss_23: 2.3497 - dense_1_loss_24: 2.4033 - dense_1_loss_25: 2.6364 - dense_1_loss_26: 2.1220 - dense_1_loss_27: 2.3866 - dense_1_loss_28: 2.4073 - dense_1_loss_29: 2.4020 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.1500 - dense_1_acc_3: 0.2500 - dense_1_acc_4: 0.2500 - dense_1_acc_5: 0.3333 - dense_1_acc_6: 0.2833 - dense_1_acc_7: 0.2667 - dense_1_acc_8: 0.3167 - dense_1_acc_9: 0.3333 - dense_1_acc_10: 0.3667 - dense_1_acc_11: 0.3167 - dense_1_acc_12: 0.2333 - dense_1_acc_13: 0.4000 - dense_1_acc_14: 0.3500 - dense_1_acc_15: 0.3500 - dense_1_acc_16: 0.2833 - dense_1_acc_17: 0.3667 - dense_1_acc_18: 0.2000 - dense_1_acc_19: 0.2833 - dense_1_acc_20: 0.3000 - dense_1_acc_21: 0.2833 - dense_1_acc_22: 0.2833 - dense_1_acc_23: 0.3333 - dense_1_acc_24: 0.2333 - dense_1_acc_25: 0.1667 - dense_1_acc_26: 0.3833 - dense_1_acc_27: 0.2667 - dense_1_acc_28: 0.2333 - dense_1_acc_29: 0.2833 - dense_1_acc_30: 0.0000e+00 Epoch 15/100 60/60 [==============================] - 0s - loss: 70.7818 - dense_1_loss_1: 4.1566 - dense_1_loss_2: 3.7716 - dense_1_loss_3: 3.2704 - dense_1_loss_4: 3.1003 - dense_1_loss_5: 2.7766 - dense_1_loss_6: 2.6157 - dense_1_loss_7: 2.6423 - dense_1_loss_8: 2.3160 - dense_1_loss_9: 2.3343 - dense_1_loss_10: 2.1863 - dense_1_loss_11: 2.3080 - dense_1_loss_12: 2.2695 - dense_1_loss_13: 2.0643 - dense_1_loss_14: 2.1616 - dense_1_loss_15: 2.2092 - dense_1_loss_16: 2.2644 - dense_1_loss_17: 2.1717 - dense_1_loss_18: 2.1806 - dense_1_loss_19: 2.1495 - dense_1_loss_20: 2.2528 - dense_1_loss_21: 2.0959 - dense_1_loss_22: 2.1184 - dense_1_loss_23: 2.2349 - dense_1_loss_24: 2.2799 - dense_1_loss_25: 2.4104 - dense_1_loss_26: 1.9154 - dense_1_loss_27: 2.1144 - dense_1_loss_28: 2.2116 - dense_1_loss_29: 2.1992 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.1500 - dense_1_acc_3: 0.2833 - dense_1_acc_4: 0.2667 - dense_1_acc_5: 0.3167 - dense_1_acc_6: 0.3167 - dense_1_acc_7: 0.3167 - dense_1_acc_8: 0.3500 - dense_1_acc_9: 0.4000 - dense_1_acc_10: 0.4000 - dense_1_acc_11: 0.2333 - dense_1_acc_12: 0.2333 - dense_1_acc_13: 0.4667 - dense_1_acc_14: 0.4167 - dense_1_acc_15: 0.2833 - dense_1_acc_16: 0.3167 - dense_1_acc_17: 0.3667 - dense_1_acc_18: 0.3167 - dense_1_acc_19: 0.3500 - dense_1_acc_20: 0.2833 - dense_1_acc_21: 0.3500 - dense_1_acc_22: 0.3667 - dense_1_acc_23: 0.4000 - dense_1_acc_24: 0.3000 - dense_1_acc_25: 0.2000 - dense_1_acc_26: 0.5167 - dense_1_acc_27: 0.3833 - dense_1_acc_28: 0.4167 - dense_1_acc_29: 0.4333 - dense_1_acc_30: 0.0000e+00 Epoch 16/100 60/60 [==============================] - 0s - loss: 67.6264 - dense_1_loss_1: 4.1490 - dense_1_loss_2: 3.7330 - dense_1_loss_3: 3.1997 - dense_1_loss_4: 2.9972 - dense_1_loss_5: 2.6689 - dense_1_loss_6: 2.4691 - dense_1_loss_7: 2.4959 - dense_1_loss_8: 2.2321 - dense_1_loss_9: 2.2149 - dense_1_loss_10: 2.0676 - dense_1_loss_11: 2.1944 - dense_1_loss_12: 2.0894 - dense_1_loss_13: 1.9174 - dense_1_loss_14: 2.0482 - dense_1_loss_15: 2.0521 - dense_1_loss_16: 2.1589 - dense_1_loss_17: 2.0443 - dense_1_loss_18: 2.0343 - dense_1_loss_19: 2.0277 - dense_1_loss_20: 2.0924 - dense_1_loss_21: 2.0356 - dense_1_loss_22: 2.0433 - dense_1_loss_23: 2.1854 - dense_1_loss_24: 2.1334 - dense_1_loss_25: 2.2683 - dense_1_loss_26: 1.8710 - dense_1_loss_27: 2.0543 - dense_1_loss_28: 2.0875 - dense_1_loss_29: 2.0611 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.1833 - dense_1_acc_3: 0.3000 - dense_1_acc_4: 0.2667 - dense_1_acc_5: 0.3333 - dense_1_acc_6: 0.3167 - dense_1_acc_7: 0.3500 - dense_1_acc_8: 0.3833 - dense_1_acc_9: 0.3833 - dense_1_acc_10: 0.3500 - dense_1_acc_11: 0.2500 - dense_1_acc_12: 0.3667 - dense_1_acc_13: 0.4167 - dense_1_acc_14: 0.4167 - dense_1_acc_15: 0.3333 - dense_1_acc_16: 0.3833 - dense_1_acc_17: 0.4000 - dense_1_acc_18: 0.4000 - dense_1_acc_19: 0.3833 - dense_1_acc_20: 0.4167 - dense_1_acc_21: 0.3833 - dense_1_acc_22: 0.3333 - dense_1_acc_23: 0.3000 - dense_1_acc_24: 0.3333 - dense_1_acc_25: 0.2167 - dense_1_acc_26: 0.4667 - dense_1_acc_27: 0.3500 - dense_1_acc_28: 0.4333 - dense_1_acc_29: 0.4333 - dense_1_acc_30: 0.0000e+00 Epoch 17/100 60/60 [==============================] - 0s - loss: 64.3102 - dense_1_loss_1: 4.1432 - dense_1_loss_2: 3.6922 - dense_1_loss_3: 3.1260 - dense_1_loss_4: 2.9039 - dense_1_loss_5: 2.5473 - dense_1_loss_6: 2.3139 - dense_1_loss_7: 2.3524 - dense_1_loss_8: 2.1075 - dense_1_loss_9: 2.1829 - dense_1_loss_10: 1.9446 - dense_1_loss_11: 2.1464 - dense_1_loss_12: 2.0344 - dense_1_loss_13: 1.8492 - dense_1_loss_14: 1.8603 - dense_1_loss_15: 1.9291 - dense_1_loss_16: 2.0644 - dense_1_loss_17: 1.9326 - dense_1_loss_18: 1.8428 - dense_1_loss_19: 1.9004 - dense_1_loss_20: 1.9474 - dense_1_loss_21: 1.9269 - dense_1_loss_22: 1.9244 - dense_1_loss_23: 1.9607 - dense_1_loss_24: 2.0257 - dense_1_loss_25: 2.1022 - dense_1_loss_26: 1.7460 - dense_1_loss_27: 1.8937 - dense_1_loss_28: 1.9563 - dense_1_loss_29: 1.9534 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.2000 - dense_1_acc_3: 0.3500 - dense_1_acc_4: 0.2667 - dense_1_acc_5: 0.3333 - dense_1_acc_6: 0.3500 - dense_1_acc_7: 0.3500 - dense_1_acc_8: 0.3667 - dense_1_acc_9: 0.3667 - dense_1_acc_10: 0.4333 - dense_1_acc_11: 0.3000 - dense_1_acc_12: 0.3500 - dense_1_acc_13: 0.4333 - dense_1_acc_14: 0.4167 - dense_1_acc_15: 0.3667 - dense_1_acc_16: 0.3333 - dense_1_acc_17: 0.3833 - dense_1_acc_18: 0.4833 - dense_1_acc_19: 0.4500 - dense_1_acc_20: 0.4667 - dense_1_acc_21: 0.4167 - dense_1_acc_22: 0.3667 - dense_1_acc_23: 0.4167 - dense_1_acc_24: 0.3667 - dense_1_acc_25: 0.2833 - dense_1_acc_26: 0.5667 - dense_1_acc_27: 0.4500 - dense_1_acc_28: 0.4167 - dense_1_acc_29: 0.4667 - dense_1_acc_30: 0.0000e+00 Epoch 18/100 60/60 [==============================] - 0s - loss: 60.9770 - dense_1_loss_1: 4.1352 - dense_1_loss_2: 3.6501 - dense_1_loss_3: 3.0557 - dense_1_loss_4: 2.8116 - dense_1_loss_5: 2.4615 - dense_1_loss_6: 2.2039 - dense_1_loss_7: 2.2144 - dense_1_loss_8: 1.9720 - dense_1_loss_9: 2.0354 - dense_1_loss_10: 1.8256 - dense_1_loss_11: 1.9682 - dense_1_loss_12: 1.8455 - dense_1_loss_13: 1.7386 - dense_1_loss_14: 1.7591 - dense_1_loss_15: 1.7897 - dense_1_loss_16: 1.9169 - dense_1_loss_17: 1.8054 - dense_1_loss_18: 1.8099 - dense_1_loss_19: 1.7484 - dense_1_loss_20: 1.7715 - dense_1_loss_21: 1.7874 - dense_1_loss_22: 1.8334 - dense_1_loss_23: 1.7951 - dense_1_loss_24: 1.9296 - dense_1_loss_25: 1.9762 - dense_1_loss_26: 1.6691 - dense_1_loss_27: 1.8107 - dense_1_loss_28: 1.8523 - dense_1_loss_29: 1.8047 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.2167 - dense_1_acc_3: 0.3500 - dense_1_acc_4: 0.2500 - dense_1_acc_5: 0.3667 - dense_1_acc_6: 0.3333 - dense_1_acc_7: 0.4000 - dense_1_acc_8: 0.4333 - dense_1_acc_9: 0.3667 - dense_1_acc_10: 0.4500 - dense_1_acc_11: 0.3833 - dense_1_acc_12: 0.4167 - dense_1_acc_13: 0.5333 - dense_1_acc_14: 0.4667 - dense_1_acc_15: 0.4667 - dense_1_acc_16: 0.3333 - dense_1_acc_17: 0.4000 - dense_1_acc_18: 0.3667 - dense_1_acc_19: 0.4000 - dense_1_acc_20: 0.4833 - dense_1_acc_21: 0.3833 - dense_1_acc_22: 0.4500 - dense_1_acc_23: 0.4833 - dense_1_acc_24: 0.3500 - dense_1_acc_25: 0.3500 - dense_1_acc_26: 0.5333 - dense_1_acc_27: 0.4333 - dense_1_acc_28: 0.4333 - dense_1_acc_29: 0.5500 - dense_1_acc_30: 0.0000e+00 Epoch 19/100 60/60 [==============================] - 0s - loss: 58.1739 - dense_1_loss_1: 4.1267 - dense_1_loss_2: 3.6067 - dense_1_loss_3: 2.9783 - dense_1_loss_4: 2.7143 - dense_1_loss_5: 2.3603 - dense_1_loss_6: 2.1084 - dense_1_loss_7: 2.1157 - dense_1_loss_8: 1.8884 - dense_1_loss_9: 1.9336 - dense_1_loss_10: 1.7485 - dense_1_loss_11: 1.9035 - dense_1_loss_12: 1.7516 - dense_1_loss_13: 1.5965 - dense_1_loss_14: 1.6437 - dense_1_loss_15: 1.6844 - dense_1_loss_16: 1.8346 - dense_1_loss_17: 1.7095 - dense_1_loss_18: 1.7362 - dense_1_loss_19: 1.6973 - dense_1_loss_20: 1.6533 - dense_1_loss_21: 1.6370 - dense_1_loss_22: 1.7230 - dense_1_loss_23: 1.7123 - dense_1_loss_24: 1.7885 - dense_1_loss_25: 1.8111 - dense_1_loss_26: 1.6029 - dense_1_loss_27: 1.7325 - dense_1_loss_28: 1.7083 - dense_1_loss_29: 1.6667 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.2167 - dense_1_acc_3: 0.3667 - dense_1_acc_4: 0.2500 - dense_1_acc_5: 0.3500 - dense_1_acc_6: 0.3833 - dense_1_acc_7: 0.4000 - dense_1_acc_8: 0.4167 - dense_1_acc_9: 0.4333 - dense_1_acc_10: 0.4500 - dense_1_acc_11: 0.3167 - dense_1_acc_12: 0.5000 - dense_1_acc_13: 0.6000 - dense_1_acc_14: 0.5000 - dense_1_acc_15: 0.5333 - dense_1_acc_16: 0.3833 - dense_1_acc_17: 0.4833 - dense_1_acc_18: 0.3833 - dense_1_acc_19: 0.4333 - dense_1_acc_20: 0.5000 - dense_1_acc_21: 0.5000 - dense_1_acc_22: 0.4833 - dense_1_acc_23: 0.4833 - dense_1_acc_24: 0.4167 - dense_1_acc_25: 0.4333 - dense_1_acc_26: 0.6333 - dense_1_acc_27: 0.5000 - dense_1_acc_28: 0.5167 - dense_1_acc_29: 0.5667 - dense_1_acc_30: 0.0000e+00 Epoch 20/100 60/60 [==============================] - 0s - loss: 55.4761 - dense_1_loss_1: 4.1189 - dense_1_loss_2: 3.5637 - dense_1_loss_3: 2.9002 - dense_1_loss_4: 2.6177 - dense_1_loss_5: 2.2596 - dense_1_loss_6: 1.9893 - dense_1_loss_7: 2.0135 - dense_1_loss_8: 1.7673 - dense_1_loss_9: 1.8833 - dense_1_loss_10: 1.6986 - dense_1_loss_11: 1.7912 - dense_1_loss_12: 1.6945 - dense_1_loss_13: 1.5218 - dense_1_loss_14: 1.5522 - dense_1_loss_15: 1.6312 - dense_1_loss_16: 1.7254 - dense_1_loss_17: 1.6705 - dense_1_loss_18: 1.5892 - dense_1_loss_19: 1.6236 - dense_1_loss_20: 1.5906 - dense_1_loss_21: 1.5903 - dense_1_loss_22: 1.6394 - dense_1_loss_23: 1.5444 - dense_1_loss_24: 1.6563 - dense_1_loss_25: 1.7084 - dense_1_loss_26: 1.4733 - dense_1_loss_27: 1.5569 - dense_1_loss_28: 1.5812 - dense_1_loss_29: 1.5237 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.2667 - dense_1_acc_3: 0.3833 - dense_1_acc_4: 0.2500 - dense_1_acc_5: 0.3500 - dense_1_acc_6: 0.4000 - dense_1_acc_7: 0.4000 - dense_1_acc_8: 0.4167 - dense_1_acc_9: 0.4167 - dense_1_acc_10: 0.4667 - dense_1_acc_11: 0.4167 - dense_1_acc_12: 0.4000 - dense_1_acc_13: 0.6000 - dense_1_acc_14: 0.5500 - dense_1_acc_15: 0.4833 - dense_1_acc_16: 0.4667 - dense_1_acc_17: 0.4667 - dense_1_acc_18: 0.4833 - dense_1_acc_19: 0.5000 - dense_1_acc_20: 0.5833 - dense_1_acc_21: 0.5833 - dense_1_acc_22: 0.5167 - dense_1_acc_23: 0.6167 - dense_1_acc_24: 0.5333 - dense_1_acc_25: 0.4167 - dense_1_acc_26: 0.6500 - dense_1_acc_27: 0.6000 - dense_1_acc_28: 0.5167 - dense_1_acc_29: 0.6167 - dense_1_acc_30: 0.0000e+00 Epoch 21/100 60/60 [==============================] - 0s - loss: 52.5952 - dense_1_loss_1: 4.1115 - dense_1_loss_2: 3.5210 - dense_1_loss_3: 2.8198 - dense_1_loss_4: 2.5191 - dense_1_loss_5: 2.1622 - dense_1_loss_6: 1.8718 - dense_1_loss_7: 1.8840 - dense_1_loss_8: 1.6437 - dense_1_loss_9: 1.7017 - dense_1_loss_10: 1.5723 - dense_1_loss_11: 1.6463 - dense_1_loss_12: 1.5608 - dense_1_loss_13: 1.3714 - dense_1_loss_14: 1.4084 - dense_1_loss_15: 1.4898 - dense_1_loss_16: 1.5919 - dense_1_loss_17: 1.5521 - dense_1_loss_18: 1.4812 - dense_1_loss_19: 1.4532 - dense_1_loss_20: 1.5159 - dense_1_loss_21: 1.4975 - dense_1_loss_22: 1.5416 - dense_1_loss_23: 1.4791 - dense_1_loss_24: 1.5756 - dense_1_loss_25: 1.6586 - dense_1_loss_26: 1.4051 - dense_1_loss_27: 1.5384 - dense_1_loss_28: 1.5311 - dense_1_loss_29: 1.4903 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.2667 - dense_1_acc_3: 0.4000 - dense_1_acc_4: 0.2667 - dense_1_acc_5: 0.3500 - dense_1_acc_6: 0.4500 - dense_1_acc_7: 0.4500 - dense_1_acc_8: 0.5000 - dense_1_acc_9: 0.5167 - dense_1_acc_10: 0.5000 - dense_1_acc_11: 0.4333 - dense_1_acc_12: 0.4667 - dense_1_acc_13: 0.7167 - dense_1_acc_14: 0.6833 - dense_1_acc_15: 0.5167 - dense_1_acc_16: 0.5500 - dense_1_acc_17: 0.4833 - dense_1_acc_18: 0.5333 - dense_1_acc_19: 0.5333 - dense_1_acc_20: 0.4833 - dense_1_acc_21: 0.6167 - dense_1_acc_22: 0.5667 - dense_1_acc_23: 0.5667 - dense_1_acc_24: 0.4833 - dense_1_acc_25: 0.4500 - dense_1_acc_26: 0.6333 - dense_1_acc_27: 0.5500 - dense_1_acc_28: 0.5833 - dense_1_acc_29: 0.6500 - dense_1_acc_30: 0.0000e+00 Epoch 22/100 60/60 [==============================] - 0s - loss: 50.2160 - dense_1_loss_1: 4.1047 - dense_1_loss_2: 3.4770 - dense_1_loss_3: 2.7407 - dense_1_loss_4: 2.4178 - dense_1_loss_5: 2.0648 - dense_1_loss_6: 1.7635 - dense_1_loss_7: 1.7659 - dense_1_loss_8: 1.5881 - dense_1_loss_9: 1.5796 - dense_1_loss_10: 1.4720 - dense_1_loss_11: 1.5638 - dense_1_loss_12: 1.4441 - dense_1_loss_13: 1.3000 - dense_1_loss_14: 1.3932 - dense_1_loss_15: 1.3870 - dense_1_loss_16: 1.5121 - dense_1_loss_17: 1.4827 - dense_1_loss_18: 1.3958 - dense_1_loss_19: 1.4016 - dense_1_loss_20: 1.4361 - dense_1_loss_21: 1.4005 - dense_1_loss_22: 1.5129 - dense_1_loss_23: 1.3737 - dense_1_loss_24: 1.4531 - dense_1_loss_25: 1.5305 - dense_1_loss_26: 1.3757 - dense_1_loss_27: 1.4563 - dense_1_loss_28: 1.4114 - dense_1_loss_29: 1.4113 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.1000 - dense_1_acc_2: 0.2667 - dense_1_acc_3: 0.4167 - dense_1_acc_4: 0.3000 - dense_1_acc_5: 0.3667 - dense_1_acc_6: 0.5000 - dense_1_acc_7: 0.4667 - dense_1_acc_8: 0.5167 - dense_1_acc_9: 0.6000 - dense_1_acc_10: 0.5500 - dense_1_acc_11: 0.4333 - dense_1_acc_12: 0.5167 - dense_1_acc_13: 0.7000 - dense_1_acc_14: 0.6333 - dense_1_acc_15: 0.6000 - dense_1_acc_16: 0.5333 - dense_1_acc_17: 0.5500 - dense_1_acc_18: 0.5833 - dense_1_acc_19: 0.6333 - dense_1_acc_20: 0.6833 - dense_1_acc_21: 0.6500 - dense_1_acc_22: 0.6167 - dense_1_acc_23: 0.6833 - dense_1_acc_24: 0.5667 - dense_1_acc_25: 0.5333 - dense_1_acc_26: 0.6500 - dense_1_acc_27: 0.5167 - dense_1_acc_28: 0.7000 - dense_1_acc_29: 0.6667 - dense_1_acc_30: 0.0000e+00 Epoch 23/100 60/60 [==============================] - 0s - loss: 47.6829 - dense_1_loss_1: 4.0972 - dense_1_loss_2: 3.4353 - dense_1_loss_3: 2.6637 - dense_1_loss_4: 2.3184 - dense_1_loss_5: 1.9563 - dense_1_loss_6: 1.6456 - dense_1_loss_7: 1.6569 - dense_1_loss_8: 1.4727 - dense_1_loss_9: 1.5131 - dense_1_loss_10: 1.3883 - dense_1_loss_11: 1.4958 - dense_1_loss_12: 1.3610 - dense_1_loss_13: 1.2473 - dense_1_loss_14: 1.3105 - dense_1_loss_15: 1.3116 - dense_1_loss_16: 1.3763 - dense_1_loss_17: 1.3985 - dense_1_loss_18: 1.3418 - dense_1_loss_19: 1.3085 - dense_1_loss_20: 1.3157 - dense_1_loss_21: 1.3183 - dense_1_loss_22: 1.4045 - dense_1_loss_23: 1.3021 - dense_1_loss_24: 1.3491 - dense_1_loss_25: 1.4308 - dense_1_loss_26: 1.2834 - dense_1_loss_27: 1.3413 - dense_1_loss_28: 1.3298 - dense_1_loss_29: 1.3091 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2667 - dense_1_acc_3: 0.4333 - dense_1_acc_4: 0.3333 - dense_1_acc_5: 0.4000 - dense_1_acc_6: 0.5000 - dense_1_acc_7: 0.5000 - dense_1_acc_8: 0.5667 - dense_1_acc_9: 0.6167 - dense_1_acc_10: 0.6167 - dense_1_acc_11: 0.5667 - dense_1_acc_12: 0.6167 - dense_1_acc_13: 0.7333 - dense_1_acc_14: 0.6167 - dense_1_acc_15: 0.5833 - dense_1_acc_16: 0.6000 - dense_1_acc_17: 0.6333 - dense_1_acc_18: 0.6500 - dense_1_acc_19: 0.6833 - dense_1_acc_20: 0.7667 - dense_1_acc_21: 0.6500 - dense_1_acc_22: 0.6500 - dense_1_acc_23: 0.7333 - dense_1_acc_24: 0.6667 - dense_1_acc_25: 0.5333 - dense_1_acc_26: 0.7333 - dense_1_acc_27: 0.6667 - dense_1_acc_28: 0.7167 - dense_1_acc_29: 0.7000 - dense_1_acc_30: 0.0000e+00 Epoch 24/100 60/60 [==============================] - 0s - loss: 45.3187 - dense_1_loss_1: 4.0901 - dense_1_loss_2: 3.3922 - dense_1_loss_3: 2.5825 - dense_1_loss_4: 2.2373 - dense_1_loss_5: 1.8703 - dense_1_loss_6: 1.5549 - dense_1_loss_7: 1.5355 - dense_1_loss_8: 1.4056 - dense_1_loss_9: 1.3904 - dense_1_loss_10: 1.3019 - dense_1_loss_11: 1.3719 - dense_1_loss_12: 1.2736 - dense_1_loss_13: 1.1612 - dense_1_loss_14: 1.2376 - dense_1_loss_15: 1.2141 - dense_1_loss_16: 1.2789 - dense_1_loss_17: 1.3098 - dense_1_loss_18: 1.2803 - dense_1_loss_19: 1.2623 - dense_1_loss_20: 1.2124 - dense_1_loss_21: 1.2315 - dense_1_loss_22: 1.3501 - dense_1_loss_23: 1.2118 - dense_1_loss_24: 1.2614 - dense_1_loss_25: 1.3370 - dense_1_loss_26: 1.2148 - dense_1_loss_27: 1.2702 - dense_1_loss_28: 1.2633 - dense_1_loss_29: 1.2155 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2833 - dense_1_acc_3: 0.4500 - dense_1_acc_4: 0.3500 - dense_1_acc_5: 0.4000 - dense_1_acc_6: 0.6000 - dense_1_acc_7: 0.5500 - dense_1_acc_8: 0.6333 - dense_1_acc_9: 0.6333 - dense_1_acc_10: 0.6500 - dense_1_acc_11: 0.5333 - dense_1_acc_12: 0.6833 - dense_1_acc_13: 0.8000 - dense_1_acc_14: 0.6000 - dense_1_acc_15: 0.6000 - dense_1_acc_16: 0.6667 - dense_1_acc_17: 0.6833 - dense_1_acc_18: 0.7167 - dense_1_acc_19: 0.7333 - dense_1_acc_20: 0.7500 - dense_1_acc_21: 0.7500 - dense_1_acc_22: 0.7000 - dense_1_acc_23: 0.7667 - dense_1_acc_24: 0.7500 - dense_1_acc_25: 0.5000 - dense_1_acc_26: 0.7167 - dense_1_acc_27: 0.6667 - dense_1_acc_28: 0.7667 - dense_1_acc_29: 0.7667 - dense_1_acc_30: 0.0000e+00 Epoch 25/100 60/60 [==============================] - 0s - loss: 43.0943 - dense_1_loss_1: 4.0826 - dense_1_loss_2: 3.3473 - dense_1_loss_3: 2.5037 - dense_1_loss_4: 2.1542 - dense_1_loss_5: 1.7748 - dense_1_loss_6: 1.4676 - dense_1_loss_7: 1.4258 - dense_1_loss_8: 1.3439 - dense_1_loss_9: 1.2775 - dense_1_loss_10: 1.2072 - dense_1_loss_11: 1.2504 - dense_1_loss_12: 1.1964 - dense_1_loss_13: 1.0910 - dense_1_loss_14: 1.1317 - dense_1_loss_15: 1.1530 - dense_1_loss_16: 1.1781 - dense_1_loss_17: 1.2662 - dense_1_loss_18: 1.2050 - dense_1_loss_19: 1.1581 - dense_1_loss_20: 1.1413 - dense_1_loss_21: 1.1878 - dense_1_loss_22: 1.2693 - dense_1_loss_23: 1.1599 - dense_1_loss_24: 1.1910 - dense_1_loss_25: 1.2612 - dense_1_loss_26: 1.1298 - dense_1_loss_27: 1.2135 - dense_1_loss_28: 1.1730 - dense_1_loss_29: 1.1530 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3000 - dense_1_acc_3: 0.4667 - dense_1_acc_4: 0.3500 - dense_1_acc_5: 0.4500 - dense_1_acc_6: 0.6000 - dense_1_acc_7: 0.5833 - dense_1_acc_8: 0.6500 - dense_1_acc_9: 0.6833 - dense_1_acc_10: 0.6833 - dense_1_acc_11: 0.6167 - dense_1_acc_12: 0.7167 - dense_1_acc_13: 0.8333 - dense_1_acc_14: 0.7167 - dense_1_acc_15: 0.7000 - dense_1_acc_16: 0.7167 - dense_1_acc_17: 0.6333 - dense_1_acc_18: 0.7000 - dense_1_acc_19: 0.8000 - dense_1_acc_20: 0.7833 - dense_1_acc_21: 0.7667 - dense_1_acc_22: 0.7000 - dense_1_acc_23: 0.7833 - dense_1_acc_24: 0.6833 - dense_1_acc_25: 0.5833 - dense_1_acc_26: 0.7500 - dense_1_acc_27: 0.6500 - dense_1_acc_28: 0.7833 - dense_1_acc_29: 0.7667 - dense_1_acc_30: 0.0000e+00 Epoch 26/100 60/60 [==============================] - 0s - loss: 40.8022 - dense_1_loss_1: 4.0748 - dense_1_loss_2: 3.3002 - dense_1_loss_3: 2.4259 - dense_1_loss_4: 2.0715 - dense_1_loss_5: 1.6825 - dense_1_loss_6: 1.3776 - dense_1_loss_7: 1.3323 - dense_1_loss_8: 1.2728 - dense_1_loss_9: 1.2005 - dense_1_loss_10: 1.1305 - dense_1_loss_11: 1.1444 - dense_1_loss_12: 1.1230 - dense_1_loss_13: 1.0321 - dense_1_loss_14: 1.0275 - dense_1_loss_15: 1.0911 - dense_1_loss_16: 1.0935 - dense_1_loss_17: 1.1465 - dense_1_loss_18: 1.1157 - dense_1_loss_19: 1.0581 - dense_1_loss_20: 1.0947 - dense_1_loss_21: 1.1024 - dense_1_loss_22: 1.1725 - dense_1_loss_23: 1.0688 - dense_1_loss_24: 1.0804 - dense_1_loss_25: 1.1933 - dense_1_loss_26: 1.0530 - dense_1_loss_27: 1.1423 - dense_1_loss_28: 1.0956 - dense_1_loss_29: 1.0991 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2833 - dense_1_acc_3: 0.5000 - dense_1_acc_4: 0.3833 - dense_1_acc_5: 0.5000 - dense_1_acc_6: 0.6833 - dense_1_acc_7: 0.7000 - dense_1_acc_8: 0.6167 - dense_1_acc_9: 0.7000 - dense_1_acc_10: 0.7833 - dense_1_acc_11: 0.7167 - dense_1_acc_12: 0.8000 - dense_1_acc_13: 0.8833 - dense_1_acc_14: 0.8667 - dense_1_acc_15: 0.7833 - dense_1_acc_16: 0.8167 - dense_1_acc_17: 0.8000 - dense_1_acc_18: 0.7333 - dense_1_acc_19: 0.8333 - dense_1_acc_20: 0.8167 - dense_1_acc_21: 0.8667 - dense_1_acc_22: 0.7667 - dense_1_acc_23: 0.8500 - dense_1_acc_24: 0.8000 - dense_1_acc_25: 0.7000 - dense_1_acc_26: 0.8667 - dense_1_acc_27: 0.7667 - dense_1_acc_28: 0.8333 - dense_1_acc_29: 0.8333 - dense_1_acc_30: 0.0000e+00 Epoch 27/100 60/60 [==============================] - 0s - loss: 38.8025 - dense_1_loss_1: 4.0663 - dense_1_loss_2: 3.2536 - dense_1_loss_3: 2.3506 - dense_1_loss_4: 1.9905 - dense_1_loss_5: 1.5981 - dense_1_loss_6: 1.3005 - dense_1_loss_7: 1.2525 - dense_1_loss_8: 1.2083 - dense_1_loss_9: 1.1123 - dense_1_loss_10: 1.0546 - dense_1_loss_11: 1.0712 - dense_1_loss_12: 1.0661 - dense_1_loss_13: 0.9613 - dense_1_loss_14: 0.9802 - dense_1_loss_15: 1.0168 - dense_1_loss_16: 1.0461 - dense_1_loss_17: 1.0501 - dense_1_loss_18: 1.0304 - dense_1_loss_19: 1.0029 - dense_1_loss_20: 1.0419 - dense_1_loss_21: 1.0439 - dense_1_loss_22: 1.0489 - dense_1_loss_23: 1.0150 - dense_1_loss_24: 0.9941 - dense_1_loss_25: 1.1266 - dense_1_loss_26: 1.0159 - dense_1_loss_27: 1.0529 - dense_1_loss_28: 1.0263 - dense_1_loss_29: 1.0247 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2833 - dense_1_acc_3: 0.5000 - dense_1_acc_4: 0.4000 - dense_1_acc_5: 0.5167 - dense_1_acc_6: 0.6667 - dense_1_acc_7: 0.7500 - dense_1_acc_8: 0.6500 - dense_1_acc_9: 0.7500 - dense_1_acc_10: 0.8333 - dense_1_acc_11: 0.7667 - dense_1_acc_12: 0.8167 - dense_1_acc_13: 0.8833 - dense_1_acc_14: 0.8667 - dense_1_acc_15: 0.8333 - dense_1_acc_16: 0.9000 - dense_1_acc_17: 0.8500 - dense_1_acc_18: 0.7500 - dense_1_acc_19: 0.8833 - dense_1_acc_20: 0.8500 - dense_1_acc_21: 0.8500 - dense_1_acc_22: 0.8833 - dense_1_acc_23: 0.8333 - dense_1_acc_24: 0.8667 - dense_1_acc_25: 0.7000 - dense_1_acc_26: 0.8500 - dense_1_acc_27: 0.8000 - dense_1_acc_28: 0.8333 - dense_1_acc_29: 0.8333 - dense_1_acc_30: 0.0000e+00 Epoch 28/100 60/60 [==============================] - 0s - loss: 36.8764 - dense_1_loss_1: 4.0580 - dense_1_loss_2: 3.2063 - dense_1_loss_3: 2.2747 - dense_1_loss_4: 1.9115 - dense_1_loss_5: 1.5232 - dense_1_loss_6: 1.2318 - dense_1_loss_7: 1.1687 - dense_1_loss_8: 1.1294 - dense_1_loss_9: 1.0486 - dense_1_loss_10: 0.9668 - dense_1_loss_11: 1.0165 - dense_1_loss_12: 1.0011 - dense_1_loss_13: 0.8906 - dense_1_loss_14: 0.8915 - dense_1_loss_15: 0.9522 - dense_1_loss_16: 0.9425 - dense_1_loss_17: 0.9959 - dense_1_loss_18: 0.9568 - dense_1_loss_19: 0.9452 - dense_1_loss_20: 0.9827 - dense_1_loss_21: 0.9702 - dense_1_loss_22: 0.9839 - dense_1_loss_23: 0.9601 - dense_1_loss_24: 0.9256 - dense_1_loss_25: 1.0594 - dense_1_loss_26: 0.9498 - dense_1_loss_27: 1.0003 - dense_1_loss_28: 0.9784 - dense_1_loss_29: 0.9549 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2833 - dense_1_acc_3: 0.5333 - dense_1_acc_4: 0.4500 - dense_1_acc_5: 0.5500 - dense_1_acc_6: 0.7167 - dense_1_acc_7: 0.8500 - dense_1_acc_8: 0.7167 - dense_1_acc_9: 0.8167 - dense_1_acc_10: 0.8833 - dense_1_acc_11: 0.7667 - dense_1_acc_12: 0.8500 - dense_1_acc_13: 0.9500 - dense_1_acc_14: 0.8833 - dense_1_acc_15: 0.8500 - dense_1_acc_16: 0.9500 - dense_1_acc_17: 0.8333 - dense_1_acc_18: 0.7667 - dense_1_acc_19: 0.8667 - dense_1_acc_20: 0.8333 - dense_1_acc_21: 0.8500 - dense_1_acc_22: 0.9000 - dense_1_acc_23: 0.8833 - dense_1_acc_24: 0.8833 - dense_1_acc_25: 0.7667 - dense_1_acc_26: 0.8833 - dense_1_acc_27: 0.8167 - dense_1_acc_28: 0.8333 - dense_1_acc_29: 0.8833 - dense_1_acc_30: 0.0000e+00 Epoch 29/100 60/60 [==============================] - 0s - loss: 34.8516 - dense_1_loss_1: 4.0505 - dense_1_loss_2: 3.1603 - dense_1_loss_3: 2.2028 - dense_1_loss_4: 1.8335 - dense_1_loss_5: 1.4395 - dense_1_loss_6: 1.1476 - dense_1_loss_7: 1.0989 - dense_1_loss_8: 1.0629 - dense_1_loss_9: 0.9678 - dense_1_loss_10: 0.9031 - dense_1_loss_11: 0.9193 - dense_1_loss_12: 0.9352 - dense_1_loss_13: 0.8223 - dense_1_loss_14: 0.8405 - dense_1_loss_15: 0.8826 - dense_1_loss_16: 0.8818 - dense_1_loss_17: 0.9120 - dense_1_loss_18: 0.8814 - dense_1_loss_19: 0.8926 - dense_1_loss_20: 0.9129 - dense_1_loss_21: 0.8975 - dense_1_loss_22: 0.9191 - dense_1_loss_23: 0.8582 - dense_1_loss_24: 0.8664 - dense_1_loss_25: 0.9873 - dense_1_loss_26: 0.8851 - dense_1_loss_27: 0.9208 - dense_1_loss_28: 0.8919 - dense_1_loss_29: 0.8781 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2833 - dense_1_acc_3: 0.5333 - dense_1_acc_4: 0.4833 - dense_1_acc_5: 0.5833 - dense_1_acc_6: 0.7667 - dense_1_acc_7: 0.9000 - dense_1_acc_8: 0.7500 - dense_1_acc_9: 0.8000 - dense_1_acc_10: 0.8833 - dense_1_acc_11: 0.8500 - dense_1_acc_12: 0.9167 - dense_1_acc_13: 0.9500 - dense_1_acc_14: 0.9500 - dense_1_acc_15: 0.8833 - dense_1_acc_16: 0.9667 - dense_1_acc_17: 0.9000 - dense_1_acc_18: 0.8833 - dense_1_acc_19: 0.8667 - dense_1_acc_20: 0.9000 - dense_1_acc_21: 0.9333 - dense_1_acc_22: 0.9167 - dense_1_acc_23: 0.9167 - dense_1_acc_24: 0.9167 - dense_1_acc_25: 0.8000 - dense_1_acc_26: 0.9167 - dense_1_acc_27: 0.9000 - dense_1_acc_28: 0.8667 - dense_1_acc_29: 0.8833 - dense_1_acc_30: 0.0000e+00 Epoch 30/100 60/60 [==============================] - 0s - loss: 33.0396 - dense_1_loss_1: 4.0425 - dense_1_loss_2: 3.1119 - dense_1_loss_3: 2.1338 - dense_1_loss_4: 1.7691 - dense_1_loss_5: 1.3593 - dense_1_loss_6: 1.0798 - dense_1_loss_7: 1.0260 - dense_1_loss_8: 0.9722 - dense_1_loss_9: 0.8939 - dense_1_loss_10: 0.8502 - dense_1_loss_11: 0.8510 - dense_1_loss_12: 0.8728 - dense_1_loss_13: 0.7624 - dense_1_loss_14: 0.8034 - dense_1_loss_15: 0.8181 - dense_1_loss_16: 0.8146 - dense_1_loss_17: 0.8315 - dense_1_loss_18: 0.8346 - dense_1_loss_19: 0.8512 - dense_1_loss_20: 0.8402 - dense_1_loss_21: 0.8208 - dense_1_loss_22: 0.8591 - dense_1_loss_23: 0.7893 - dense_1_loss_24: 0.8128 - dense_1_loss_25: 0.9424 - dense_1_loss_26: 0.7967 - dense_1_loss_27: 0.8467 - dense_1_loss_28: 0.8232 - dense_1_loss_29: 0.8301 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2833 - dense_1_acc_3: 0.5500 - dense_1_acc_4: 0.4833 - dense_1_acc_5: 0.6333 - dense_1_acc_6: 0.7833 - dense_1_acc_7: 0.9500 - dense_1_acc_8: 0.8000 - dense_1_acc_9: 0.8333 - dense_1_acc_10: 0.9000 - dense_1_acc_11: 0.9000 - dense_1_acc_12: 0.9333 - dense_1_acc_13: 0.9667 - dense_1_acc_14: 0.9500 - dense_1_acc_15: 0.8833 - dense_1_acc_16: 0.9667 - dense_1_acc_17: 0.9333 - dense_1_acc_18: 0.9000 - dense_1_acc_19: 0.9000 - dense_1_acc_20: 0.9167 - dense_1_acc_21: 0.9500 - dense_1_acc_22: 0.9333 - dense_1_acc_23: 0.9333 - dense_1_acc_24: 0.9667 - dense_1_acc_25: 0.8333 - dense_1_acc_26: 0.9500 - dense_1_acc_27: 0.9167 - dense_1_acc_28: 0.9000 - dense_1_acc_29: 0.8833 - dense_1_acc_30: 0.0000e+00 Epoch 31/100 60/60 [==============================] - 0s - loss: 31.2115 - dense_1_loss_1: 4.0347 - dense_1_loss_2: 3.0670 - dense_1_loss_3: 2.0677 - dense_1_loss_4: 1.6894 - dense_1_loss_5: 1.2791 - dense_1_loss_6: 1.0036 - dense_1_loss_7: 0.9697 - dense_1_loss_8: 0.9078 - dense_1_loss_9: 0.8270 - dense_1_loss_10: 0.7842 - dense_1_loss_11: 0.7984 - dense_1_loss_12: 0.7919 - dense_1_loss_13: 0.6998 - dense_1_loss_14: 0.7360 - dense_1_loss_15: 0.7607 - dense_1_loss_16: 0.7495 - dense_1_loss_17: 0.7710 - dense_1_loss_18: 0.7747 - dense_1_loss_19: 0.7774 - dense_1_loss_20: 0.7782 - dense_1_loss_21: 0.7584 - dense_1_loss_22: 0.7907 - dense_1_loss_23: 0.7403 - dense_1_loss_24: 0.7389 - dense_1_loss_25: 0.8684 - dense_1_loss_26: 0.7347 - dense_1_loss_27: 0.7808 - dense_1_loss_28: 0.7643 - dense_1_loss_29: 0.7672 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2833 - dense_1_acc_3: 0.5667 - dense_1_acc_4: 0.4833 - dense_1_acc_5: 0.7000 - dense_1_acc_6: 0.8167 - dense_1_acc_7: 0.9333 - dense_1_acc_8: 0.8667 - dense_1_acc_9: 0.8500 - dense_1_acc_10: 0.9500 - dense_1_acc_11: 0.9000 - dense_1_acc_12: 0.9500 - dense_1_acc_13: 0.9833 - dense_1_acc_14: 0.9500 - dense_1_acc_15: 0.9167 - dense_1_acc_16: 0.9833 - dense_1_acc_17: 0.9333 - dense_1_acc_18: 0.9500 - dense_1_acc_19: 0.9500 - dense_1_acc_20: 0.9500 - dense_1_acc_21: 0.9333 - dense_1_acc_22: 0.9500 - dense_1_acc_23: 0.9833 - dense_1_acc_24: 0.9667 - dense_1_acc_25: 0.8500 - dense_1_acc_26: 0.9500 - dense_1_acc_27: 0.9500 - dense_1_acc_28: 0.9167 - dense_1_acc_29: 0.9000 - dense_1_acc_30: 0.0000e+00 Epoch 32/100 60/60 [==============================] - 0s - loss: 29.5748 - dense_1_loss_1: 4.0282 - dense_1_loss_2: 3.0201 - dense_1_loss_3: 2.0028 - dense_1_loss_4: 1.6073 - dense_1_loss_5: 1.1976 - dense_1_loss_6: 0.9391 - dense_1_loss_7: 0.8996 - dense_1_loss_8: 0.8674 - dense_1_loss_9: 0.7689 - dense_1_loss_10: 0.7085 - dense_1_loss_11: 0.7296 - dense_1_loss_12: 0.7143 - dense_1_loss_13: 0.6478 - dense_1_loss_14: 0.6798 - dense_1_loss_15: 0.7037 - dense_1_loss_16: 0.6957 - dense_1_loss_17: 0.7155 - dense_1_loss_18: 0.7009 - dense_1_loss_19: 0.7276 - dense_1_loss_20: 0.7203 - dense_1_loss_21: 0.7206 - dense_1_loss_22: 0.7310 - dense_1_loss_23: 0.6957 - dense_1_loss_24: 0.6805 - dense_1_loss_25: 0.8066 - dense_1_loss_26: 0.6915 - dense_1_loss_27: 0.7420 - dense_1_loss_28: 0.7182 - dense_1_loss_29: 0.7141 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.2833 - dense_1_acc_3: 0.5500 - dense_1_acc_4: 0.5500 - dense_1_acc_5: 0.7333 - dense_1_acc_6: 0.8000 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.8167 - dense_1_acc_9: 0.8833 - dense_1_acc_10: 0.9167 - dense_1_acc_11: 0.9667 - dense_1_acc_12: 0.9667 - dense_1_acc_13: 0.9833 - dense_1_acc_14: 0.9500 - dense_1_acc_15: 0.9500 - dense_1_acc_16: 0.9833 - dense_1_acc_17: 0.9500 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 0.9667 - dense_1_acc_20: 0.9667 - dense_1_acc_21: 0.9667 - dense_1_acc_22: 0.9833 - dense_1_acc_23: 0.9833 - dense_1_acc_24: 0.9833 - dense_1_acc_25: 0.8667 - dense_1_acc_26: 0.9667 - dense_1_acc_27: 0.9500 - dense_1_acc_28: 0.9500 - dense_1_acc_29: 0.9000 - dense_1_acc_30: 0.0000e+00 Epoch 33/100 60/60 [==============================] - 0s - loss: 27.9737 - dense_1_loss_1: 4.0216 - dense_1_loss_2: 2.9721 - dense_1_loss_3: 1.9386 - dense_1_loss_4: 1.5331 - dense_1_loss_5: 1.1200 - dense_1_loss_6: 0.8744 - dense_1_loss_7: 0.8335 - dense_1_loss_8: 0.7904 - dense_1_loss_9: 0.7247 - dense_1_loss_10: 0.6550 - dense_1_loss_11: 0.6820 - dense_1_loss_12: 0.6602 - dense_1_loss_13: 0.6051 - dense_1_loss_14: 0.6329 - dense_1_loss_15: 0.6448 - dense_1_loss_16: 0.6574 - dense_1_loss_17: 0.6564 - dense_1_loss_18: 0.6470 - dense_1_loss_19: 0.6808 - dense_1_loss_20: 0.6682 - dense_1_loss_21: 0.6471 - dense_1_loss_22: 0.6879 - dense_1_loss_23: 0.6490 - dense_1_loss_24: 0.6138 - dense_1_loss_25: 0.7459 - dense_1_loss_26: 0.6281 - dense_1_loss_27: 0.6810 - dense_1_loss_28: 0.6606 - dense_1_loss_29: 0.6623 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3000 - dense_1_acc_3: 0.5500 - dense_1_acc_4: 0.5667 - dense_1_acc_5: 0.7500 - dense_1_acc_6: 0.8333 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.9167 - dense_1_acc_9: 0.9000 - dense_1_acc_10: 0.9833 - dense_1_acc_11: 0.9167 - dense_1_acc_12: 0.9833 - dense_1_acc_13: 0.9833 - dense_1_acc_14: 0.9667 - dense_1_acc_15: 0.9667 - dense_1_acc_16: 0.9833 - dense_1_acc_17: 0.9667 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 0.9667 - dense_1_acc_20: 0.9833 - dense_1_acc_21: 0.9833 - dense_1_acc_22: 0.9833 - dense_1_acc_23: 0.9833 - dense_1_acc_24: 0.9833 - dense_1_acc_25: 0.9000 - dense_1_acc_26: 0.9833 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 0.9833 - dense_1_acc_29: 0.9000 - dense_1_acc_30: 0.0000e+00 Epoch 34/100 60/60 [==============================] - 0s - loss: 26.4537 - dense_1_loss_1: 4.0143 - dense_1_loss_2: 2.9284 - dense_1_loss_3: 1.8764 - dense_1_loss_4: 1.4577 - dense_1_loss_5: 1.0544 - dense_1_loss_6: 0.8088 - dense_1_loss_7: 0.7924 - dense_1_loss_8: 0.7171 - dense_1_loss_9: 0.6815 - dense_1_loss_10: 0.6032 - dense_1_loss_11: 0.6289 - dense_1_loss_12: 0.6122 - dense_1_loss_13: 0.5585 - dense_1_loss_14: 0.5847 - dense_1_loss_15: 0.6003 - dense_1_loss_16: 0.5864 - dense_1_loss_17: 0.6140 - dense_1_loss_18: 0.6007 - dense_1_loss_19: 0.6066 - dense_1_loss_20: 0.6227 - dense_1_loss_21: 0.6074 - dense_1_loss_22: 0.6242 - dense_1_loss_23: 0.5913 - dense_1_loss_24: 0.5784 - dense_1_loss_25: 0.6959 - dense_1_loss_26: 0.5685 - dense_1_loss_27: 0.6152 - dense_1_loss_28: 0.6178 - dense_1_loss_29: 0.6057 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3000 - dense_1_acc_3: 0.5667 - dense_1_acc_4: 0.6167 - dense_1_acc_5: 0.7833 - dense_1_acc_6: 0.8333 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.9167 - dense_1_acc_9: 0.9167 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 0.9667 - dense_1_acc_12: 0.9667 - dense_1_acc_13: 0.9500 - dense_1_acc_14: 0.9833 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 0.9667 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 0.9833 - dense_1_acc_20: 0.9833 - dense_1_acc_21: 0.9833 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 0.9833 - dense_1_acc_24: 0.9833 - dense_1_acc_25: 0.9000 - dense_1_acc_26: 0.9833 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 0.9833 - dense_1_acc_29: 0.9333 - dense_1_acc_30: 0.0000e+00 Epoch 35/100 60/60 [==============================] - 0s - loss: 25.3020 - dense_1_loss_1: 4.0076 - dense_1_loss_2: 2.8809 - dense_1_loss_3: 1.8127 - dense_1_loss_4: 1.3818 - dense_1_loss_5: 0.9883 - dense_1_loss_6: 0.7545 - dense_1_loss_7: 0.7398 - dense_1_loss_8: 0.6632 - dense_1_loss_9: 0.6211 - dense_1_loss_10: 0.5547 - dense_1_loss_11: 0.5828 - dense_1_loss_12: 0.5680 - dense_1_loss_13: 0.5074 - dense_1_loss_14: 0.5488 - dense_1_loss_15: 0.5518 - dense_1_loss_16: 0.5706 - dense_1_loss_17: 0.5637 - dense_1_loss_18: 0.5603 - dense_1_loss_19: 0.5770 - dense_1_loss_20: 0.5882 - dense_1_loss_21: 0.5975 - dense_1_loss_22: 0.5684 - dense_1_loss_23: 0.5613 - dense_1_loss_24: 0.5658 - dense_1_loss_25: 0.6647 - dense_1_loss_26: 0.5433 - dense_1_loss_27: 0.6020 - dense_1_loss_28: 0.5876 - dense_1_loss_29: 0.5882 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3500 - dense_1_acc_3: 0.6000 - dense_1_acc_4: 0.6500 - dense_1_acc_5: 0.7833 - dense_1_acc_6: 0.8667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.9500 - dense_1_acc_9: 0.9000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 0.9667 - dense_1_acc_12: 0.9833 - dense_1_acc_13: 0.9833 - dense_1_acc_14: 0.9667 - dense_1_acc_15: 0.9833 - dense_1_acc_16: 0.9833 - dense_1_acc_17: 0.9833 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 0.9667 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 0.9833 - dense_1_acc_22: 0.9833 - dense_1_acc_23: 0.9833 - dense_1_acc_24: 0.9833 - dense_1_acc_25: 0.8833 - dense_1_acc_26: 0.9833 - dense_1_acc_27: 0.9833 - dense_1_acc_28: 0.9667 - dense_1_acc_29: 0.9000 - dense_1_acc_30: 0.0000e+00 Epoch 36/100 60/60 [==============================] - 0s - loss: 24.0212 - dense_1_loss_1: 4.0018 - dense_1_loss_2: 2.8350 - dense_1_loss_3: 1.7567 - dense_1_loss_4: 1.3129 - dense_1_loss_5: 0.9261 - dense_1_loss_6: 0.7048 - dense_1_loss_7: 0.7056 - dense_1_loss_8: 0.5899 - dense_1_loss_9: 0.5723 - dense_1_loss_10: 0.5290 - dense_1_loss_11: 0.5672 - dense_1_loss_12: 0.5433 - dense_1_loss_13: 0.4691 - dense_1_loss_14: 0.5052 - dense_1_loss_15: 0.5154 - dense_1_loss_16: 0.5191 - dense_1_loss_17: 0.5147 - dense_1_loss_18: 0.5275 - dense_1_loss_19: 0.5322 - dense_1_loss_20: 0.5460 - dense_1_loss_21: 0.5364 - dense_1_loss_22: 0.5149 - dense_1_loss_23: 0.5171 - dense_1_loss_24: 0.5470 - dense_1_loss_25: 0.6014 - dense_1_loss_26: 0.5223 - dense_1_loss_27: 0.5298 - dense_1_loss_28: 0.5323 - dense_1_loss_29: 0.5465 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6167 - dense_1_acc_4: 0.6500 - dense_1_acc_5: 0.8000 - dense_1_acc_6: 0.8833 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.9500 - dense_1_acc_9: 0.9333 - dense_1_acc_10: 0.9833 - dense_1_acc_11: 0.9333 - dense_1_acc_12: 0.9667 - dense_1_acc_13: 0.9833 - dense_1_acc_14: 0.9833 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 0.9833 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 0.9833 - dense_1_acc_19: 0.9833 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 0.9833 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 0.9833 - dense_1_acc_25: 0.9333 - dense_1_acc_26: 0.9667 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 0.9833 - dense_1_acc_29: 0.9333 - dense_1_acc_30: 0.0000e+00 Epoch 37/100 60/60 [==============================] - 0s - loss: 22.7911 - dense_1_loss_1: 3.9958 - dense_1_loss_2: 2.7889 - dense_1_loss_3: 1.6957 - dense_1_loss_4: 1.2449 - dense_1_loss_5: 0.8664 - dense_1_loss_6: 0.6546 - dense_1_loss_7: 0.6433 - dense_1_loss_8: 0.5611 - dense_1_loss_9: 0.5342 - dense_1_loss_10: 0.4752 - dense_1_loss_11: 0.4942 - dense_1_loss_12: 0.4948 - dense_1_loss_13: 0.4327 - dense_1_loss_14: 0.4618 - dense_1_loss_15: 0.4805 - dense_1_loss_16: 0.4725 - dense_1_loss_17: 0.4893 - dense_1_loss_18: 0.4711 - dense_1_loss_19: 0.5226 - dense_1_loss_20: 0.4978 - dense_1_loss_21: 0.4973 - dense_1_loss_22: 0.5085 - dense_1_loss_23: 0.4918 - dense_1_loss_24: 0.4780 - dense_1_loss_25: 0.5687 - dense_1_loss_26: 0.5094 - dense_1_loss_27: 0.4872 - dense_1_loss_28: 0.4759 - dense_1_loss_29: 0.4969 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6167 - dense_1_acc_4: 0.6833 - dense_1_acc_5: 0.8333 - dense_1_acc_6: 0.9167 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.9667 - dense_1_acc_9: 0.9833 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 0.9667 - dense_1_acc_12: 0.9833 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 0.9833 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 0.9833 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 0.9833 - dense_1_acc_20: 0.9667 - dense_1_acc_21: 0.9500 - dense_1_acc_22: 0.9833 - dense_1_acc_23: 0.9833 - dense_1_acc_24: 0.9833 - dense_1_acc_25: 0.9333 - dense_1_acc_26: 0.9500 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 0.9833 - dense_1_acc_29: 0.9667 - dense_1_acc_30: 0.0000e+00 Epoch 38/100 60/60 [==============================] - 0s - loss: 21.5434 - dense_1_loss_1: 3.9901 - dense_1_loss_2: 2.7436 - dense_1_loss_3: 1.6372 - dense_1_loss_4: 1.1774 - dense_1_loss_5: 0.8135 - dense_1_loss_6: 0.6036 - dense_1_loss_7: 0.5891 - dense_1_loss_8: 0.5241 - dense_1_loss_9: 0.4945 - dense_1_loss_10: 0.4275 - dense_1_loss_11: 0.4470 - dense_1_loss_12: 0.4405 - dense_1_loss_13: 0.3920 - dense_1_loss_14: 0.4135 - dense_1_loss_15: 0.4474 - dense_1_loss_16: 0.4103 - dense_1_loss_17: 0.4569 - dense_1_loss_18: 0.4348 - dense_1_loss_19: 0.4566 - dense_1_loss_20: 0.4556 - dense_1_loss_21: 0.4633 - dense_1_loss_22: 0.4516 - dense_1_loss_23: 0.4803 - dense_1_loss_24: 0.4165 - dense_1_loss_25: 0.5328 - dense_1_loss_26: 0.4615 - dense_1_loss_27: 0.4715 - dense_1_loss_28: 0.4605 - dense_1_loss_29: 0.4501 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6167 - dense_1_acc_4: 0.7500 - dense_1_acc_5: 0.9000 - dense_1_acc_6: 0.9333 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.9833 - dense_1_acc_9: 0.9833 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 0.9833 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 0.9833 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 0.9833 - dense_1_acc_25: 0.9667 - dense_1_acc_26: 0.9833 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 0.9833 - dense_1_acc_30: 0.0000e+00 Epoch 39/100 60/60 [==============================] - 0s - loss: 20.4096 - dense_1_loss_1: 3.9847 - dense_1_loss_2: 2.6981 - dense_1_loss_3: 1.5830 - dense_1_loss_4: 1.1062 - dense_1_loss_5: 0.7642 - dense_1_loss_6: 0.5615 - dense_1_loss_7: 0.5547 - dense_1_loss_8: 0.4929 - dense_1_loss_9: 0.4626 - dense_1_loss_10: 0.3892 - dense_1_loss_11: 0.4192 - dense_1_loss_12: 0.4008 - dense_1_loss_13: 0.3720 - dense_1_loss_14: 0.3958 - dense_1_loss_15: 0.4000 - dense_1_loss_16: 0.3839 - dense_1_loss_17: 0.4178 - dense_1_loss_18: 0.4045 - dense_1_loss_19: 0.4057 - dense_1_loss_20: 0.4201 - dense_1_loss_21: 0.4235 - dense_1_loss_22: 0.3947 - dense_1_loss_23: 0.4373 - dense_1_loss_24: 0.3695 - dense_1_loss_25: 0.4961 - dense_1_loss_26: 0.4024 - dense_1_loss_27: 0.4213 - dense_1_loss_28: 0.4212 - dense_1_loss_29: 0.4268 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6167 - dense_1_acc_4: 0.7667 - dense_1_acc_5: 0.9000 - dense_1_acc_6: 0.9500 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 0.9833 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 0.9833 - dense_1_acc_12: 0.9833 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 0.9833 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 0.9833 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 0.9833 - dense_1_acc_30: 0.0000e+00 Epoch 40/100 60/60 [==============================] - 0s - loss: 19.3388 - dense_1_loss_1: 3.9784 - dense_1_loss_2: 2.6535 - dense_1_loss_3: 1.5344 - dense_1_loss_4: 1.0452 - dense_1_loss_5: 0.7121 - dense_1_loss_6: 0.5257 - dense_1_loss_7: 0.5184 - dense_1_loss_8: 0.4365 - dense_1_loss_9: 0.4269 - dense_1_loss_10: 0.3572 - dense_1_loss_11: 0.3919 - dense_1_loss_12: 0.3681 - dense_1_loss_13: 0.3411 - dense_1_loss_14: 0.3671 - dense_1_loss_15: 0.3574 - dense_1_loss_16: 0.3562 - dense_1_loss_17: 0.3790 - dense_1_loss_18: 0.3677 - dense_1_loss_19: 0.3747 - dense_1_loss_20: 0.3929 - dense_1_loss_21: 0.3757 - dense_1_loss_22: 0.3625 - dense_1_loss_23: 0.3857 - dense_1_loss_24: 0.3530 - dense_1_loss_25: 0.4471 - dense_1_loss_26: 0.3708 - dense_1_loss_27: 0.3588 - dense_1_loss_28: 0.3926 - dense_1_loss_29: 0.4082 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3667 - dense_1_acc_3: 0.6167 - dense_1_acc_4: 0.7833 - dense_1_acc_5: 0.9000 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.9833 - dense_1_acc_9: 0.9833 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 0.9833 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 0.9833 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 0.9833 - dense_1_acc_30: 0.0000e+00 Epoch 41/100 60/60 [==============================] - 0s - loss: 18.4427 - dense_1_loss_1: 3.9727 - dense_1_loss_2: 2.6086 - dense_1_loss_3: 1.4855 - dense_1_loss_4: 0.9911 - dense_1_loss_5: 0.6693 - dense_1_loss_6: 0.4911 - dense_1_loss_7: 0.4898 - dense_1_loss_8: 0.3894 - dense_1_loss_9: 0.3934 - dense_1_loss_10: 0.3294 - dense_1_loss_11: 0.3655 - dense_1_loss_12: 0.3469 - dense_1_loss_13: 0.3107 - dense_1_loss_14: 0.3240 - dense_1_loss_15: 0.3409 - dense_1_loss_16: 0.3289 - dense_1_loss_17: 0.3514 - dense_1_loss_18: 0.3306 - dense_1_loss_19: 0.3551 - dense_1_loss_20: 0.3632 - dense_1_loss_21: 0.3502 - dense_1_loss_22: 0.3416 - dense_1_loss_23: 0.3415 - dense_1_loss_24: 0.3464 - dense_1_loss_25: 0.4083 - dense_1_loss_26: 0.3446 - dense_1_loss_27: 0.3314 - dense_1_loss_28: 0.3591 - dense_1_loss_29: 0.3821 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6333 - dense_1_acc_4: 0.7833 - dense_1_acc_5: 0.9167 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 0.9833 - dense_1_acc_9: 0.9833 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 0.9833 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 0.9833 - dense_1_acc_26: 0.9833 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 0.9833 - dense_1_acc_30: 0.0000e+00 Epoch 42/100 60/60 [==============================] - 0s - loss: 17.5646 - dense_1_loss_1: 3.9672 - dense_1_loss_2: 2.5679 - dense_1_loss_3: 1.4368 - dense_1_loss_4: 0.9286 - dense_1_loss_5: 0.6270 - dense_1_loss_6: 0.4565 - dense_1_loss_7: 0.4589 - dense_1_loss_8: 0.3703 - dense_1_loss_9: 0.3668 - dense_1_loss_10: 0.3025 - dense_1_loss_11: 0.3322 - dense_1_loss_12: 0.3195 - dense_1_loss_13: 0.2819 - dense_1_loss_14: 0.2898 - dense_1_loss_15: 0.3175 - dense_1_loss_16: 0.3034 - dense_1_loss_17: 0.3205 - dense_1_loss_18: 0.3035 - dense_1_loss_19: 0.3269 - dense_1_loss_20: 0.3342 - dense_1_loss_21: 0.3339 - dense_1_loss_22: 0.3222 - dense_1_loss_23: 0.3123 - dense_1_loss_24: 0.3104 - dense_1_loss_25: 0.3722 - dense_1_loss_26: 0.3139 - dense_1_loss_27: 0.3122 - dense_1_loss_28: 0.3210 - dense_1_loss_29: 0.3546 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6333 - dense_1_acc_4: 0.8167 - dense_1_acc_5: 0.9333 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 0.9833 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 0.9833 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 0.9833 - dense_1_acc_30: 0.0000e+00 Epoch 43/100 60/60 [==============================] - 0s - loss: 16.7271 - dense_1_loss_1: 3.9624 - dense_1_loss_2: 2.5229 - dense_1_loss_3: 1.3891 - dense_1_loss_4: 0.8766 - dense_1_loss_5: 0.5865 - dense_1_loss_6: 0.4224 - dense_1_loss_7: 0.4348 - dense_1_loss_8: 0.3382 - dense_1_loss_9: 0.3413 - dense_1_loss_10: 0.2778 - dense_1_loss_11: 0.3062 - dense_1_loss_12: 0.2913 - dense_1_loss_13: 0.2632 - dense_1_loss_14: 0.2670 - dense_1_loss_15: 0.2825 - dense_1_loss_16: 0.2767 - dense_1_loss_17: 0.2871 - dense_1_loss_18: 0.2843 - dense_1_loss_19: 0.2982 - dense_1_loss_20: 0.2984 - dense_1_loss_21: 0.3035 - dense_1_loss_22: 0.2955 - dense_1_loss_23: 0.2923 - dense_1_loss_24: 0.2757 - dense_1_loss_25: 0.3403 - dense_1_loss_26: 0.2941 - dense_1_loss_27: 0.2997 - dense_1_loss_28: 0.2961 - dense_1_loss_29: 0.3230 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6500 - dense_1_acc_4: 0.8500 - dense_1_acc_5: 0.9333 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 0.9833 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 0.9833 - dense_1_acc_30: 0.0000e+00 Epoch 44/100 60/60 [==============================] - 0s - loss: 16.0275 - dense_1_loss_1: 3.9571 - dense_1_loss_2: 2.4818 - dense_1_loss_3: 1.3443 - dense_1_loss_4: 0.8290 - dense_1_loss_5: 0.5499 - dense_1_loss_6: 0.3926 - dense_1_loss_7: 0.4075 - dense_1_loss_8: 0.3038 - dense_1_loss_9: 0.3181 - dense_1_loss_10: 0.2565 - dense_1_loss_11: 0.2853 - dense_1_loss_12: 0.2629 - dense_1_loss_13: 0.2446 - dense_1_loss_14: 0.2522 - dense_1_loss_15: 0.2562 - dense_1_loss_16: 0.2535 - dense_1_loss_17: 0.2654 - dense_1_loss_18: 0.2687 - dense_1_loss_19: 0.2804 - dense_1_loss_20: 0.2723 - dense_1_loss_21: 0.2774 - dense_1_loss_22: 0.2747 - dense_1_loss_23: 0.2851 - dense_1_loss_24: 0.2579 - dense_1_loss_25: 0.3207 - dense_1_loss_26: 0.2677 - dense_1_loss_27: 0.2866 - dense_1_loss_28: 0.2823 - dense_1_loss_29: 0.2931 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6500 - dense_1_acc_4: 0.8667 - dense_1_acc_5: 0.9333 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 45/100 60/60 [==============================] - 0s - loss: 15.2892 - dense_1_loss_1: 3.9528 - dense_1_loss_2: 2.4409 - dense_1_loss_3: 1.2989 - dense_1_loss_4: 0.7792 - dense_1_loss_5: 0.5144 - dense_1_loss_6: 0.3656 - dense_1_loss_7: 0.3803 - dense_1_loss_8: 0.2880 - dense_1_loss_9: 0.2896 - dense_1_loss_10: 0.2389 - dense_1_loss_11: 0.2588 - dense_1_loss_12: 0.2401 - dense_1_loss_13: 0.2234 - dense_1_loss_14: 0.2316 - dense_1_loss_15: 0.2370 - dense_1_loss_16: 0.2391 - dense_1_loss_17: 0.2397 - dense_1_loss_18: 0.2462 - dense_1_loss_19: 0.2602 - dense_1_loss_20: 0.2470 - dense_1_loss_21: 0.2554 - dense_1_loss_22: 0.2459 - dense_1_loss_23: 0.2618 - dense_1_loss_24: 0.2310 - dense_1_loss_25: 0.2938 - dense_1_loss_26: 0.2389 - dense_1_loss_27: 0.2560 - dense_1_loss_28: 0.2606 - dense_1_loss_29: 0.2741 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.3833 - dense_1_acc_3: 0.6500 - dense_1_acc_4: 0.8667 - dense_1_acc_5: 0.9500 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 46/100 60/60 [==============================] - 0s - loss: 14.6790 - dense_1_loss_1: 3.9472 - dense_1_loss_2: 2.3999 - dense_1_loss_3: 1.2582 - dense_1_loss_4: 0.7286 - dense_1_loss_5: 0.4830 - dense_1_loss_6: 0.3464 - dense_1_loss_7: 0.3574 - dense_1_loss_8: 0.2724 - dense_1_loss_9: 0.2679 - dense_1_loss_10: 0.2224 - dense_1_loss_11: 0.2386 - dense_1_loss_12: 0.2260 - dense_1_loss_13: 0.2022 - dense_1_loss_14: 0.2122 - dense_1_loss_15: 0.2253 - dense_1_loss_16: 0.2226 - dense_1_loss_17: 0.2267 - dense_1_loss_18: 0.2232 - dense_1_loss_19: 0.2375 - dense_1_loss_20: 0.2378 - dense_1_loss_21: 0.2357 - dense_1_loss_22: 0.2314 - dense_1_loss_23: 0.2384 - dense_1_loss_24: 0.2123 - dense_1_loss_25: 0.2717 - dense_1_loss_26: 0.2229 - dense_1_loss_27: 0.2288 - dense_1_loss_28: 0.2388 - dense_1_loss_29: 0.2635 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4333 - dense_1_acc_3: 0.6500 - dense_1_acc_4: 0.8667 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 47/100 60/60 [==============================] - 0s - loss: 14.0970 - dense_1_loss_1: 3.9425 - dense_1_loss_2: 2.3600 - dense_1_loss_3: 1.2195 - dense_1_loss_4: 0.6886 - dense_1_loss_5: 0.4519 - dense_1_loss_6: 0.3266 - dense_1_loss_7: 0.3332 - dense_1_loss_8: 0.2453 - dense_1_loss_9: 0.2445 - dense_1_loss_10: 0.2051 - dense_1_loss_11: 0.2236 - dense_1_loss_12: 0.2130 - dense_1_loss_13: 0.1839 - dense_1_loss_14: 0.1961 - dense_1_loss_15: 0.2109 - dense_1_loss_16: 0.2001 - dense_1_loss_17: 0.2142 - dense_1_loss_18: 0.2039 - dense_1_loss_19: 0.2189 - dense_1_loss_20: 0.2237 - dense_1_loss_21: 0.2180 - dense_1_loss_22: 0.2095 - dense_1_loss_23: 0.2203 - dense_1_loss_24: 0.2009 - dense_1_loss_25: 0.2488 - dense_1_loss_26: 0.2097 - dense_1_loss_27: 0.2100 - dense_1_loss_28: 0.2275 - dense_1_loss_29: 0.2467 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4500 - dense_1_acc_3: 0.6667 - dense_1_acc_4: 0.8667 - dense_1_acc_5: 0.9667 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 48/100 60/60 [==============================] - 0s - loss: 13.5650 - dense_1_loss_1: 3.9380 - dense_1_loss_2: 2.3213 - dense_1_loss_3: 1.1828 - dense_1_loss_4: 0.6479 - dense_1_loss_5: 0.4232 - dense_1_loss_6: 0.3065 - dense_1_loss_7: 0.3102 - dense_1_loss_8: 0.2231 - dense_1_loss_9: 0.2256 - dense_1_loss_10: 0.1906 - dense_1_loss_11: 0.2100 - dense_1_loss_12: 0.1964 - dense_1_loss_13: 0.1716 - dense_1_loss_14: 0.1827 - dense_1_loss_15: 0.1923 - dense_1_loss_16: 0.1898 - dense_1_loss_17: 0.1986 - dense_1_loss_18: 0.1855 - dense_1_loss_19: 0.2080 - dense_1_loss_20: 0.2070 - dense_1_loss_21: 0.1993 - dense_1_loss_22: 0.1943 - dense_1_loss_23: 0.2010 - dense_1_loss_24: 0.1894 - dense_1_loss_25: 0.2308 - dense_1_loss_26: 0.1998 - dense_1_loss_27: 0.2002 - dense_1_loss_28: 0.2135 - dense_1_loss_29: 0.2255 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4667 - dense_1_acc_3: 0.6833 - dense_1_acc_4: 0.8667 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 49/100 60/60 [==============================] - 0s - loss: 13.0597 - dense_1_loss_1: 3.9339 - dense_1_loss_2: 2.2832 - dense_1_loss_3: 1.1444 - dense_1_loss_4: 0.6110 - dense_1_loss_5: 0.3977 - dense_1_loss_6: 0.2916 - dense_1_loss_7: 0.2918 - dense_1_loss_8: 0.2103 - dense_1_loss_9: 0.2145 - dense_1_loss_10: 0.1770 - dense_1_loss_11: 0.1955 - dense_1_loss_12: 0.1818 - dense_1_loss_13: 0.1640 - dense_1_loss_14: 0.1702 - dense_1_loss_15: 0.1761 - dense_1_loss_16: 0.1776 - dense_1_loss_17: 0.1823 - dense_1_loss_18: 0.1722 - dense_1_loss_19: 0.1921 - dense_1_loss_20: 0.1922 - dense_1_loss_21: 0.1825 - dense_1_loss_22: 0.1838 - dense_1_loss_23: 0.1810 - dense_1_loss_24: 0.1717 - dense_1_loss_25: 0.2133 - dense_1_loss_26: 0.1796 - dense_1_loss_27: 0.1877 - dense_1_loss_28: 0.1932 - dense_1_loss_29: 0.2074 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4667 - dense_1_acc_3: 0.7167 - dense_1_acc_4: 0.8667 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 50/100 60/60 [==============================] - 0s - loss: 12.6026 - dense_1_loss_1: 3.9298 - dense_1_loss_2: 2.2459 - dense_1_loss_3: 1.1076 - dense_1_loss_4: 0.5701 - dense_1_loss_5: 0.3745 - dense_1_loss_6: 0.2743 - dense_1_loss_7: 0.2741 - dense_1_loss_8: 0.1958 - dense_1_loss_9: 0.2036 - dense_1_loss_10: 0.1632 - dense_1_loss_11: 0.1818 - dense_1_loss_12: 0.1658 - dense_1_loss_13: 0.1562 - dense_1_loss_14: 0.1593 - dense_1_loss_15: 0.1637 - dense_1_loss_16: 0.1626 - dense_1_loss_17: 0.1675 - dense_1_loss_18: 0.1637 - dense_1_loss_19: 0.1737 - dense_1_loss_20: 0.1801 - dense_1_loss_21: 0.1730 - dense_1_loss_22: 0.1703 - dense_1_loss_23: 0.1721 - dense_1_loss_24: 0.1558 - dense_1_loss_25: 0.1999 - dense_1_loss_26: 0.1673 - dense_1_loss_27: 0.1757 - dense_1_loss_28: 0.1815 - dense_1_loss_29: 0.1938 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4667 - dense_1_acc_3: 0.7167 - dense_1_acc_4: 0.9333 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 51/100 60/60 [==============================] - 0s - loss: 12.1969 - dense_1_loss_1: 3.9255 - dense_1_loss_2: 2.2080 - dense_1_loss_3: 1.0726 - dense_1_loss_4: 0.5390 - dense_1_loss_5: 0.3548 - dense_1_loss_6: 0.2577 - dense_1_loss_7: 0.2564 - dense_1_loss_8: 0.1868 - dense_1_loss_9: 0.1882 - dense_1_loss_10: 0.1529 - dense_1_loss_11: 0.1652 - dense_1_loss_12: 0.1568 - dense_1_loss_13: 0.1446 - dense_1_loss_14: 0.1448 - dense_1_loss_15: 0.1528 - dense_1_loss_16: 0.1549 - dense_1_loss_17: 0.1573 - dense_1_loss_18: 0.1558 - dense_1_loss_19: 0.1594 - dense_1_loss_20: 0.1655 - dense_1_loss_21: 0.1656 - dense_1_loss_22: 0.1577 - dense_1_loss_23: 0.1639 - dense_1_loss_24: 0.1475 - dense_1_loss_25: 0.1874 - dense_1_loss_26: 0.1574 - dense_1_loss_27: 0.1644 - dense_1_loss_28: 0.1717 - dense_1_loss_29: 0.1822 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4667 - dense_1_acc_3: 0.7333 - dense_1_acc_4: 0.9333 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 52/100 60/60 [==============================] - 0s - loss: 11.7999 - dense_1_loss_1: 3.9217 - dense_1_loss_2: 2.1720 - dense_1_loss_3: 1.0392 - dense_1_loss_4: 0.5107 - dense_1_loss_5: 0.3348 - dense_1_loss_6: 0.2429 - dense_1_loss_7: 0.2425 - dense_1_loss_8: 0.1760 - dense_1_loss_9: 0.1731 - dense_1_loss_10: 0.1442 - dense_1_loss_11: 0.1543 - dense_1_loss_12: 0.1459 - dense_1_loss_13: 0.1337 - dense_1_loss_14: 0.1360 - dense_1_loss_15: 0.1401 - dense_1_loss_16: 0.1463 - dense_1_loss_17: 0.1466 - dense_1_loss_18: 0.1444 - dense_1_loss_19: 0.1508 - dense_1_loss_20: 0.1517 - dense_1_loss_21: 0.1525 - dense_1_loss_22: 0.1483 - dense_1_loss_23: 0.1505 - dense_1_loss_24: 0.1387 - dense_1_loss_25: 0.1725 - dense_1_loss_26: 0.1485 - dense_1_loss_27: 0.1516 - dense_1_loss_28: 0.1598 - dense_1_loss_29: 0.1704 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4667 - dense_1_acc_3: 0.7333 - dense_1_acc_4: 0.9333 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 53/100 60/60 [==============================] - 0s - loss: 11.4567 - dense_1_loss_1: 3.9172 - dense_1_loss_2: 2.1361 - dense_1_loss_3: 1.0079 - dense_1_loss_4: 0.4832 - dense_1_loss_5: 0.3156 - dense_1_loss_6: 0.2311 - dense_1_loss_7: 0.2329 - dense_1_loss_8: 0.1637 - dense_1_loss_9: 0.1621 - dense_1_loss_10: 0.1354 - dense_1_loss_11: 0.1477 - dense_1_loss_12: 0.1379 - dense_1_loss_13: 0.1248 - dense_1_loss_14: 0.1295 - dense_1_loss_15: 0.1316 - dense_1_loss_16: 0.1364 - dense_1_loss_17: 0.1378 - dense_1_loss_18: 0.1345 - dense_1_loss_19: 0.1434 - dense_1_loss_20: 0.1433 - dense_1_loss_21: 0.1424 - dense_1_loss_22: 0.1385 - dense_1_loss_23: 0.1395 - dense_1_loss_24: 0.1307 - dense_1_loss_25: 0.1613 - dense_1_loss_26: 0.1397 - dense_1_loss_27: 0.1392 - dense_1_loss_28: 0.1497 - dense_1_loss_29: 0.1637 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4500 - dense_1_acc_3: 0.7833 - dense_1_acc_4: 0.9333 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 54/100 60/60 [==============================] - 0s - loss: 11.1297 - dense_1_loss_1: 3.9131 - dense_1_loss_2: 2.1021 - dense_1_loss_3: 0.9777 - dense_1_loss_4: 0.4565 - dense_1_loss_5: 0.2980 - dense_1_loss_6: 0.2192 - dense_1_loss_7: 0.2220 - dense_1_loss_8: 0.1525 - dense_1_loss_9: 0.1522 - dense_1_loss_10: 0.1263 - dense_1_loss_11: 0.1413 - dense_1_loss_12: 0.1293 - dense_1_loss_13: 0.1164 - dense_1_loss_14: 0.1224 - dense_1_loss_15: 0.1257 - dense_1_loss_16: 0.1262 - dense_1_loss_17: 0.1297 - dense_1_loss_18: 0.1259 - dense_1_loss_19: 0.1343 - dense_1_loss_20: 0.1374 - dense_1_loss_21: 0.1325 - dense_1_loss_22: 0.1280 - dense_1_loss_23: 0.1302 - dense_1_loss_24: 0.1229 - dense_1_loss_25: 0.1515 - dense_1_loss_26: 0.1306 - dense_1_loss_27: 0.1295 - dense_1_loss_28: 0.1410 - dense_1_loss_29: 0.1554 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4833 - dense_1_acc_3: 0.7833 - dense_1_acc_4: 0.9333 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 55/100 60/60 [==============================] - 0s - loss: 10.8305 - dense_1_loss_1: 3.9091 - dense_1_loss_2: 2.0676 - dense_1_loss_3: 0.9481 - dense_1_loss_4: 0.4333 - dense_1_loss_5: 0.2845 - dense_1_loss_6: 0.2080 - dense_1_loss_7: 0.2110 - dense_1_loss_8: 0.1461 - dense_1_loss_9: 0.1429 - dense_1_loss_10: 0.1194 - dense_1_loss_11: 0.1314 - dense_1_loss_12: 0.1221 - dense_1_loss_13: 0.1087 - dense_1_loss_14: 0.1134 - dense_1_loss_15: 0.1198 - dense_1_loss_16: 0.1214 - dense_1_loss_17: 0.1213 - dense_1_loss_18: 0.1185 - dense_1_loss_19: 0.1254 - dense_1_loss_20: 0.1298 - dense_1_loss_21: 0.1259 - dense_1_loss_22: 0.1191 - dense_1_loss_23: 0.1219 - dense_1_loss_24: 0.1151 - dense_1_loss_25: 0.1430 - dense_1_loss_26: 0.1220 - dense_1_loss_27: 0.1234 - dense_1_loss_28: 0.1338 - dense_1_loss_29: 0.1443 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4833 - dense_1_acc_3: 0.8000 - dense_1_acc_4: 0.9333 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 56/100 60/60 [==============================] - 0s - loss: 10.5476 - dense_1_loss_1: 3.9054 - dense_1_loss_2: 2.0361 - dense_1_loss_3: 0.9192 - dense_1_loss_4: 0.4097 - dense_1_loss_5: 0.2699 - dense_1_loss_6: 0.1966 - dense_1_loss_7: 0.1976 - dense_1_loss_8: 0.1380 - dense_1_loss_9: 0.1355 - dense_1_loss_10: 0.1130 - dense_1_loss_11: 0.1203 - dense_1_loss_12: 0.1168 - dense_1_loss_13: 0.1029 - dense_1_loss_14: 0.1052 - dense_1_loss_15: 0.1129 - dense_1_loss_16: 0.1181 - dense_1_loss_17: 0.1149 - dense_1_loss_18: 0.1110 - dense_1_loss_19: 0.1188 - dense_1_loss_20: 0.1210 - dense_1_loss_21: 0.1195 - dense_1_loss_22: 0.1123 - dense_1_loss_23: 0.1150 - dense_1_loss_24: 0.1095 - dense_1_loss_25: 0.1334 - dense_1_loss_26: 0.1156 - dense_1_loss_27: 0.1183 - dense_1_loss_28: 0.1270 - dense_1_loss_29: 0.1341 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4833 - dense_1_acc_3: 0.8333 - dense_1_acc_4: 0.9333 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9833 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 57/100 60/60 [==============================] - 0s - loss: 10.2853 - dense_1_loss_1: 3.9014 - dense_1_loss_2: 2.0034 - dense_1_loss_3: 0.8925 - dense_1_loss_4: 0.3899 - dense_1_loss_5: 0.2565 - dense_1_loss_6: 0.1892 - dense_1_loss_7: 0.1875 - dense_1_loss_8: 0.1288 - dense_1_loss_9: 0.1290 - dense_1_loss_10: 0.1055 - dense_1_loss_11: 0.1154 - dense_1_loss_12: 0.1096 - dense_1_loss_13: 0.0982 - dense_1_loss_14: 0.1008 - dense_1_loss_15: 0.1044 - dense_1_loss_16: 0.1075 - dense_1_loss_17: 0.1089 - dense_1_loss_18: 0.1047 - dense_1_loss_19: 0.1123 - dense_1_loss_20: 0.1138 - dense_1_loss_21: 0.1113 - dense_1_loss_22: 0.1068 - dense_1_loss_23: 0.1086 - dense_1_loss_24: 0.1038 - dense_1_loss_25: 0.1242 - dense_1_loss_26: 0.1097 - dense_1_loss_27: 0.1119 - dense_1_loss_28: 0.1217 - dense_1_loss_29: 0.1279 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4833 - dense_1_acc_3: 0.8667 - dense_1_acc_4: 0.9333 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 58/100 60/60 [==============================] - 0s - loss: 10.0500 - dense_1_loss_1: 3.8974 - dense_1_loss_2: 1.9716 - dense_1_loss_3: 0.8691 - dense_1_loss_4: 0.3699 - dense_1_loss_5: 0.2441 - dense_1_loss_6: 0.1818 - dense_1_loss_7: 0.1776 - dense_1_loss_8: 0.1225 - dense_1_loss_9: 0.1224 - dense_1_loss_10: 0.0987 - dense_1_loss_11: 0.1126 - dense_1_loss_12: 0.1019 - dense_1_loss_13: 0.0948 - dense_1_loss_14: 0.0996 - dense_1_loss_15: 0.0984 - dense_1_loss_16: 0.0987 - dense_1_loss_17: 0.1031 - dense_1_loss_18: 0.0995 - dense_1_loss_19: 0.1077 - dense_1_loss_20: 0.1079 - dense_1_loss_21: 0.1046 - dense_1_loss_22: 0.1028 - dense_1_loss_23: 0.1023 - dense_1_loss_24: 0.0975 - dense_1_loss_25: 0.1167 - dense_1_loss_26: 0.1035 - dense_1_loss_27: 0.1052 - dense_1_loss_28: 0.1159 - dense_1_loss_29: 0.1220 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4833 - dense_1_acc_3: 0.8667 - dense_1_acc_4: 0.9500 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9667 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 59/100 60/60 [==============================] - 0s - loss: 9.8203 - dense_1_loss_1: 3.8939 - dense_1_loss_2: 1.9422 - dense_1_loss_3: 0.8429 - dense_1_loss_4: 0.3533 - dense_1_loss_5: 0.2323 - dense_1_loss_6: 0.1734 - dense_1_loss_7: 0.1676 - dense_1_loss_8: 0.1176 - dense_1_loss_9: 0.1153 - dense_1_loss_10: 0.0942 - dense_1_loss_11: 0.1060 - dense_1_loss_12: 0.0960 - dense_1_loss_13: 0.0904 - dense_1_loss_14: 0.0930 - dense_1_loss_15: 0.0923 - dense_1_loss_16: 0.0952 - dense_1_loss_17: 0.0974 - dense_1_loss_18: 0.0942 - dense_1_loss_19: 0.1022 - dense_1_loss_20: 0.1018 - dense_1_loss_21: 0.0990 - dense_1_loss_22: 0.0968 - dense_1_loss_23: 0.0966 - dense_1_loss_24: 0.0918 - dense_1_loss_25: 0.1124 - dense_1_loss_26: 0.0959 - dense_1_loss_27: 0.1002 - dense_1_loss_28: 0.1087 - dense_1_loss_29: 0.1179 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4833 - dense_1_acc_3: 0.8667 - dense_1_acc_4: 0.9667 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 0.9833 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 60/100 60/60 [==============================] - 0s - loss: 9.6102 - dense_1_loss_1: 3.8901 - dense_1_loss_2: 1.9130 - dense_1_loss_3: 0.8196 - dense_1_loss_4: 0.3378 - dense_1_loss_5: 0.2224 - dense_1_loss_6: 0.1642 - dense_1_loss_7: 0.1581 - dense_1_loss_8: 0.1135 - dense_1_loss_9: 0.1082 - dense_1_loss_10: 0.0912 - dense_1_loss_11: 0.0977 - dense_1_loss_12: 0.0921 - dense_1_loss_13: 0.0849 - dense_1_loss_14: 0.0853 - dense_1_loss_15: 0.0877 - dense_1_loss_16: 0.0951 - dense_1_loss_17: 0.0930 - dense_1_loss_18: 0.0896 - dense_1_loss_19: 0.0958 - dense_1_loss_20: 0.0967 - dense_1_loss_21: 0.0938 - dense_1_loss_22: 0.0912 - dense_1_loss_23: 0.0920 - dense_1_loss_24: 0.0874 - dense_1_loss_25: 0.1076 - dense_1_loss_26: 0.0912 - dense_1_loss_27: 0.0961 - dense_1_loss_28: 0.1043 - dense_1_loss_29: 0.1108 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4833 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9667 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 61/100 60/60 [==============================] - 0s - loss: 9.4133 - dense_1_loss_1: 3.8863 - dense_1_loss_2: 1.8849 - dense_1_loss_3: 0.7978 - dense_1_loss_4: 0.3220 - dense_1_loss_5: 0.2128 - dense_1_loss_6: 0.1572 - dense_1_loss_7: 0.1521 - dense_1_loss_8: 0.1070 - dense_1_loss_9: 0.1030 - dense_1_loss_10: 0.0857 - dense_1_loss_11: 0.0937 - dense_1_loss_12: 0.0873 - dense_1_loss_13: 0.0807 - dense_1_loss_14: 0.0805 - dense_1_loss_15: 0.0843 - dense_1_loss_16: 0.0901 - dense_1_loss_17: 0.0880 - dense_1_loss_18: 0.0856 - dense_1_loss_19: 0.0903 - dense_1_loss_20: 0.0918 - dense_1_loss_21: 0.0891 - dense_1_loss_22: 0.0861 - dense_1_loss_23: 0.0880 - dense_1_loss_24: 0.0832 - dense_1_loss_25: 0.1017 - dense_1_loss_26: 0.0877 - dense_1_loss_27: 0.0909 - dense_1_loss_28: 0.1005 - dense_1_loss_29: 0.1050 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.4833 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9667 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 62/100 60/60 [==============================] - 0s - loss: 9.2328 - dense_1_loss_1: 3.8825 - dense_1_loss_2: 1.8573 - dense_1_loss_3: 0.7768 - dense_1_loss_4: 0.3090 - dense_1_loss_5: 0.2032 - dense_1_loss_6: 0.1514 - dense_1_loss_7: 0.1455 - dense_1_loss_8: 0.1013 - dense_1_loss_9: 0.0986 - dense_1_loss_10: 0.0806 - dense_1_loss_11: 0.0914 - dense_1_loss_12: 0.0821 - dense_1_loss_13: 0.0775 - dense_1_loss_14: 0.0791 - dense_1_loss_15: 0.0815 - dense_1_loss_16: 0.0832 - dense_1_loss_17: 0.0835 - dense_1_loss_18: 0.0818 - dense_1_loss_19: 0.0859 - dense_1_loss_20: 0.0878 - dense_1_loss_21: 0.0853 - dense_1_loss_22: 0.0816 - dense_1_loss_23: 0.0841 - dense_1_loss_24: 0.0788 - dense_1_loss_25: 0.0971 - dense_1_loss_26: 0.0838 - dense_1_loss_27: 0.0861 - dense_1_loss_28: 0.0956 - dense_1_loss_29: 0.1004 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.5333 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 63/100 60/60 [==============================] - 0s - loss: 9.0548 - dense_1_loss_1: 3.8792 - dense_1_loss_2: 1.8308 - dense_1_loss_3: 0.7560 - dense_1_loss_4: 0.2950 - dense_1_loss_5: 0.1932 - dense_1_loss_6: 0.1463 - dense_1_loss_7: 0.1384 - dense_1_loss_8: 0.0973 - dense_1_loss_9: 0.0945 - dense_1_loss_10: 0.0765 - dense_1_loss_11: 0.0868 - dense_1_loss_12: 0.0783 - dense_1_loss_13: 0.0737 - dense_1_loss_14: 0.0760 - dense_1_loss_15: 0.0766 - dense_1_loss_16: 0.0795 - dense_1_loss_17: 0.0796 - dense_1_loss_18: 0.0771 - dense_1_loss_19: 0.0826 - dense_1_loss_20: 0.0834 - dense_1_loss_21: 0.0810 - dense_1_loss_22: 0.0785 - dense_1_loss_23: 0.0788 - dense_1_loss_24: 0.0750 - dense_1_loss_25: 0.0926 - dense_1_loss_26: 0.0799 - dense_1_loss_27: 0.0815 - dense_1_loss_28: 0.0906 - dense_1_loss_29: 0.0961 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.5333 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 0.9833 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 64/100 60/60 [==============================] - 0s - loss: 8.8914 - dense_1_loss_1: 3.8755 - dense_1_loss_2: 1.8055 - dense_1_loss_3: 0.7353 - dense_1_loss_4: 0.2827 - dense_1_loss_5: 0.1841 - dense_1_loss_6: 0.1409 - dense_1_loss_7: 0.1301 - dense_1_loss_8: 0.0936 - dense_1_loss_9: 0.0905 - dense_1_loss_10: 0.0738 - dense_1_loss_11: 0.0813 - dense_1_loss_12: 0.0759 - dense_1_loss_13: 0.0702 - dense_1_loss_14: 0.0716 - dense_1_loss_15: 0.0723 - dense_1_loss_16: 0.0781 - dense_1_loss_17: 0.0761 - dense_1_loss_18: 0.0736 - dense_1_loss_19: 0.0786 - dense_1_loss_20: 0.0793 - dense_1_loss_21: 0.0781 - dense_1_loss_22: 0.0746 - dense_1_loss_23: 0.0752 - dense_1_loss_24: 0.0720 - dense_1_loss_25: 0.0887 - dense_1_loss_26: 0.0761 - dense_1_loss_27: 0.0778 - dense_1_loss_28: 0.0867 - dense_1_loss_29: 0.0931 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.5500 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 65/100 60/60 [==============================] - 0s - loss: 8.7391 - dense_1_loss_1: 3.8723 - dense_1_loss_2: 1.7804 - dense_1_loss_3: 0.7166 - dense_1_loss_4: 0.2720 - dense_1_loss_5: 0.1767 - dense_1_loss_6: 0.1358 - dense_1_loss_7: 0.1244 - dense_1_loss_8: 0.0892 - dense_1_loss_9: 0.0861 - dense_1_loss_10: 0.0708 - dense_1_loss_11: 0.0780 - dense_1_loss_12: 0.0725 - dense_1_loss_13: 0.0674 - dense_1_loss_14: 0.0686 - dense_1_loss_15: 0.0695 - dense_1_loss_16: 0.0742 - dense_1_loss_17: 0.0727 - dense_1_loss_18: 0.0706 - dense_1_loss_19: 0.0747 - dense_1_loss_20: 0.0758 - dense_1_loss_21: 0.0747 - dense_1_loss_22: 0.0707 - dense_1_loss_23: 0.0721 - dense_1_loss_24: 0.0693 - dense_1_loss_25: 0.0842 - dense_1_loss_26: 0.0727 - dense_1_loss_27: 0.0748 - dense_1_loss_28: 0.0830 - dense_1_loss_29: 0.0892 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.5500 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 66/100 60/60 [==============================] - 0s - loss: 8.5942 - dense_1_loss_1: 3.8689 - dense_1_loss_2: 1.7559 - dense_1_loss_3: 0.6982 - dense_1_loss_4: 0.2609 - dense_1_loss_5: 0.1701 - dense_1_loss_6: 0.1306 - dense_1_loss_7: 0.1191 - dense_1_loss_8: 0.0851 - dense_1_loss_9: 0.0818 - dense_1_loss_10: 0.0680 - dense_1_loss_11: 0.0746 - dense_1_loss_12: 0.0693 - dense_1_loss_13: 0.0646 - dense_1_loss_14: 0.0660 - dense_1_loss_15: 0.0671 - dense_1_loss_16: 0.0708 - dense_1_loss_17: 0.0694 - dense_1_loss_18: 0.0676 - dense_1_loss_19: 0.0714 - dense_1_loss_20: 0.0726 - dense_1_loss_21: 0.0712 - dense_1_loss_22: 0.0675 - dense_1_loss_23: 0.0694 - dense_1_loss_24: 0.0669 - dense_1_loss_25: 0.0796 - dense_1_loss_26: 0.0696 - dense_1_loss_27: 0.0718 - dense_1_loss_28: 0.0809 - dense_1_loss_29: 0.0853 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.5667 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 67/100 60/60 [==============================] - 0s - loss: 8.4597 - dense_1_loss_1: 3.8653 - dense_1_loss_2: 1.7322 - dense_1_loss_3: 0.6813 - dense_1_loss_4: 0.2504 - dense_1_loss_5: 0.1646 - dense_1_loss_6: 0.1260 - dense_1_loss_7: 0.1150 - dense_1_loss_8: 0.0816 - dense_1_loss_9: 0.0788 - dense_1_loss_10: 0.0654 - dense_1_loss_11: 0.0715 - dense_1_loss_12: 0.0665 - dense_1_loss_13: 0.0620 - dense_1_loss_14: 0.0631 - dense_1_loss_15: 0.0643 - dense_1_loss_16: 0.0678 - dense_1_loss_17: 0.0667 - dense_1_loss_18: 0.0647 - dense_1_loss_19: 0.0686 - dense_1_loss_20: 0.0697 - dense_1_loss_21: 0.0677 - dense_1_loss_22: 0.0649 - dense_1_loss_23: 0.0665 - dense_1_loss_24: 0.0640 - dense_1_loss_25: 0.0765 - dense_1_loss_26: 0.0670 - dense_1_loss_27: 0.0691 - dense_1_loss_28: 0.0773 - dense_1_loss_29: 0.0814 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.5833 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 0.9833 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 68/100 60/60 [==============================] - 0s - loss: 8.3313 - dense_1_loss_1: 3.8621 - dense_1_loss_2: 1.7093 - dense_1_loss_3: 0.6644 - dense_1_loss_4: 0.2410 - dense_1_loss_5: 0.1584 - dense_1_loss_6: 0.1217 - dense_1_loss_7: 0.1098 - dense_1_loss_8: 0.0788 - dense_1_loss_9: 0.0759 - dense_1_loss_10: 0.0629 - dense_1_loss_11: 0.0685 - dense_1_loss_12: 0.0640 - dense_1_loss_13: 0.0594 - dense_1_loss_14: 0.0603 - dense_1_loss_15: 0.0613 - dense_1_loss_16: 0.0656 - dense_1_loss_17: 0.0639 - dense_1_loss_18: 0.0619 - dense_1_loss_19: 0.0660 - dense_1_loss_20: 0.0670 - dense_1_loss_21: 0.0648 - dense_1_loss_22: 0.0628 - dense_1_loss_23: 0.0633 - dense_1_loss_24: 0.0610 - dense_1_loss_25: 0.0739 - dense_1_loss_26: 0.0641 - dense_1_loss_27: 0.0664 - dense_1_loss_28: 0.0742 - dense_1_loss_29: 0.0783 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 69/100 60/60 [==============================] - 0s - loss: 8.2080 - dense_1_loss_1: 3.8588 - dense_1_loss_2: 1.6867 - dense_1_loss_3: 0.6497 - dense_1_loss_4: 0.2312 - dense_1_loss_5: 0.1523 - dense_1_loss_6: 0.1170 - dense_1_loss_7: 0.1045 - dense_1_loss_8: 0.0760 - dense_1_loss_9: 0.0730 - dense_1_loss_10: 0.0605 - dense_1_loss_11: 0.0660 - dense_1_loss_12: 0.0614 - dense_1_loss_13: 0.0570 - dense_1_loss_14: 0.0581 - dense_1_loss_15: 0.0590 - dense_1_loss_16: 0.0633 - dense_1_loss_17: 0.0611 - dense_1_loss_18: 0.0595 - dense_1_loss_19: 0.0633 - dense_1_loss_20: 0.0644 - dense_1_loss_21: 0.0625 - dense_1_loss_22: 0.0602 - dense_1_loss_23: 0.0608 - dense_1_loss_24: 0.0586 - dense_1_loss_25: 0.0714 - dense_1_loss_26: 0.0612 - dense_1_loss_27: 0.0638 - dense_1_loss_28: 0.0713 - dense_1_loss_29: 0.0755 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 70/100 60/60 [==============================] - 0s - loss: 8.0932 - dense_1_loss_1: 3.8554 - dense_1_loss_2: 1.6644 - dense_1_loss_3: 0.6344 - dense_1_loss_4: 0.2231 - dense_1_loss_5: 0.1467 - dense_1_loss_6: 0.1132 - dense_1_loss_7: 0.1002 - dense_1_loss_8: 0.0731 - dense_1_loss_9: 0.0704 - dense_1_loss_10: 0.0579 - dense_1_loss_11: 0.0641 - dense_1_loss_12: 0.0590 - dense_1_loss_13: 0.0547 - dense_1_loss_14: 0.0564 - dense_1_loss_15: 0.0570 - dense_1_loss_16: 0.0605 - dense_1_loss_17: 0.0586 - dense_1_loss_18: 0.0574 - dense_1_loss_19: 0.0612 - dense_1_loss_20: 0.0617 - dense_1_loss_21: 0.0602 - dense_1_loss_22: 0.0581 - dense_1_loss_23: 0.0583 - dense_1_loss_24: 0.0565 - dense_1_loss_25: 0.0686 - dense_1_loss_26: 0.0590 - dense_1_loss_27: 0.0614 - dense_1_loss_28: 0.0689 - dense_1_loss_29: 0.0725 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 71/100 60/60 [==============================] - 0s - loss: 7.9833 - dense_1_loss_1: 3.8521 - dense_1_loss_2: 1.6440 - dense_1_loss_3: 0.6199 - dense_1_loss_4: 0.2144 - dense_1_loss_5: 0.1410 - dense_1_loss_6: 0.1097 - dense_1_loss_7: 0.0962 - dense_1_loss_8: 0.0707 - dense_1_loss_9: 0.0678 - dense_1_loss_10: 0.0558 - dense_1_loss_11: 0.0620 - dense_1_loss_12: 0.0569 - dense_1_loss_13: 0.0526 - dense_1_loss_14: 0.0543 - dense_1_loss_15: 0.0549 - dense_1_loss_16: 0.0583 - dense_1_loss_17: 0.0564 - dense_1_loss_18: 0.0554 - dense_1_loss_19: 0.0587 - dense_1_loss_20: 0.0594 - dense_1_loss_21: 0.0580 - dense_1_loss_22: 0.0557 - dense_1_loss_23: 0.0561 - dense_1_loss_24: 0.0546 - dense_1_loss_25: 0.0658 - dense_1_loss_26: 0.0568 - dense_1_loss_27: 0.0590 - dense_1_loss_28: 0.0666 - dense_1_loss_29: 0.0701 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 72/100 60/60 [==============================] - 0s - loss: 7.8793 - dense_1_loss_1: 3.8491 - dense_1_loss_2: 1.6233 - dense_1_loss_3: 0.6055 - dense_1_loss_4: 0.2075 - dense_1_loss_5: 0.1360 - dense_1_loss_6: 0.1063 - dense_1_loss_7: 0.0922 - dense_1_loss_8: 0.0684 - dense_1_loss_9: 0.0655 - dense_1_loss_10: 0.0538 - dense_1_loss_11: 0.0597 - dense_1_loss_12: 0.0548 - dense_1_loss_13: 0.0509 - dense_1_loss_14: 0.0519 - dense_1_loss_15: 0.0528 - dense_1_loss_16: 0.0567 - dense_1_loss_17: 0.0545 - dense_1_loss_18: 0.0533 - dense_1_loss_19: 0.0564 - dense_1_loss_20: 0.0573 - dense_1_loss_21: 0.0558 - dense_1_loss_22: 0.0536 - dense_1_loss_23: 0.0542 - dense_1_loss_24: 0.0528 - dense_1_loss_25: 0.0632 - dense_1_loss_26: 0.0548 - dense_1_loss_27: 0.0568 - dense_1_loss_28: 0.0645 - dense_1_loss_29: 0.0678 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.8833 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 73/100 60/60 [==============================] - 0s - loss: 7.7824 - dense_1_loss_1: 3.8456 - dense_1_loss_2: 1.6041 - dense_1_loss_3: 0.5923 - dense_1_loss_4: 0.2004 - dense_1_loss_5: 0.1317 - dense_1_loss_6: 0.1029 - dense_1_loss_7: 0.0889 - dense_1_loss_8: 0.0664 - dense_1_loss_9: 0.0632 - dense_1_loss_10: 0.0521 - dense_1_loss_11: 0.0574 - dense_1_loss_12: 0.0529 - dense_1_loss_13: 0.0493 - dense_1_loss_14: 0.0501 - dense_1_loss_15: 0.0510 - dense_1_loss_16: 0.0550 - dense_1_loss_17: 0.0527 - dense_1_loss_18: 0.0514 - dense_1_loss_19: 0.0545 - dense_1_loss_20: 0.0553 - dense_1_loss_21: 0.0539 - dense_1_loss_22: 0.0516 - dense_1_loss_23: 0.0523 - dense_1_loss_24: 0.0509 - dense_1_loss_25: 0.0612 - dense_1_loss_26: 0.0528 - dense_1_loss_27: 0.0548 - dense_1_loss_28: 0.0620 - dense_1_loss_29: 0.0656 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 74/100 60/60 [==============================] - 0s - loss: 7.6862 - dense_1_loss_1: 3.8425 - dense_1_loss_2: 1.5849 - dense_1_loss_3: 0.5793 - dense_1_loss_4: 0.1927 - dense_1_loss_5: 0.1273 - dense_1_loss_6: 0.0993 - dense_1_loss_7: 0.0849 - dense_1_loss_8: 0.0644 - dense_1_loss_9: 0.0610 - dense_1_loss_10: 0.0502 - dense_1_loss_11: 0.0555 - dense_1_loss_12: 0.0509 - dense_1_loss_13: 0.0478 - dense_1_loss_14: 0.0484 - dense_1_loss_15: 0.0494 - dense_1_loss_16: 0.0531 - dense_1_loss_17: 0.0509 - dense_1_loss_18: 0.0496 - dense_1_loss_19: 0.0527 - dense_1_loss_20: 0.0534 - dense_1_loss_21: 0.0521 - dense_1_loss_22: 0.0499 - dense_1_loss_23: 0.0506 - dense_1_loss_24: 0.0491 - dense_1_loss_25: 0.0593 - dense_1_loss_26: 0.0512 - dense_1_loss_27: 0.0528 - dense_1_loss_28: 0.0598 - dense_1_loss_29: 0.0633 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 75/100 60/60 [==============================] - 0s - loss: 7.5985 - dense_1_loss_1: 3.8396 - dense_1_loss_2: 1.5661 - dense_1_loss_3: 0.5669 - dense_1_loss_4: 0.1867 - dense_1_loss_5: 0.1231 - dense_1_loss_6: 0.0963 - dense_1_loss_7: 0.0820 - dense_1_loss_8: 0.0623 - dense_1_loss_9: 0.0592 - dense_1_loss_10: 0.0486 - dense_1_loss_11: 0.0536 - dense_1_loss_12: 0.0492 - dense_1_loss_13: 0.0461 - dense_1_loss_14: 0.0469 - dense_1_loss_15: 0.0479 - dense_1_loss_16: 0.0513 - dense_1_loss_17: 0.0492 - dense_1_loss_18: 0.0480 - dense_1_loss_19: 0.0510 - dense_1_loss_20: 0.0517 - dense_1_loss_21: 0.0506 - dense_1_loss_22: 0.0483 - dense_1_loss_23: 0.0489 - dense_1_loss_24: 0.0476 - dense_1_loss_25: 0.0574 - dense_1_loss_26: 0.0496 - dense_1_loss_27: 0.0511 - dense_1_loss_28: 0.0580 - dense_1_loss_29: 0.0615 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 76/100 60/60 [==============================] - 0s - loss: 7.5125 - dense_1_loss_1: 3.8364 - dense_1_loss_2: 1.5483 - dense_1_loss_3: 0.5546 - dense_1_loss_4: 0.1802 - dense_1_loss_5: 0.1192 - dense_1_loss_6: 0.0934 - dense_1_loss_7: 0.0791 - dense_1_loss_8: 0.0603 - dense_1_loss_9: 0.0576 - dense_1_loss_10: 0.0469 - dense_1_loss_11: 0.0519 - dense_1_loss_12: 0.0477 - dense_1_loss_13: 0.0446 - dense_1_loss_14: 0.0454 - dense_1_loss_15: 0.0463 - dense_1_loss_16: 0.0498 - dense_1_loss_17: 0.0475 - dense_1_loss_18: 0.0464 - dense_1_loss_19: 0.0493 - dense_1_loss_20: 0.0500 - dense_1_loss_21: 0.0490 - dense_1_loss_22: 0.0467 - dense_1_loss_23: 0.0472 - dense_1_loss_24: 0.0460 - dense_1_loss_25: 0.0555 - dense_1_loss_26: 0.0479 - dense_1_loss_27: 0.0494 - dense_1_loss_28: 0.0563 - dense_1_loss_29: 0.0593 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 77/100 60/60 [==============================] - 0s - loss: 7.4341 - dense_1_loss_1: 3.8334 - dense_1_loss_2: 1.5311 - dense_1_loss_3: 0.5435 - dense_1_loss_4: 0.1750 - dense_1_loss_5: 0.1158 - dense_1_loss_6: 0.0910 - dense_1_loss_7: 0.0767 - dense_1_loss_8: 0.0585 - dense_1_loss_9: 0.0560 - dense_1_loss_10: 0.0454 - dense_1_loss_11: 0.0504 - dense_1_loss_12: 0.0463 - dense_1_loss_13: 0.0432 - dense_1_loss_14: 0.0440 - dense_1_loss_15: 0.0450 - dense_1_loss_16: 0.0483 - dense_1_loss_17: 0.0461 - dense_1_loss_18: 0.0450 - dense_1_loss_19: 0.0478 - dense_1_loss_20: 0.0483 - dense_1_loss_21: 0.0475 - dense_1_loss_22: 0.0451 - dense_1_loss_23: 0.0458 - dense_1_loss_24: 0.0446 - dense_1_loss_25: 0.0535 - dense_1_loss_26: 0.0465 - dense_1_loss_27: 0.0480 - dense_1_loss_28: 0.0547 - dense_1_loss_29: 0.0576 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 78/100 60/60 [==============================] - 0s - loss: 7.3557 - dense_1_loss_1: 3.8301 - dense_1_loss_2: 1.5137 - dense_1_loss_3: 0.5326 - dense_1_loss_4: 0.1695 - dense_1_loss_5: 0.1125 - dense_1_loss_6: 0.0882 - dense_1_loss_7: 0.0739 - dense_1_loss_8: 0.0568 - dense_1_loss_9: 0.0543 - dense_1_loss_10: 0.0441 - dense_1_loss_11: 0.0487 - dense_1_loss_12: 0.0448 - dense_1_loss_13: 0.0419 - dense_1_loss_14: 0.0426 - dense_1_loss_15: 0.0435 - dense_1_loss_16: 0.0469 - dense_1_loss_17: 0.0447 - dense_1_loss_18: 0.0435 - dense_1_loss_19: 0.0463 - dense_1_loss_20: 0.0468 - dense_1_loss_21: 0.0460 - dense_1_loss_22: 0.0437 - dense_1_loss_23: 0.0444 - dense_1_loss_24: 0.0434 - dense_1_loss_25: 0.0519 - dense_1_loss_26: 0.0451 - dense_1_loss_27: 0.0468 - dense_1_loss_28: 0.0532 - dense_1_loss_29: 0.0558 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6000 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 79/100 60/60 [==============================] - 0s - loss: 7.2792 - dense_1_loss_1: 3.8274 - dense_1_loss_2: 1.4976 - dense_1_loss_3: 0.5203 - dense_1_loss_4: 0.1640 - dense_1_loss_5: 0.1092 - dense_1_loss_6: 0.0854 - dense_1_loss_7: 0.0712 - dense_1_loss_8: 0.0553 - dense_1_loss_9: 0.0525 - dense_1_loss_10: 0.0428 - dense_1_loss_11: 0.0473 - dense_1_loss_12: 0.0433 - dense_1_loss_13: 0.0407 - dense_1_loss_14: 0.0412 - dense_1_loss_15: 0.0423 - dense_1_loss_16: 0.0455 - dense_1_loss_17: 0.0433 - dense_1_loss_18: 0.0422 - dense_1_loss_19: 0.0449 - dense_1_loss_20: 0.0454 - dense_1_loss_21: 0.0445 - dense_1_loss_22: 0.0423 - dense_1_loss_23: 0.0432 - dense_1_loss_24: 0.0420 - dense_1_loss_25: 0.0507 - dense_1_loss_26: 0.0437 - dense_1_loss_27: 0.0455 - dense_1_loss_28: 0.0514 - dense_1_loss_29: 0.0542 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 80/100 60/60 [==============================] - 0s - loss: 7.2088 - dense_1_loss_1: 3.8242 - dense_1_loss_2: 1.4811 - dense_1_loss_3: 0.5108 - dense_1_loss_4: 0.1594 - dense_1_loss_5: 0.1063 - dense_1_loss_6: 0.0832 - dense_1_loss_7: 0.0692 - dense_1_loss_8: 0.0537 - dense_1_loss_9: 0.0511 - dense_1_loss_10: 0.0415 - dense_1_loss_11: 0.0459 - dense_1_loss_12: 0.0420 - dense_1_loss_13: 0.0396 - dense_1_loss_14: 0.0399 - dense_1_loss_15: 0.0412 - dense_1_loss_16: 0.0442 - dense_1_loss_17: 0.0419 - dense_1_loss_18: 0.0410 - dense_1_loss_19: 0.0436 - dense_1_loss_20: 0.0441 - dense_1_loss_21: 0.0431 - dense_1_loss_22: 0.0411 - dense_1_loss_23: 0.0420 - dense_1_loss_24: 0.0408 - dense_1_loss_25: 0.0494 - dense_1_loss_26: 0.0423 - dense_1_loss_27: 0.0441 - dense_1_loss_28: 0.0497 - dense_1_loss_29: 0.0524 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 0.9833 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 81/100 60/60 [==============================] - 0s - loss: 7.1396 - dense_1_loss_1: 3.8213 - dense_1_loss_2: 1.4653 - dense_1_loss_3: 0.5001 - dense_1_loss_4: 0.1549 - dense_1_loss_5: 0.1030 - dense_1_loss_6: 0.0809 - dense_1_loss_7: 0.0669 - dense_1_loss_8: 0.0520 - dense_1_loss_9: 0.0497 - dense_1_loss_10: 0.0404 - dense_1_loss_11: 0.0447 - dense_1_loss_12: 0.0409 - dense_1_loss_13: 0.0384 - dense_1_loss_14: 0.0388 - dense_1_loss_15: 0.0400 - dense_1_loss_16: 0.0428 - dense_1_loss_17: 0.0407 - dense_1_loss_18: 0.0398 - dense_1_loss_19: 0.0422 - dense_1_loss_20: 0.0428 - dense_1_loss_21: 0.0420 - dense_1_loss_22: 0.0400 - dense_1_loss_23: 0.0406 - dense_1_loss_24: 0.0398 - dense_1_loss_25: 0.0478 - dense_1_loss_26: 0.0411 - dense_1_loss_27: 0.0429 - dense_1_loss_28: 0.0486 - dense_1_loss_29: 0.0511 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 82/100 60/60 [==============================] - 0s - loss: 7.0758 - dense_1_loss_1: 3.8183 - dense_1_loss_2: 1.4507 - dense_1_loss_3: 0.4906 - dense_1_loss_4: 0.1508 - dense_1_loss_5: 0.1003 - dense_1_loss_6: 0.0791 - dense_1_loss_7: 0.0651 - dense_1_loss_8: 0.0507 - dense_1_loss_9: 0.0485 - dense_1_loss_10: 0.0393 - dense_1_loss_11: 0.0435 - dense_1_loss_12: 0.0398 - dense_1_loss_13: 0.0374 - dense_1_loss_14: 0.0377 - dense_1_loss_15: 0.0388 - dense_1_loss_16: 0.0418 - dense_1_loss_17: 0.0396 - dense_1_loss_18: 0.0386 - dense_1_loss_19: 0.0410 - dense_1_loss_20: 0.0417 - dense_1_loss_21: 0.0407 - dense_1_loss_22: 0.0388 - dense_1_loss_23: 0.0395 - dense_1_loss_24: 0.0387 - dense_1_loss_25: 0.0460 - dense_1_loss_26: 0.0400 - dense_1_loss_27: 0.0417 - dense_1_loss_28: 0.0475 - dense_1_loss_29: 0.0497 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 83/100 60/60 [==============================] - 0s - loss: 7.0105 - dense_1_loss_1: 3.8154 - dense_1_loss_2: 1.4350 - dense_1_loss_3: 0.4811 - dense_1_loss_4: 0.1461 - dense_1_loss_5: 0.0975 - dense_1_loss_6: 0.0767 - dense_1_loss_7: 0.0629 - dense_1_loss_8: 0.0494 - dense_1_loss_9: 0.0473 - dense_1_loss_10: 0.0382 - dense_1_loss_11: 0.0422 - dense_1_loss_12: 0.0388 - dense_1_loss_13: 0.0363 - dense_1_loss_14: 0.0366 - dense_1_loss_15: 0.0375 - dense_1_loss_16: 0.0410 - dense_1_loss_17: 0.0385 - dense_1_loss_18: 0.0375 - dense_1_loss_19: 0.0398 - dense_1_loss_20: 0.0405 - dense_1_loss_21: 0.0396 - dense_1_loss_22: 0.0378 - dense_1_loss_23: 0.0382 - dense_1_loss_24: 0.0377 - dense_1_loss_25: 0.0446 - dense_1_loss_26: 0.0389 - dense_1_loss_27: 0.0406 - dense_1_loss_28: 0.0463 - dense_1_loss_29: 0.0484 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 84/100 60/60 [==============================] - 0s - loss: 6.9501 - dense_1_loss_1: 3.8125 - dense_1_loss_2: 1.4203 - dense_1_loss_3: 0.4719 - dense_1_loss_4: 0.1424 - dense_1_loss_5: 0.0951 - dense_1_loss_6: 0.0747 - dense_1_loss_7: 0.0612 - dense_1_loss_8: 0.0482 - dense_1_loss_9: 0.0460 - dense_1_loss_10: 0.0372 - dense_1_loss_11: 0.0412 - dense_1_loss_12: 0.0377 - dense_1_loss_13: 0.0353 - dense_1_loss_14: 0.0357 - dense_1_loss_15: 0.0366 - dense_1_loss_16: 0.0398 - dense_1_loss_17: 0.0374 - dense_1_loss_18: 0.0365 - dense_1_loss_19: 0.0388 - dense_1_loss_20: 0.0393 - dense_1_loss_21: 0.0385 - dense_1_loss_22: 0.0368 - dense_1_loss_23: 0.0372 - dense_1_loss_24: 0.0367 - dense_1_loss_25: 0.0435 - dense_1_loss_26: 0.0379 - dense_1_loss_27: 0.0395 - dense_1_loss_28: 0.0449 - dense_1_loss_29: 0.0473 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 85/100 60/60 [==============================] - 0s - loss: 6.8908 - dense_1_loss_1: 3.8096 - dense_1_loss_2: 1.4065 - dense_1_loss_3: 0.4622 - dense_1_loss_4: 0.1385 - dense_1_loss_5: 0.0926 - dense_1_loss_6: 0.0726 - dense_1_loss_7: 0.0596 - dense_1_loss_8: 0.0470 - dense_1_loss_9: 0.0448 - dense_1_loss_10: 0.0362 - dense_1_loss_11: 0.0401 - dense_1_loss_12: 0.0367 - dense_1_loss_13: 0.0344 - dense_1_loss_14: 0.0348 - dense_1_loss_15: 0.0358 - dense_1_loss_16: 0.0387 - dense_1_loss_17: 0.0364 - dense_1_loss_18: 0.0355 - dense_1_loss_19: 0.0379 - dense_1_loss_20: 0.0381 - dense_1_loss_21: 0.0375 - dense_1_loss_22: 0.0358 - dense_1_loss_23: 0.0363 - dense_1_loss_24: 0.0357 - dense_1_loss_25: 0.0427 - dense_1_loss_26: 0.0369 - dense_1_loss_27: 0.0384 - dense_1_loss_28: 0.0436 - dense_1_loss_29: 0.0460 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 86/100 60/60 [==============================] - 0s - loss: 6.8355 - dense_1_loss_1: 3.8069 - dense_1_loss_2: 1.3923 - dense_1_loss_3: 0.4541 - dense_1_loss_4: 0.1352 - dense_1_loss_5: 0.0904 - dense_1_loss_6: 0.0708 - dense_1_loss_7: 0.0582 - dense_1_loss_8: 0.0458 - dense_1_loss_9: 0.0436 - dense_1_loss_10: 0.0352 - dense_1_loss_11: 0.0392 - dense_1_loss_12: 0.0357 - dense_1_loss_13: 0.0335 - dense_1_loss_14: 0.0339 - dense_1_loss_15: 0.0349 - dense_1_loss_16: 0.0376 - dense_1_loss_17: 0.0355 - dense_1_loss_18: 0.0347 - dense_1_loss_19: 0.0370 - dense_1_loss_20: 0.0371 - dense_1_loss_21: 0.0366 - dense_1_loss_22: 0.0349 - dense_1_loss_23: 0.0354 - dense_1_loss_24: 0.0348 - dense_1_loss_25: 0.0417 - dense_1_loss_26: 0.0360 - dense_1_loss_27: 0.0373 - dense_1_loss_28: 0.0424 - dense_1_loss_29: 0.0448 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 87/100 60/60 [==============================] - 0s - loss: 6.7820 - dense_1_loss_1: 3.8038 - dense_1_loss_2: 1.3795 - dense_1_loss_3: 0.4460 - dense_1_loss_4: 0.1318 - dense_1_loss_5: 0.0884 - dense_1_loss_6: 0.0693 - dense_1_loss_7: 0.0568 - dense_1_loss_8: 0.0447 - dense_1_loss_9: 0.0426 - dense_1_loss_10: 0.0343 - dense_1_loss_11: 0.0381 - dense_1_loss_12: 0.0348 - dense_1_loss_13: 0.0326 - dense_1_loss_14: 0.0330 - dense_1_loss_15: 0.0340 - dense_1_loss_16: 0.0367 - dense_1_loss_17: 0.0346 - dense_1_loss_18: 0.0337 - dense_1_loss_19: 0.0359 - dense_1_loss_20: 0.0362 - dense_1_loss_21: 0.0356 - dense_1_loss_22: 0.0339 - dense_1_loss_23: 0.0346 - dense_1_loss_24: 0.0339 - dense_1_loss_25: 0.0404 - dense_1_loss_26: 0.0351 - dense_1_loss_27: 0.0364 - dense_1_loss_28: 0.0414 - dense_1_loss_29: 0.0438 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 88/100 60/60 [==============================] - 0s - loss: 6.7287 - dense_1_loss_1: 3.8010 - dense_1_loss_2: 1.3661 - dense_1_loss_3: 0.4374 - dense_1_loss_4: 0.1286 - dense_1_loss_5: 0.0861 - dense_1_loss_6: 0.0677 - dense_1_loss_7: 0.0553 - dense_1_loss_8: 0.0436 - dense_1_loss_9: 0.0417 - dense_1_loss_10: 0.0335 - dense_1_loss_11: 0.0371 - dense_1_loss_12: 0.0341 - dense_1_loss_13: 0.0317 - dense_1_loss_14: 0.0321 - dense_1_loss_15: 0.0331 - dense_1_loss_16: 0.0360 - dense_1_loss_17: 0.0338 - dense_1_loss_18: 0.0329 - dense_1_loss_19: 0.0350 - dense_1_loss_20: 0.0353 - dense_1_loss_21: 0.0347 - dense_1_loss_22: 0.0330 - dense_1_loss_23: 0.0336 - dense_1_loss_24: 0.0331 - dense_1_loss_25: 0.0393 - dense_1_loss_26: 0.0341 - dense_1_loss_27: 0.0356 - dense_1_loss_28: 0.0406 - dense_1_loss_29: 0.0427 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 89/100 60/60 [==============================] - 0s - loss: 6.6773 - dense_1_loss_1: 3.7981 - dense_1_loss_2: 1.3530 - dense_1_loss_3: 0.4293 - dense_1_loss_4: 0.1254 - dense_1_loss_5: 0.0840 - dense_1_loss_6: 0.0658 - dense_1_loss_7: 0.0538 - dense_1_loss_8: 0.0427 - dense_1_loss_9: 0.0406 - dense_1_loss_10: 0.0327 - dense_1_loss_11: 0.0361 - dense_1_loss_12: 0.0332 - dense_1_loss_13: 0.0309 - dense_1_loss_14: 0.0313 - dense_1_loss_15: 0.0324 - dense_1_loss_16: 0.0353 - dense_1_loss_17: 0.0330 - dense_1_loss_18: 0.0321 - dense_1_loss_19: 0.0341 - dense_1_loss_20: 0.0345 - dense_1_loss_21: 0.0339 - dense_1_loss_22: 0.0322 - dense_1_loss_23: 0.0328 - dense_1_loss_24: 0.0324 - dense_1_loss_25: 0.0384 - dense_1_loss_26: 0.0333 - dense_1_loss_27: 0.0348 - dense_1_loss_28: 0.0397 - dense_1_loss_29: 0.0417 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 90/100 60/60 [==============================] - 0s - loss: 6.6297 - dense_1_loss_1: 3.7953 - dense_1_loss_2: 1.3410 - dense_1_loss_3: 0.4219 - dense_1_loss_4: 0.1224 - dense_1_loss_5: 0.0822 - dense_1_loss_6: 0.0643 - dense_1_loss_7: 0.0526 - dense_1_loss_8: 0.0418 - dense_1_loss_9: 0.0397 - dense_1_loss_10: 0.0319 - dense_1_loss_11: 0.0354 - dense_1_loss_12: 0.0324 - dense_1_loss_13: 0.0302 - dense_1_loss_14: 0.0305 - dense_1_loss_15: 0.0317 - dense_1_loss_16: 0.0344 - dense_1_loss_17: 0.0321 - dense_1_loss_18: 0.0313 - dense_1_loss_19: 0.0334 - dense_1_loss_20: 0.0336 - dense_1_loss_21: 0.0331 - dense_1_loss_22: 0.0315 - dense_1_loss_23: 0.0320 - dense_1_loss_24: 0.0315 - dense_1_loss_25: 0.0376 - dense_1_loss_26: 0.0325 - dense_1_loss_27: 0.0340 - dense_1_loss_28: 0.0387 - dense_1_loss_29: 0.0407 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6167 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 91/100 60/60 [==============================] - 0s - loss: 6.5828 - dense_1_loss_1: 3.7927 - dense_1_loss_2: 1.3288 - dense_1_loss_3: 0.4146 - dense_1_loss_4: 0.1198 - dense_1_loss_5: 0.0803 - dense_1_loss_6: 0.0626 - dense_1_loss_7: 0.0514 - dense_1_loss_8: 0.0409 - dense_1_loss_9: 0.0387 - dense_1_loss_10: 0.0311 - dense_1_loss_11: 0.0347 - dense_1_loss_12: 0.0316 - dense_1_loss_13: 0.0295 - dense_1_loss_14: 0.0299 - dense_1_loss_15: 0.0311 - dense_1_loss_16: 0.0334 - dense_1_loss_17: 0.0313 - dense_1_loss_18: 0.0306 - dense_1_loss_19: 0.0326 - dense_1_loss_20: 0.0328 - dense_1_loss_21: 0.0323 - dense_1_loss_22: 0.0307 - dense_1_loss_23: 0.0313 - dense_1_loss_24: 0.0308 - dense_1_loss_25: 0.0368 - dense_1_loss_26: 0.0317 - dense_1_loss_27: 0.0332 - dense_1_loss_28: 0.0377 - dense_1_loss_29: 0.0397 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 92/100 60/60 [==============================] - 0s - loss: 6.5360 - dense_1_loss_1: 3.7898 - dense_1_loss_2: 1.3168 - dense_1_loss_3: 0.4072 - dense_1_loss_4: 0.1168 - dense_1_loss_5: 0.0785 - dense_1_loss_6: 0.0611 - dense_1_loss_7: 0.0502 - dense_1_loss_8: 0.0400 - dense_1_loss_9: 0.0379 - dense_1_loss_10: 0.0304 - dense_1_loss_11: 0.0339 - dense_1_loss_12: 0.0309 - dense_1_loss_13: 0.0289 - dense_1_loss_14: 0.0292 - dense_1_loss_15: 0.0304 - dense_1_loss_16: 0.0326 - dense_1_loss_17: 0.0306 - dense_1_loss_18: 0.0298 - dense_1_loss_19: 0.0319 - dense_1_loss_20: 0.0320 - dense_1_loss_21: 0.0315 - dense_1_loss_22: 0.0301 - dense_1_loss_23: 0.0306 - dense_1_loss_24: 0.0301 - dense_1_loss_25: 0.0358 - dense_1_loss_26: 0.0310 - dense_1_loss_27: 0.0324 - dense_1_loss_28: 0.0369 - dense_1_loss_29: 0.0387 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 93/100 60/60 [==============================] - 0s - loss: 6.4926 - dense_1_loss_1: 3.7870 - dense_1_loss_2: 1.3053 - dense_1_loss_3: 0.4006 - dense_1_loss_4: 0.1143 - dense_1_loss_5: 0.0767 - dense_1_loss_6: 0.0598 - dense_1_loss_7: 0.0491 - dense_1_loss_8: 0.0391 - dense_1_loss_9: 0.0371 - dense_1_loss_10: 0.0297 - dense_1_loss_11: 0.0331 - dense_1_loss_12: 0.0302 - dense_1_loss_13: 0.0282 - dense_1_loss_14: 0.0286 - dense_1_loss_15: 0.0296 - dense_1_loss_16: 0.0320 - dense_1_loss_17: 0.0299 - dense_1_loss_18: 0.0292 - dense_1_loss_19: 0.0311 - dense_1_loss_20: 0.0313 - dense_1_loss_21: 0.0307 - dense_1_loss_22: 0.0295 - dense_1_loss_23: 0.0299 - dense_1_loss_24: 0.0295 - dense_1_loss_25: 0.0349 - dense_1_loss_26: 0.0304 - dense_1_loss_27: 0.0317 - dense_1_loss_28: 0.0362 - dense_1_loss_29: 0.0379 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 94/100 60/60 [==============================] - 0s - loss: 6.4500 - dense_1_loss_1: 3.7845 - dense_1_loss_2: 1.2939 - dense_1_loss_3: 0.3931 - dense_1_loss_4: 0.1121 - dense_1_loss_5: 0.0750 - dense_1_loss_6: 0.0586 - dense_1_loss_7: 0.0481 - dense_1_loss_8: 0.0383 - dense_1_loss_9: 0.0363 - dense_1_loss_10: 0.0291 - dense_1_loss_11: 0.0323 - dense_1_loss_12: 0.0296 - dense_1_loss_13: 0.0275 - dense_1_loss_14: 0.0280 - dense_1_loss_15: 0.0289 - dense_1_loss_16: 0.0315 - dense_1_loss_17: 0.0293 - dense_1_loss_18: 0.0285 - dense_1_loss_19: 0.0305 - dense_1_loss_20: 0.0306 - dense_1_loss_21: 0.0300 - dense_1_loss_22: 0.0288 - dense_1_loss_23: 0.0292 - dense_1_loss_24: 0.0289 - dense_1_loss_25: 0.0341 - dense_1_loss_26: 0.0297 - dense_1_loss_27: 0.0311 - dense_1_loss_28: 0.0354 - dense_1_loss_29: 0.0371 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9000 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 95/100 60/60 [==============================] - 0s - loss: 6.4081 - dense_1_loss_1: 3.7817 - dense_1_loss_2: 1.2828 - dense_1_loss_3: 0.3861 - dense_1_loss_4: 0.1097 - dense_1_loss_5: 0.0733 - dense_1_loss_6: 0.0572 - dense_1_loss_7: 0.0471 - dense_1_loss_8: 0.0375 - dense_1_loss_9: 0.0355 - dense_1_loss_10: 0.0285 - dense_1_loss_11: 0.0317 - dense_1_loss_12: 0.0289 - dense_1_loss_13: 0.0269 - dense_1_loss_14: 0.0273 - dense_1_loss_15: 0.0283 - dense_1_loss_16: 0.0308 - dense_1_loss_17: 0.0286 - dense_1_loss_18: 0.0279 - dense_1_loss_19: 0.0297 - dense_1_loss_20: 0.0300 - dense_1_loss_21: 0.0294 - dense_1_loss_22: 0.0282 - dense_1_loss_23: 0.0286 - dense_1_loss_24: 0.0283 - dense_1_loss_25: 0.0334 - dense_1_loss_26: 0.0291 - dense_1_loss_27: 0.0304 - dense_1_loss_28: 0.0348 - dense_1_loss_29: 0.0364 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9167 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 96/100 60/60 [==============================] - 0s - loss: 6.3681 - dense_1_loss_1: 3.7790 - dense_1_loss_2: 1.2717 - dense_1_loss_3: 0.3798 - dense_1_loss_4: 0.1075 - dense_1_loss_5: 0.0719 - dense_1_loss_6: 0.0562 - dense_1_loss_7: 0.0461 - dense_1_loss_8: 0.0367 - dense_1_loss_9: 0.0348 - dense_1_loss_10: 0.0279 - dense_1_loss_11: 0.0311 - dense_1_loss_12: 0.0283 - dense_1_loss_13: 0.0264 - dense_1_loss_14: 0.0268 - dense_1_loss_15: 0.0278 - dense_1_loss_16: 0.0301 - dense_1_loss_17: 0.0280 - dense_1_loss_18: 0.0273 - dense_1_loss_19: 0.0291 - dense_1_loss_20: 0.0293 - dense_1_loss_21: 0.0287 - dense_1_loss_22: 0.0275 - dense_1_loss_23: 0.0279 - dense_1_loss_24: 0.0277 - dense_1_loss_25: 0.0327 - dense_1_loss_26: 0.0284 - dense_1_loss_27: 0.0298 - dense_1_loss_28: 0.0339 - dense_1_loss_29: 0.0356 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9167 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 97/100 60/60 [==============================] - 0s - loss: 6.3282 - dense_1_loss_1: 3.7764 - dense_1_loss_2: 1.2606 - dense_1_loss_3: 0.3734 - dense_1_loss_4: 0.1054 - dense_1_loss_5: 0.0702 - dense_1_loss_6: 0.0549 - dense_1_loss_7: 0.0451 - dense_1_loss_8: 0.0359 - dense_1_loss_9: 0.0341 - dense_1_loss_10: 0.0273 - dense_1_loss_11: 0.0305 - dense_1_loss_12: 0.0276 - dense_1_loss_13: 0.0258 - dense_1_loss_14: 0.0263 - dense_1_loss_15: 0.0273 - dense_1_loss_16: 0.0293 - dense_1_loss_17: 0.0274 - dense_1_loss_18: 0.0268 - dense_1_loss_19: 0.0285 - dense_1_loss_20: 0.0287 - dense_1_loss_21: 0.0282 - dense_1_loss_22: 0.0270 - dense_1_loss_23: 0.0274 - dense_1_loss_24: 0.0271 - dense_1_loss_25: 0.0320 - dense_1_loss_26: 0.0278 - dense_1_loss_27: 0.0292 - dense_1_loss_28: 0.0332 - dense_1_loss_29: 0.0348 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9167 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 98/100 60/60 [==============================] - 0s - loss: 6.2921 - dense_1_loss_1: 3.7737 - dense_1_loss_2: 1.2509 - dense_1_loss_3: 0.3680 - dense_1_loss_4: 0.1033 - dense_1_loss_5: 0.0688 - dense_1_loss_6: 0.0540 - dense_1_loss_7: 0.0442 - dense_1_loss_8: 0.0352 - dense_1_loss_9: 0.0335 - dense_1_loss_10: 0.0267 - dense_1_loss_11: 0.0299 - dense_1_loss_12: 0.0271 - dense_1_loss_13: 0.0253 - dense_1_loss_14: 0.0257 - dense_1_loss_15: 0.0267 - dense_1_loss_16: 0.0288 - dense_1_loss_17: 0.0268 - dense_1_loss_18: 0.0263 - dense_1_loss_19: 0.0279 - dense_1_loss_20: 0.0281 - dense_1_loss_21: 0.0276 - dense_1_loss_22: 0.0264 - dense_1_loss_23: 0.0268 - dense_1_loss_24: 0.0265 - dense_1_loss_25: 0.0313 - dense_1_loss_26: 0.0272 - dense_1_loss_27: 0.0286 - dense_1_loss_28: 0.0326 - dense_1_loss_29: 0.0341 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9167 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 99/100 60/60 [==============================] - 0s - loss: 6.2552 - dense_1_loss_1: 3.7710 - dense_1_loss_2: 1.2403 - dense_1_loss_3: 0.3620 - dense_1_loss_4: 0.1015 - dense_1_loss_5: 0.0674 - dense_1_loss_6: 0.0530 - dense_1_loss_7: 0.0433 - dense_1_loss_8: 0.0346 - dense_1_loss_9: 0.0329 - dense_1_loss_10: 0.0262 - dense_1_loss_11: 0.0292 - dense_1_loss_12: 0.0266 - dense_1_loss_13: 0.0247 - dense_1_loss_14: 0.0251 - dense_1_loss_15: 0.0261 - dense_1_loss_16: 0.0285 - dense_1_loss_17: 0.0263 - dense_1_loss_18: 0.0257 - dense_1_loss_19: 0.0273 - dense_1_loss_20: 0.0275 - dense_1_loss_21: 0.0270 - dense_1_loss_22: 0.0258 - dense_1_loss_23: 0.0263 - dense_1_loss_24: 0.0260 - dense_1_loss_25: 0.0307 - dense_1_loss_26: 0.0267 - dense_1_loss_27: 0.0280 - dense_1_loss_28: 0.0320 - dense_1_loss_29: 0.0334 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6333 - dense_1_acc_3: 0.9167 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 Epoch 100/100 60/60 [==============================] - 0s - loss: 6.2195 - dense_1_loss_1: 3.7686 - dense_1_loss_2: 1.2304 - dense_1_loss_3: 0.3562 - dense_1_loss_4: 0.0996 - dense_1_loss_5: 0.0661 - dense_1_loss_6: 0.0518 - dense_1_loss_7: 0.0425 - dense_1_loss_8: 0.0339 - dense_1_loss_9: 0.0322 - dense_1_loss_10: 0.0257 - dense_1_loss_11: 0.0285 - dense_1_loss_12: 0.0261 - dense_1_loss_13: 0.0242 - dense_1_loss_14: 0.0246 - dense_1_loss_15: 0.0256 - dense_1_loss_16: 0.0281 - dense_1_loss_17: 0.0257 - dense_1_loss_18: 0.0252 - dense_1_loss_19: 0.0267 - dense_1_loss_20: 0.0269 - dense_1_loss_21: 0.0265 - dense_1_loss_22: 0.0253 - dense_1_loss_23: 0.0257 - dense_1_loss_24: 0.0255 - dense_1_loss_25: 0.0301 - dense_1_loss_26: 0.0261 - dense_1_loss_27: 0.0275 - dense_1_loss_28: 0.0314 - dense_1_loss_29: 0.0328 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.6500 - dense_1_acc_3: 0.9167 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00 &lt;keras.callbacks.History at 0x7fcff481d908&gt; You should see the model loss going down. Now that you have trained a model, lets go on the the final section to implement an inference algorithm, and generate some music! 3 - Generating musicYou now have a trained model which has learned the patterns of the jazz soloist. Lets now use this model to synthesize new music. 3.1 - Predicting &amp; Sampling At each step of sampling, you will take as input the activation a and cell state c from the previous state of the LSTM, forward propagate by one step, and get a new output activation as well as cell state. The new activation a can then be used to generate the output, using densor as before. To start off the model, we will initialize x0 as well as the LSTM activation and and cell value a0 and c0 to be zeros. Exercise: Implement the function below to sample a sequence of musical values. Here are some of the key steps you’ll need to implement inside the for-loop that generates the $T_y$ output characters: Step 2.A: Use LSTM_Cell, which inputs the previous step’s c and a to generate the current step’s c and a. Step 2.B: Use densor (defined previously) to compute a softmax on a to get the output for the current step. Step 2.C: Save the output you have just generated by appending it to outputs. Step 2.D: Sample x to the be “out”‘s one-hot version (the prediction) so that you can pass it to the next LSTM’s step. We have already provided this line of code, which uses a Lambda function.1x = Lambda(one_hot)(out) [Minor technical note: Rather than sampling a value at random according to the probabilities in out, this line of code actually chooses the single most likely note at each step using an argmax.] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# GRADED FUNCTION: music_inference_modeldef music_inference_model(LSTM_cell, densor, n_values = 78, n_a = 64, Ty = 100): """ Uses the trained "LSTM_cell" and "densor" from model() to generate a sequence of values. Arguments: LSTM_cell -- the trained "LSTM_cell" from model(), Keras layer object densor -- the trained "densor" from model(), Keras layer object n_values -- integer, umber of unique values n_a -- number of units in the LSTM_cell Ty -- integer, number of time steps to generate Returns: inference_model -- Keras model instance """ # Define the input of your model with a shape x0 = Input(shape=(1, n_values)) # Define s0, initial hidden state for the decoder LSTM a0 = Input(shape=(n_a,), name='a0') c0 = Input(shape=(n_a,), name='c0') a = a0 c = c0 x = x0 ### START CODE HERE ### # Step 1: Create an empty list of "outputs" to later store your predicted values (≈1 line) outputs = [] # Step 2: Loop over Ty and generate a value at every time step for t in range(Ty): # Step 2.A: Perform one step of LSTM_cell (≈1 line) a, _, c = LSTM_cell(x, initial_state=[a, c]); # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line) out = densor(a); # Step 2.C: Append the prediction "out" to "outputs". out.shape = (None, 78) (≈1 line) outputs.append(out); # Step 2.D: Select the next value according to "out", and set "x" to be the one-hot representation of the # selected value, which will be passed as the input to LSTM_cell on the next step. We have provided # the line of code you need to do this. x = Lambda(one_hot)(out); # Step 3: Create model instance with the correct "inputs" and "outputs" (≈1 line) inference_model = Model(inputs=[x0, a0, c0], outputs=outputs); ### END CODE HERE ### return inference_model Run the cell below to define your inference model. This model is hard coded to generate 50 values. 1inference_model = music_inference_model(LSTM_cell, densor, n_values = 78, n_a = 64, Ty = 50) Finally, this creates the zero-valued vectors you will use to initialize x and the LSTM state variables a and c. 123x_initializer = np.zeros((1, 1, 78))a_initializer = np.zeros((1, n_a))c_initializer = np.zeros((1, n_a)) Exercise: Implement predict_and_sample(). This function takes many arguments including the inputs [x_initializer, a_initializer, c_initializer]. In order to predict the output corresponding to this input, you will need to carry-out 3 steps: Use your inference model to predict an output given your set of inputs. The output pred should be a list of length $T_y$ where each element is a numpy-array of shape (1, n_values). Convert pred into a numpy array of $T_y$ indices. Each index corresponds is computed by taking the argmax of an element of the pred list. Hint. Convert the indices into their one-hot vector representations. Hint. 12345678910111213141516171819202122232425262728# GRADED FUNCTION: predict_and_sampledef predict_and_sample(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, c_initializer = c_initializer): """ Predicts the next value of values using the inference model. Arguments: inference_model -- Keras model instance for inference time x_initializer -- numpy array of shape (1, 1, 78), one-hot vector initializing the values generation a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel Returns: results -- numpy-array of shape (Ty, 78), matrix of one-hot vectors representing the values generated indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated """ ### START CODE HERE ### # Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer. pred = inference_model.predict([x_initializer, a_initializer, c_initializer]); # Step 2: Convert "pred" into an np.array() of indices with the maximum probabilities indices = np.argmax(np.array(pred), axis = -1); # Step 3: Convert indices to one-hot vectors, the shape of the results should be (1, ) results = to_categorical(indices, num_classes = x_initializer.shape[-1]); ### END CODE HERE ### return results, indices 1234results, indices = predict_and_sample(inference_model, x_initializer, a_initializer, c_initializer)print("np.argmax(results[12]) =", np.argmax(results[12]))print("np.argmax(results[17]) =", np.argmax(results[17]))print("list(indices[12:18]) =", list(indices[12:18])) np.argmax(results[12]) = 21 np.argmax(results[17]) = 7 list(indices[12:18]) = [array([21]), array([10]), array([57]), array([43]), array([12]), array([7])] Expected Output: Your results may differ because Keras’ results are not completely predictable. However, if you have trained your LSTM_cell with model.fit() for exactly 100 epochs as described above, you should very likely observe a sequence of indices that are not all identical. Moreover, you should observe that: np.argmax(results[12]) is the first element of list(indices[12:18]) and np.argmax(results[17]) is the last element of list(indices[12:18]). np.argmax(results[12]) = 1 np.argmax(results[12]) = 42 list(indices[12:18]) = [array([1]), array([42]), array([54]), array([17]), array([1]), array([42])] 3.3 - Generate musicFinally, you are ready to generate music. Your RNN generates a sequence of values. The following code generates music by first calling your predict_and_sample() function. These values are then post-processed into musical chords (meaning that multiple values or notes can be played at the same time). Most computational music algorithms use some post-processing because it is difficult to generate music that sounds good without such post-processing. The post-processing does things such as clean up the generated audio by making sure the same sound is not repeated too many times, that two successive notes are not too far from each other in pitch, and so on. One could argue that a lot of these post-processing steps are hacks; also, a lot the music generation literature has also focused on hand-crafting post-processors, and a lot of the output quality depends on the quality of the post-processing and not just the quality of the RNN. But this post-processing does make a huge difference, so lets use it in our implementation as well. Lets make some music! Run the following cell to generate music and record it into your out_stream. This can take a couple of minutes. 1out_stream = generate_music(inference_model) Predicting new values for different set of chords. Generated 51 sounds using the predicted values for the set of chords (&quot;1&quot;) and after pruning Generated 50 sounds using the predicted values for the set of chords (&quot;2&quot;) and after pruning Generated 50 sounds using the predicted values for the set of chords (&quot;3&quot;) and after pruning Generated 51 sounds using the predicted values for the set of chords (&quot;4&quot;) and after pruning Generated 51 sounds using the predicted values for the set of chords (&quot;5&quot;) and after pruning Your generated music is saved in output/my_music.midi To listen to your music, click File-&gt;Open… Then go to “output/“ and download “my_music.midi”. Either play it on your computer with an application that can read midi files if you have one, or use one of the free online “MIDI to mp3” conversion tools to convert this to mp3. As reference, here also is a 30sec audio clip we generated using this algorithm. 1IPython.display.Audio('./data/30s_trained_model.mp3') &lt;audio controls=&quot;controls&quot; &gt; y Your browser does not support the audio element. Congratulations!You have come to the end of the notebook. Here’s what you should remember: A sequence model can be used to generate musical values, which are then post-processed into midi music. Fairly similar models can be used to generate dinosaur names or to generate music, with the major difference being the input fed to the model. In Keras, sequence generation involves defining layers with shared weights, which are then repeated for the different time steps $1, \ldots, T_x$. Congratulations on completing this assignment and generating a jazz solo! References The ideas presented in this notebook came primarily from three computational music papers cited below. The implementation here also took significant inspiration and used many components from Ji-Sung Kim’s github repository. Ji-Sung Kim, 2016, deepjazz Jon Gillick, Kevin Tang and Robert Keller, 2009. Learning Jazz Grammars Robert Keller and David Morrison, 2007, A Grammatical Approach to Automatic Improvisation François Pachet, 1999, Surprising Harmonies We’re also grateful to François Germain for valuable feedback.]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dinosaurus Island Character level language model final]]></title>
    <url>%2F2018%2F06%2F02%2FDinosaurus%2BIsland%2B--%2BCharacter%2Blevel%2Blanguage%2Bmodel%2Bfinal%2B-%2Bv3%2F</url>
    <content type="text"><![CDATA[NoteThis is one of my personal programming assignments after studying the course nlp sequence models at the 1st week and the copyright belongs to deeplearning.ai. Character level language model - Dinosaurus landWelcome to Dinosaurus Island! 65 million years ago, dinosaurs existed, and in this assignment they are back. You are in charge of a special task. Leading biology researchers are creating new breeds of dinosaurs and bringing them to life on earth, and your job is to give names to these dinosaurs. If a dinosaur does not like its name, it might go beserk, so choose wisely! Luckily you have learned some deep learning and you will use it to save the day. Your assistant has collected a list of all the dinosaur names they could find, and compiled them into this dataset. (Feel free to take a look by clicking the previous link.) To create new dinosaur names, you will build a character level language model to generate new names. Your algorithm will learn the different name patterns, and randomly generate new names. Hopefully this algorithm will keep you and your team safe from the dinosaurs’ wrath! By completing this assignment you will learn: How to store text data for processing using an RNN How to synthesize data, by sampling predictions at each time step and passing it to the next RNN-cell unit How to build a character-level text generation recurrent neural network Why clipping the gradients is important We will begin by loading in some functions that we have provided for you in rnn_utils. Specifically, you have access to functions such as rnn_forward and rnn_backward which are equivalent to those you’ve implemented in the previous assignment. 123import numpy as npfrom utils import *import random 1 - Problem Statement1.1 - Dataset and PreprocessingRun the following cell to read the dataset of dinosaur names, create a list of unique characters (such as a-z), and compute the dataset and vocabulary size. 12345data = open('dinos.txt', 'r').read()data= data.lower()chars = list(set(data))data_size, vocab_size = len(data), len(chars)print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size)) There are 19909 total characters and 27 unique characters in your data. The characters are a-z (26 characters) plus the “\n” (or newline character), which in this assignment plays a role similar to the &lt;EOS&gt; (or “End of sentence”) token we had discussed in lecture, only here it indicates the end of the dinosaur name rather than the end of a sentence. In the cell below, we create a python dictionary (i.e., a hash table) to map each character to an index from 0-26. We also create a second python dictionary that maps each index back to the corresponding character character. This will help you figure out what index corresponds to what character in the probability distribution output of the softmax layer. Below, char_to_ix and ix_to_char are the python dictionaries. 123char_to_ix = &#123; ch:i for i,ch in enumerate(sorted(chars)) &#125;ix_to_char = &#123; i:ch for i,ch in enumerate(sorted(chars)) &#125;print(ix_to_char) {0: &apos;\n&apos;, 1: &apos;a&apos;, 2: &apos;b&apos;, 3: &apos;c&apos;, 4: &apos;d&apos;, 5: &apos;e&apos;, 6: &apos;f&apos;, 7: &apos;g&apos;, 8: &apos;h&apos;, 9: &apos;i&apos;, 10: &apos;j&apos;, 11: &apos;k&apos;, 12: &apos;l&apos;, 13: &apos;m&apos;, 14: &apos;n&apos;, 15: &apos;o&apos;, 16: &apos;p&apos;, 17: &apos;q&apos;, 18: &apos;r&apos;, 19: &apos;s&apos;, 20: &apos;t&apos;, 21: &apos;u&apos;, 22: &apos;v&apos;, 23: &apos;w&apos;, 24: &apos;x&apos;, 25: &apos;y&apos;, 26: &apos;z&apos;} 1.2 - Overview of the modelYour model will have the following structure: Initialize parameters Run the optimization loop Forward propagation to compute the loss function Backward propagation to compute the gradients with respect to the loss function Clip the gradients to avoid exploding gradients Using the gradients, update your parameter with the gradient descent update rule. Return the learned parameters Figure 1: Recurrent Neural Network, similar to what you had built in the previous notebook “Building a RNN - Step by Step”. At each time-step, the RNN tries to predict what is the next character given the previous characters. The dataset $X = (x^{\langle 1 \rangle}, x^{\langle 2 \rangle}, …, x^{\langle T_x \rangle})$ is a list of characters in the training set, while $Y = (y^{\langle 1 \rangle}, y^{\langle 2 \rangle}, …, y^{\langle T_x \rangle})$ is such that at every time-step $t$, we have $y^{\langle t \rangle} = x^{\langle t+1 \rangle}$. 2 - Building blocks of the modelIn this part, you will build two important blocks of the overall model: Gradient clipping: to avoid exploding gradients Sampling: a technique used to generate characters You will then apply these two functions to build the model. 2.1 - Clipping the gradients in the optimization loopIn this section you will implement the clip function that you will call inside of your optimization loop. Recall that your overall loop structure usually consists of a forward pass, a cost computation, a backward pass, and a parameter update. Before updating the parameters, you will perform gradient clipping when needed to make sure that your gradients are not “exploding,” meaning taking on overly large values. In the exercise below, you will implement a function clip that takes in a dictionary of gradients and returns a clipped version of gradients if needed. There are different ways to clip gradients; we will use a simple element-wise clipping procedure, in which every element of the gradient vector is clipped to lie between some range [-N, N]. More generally, you will provide a maxValue (say 10). In this example, if any component of the gradient vector is greater than 10, it would be set to 10; and if any component of the gradient vector is less than -10, it would be set to -10. If it is between -10 and 10, it is left alone. Figure 2: Visualization of gradient descent with and without gradient clipping, in a case where the network is running into slight “exploding gradient” problems. Exercise: Implement the function below to return the clipped gradients of your dictionary gradients. Your function takes in a maximum threshold and returns the clipped versions of your gradients. You can check out this hint for examples of how to clip in numpy. You will need to use the argument out = .... 12345678910111213141516171819202122232425### GRADED FUNCTION: clipdef clip(gradients, maxValue): ''' Clips the gradients' values between minimum and maximum. Arguments: gradients -- a dictionary containing the gradients "dWaa", "dWax", "dWya", "db", "dby" maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue Returns: gradients -- a dictionary with the clipped gradients. ''' dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby'] ### START CODE HERE ### # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines) for gradient in [dWax, dWaa, dWya, db, dby]: np.clip(gradient, -maxValue, maxValue, out = gradient); ### END CODE HERE ### gradients = &#123;"dWaa": dWaa, "dWax": dWax, "dWya": dWya, "db": db, "dby": dby&#125; return gradients 12345678910111213np.random.seed(3)dWax = np.random.randn(5,3)*10dWaa = np.random.randn(5,5)*10dWya = np.random.randn(2,5)*10db = np.random.randn(5,1)*10dby = np.random.randn(2,1)*10gradients = &#123;"dWax": dWax, "dWaa": dWaa, "dWya": dWya, "db": db, "dby": dby&#125;gradients = clip(gradients, 10)print("gradients[\"dWaa\"][1][2] =", gradients["dWaa"][1][2])print("gradients[\"dWax\"][3][1] =", gradients["dWax"][3][1])print("gradients[\"dWya\"][1][2] =", gradients["dWya"][1][2])print("gradients[\"db\"][4] =", gradients["db"][4])print("gradients[\"dby\"][1] =", gradients["dby"][1]) gradients[&quot;dWaa&quot;][1][2] = 10.0 gradients[&quot;dWax&quot;][3][1] = -10.0 gradients[&quot;dWya&quot;][1][2] = 0.2971381536101662 gradients[&quot;db&quot;][4] = [10.] gradients[&quot;dby&quot;][1] = [8.45833407] Expected output: gradients[“dWaa”][1][2] 10.0 gradients[“dWax”][3][1] -10.0 gradients[“dWya”][1][2] 0.29713815361 gradients[“db”][4] [ 10.] gradients[“dby”][1] [ 8.45833407] 2.2 - SamplingNow assume that your model is trained. You would like to generate new text (characters). The process of generation is explained in the picture below: Figure 3: In this picture, we assume the model is already trained. We pass in $x^{\langle 1\rangle} = \vec{0}$ at the first time step, and have the network then sample one character at a time. Exercise: Implement the sample function below to sample characters. You need to carry out 4 steps: Step 1: Pass the network the first “dummy” input $x^{\langle 1 \rangle} = \vec{0}$ (the vector of zeros). This is the default input before we’ve generated any characters. We also set $a^{\langle 0 \rangle} = \vec{0}$ Step 2: Run one step of forward propagation to get $a^{\langle 1 \rangle}$ and $\hat{y}^{\langle 1 \rangle}$. Here are the equations: $$ a^{\langle t+1 \rangle} = \tanh(W_{ax} x^{\langle t \rangle } + W_{aa} a^{\langle t \rangle } + b)\tag{1}$$ $$ z^{\langle t + 1 \rangle } = W_{ya} a^{\langle t + 1 \rangle } + b_y \tag{2}$$ $$ \hat{y}^{\langle t+1 \rangle } = softmax(z^{\langle t + 1 \rangle })\tag{3}$$ Note that $\hat{y}^{\langle t+1 \rangle }$ is a (softmax) probability vector (its entries are between 0 and 1 and sum to 1). $\hat{y}^{\langle t+1 \rangle}_i$ represents the probability that the character indexed by “i” is the next character. We have provided a softmax() function that you can use. Step 3: Carry out sampling: Pick the next character’s index according to the probability distribution specified by $\hat{y}^{\langle t+1 \rangle }$. This means that if $\hat{y}^{\langle t+1 \rangle }_i = 0.16$, you will pick the index “i” with 16% probability. To implement it, you can use np.random.choice. Here is an example of how to use np.random.choice():123np.random.seed(0)p = np.array([0.1, 0.0, 0.7, 0.2])index = np.random.choice([0, 1, 2, 3], p = p.ravel()) This means that you will pick the index according to the distribution:$P(index = 0) = 0.1, P(index = 1) = 0.0, P(index = 2) = 0.7, P(index = 3) = 0.2$. Step 4: The last step to implement in sample() is to overwrite the variable x, which currently stores $x^{\langle t \rangle }$, with the value of $x^{\langle t + 1 \rangle }$. You will represent $x^{\langle t + 1 \rangle }$ by creating a one-hot vector corresponding to the character you’ve chosen as your prediction. You will then forward propagate $x^{\langle t + 1 \rangle }$ in Step 1 and keep repeating the process until you get a “\n” character, indicating you’ve reached the end of the dinosaur name. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# GRADED FUNCTION: sampledef sample(parameters, char_to_ix, seed): """ Sample a sequence of characters according to a sequence of probability distributions output of the RNN Arguments: parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. char_to_ix -- python dictionary mapping each character to an index. seed -- used for grading purposes. Do not worry about it. Returns: indices -- a list of length n containing the indices of the sampled characters. """ # Retrieve parameters and relevant shapes from "parameters" dictionary Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b'] vocab_size = by.shape[0] n_a = Waa.shape[1] ### START CODE HERE ### # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line) x = np.zeros((vocab_size, 1)); # Step 1': Initialize a_prev as zeros (≈1 line) a_prev = np.zeros((n_a, 1)); # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line) indices = [] # Idx is a flag to detect a newline character, we initialize it to -1 idx = -1 # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append # its index to "indices". We'll stop if we reach 50 characters (which should be very unlikely with a well # trained model), which helps debugging and prevents entering an infinite loop. counter = 0 newline_character = char_to_ix['\n'] while (idx != newline_character and counter != 50): # Step 2: Forward propagate x using the equations (1), (2) and (3) a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b); z = np.dot(Wya, a) + by; y = softmax(z); # for grading purposes np.random.seed(counter+seed) # Step 3: Sample the index of a character within the vocabulary from the probability distribution y idx = np.random.choice(range(len(y)), p = y.ravel()); # Append the index to "indices" indices.append(idx); # Step 4: Overwrite the input character as the one corresponding to the sampled index. x = np.zeros((vocab_size, 1)); x[idx] = 1; # Update "a_prev" to be "a" a_prev = a; # for grading purposes seed += 1 counter +=1 ### END CODE HERE ### if (counter == 50): indices.append(char_to_ix['\n']) return indices 1234567891011np.random.seed(2)_, n_a = 20, 100Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)parameters = &#123;"Wax": Wax, "Waa": Waa, "Wya": Wya, "b": b, "by": by&#125;indices = sample(parameters, char_to_ix, 0)print("Sampling:")print("list of sampled indices:", indices)print("list of sampled characters:", [ix_to_char[i] for i in indices]) Sampling: list of sampled indices: [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 3, 6, 23, 13, 1, 0] list of sampled characters: [&apos;l&apos;, &apos;q&apos;, &apos;x&apos;, &apos;n&apos;, &apos;m&apos;, &apos;i&apos;, &apos;j&apos;, &apos;v&apos;, &apos;x&apos;, &apos;f&apos;, &apos;m&apos;, &apos;k&apos;, &apos;l&apos;, &apos;f&apos;, &apos;u&apos;, &apos;o&apos;, &apos;u&apos;, &apos;n&apos;, &apos;c&apos;, &apos;b&apos;, &apos;a&apos;, &apos;u&apos;, &apos;r&apos;, &apos;x&apos;, &apos;g&apos;, &apos;y&apos;, &apos;f&apos;, &apos;y&apos;, &apos;r&apos;, &apos;j&apos;, &apos;p&apos;, &apos;b&apos;, &apos;c&apos;, &apos;h&apos;, &apos;o&apos;, &apos;l&apos;, &apos;k&apos;, &apos;g&apos;, &apos;a&apos;, &apos;l&apos;, &apos;j&apos;, &apos;b&apos;, &apos;g&apos;, &apos;g&apos;, &apos;k&apos;, &apos;c&apos;, &apos;f&apos;, &apos;w&apos;, &apos;m&apos;, &apos;a&apos;, &apos;\n&apos;] Expected output: list of sampled indices: [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 5, 6, 12, 25, 0, 0] list of sampled characters: [‘l’, ‘q’, ‘x’, ‘n’, ‘m’, ‘i’, ‘j’, ‘v’, ‘x’, ‘f’, ‘m’, ‘k’, ‘l’, ‘f’, ‘u’, ‘o’, ‘u’, ‘n’, ‘c’, ‘b’, ‘a’, ‘u’, ‘r’, ‘x’, ‘g’, ‘y’, ‘f’, ‘y’, ‘r’, ‘j’, ‘p’, ‘b’, ‘c’, ‘h’, ‘o’, ‘l’, ‘k’, ‘g’, ‘a’, ‘l’, ‘j’, ‘b’, ‘g’, ‘g’, ‘k’, ‘e’, ‘f’, ‘l’, ‘y’, ‘\n’, ‘\n’] 3 - Building the language modelIt is time to build the character-level language model for text generation. 3.1 - Gradient descentIn this section you will implement a function performing one step of stochastic gradient descent (with clipped gradients). You will go through the training examples one at a time, so the optimization algorithm will be stochastic gradient descent. As a reminder, here are the steps of a common optimization loop for an RNN: Forward propagate through the RNN to compute the loss Backward propagate through time to compute the gradients of the loss with respect to the parameters Clip the gradients if necessary Update your parameters using gradient descent Exercise: Implement this optimization process (one step of stochastic gradient descent). We provide you with the following functions: 12345678910111213141516def rnn_forward(X, Y, a_prev, parameters): """ Performs the forward propagation through the RNN and computes the cross-entropy loss. It returns the loss' value as well as a "cache" storing values to be used in the backpropagation.""" .... return loss, cache def rnn_backward(X, Y, parameters, cache): """ Performs the backward propagation through time to compute the gradients of the loss with respect to the parameters. It returns also all the hidden states.""" ... return gradients, adef update_parameters(parameters, gradients, learning_rate): """ Updates parameters using the Gradient Descent Update Rule.""" ... return parameters 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# GRADED FUNCTION: optimizedef optimize(X, Y, a_prev, parameters, learning_rate = 0.01): """ Execute one step of the optimization to train the model. Arguments: X -- list of integers, where each integer is a number that maps to a character in the vocabulary. Y -- list of integers, exactly the same as X but shifted one index to the left. a_prev -- previous hidden state. parameters -- python dictionary containing: Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x) Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a) Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a) b -- Bias, numpy array of shape (n_a, 1) by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1) learning_rate -- learning rate for the model. Returns: loss -- value of the loss function (cross-entropy) gradients -- python dictionary containing: dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x) dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a) dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a) db -- Gradients of bias vector, of shape (n_a, 1) dby -- Gradients of output bias vector, of shape (n_y, 1) a[len(X)-1] -- the last hidden state, of shape (n_a, 1) """ ### START CODE HERE ### # Forward propagate through time (≈1 line) loss, cache = rnn_forward(X, Y, a_prev, parameters); # Backpropagate through time (≈1 line) gradients, a = rnn_backward(X, Y, parameters, cache); # Clip your gradients between -5 (min) and 5 (max) (≈1 line) gradients = clip(gradients, maxValue = 5); # Update parameters (≈1 line) parameters = update_parameters(parameters, gradients, learning_rate); ### END CODE HERE ### return loss, gradients, a[len(X)-1] 1234567891011121314151617np.random.seed(1)vocab_size, n_a = 27, 100a_prev = np.random.randn(n_a, 1)Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)parameters = &#123;"Wax": Wax, "Waa": Waa, "Wya": Wya, "b": b, "by": by&#125;X = [12,3,5,11,22,3]Y = [4,14,11,22,25, 26]loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)print("Loss =", loss)print("gradients[\"dWaa\"][1][2] =", gradients["dWaa"][1][2])print("np.argmax(gradients[\"dWax\"]) =", np.argmax(gradients["dWax"]))print("gradients[\"dWya\"][1][2] =", gradients["dWya"][1][2])print("gradients[\"db\"][4] =", gradients["db"][4])print("gradients[\"dby\"][1] =", gradients["dby"][1])print("a_last[4] =", a_last[4]) Loss = 126.50397572165383 gradients[&quot;dWaa&quot;][1][2] = 0.1947093153471825 np.argmax(gradients[&quot;dWax&quot;]) = 93 gradients[&quot;dWya&quot;][1][2] = -0.007773876032003897 gradients[&quot;db&quot;][4] = [-0.06809825] gradients[&quot;dby&quot;][1] = [0.01538192] a_last[4] = [-1.] Expected output: Loss 126.503975722 gradients[“dWaa”][1][2] 0.194709315347 np.argmax(gradients[“dWax”]) 93 gradients[“dWya”][1][2] -0.007773876032 gradients[“db”][4] [-0.06809825] gradients[“dby”][1] [ 0.01538192] a_last[4] [-1.] 3.2 - Training the modelGiven the dataset of dinosaur names, we use each line of the dataset (one name) as one training example. Every 100 steps of stochastic gradient descent, you will sample 10 randomly chosen names to see how the algorithm is doing. Remember to shuffle the dataset, so that stochastic gradient descent visits the examples in random order. Exercise: Follow the instructions and implement model(). When examples[index] contains one dinosaur name (string), to create an example (X, Y), you can use this:123index = j % len(examples)X = [None] + [char_to_ix[ch] for ch in examples[index]] Y = X[1:] + [char_to_ix["\n"]] Note that we use: index= j % len(examples), where j = 1....num_iterations, to make sure that examples[index] is always a valid statement (index is smaller than len(examples)).The first entry of X being None will be interpreted by rnn_forward() as setting $x^{\langle 0 \rangle} = \vec{0}$. Further, this ensures that Y is equal to X but shifted one step to the left, and with an additional “\n” appended to signify the end of the dinosaur name. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# GRADED FUNCTION: modeldef model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27): """ Trains the model and generates dinosaur names. Arguments: data -- text corpus ix_to_char -- dictionary that maps the index to a character char_to_ix -- dictionary that maps a character to an index num_iterations -- number of iterations to train the model for n_a -- number of units of the RNN cell dino_names -- number of dinosaur names you want to sample at each iteration. vocab_size -- number of unique characters found in the text, size of the vocabulary Returns: parameters -- learned parameters """ # Retrieve n_x and n_y from vocab_size n_x, n_y = vocab_size, vocab_size # Initialize parameters parameters = initialize_parameters(n_a, n_x, n_y) # Initialize loss (this is required because we want to smooth our loss, don't worry about it) loss = get_initial_loss(vocab_size, dino_names) # Build list of all dinosaur names (training examples). with open("dinos.txt") as f: examples = f.readlines() examples = [x.lower().strip() for x in examples] # Shuffle list of all dinosaur names np.random.seed(0) np.random.shuffle(examples) # Initialize the hidden state of your LSTM a_prev = np.zeros((n_a, 1)) # Optimization loop for j in range(num_iterations): ### START CODE HERE ### # Use the hint above to define one training example (X,Y) (≈ 2 lines) index = j % len(examples); X = [None] + [char_to_ix[ch] for ch in examples[index]]; Y = X[1:] + [char_to_ix["\n"]]; learning_rate = 0.01; # num_partition = num_iterations / 10; # if j / num_partition &gt; 0 : # if j % num_partition == 0 : # learning_rate = 0.01 * (0.95 ** (j / num_partition)); # print("current learning rate: " + str(learning_rate)); # Perform one optimization step: Forward-prop -&gt; Backward-prop -&gt; Clip -&gt; Update parameters # Choose a learning rate of 0.01 curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate); ### END CODE HERE ### # Use a latency trick to keep the loss smooth. It happens here to accelerate the training. loss = smooth(loss, curr_loss) # Every 2000 Iteration, generate "n" characters thanks to sample() to check if the model is learning properly if j % 2000 == 0: print('Iteration: %d, Loss: %f' % (j, loss) + '\n') # The number of dinosaur names to print seed = 0 for name in range(dino_names): # Sample indices and print them sampled_indices = sample(parameters, char_to_ix, seed) print_sample(sampled_indices, ix_to_char) seed += 1 # To get the same result for grading purposed, increment the seed by one. print('\n') return parameters Run the following cell, you should observe your model outputting random-looking characters at the first iteration. After a few thousand iterations, your model should learn to generate reasonable-looking names. 1parameters = model(data, ix_to_char, char_to_ix) Iteration: 0, Loss: 23.087336 Nkzxwtdmfqoeyhsqwasjkjvu Kneb Kzxwtdmfqoeyhsqwasjkjvu Neb Zxwtdmfqoeyhsqwasjkjvu Eb Xwtdmfqoeyhsqwasjkjvu Iteration: 2000, Loss: 27.884160 Liusskeomnolxeros Hmdaairus Hytroligoraurus Lecalosapaus Xusicikoraurus Abalpsamantisaurus Tpraneronxeros Iteration: 4000, Loss: 25.901815 Mivrosaurus Inee Ivtroplisaurus Mbaaisaurus Wusichisaurus Cabaselachus Toraperlethosdarenitochusthiamamumamaon Iteration: 6000, Loss: 24.608779 Onwusceomosaurus Lieeaerosaurus Lxussaurus Oma Xusteonosaurus Eeahosaurus Toreonosaurus Iteration: 8000, Loss: 24.070350 Onxusichepriuon Kilabersaurus Lutrodon Omaaerosaurus Xutrcheps Edaksoje Trodiktonus Iteration: 10000, Loss: 23.844446 Onyusaurus Klecalosaurus Lustodon Ola Xusodonia Eeaeosaurus Troceosaurus Iteration: 12000, Loss: 23.291971 Onyxosaurus Kica Lustrepiosaurus Olaagrraiansaurus Yuspangosaurus Eealosaurus Trognesaurus Iteration: 14000, Loss: 23.382339 Meutromodromurus Inda Iutroinatorsaurus Maca Yusteratoptititan Ca Troclosaurus Iteration: 16000, Loss: 23.259291 Meustomia Indaadps Justolongchudosatrus Macabosaurus Yuspanhosaurus Caaerosaurus Trodon Iteration: 18000, Loss: 22.940799 Phusaurus Meicamitheastosaurus Mussteratops Peg Ytrong Egaltor Trolome Iteration: 20000, Loss: 22.894192 Meutrodon Lledansteh Lwuspconyxauosaurus Macalosaurus Yusocichugus Eiagosaurus Trrangosaurus Iteration: 22000, Loss: 22.851820 Onustolia Midcagosaurus Mwrrodonnonus Ola Yurodon Eiaeptia Trodoniohus Iteration: 24000, Loss: 22.700408 Meutosaurus Jmacagosaurus Kurrodon Macaistel Yuroeleton Eiaeror Trodonosaurus Iteration: 26000, Loss: 22.736918 Niutosaurus Liga Lustoingosaurus Necakroia Xrprinhtilus Eiaestehastes Trocilosaurus Iteration: 28000, Loss: 22.595568 Meutosaurus Kolaaeus Kystodonisaurus Macahtopadrus Xtrrararkaumurpasaurus Eiaeosaurus Trodmanolus Iteration: 30000, Loss: 22.609381 Meutosaurus Kracakosaurus Lustodon Macaisthachwisaurus Wusqandosaurus Eiacosaurus Trsatisaurus Iteration: 32000, Loss: 22.251308 Mausinasaurus Incaadropeglsaurus Itrosaurus Macamisaurus Wuroenatoraerax Ehanosaurus Trnanclodratosaurus Iteration: 34000, Loss: 22.477910 Mawspichaniaekorocimamroberax Inda Itrus Macaesis Wrosaurus Elaeosaurus Stegngosaurus ConclusionYou can see that your algorithm has started to generate plausible dinosaur names towards the end of the training. At first, it was generating random characters, but towards the end you could see dinosaur names with cool endings. Feel free to run the algorithm even longer and play with hyperparameters to see if you can get even better results. Our implemetation generated some really cool names like maconucon, marloralus and macingsersaurus. Your model hopefully also learned that dinosaur names tend to end in saurus, don, aura, tor, etc. If your model generates some non-cool names, don’t blame the model entirely–not all actual dinosaur names sound cool. (For example, dromaeosauroides is an actual dinosaur name and is in the training set.) But this model should give you a set of candidates from which you can pick the coolest! This assignment had used a relatively small dataset, so that you could train an RNN quickly on a CPU. Training a model of the english language requires a much bigger dataset, and usually needs much more computation, and could run for many hours on GPUs. We ran our dinosaur name for quite some time, and so far our favoriate name is the great, undefeatable, and fierce: Mangosaurus! 4 - Writing like ShakespeareThe rest of this notebook is optional and is not graded, but we hope you’ll do it anyway since it’s quite fun and informative. A similar (but more complicated) task is to generate Shakespeare poems. Instead of learning from a dataset of Dinosaur names you can use a collection of Shakespearian poems. Using LSTM cells, you can learn longer term dependencies that span many characters in the text–e.g., where a character appearing somewhere a sequence can influence what should be a different character much much later in ths sequence. These long term dependencies were less important with dinosaur names, since the names were quite short. Let’s become poets! We have implemented a Shakespeare poem generator with Keras. Run the following cell to load the required packages and models. This may take a few minutes. 12345678910from __future__ import print_functionfrom keras.callbacks import LambdaCallbackfrom keras.models import Model, load_model, Sequentialfrom keras.layers import Dense, Activation, Dropout, Input, Maskingfrom keras.layers import LSTMfrom keras.utils.data_utils import get_filefrom keras.preprocessing.sequence import pad_sequencesfrom shakespeare_utils import *import sysimport io C:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Using TensorFlow backend. Loading text data... Creating training set... number of training examples: 31412 Vectorizing training set... Loading model... To save you some time, we have already trained a model for ~1000 epochs on a collection of Shakespearian poems called “The Sonnets”. Let’s train the model for one more epoch. When it finishes training for an epoch—this will also take a few minutes—you can run generate_output, which will prompt asking you for an input (&lt;40 characters). The poem will start with your sentence, and our RNN-Shakespeare will complete the rest of the poem for you! For example, try “Forsooth this maketh no sense “ (don’t enter the quotation marks). Depending on whether you include the space at the end, your results might also differ–try it both ways, and try other inputs as well. 123print_callback = LambdaCallback(on_epoch_end=on_epoch_end)model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback]) Epoch 1/1 31412/31412 [==============================] - 244s 8ms/step - loss: 2.7302 &lt;keras.callbacks.History at 0x1c3ef0e8978&gt; 12# Run this cell to try with different inputs without having to re-train the model generate_output() Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: You are a flower Here is your poem: You are a flower, and tines wo why doaoty loving friel be lifles it, whene the ford, eoreing oned his byfor mine, the beauty astore, with the dune still weel, doth nof berioner others should best ay commors shall&apos;s feel how the, ti the vere datef me wenden conse, now this, and mateh and haris by deigh doy, how raccersake wiming to be the worlts in thine sho nuch their astaver beloned i ustind, that youn thou The RNN-Shakespeare model is very similar to the one you have built for dinosaur names. The only major differences are: LSTMs instead of the basic RNN to capture longer-range dependencies The model is a deeper, stacked LSTM model (2 layer) Using Keras instead of python to simplify the code If you want to learn more, you can also check out the Keras Team’s text generation implementation on GitHub: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py. Congratulations on finishing this notebook! References: This exercise took inspiration from Andrej Karpathy’s implementation: https://gist.github.com/karpathy/d4dee566867f8291f086. To learn more about text generation, also check out Karpathy’s blog post. For the Shakespearian poem generator, our implementation was based on the implementation of an LSTM text generator by the Keras team: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[natural language processing word embeddings]]></title>
    <url>%2F2018%2F06%2F02%2F02_natural-language-processing-word-embeddings%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal lecture note after studying the course nlp sequence models at the 2nd week and the copyright belongs to deeplearning.ai. 01_introduction-to-word-embeddings01_word-representationHello, and welcome back. Last week, we learned about RNNs, GRUs, and LSTMs. In this week, you see how many of these ideas can be applied to NLP, to Natural Language Processing, which is one of the features of AI because it’s really being revolutionized by deep learning. One of the key ideas you learn about is word embeddings, which is a way of representing words. That let your algorithms automatically understand analogies like that, man is to woman, as king is to queen, and many other examples. And through these ideas of word embeddings, you’ll be able to build NPL applications, even with models the size of, usually of relatively small label training sets. Finally towards the end of the week, you’ll see how to debias word embeddings. That’s to reduce undesirable gender or ethnicity or other types of bias that learning algorithms can sometimes pick up. So with that, let’s get started with a discussion on word representation. So far, we’ve been representing words using a vocabulary of words, and a vocabulary from the previous week might be say, 10,000 words. And we’ve been representing words using a one-hot vector. So for example, if man is word number 5391 in this dictionary, then you represent him with a vector with one in position 5391. And I’m also going to use O subscript 5391 to represent this factor, where O here stands for one-hot. And then, if woman is word number 9853, then you represent it with O subscript 9853 which just has a one in position 9853 and zeros elsewhere. And then other words king, queen, apple, orange will be similarly represented with one-hot vector. One of the weaknesses of this representation is that it treats each word as a thing unto itself, and it doesn’t allow an algorithm to easily generalize the cross words. For example, let’s say you have a language model that has learned that when you see “I want a glass of orange “. Well, what do you think the next word will be? Very likely, it’ll be “juice”. But even if the learning algorithm has learned that “I want a glass of orange juice” is a likely sentence, if it sees “I want a glass of apple _“. As far as it knows the relationship between apple and orange is not any closer as the relationship between any of the other words man, woman, king, queen, and orange. And so, it’s not easy for the learning algorithm to generalize from knowing that orange juice is a popular thing, to recognizing that apple juice might also be a popular thing or a popular phrase. And this is because the any product between any two different one-hot vector is zero. If you take any two vectors say, queen and king and any product of them, the end product is zero. If you take apple and orange and any product of them, the end product is zero. And you couldn’t get distance between any pair of these vectors, which is also the same. So it just doesn’t know that somehow apple and orange are much more similar than king and orange or queen and orange. So, won’t it be nice if instead of a one-hot presentation we can instead learn a featurized representation with each of these words, a man, woman, king, queen, apple, orange or really for every word in the dictionary, we could learn a set of features and values for each of them. So for example, each of these words, we want to know what is the gender associated with each of these things. So, if gender goes from minus one for male to plus one for female, then the gender associated with man might be minus one, for woman might be plus one. And then eventually, learning these things maybe for king you get minus 0.95, for queen plus 0.97, and for apple and orange sort of genderless. Another feature might be, well how royal are these things. And so the terms, man and woman are not really royal, so they might have feature values close to zero. Whereas king and queen are highly royal. And apple and orange are not really loyal. How about age? Well, man and woman doesn’t connotes much about age. Maybe men and woman implies that they’re adults, but maybe neither necessarily young nor old. So maybe values close to zero. Whereas kings and queens are always almost always adults. And apple and orange might be more neutral with respect to age. And then, another feature for here, is this is a food? Well, man is not a food, woman is not a food, neither are kings and queens, but apples and oranges are foods. And they can be many other features as well ranging from, what is the size of this? What is the cost? Is this something that is a live? Is this an action, or is this a noun, or is this a verb, or is it something else? And so on. So you can imagine coming up with many features. And for the sake of the illustration let’s say, 300 different features, and what that does is, it allows you to take this list of numbers, I’ve only written four here, but this could be a list of 300 numbers, that then becomes a 300 dimensional vector for representing the word man. And I’m going to use the notation e subscript 5391 to denote a representation like this. And similarly, this vector, this 300 dimensional vector or 300 dimensional vector like this, I would denote e9853 to denote a 300 dimensional vector we could use to represent the word woman. And similarly, for the other examples here. Now, if you use this representation to represent the words orange and apple, then notice that the representations for orange and apple are now quite similar. Some of the features will differ because of the color of an orange, the color an apple, the taste, or some of the features would differ. But by a large, a lot of the features of apple and orange are actually the same, or take on very similar values. And so, this increases the odds of the learning algorithm that has figured out that orange juice is a thing, to also quickly figure out that apple juice is a thing. So this allows it to generalize better across different words. So over the next few videos, we’ll find a way to learn words embeddings. We just need you to learn high dimensional feature vectors like these, that gives a better representation than one-hot vectors for representing different words. And the features we’ll end up learning, won’t have a easy to interpret interpretation like that component one is gender, component two is royal, component three is age and so on. Exactly what they’re representing will be a bit harder to figure out. But nonetheless, the featurized representations we will learn, will allow an algorithm to quickly figure out that apple and orange are more similar than say, king and orange or queen and orange. If we’re able to learn a 300 dimensional feature vector or 300 dimensional embedding for each words, one of the popular things to do is also to take this 300 dimensional data and embed it say, in a two dimensional space so that you can visualize them. And so, one common algorithm for doing this is the t-SNE algorithm due to Laurens van der Maaten and Geoff Hinton. And if you look at one of these embeddings, one of these representations, you find that words like man and woman tend to get grouped together, king and queen tend to get grouped together, and these are the people which tends to get grouped together. Those are animals who can get grouped together. Fruits will tend to be close to each other. Numbers like one, two, three, four, will be close to each other. And then, maybe the animate objects as whole will also tend to be grouped together. But you see plots like these sometimes on the internet to visualize some of these 300 or higher dimensional embeddings. And maybe this gives you a sense that, word embeddings algorithms like this can learn similar features for concepts that feel like they should be more related, as visualized by that concept that seem to you and me like they should be more similar, end up getting mapped to a more similar feature vectors. And these representations will use these sort of featurized representations in maybe a 300 dimensional space, these are called embeddings. And the reason we call them embeddings is, you can think of a 300 dimensional space. And again, they can’t draw out here in two dimensional space because it’s a 3D one. And what you do is you take every words like orange, and have a three dimensional feature vector so that word orange gets embedded to a point in this 300 dimensional space. And the word apple, gets embedded to a different point in this 300 dimensional space. And of course to visualize it, algorithms like t-SNE, map this to a much lower dimensional space, you can actually plot the 2D data and look at it. But that’s what the term embedding comes from. Word embeddings has been one of the most important ideas in NLP, in Natural Language Processing. In this video, you saw why you might want to learn or use word embeddings. In the next video, let’s take a deeper look at how you’ll be able to use these algorithms, to build NLP algorithims. 02_using-word-embeddingsIn the last video, you saw what it might mean to learn a featurized representations of different words. In this video, you see how we can take these representations and plug them into NLP applications. Let’s start with an example. Continuing with the named entity recognition example, if you’re trying to detect people’s names. Given a sentence like Sally Johnson is an orange farmer, hopefully, you’ll figure out that Sally Johnson is a person’s name, hence, the outputs 1 like that. And one way to be sure that Sally Johnson has to be a person, rather than say the name of the corporation is that you know orange farmer is a person. So previously, we had talked about one hot representations to represent these words, x(1), x(2), and so on. But if you can now use the featurized representations, the embedding vectors that we talked about in the last video. Then after having trained a model that uses word embeddings as the inputs, if you now see a new input, Robert Lin is an apple farmer. Knowing that orange and apple are very similar will make it easier for your learning algorithm to generalize to figure out that Robert Lin is also a human, is also a person’s name. One of the most interesting cases will be, what if in your test set you see not Robert Lin is an apple farmer, but you see much less common words? What if you see Robert Lin is a durian cultivator? A durian is a rare type of fruit, popular in Singapore and a few other countries. But if you have a small label training set for the named entity recognition task, you might not even have seen the word durian or seen the word cultivator in your training set. I guess technically, this should be a durian cultivator. But if you have learned a word embedding that tells you that durian is a fruit, so it’s like an orange, and a cultivator, someone that cultivates is like a farmer, then you might still be generalize from having seen an orange farmer in your training set to knowing that a durian cultivator is also probably a person. So one of the reasons that word embeddings will be able to do this is the algorithms to learning word embeddings can examine very large text corpuses, maybe found off the Internet. So you can examine very large data sets, maybe a billion words, maybe even up to 100 billion words would be quite reasonable. So very large training sets of just unlabeled text. And by examining tons of unlabeled text, which you can download more or less for free, you can figure out that orange and durian are similar. And farmer and cultivator are similar, and therefore, learn embeddings, that groups them together. Now having discovered that orange and durian are both fruits by reading massive amounts of Internet text, what you can do is then take this word embedding and apply it to your named entity recognition task, for which you might have a much smaller training set, maybe just 100,000 words in your training set, or even much smaller. And so this allows you to carry out transfer learning, where you take information you’ve learned from huge amounts of unlabeled text that you can suck down essentially for free off the Internet to figure out that orange, apple, and durian are fruits. And then transfer that knowledge to a task, such as named entity recognition, for which you may have a relatively small labeled training set. And, of course, for simplicity, l drew this for it only as a unidirectional RNN. If you actually want to carry out the named entity recognition task, you should, of course, use a bidirectional RNN rather than a simpler one I’ve drawn here. But to summarize, this is how you can carry out transfer learning using word embeddings. Step 1 is to learn word embeddings from a large text corpus, a very large text corpus or you can also download pre-trained word embeddings online. There are several word embeddings that you can find online under very permissive licenses. And you can then take these word embeddings and transfer the embedding to new task, where you have a much smaller labeled training sets. And use this, let’s say, 300 dimensional embedding, to represent your words. One nice thing also about this is you can now use relatively lower dimensional feature vectors. So rather than using a 10,000 dimensional one-hot vector, you can now instead use maybe a 300 dimensional dense vector. Although the one-hot vector is fast and the 300 dimensional vector that you might learn for your embedding will be a dense vector. And then, finally, as you train your model on your new task, on your named entity recognition task with a smaller label data set, one thing you can optionally do is to continue to fine tune, continue to adjust the word embeddings with the new data. In practice, you would do this only if this task 2 has a pretty big data set. If your label data set for step 2 is quite small, then usually, I would not bother to continue to fine tune the word embeddings. So word embeddings tend to make the biggest difference when the task you’re trying to carry out has a relatively smaller training set. So it has been useful for many NLP tasks. And I’ll just name a few. Don’t worry if you don’t know these terms. It has been useful for named entity recognition, for text summarization, for co-reference resolution, for parsing. These are all maybe pretty standard NLP tasks. It has been less useful for language modeling, machine translation, especially if you’re accessing a language modeling or machine translation task for which you have a lot of data just dedicated to that task. So as seen in other transfer learning settings, if you’re trying to transfer from some task A to some task B, the process of transfer learning is just most useful when you happen to have a ton of data for A and a relatively smaller data set for B. And so that’s true for a lot of NLP tasks, and just less true for some language modeling and machine translation settings. Finally, word embeddings has a interesting relationship to the face encoding ideas that you learned about in the previous course, if you took the convolutional neural networks course. So you will remember that for face recognition, we train this Siamese network architecture that would learn, say, a 128 dimensional representation for different faces. And then you can compare these encodings in order to figure out if these two pictures are of the same face. The words encoding and embedding mean fairly similar things. So in the face recognition literature, people also use the term encoding to refer to these vectors, f(x(i)) and f(x(j)). One difference between the face recognition literature and what we do in word embeddings is that, for face recognition, you wanted to train a neural network that can take as input any face picture, even a picture you’ve never seen before, and have a neural network compute an encoding for that new picture. Whereas what we’ll do, and you’ll understand this better when we go through the next few videos, whereas what we’ll do for learning word embeddings is that we’ll have a fixed vocabulary of, say, 10,000 words. And we’ll learn a vector e1 through, say, e10,000 that just learns a fixed encoding or learns a fixed embedding for each of the words in our vocabulary. So that’s one difference between the set of ideas you saw for face recognition versus what the algorithms we’ll discuss in the next few videos. But the terms encoding and embedding are used somewhat interchangeably. So the difference I just described is not represented by the difference in terminologies. It’s just a difference in how we need to use these algorithms in face recognition, where there’s unlimited sea of pictures you could see in the future. Versus natural language processing, where there might be just a fixed vocabulary, and everything else like that we’ll just declare as an unknown word. So in this video, you saw how using word embeddings allows you to implement this type of transfer learning. And how, by replacing the one-hot vectors we’re using previously with the embedding vectors, you can allow your algorithms to generalize much better, or you can learn from much less label data. Next, I want to show you just a few more properties of these word embeddings. And then after that, we will talk about algorithms for actually learning these word embeddings. Let’s go on to the next video, where you’ll see how word embeddings can help with reasoning about analogies. 03_properties-of-word-embeddingsBy now, you should have a sense of how word embeddings can help you build NLP applications. One of the most fascinating properties of word embeddings is that they can also help with analogy reasoning. And while reasonable analogies may not be by itself the most important NLP application, they might also help convey a sense of what these word embeddings are doing, what these word embeddings can do. Let me show you what I mean here are the featurized representations of a set of words that you might hope a word embedding could capture. Let’s say I pose a question, man is to woman as king is to what? Many of you will say, man is to woman as king is to queen. But is it possible to have an algorithm figure this out automatically? Well, here’s how you could do it, let’s say that you’re using this four dimensional vector to represent man. So this will be your E5391, although just for this video, let me call this e subscript man. And let’s say that’s the embedding vector for woman, so I’m going to call that e subscript woman, and similarly for king and queen. And for this example, I’m just going to assume you’re using four dimensional embeddings, rather than anywhere from 50 to 1,000 dimensional, which would be more typical. One interesting property of these vectors is that if you take the vector, e man, and subtract the vector e woman, then, You end up with approximately -1, negative another 1 is -2, decimal 0- 0, 0- 0, close to 0- 0, so you get roughly -2 0 0 0. And similarly if you take e king minus e queen, then that’s approximately the same thing. That’s about -1- 0.97, it’s about -2. This is about 1- 1, since kings and queens are both about equally royal. So that’s 0, and then age difference, food difference, 0. And so what this is capturing is that the main difference between man and woman is the gender. And the main difference between king and queen, as represented by these vectors, is also the gender. Which is why the difference e man- e woman, and the difference e king- e queen, are about the same. So one way to carry out this analogy reasoning is, if the algorithm is asked, man is to woman as king is to what? What it can do is compute e man- e woman, and try to find a vector, try to find a word so that e man- e woman is close to e king- e of that new word. And it turns out that when queen is the word plugged in here, then the left hand side is close to the the right hand side. So these ideas were first pointed out by Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. And it’s been one of the most remarkable and surprisingly influential results about word embeddings. And I think has helped the whole community get better intuitions about what word embeddings are doing. So let’s formalize how you can turn this into an algorithm. In pictures, the word embeddings live in maybe a 300 dimensional space. And so the word man is represented as a point in the space, and the word woman is represented as a point in the space. And the word king is represented as another point, and the word queen is represented as another point. And what we pointed out really on the last slide is that the vector difference between man and woman is very similar to the vector difference between king and queen. And this arrow I just drew is really the vector that represents a difference in gender. And remember, these are points we’re plotting in a 300 dimensional space. So in order to carry out this kind of analogical reasoning to figure out, man is to woman is king is to what, what you can do is try to find the word w, So that, This equation holds true, so you want there to be, A high degree of a similarity, between I’m going to use s, And so what you want is to find the word w that maximizes the similarity between, e w compared to e king- e man + e woman Right, so what I did is, I took this e question mark, and replaced that with ew, and then brought ew to just one side of the equation. And then the other three terms to the right hand side of this equation. So we have some appropriate similarity function for measuring how similar is the embedding of some word w to this quantity of the right. Then finding the word that maximizes the similarity should hopefully let you pick out the word queen. And the remarkable thing is, this actually works. If you learn a set of word embeddings and find a word w that maximizes this type of similarity, you can actually get the exact right answer. Depending on the details of the task, but if you look at research papers, it’s not uncommon for research papers to report anywhere from, say, 30% to 75% accuracy on analogy using tasks like these. Where you count an anology attempt as correct only if it guesses the exact word right. So only if, in this case, it picks out the word queen. Before moving on, I just want to clarify what this plot on the left is. Previously, we talked about using algorithms like t-SNE to visualize words. What t-SNE does is, it takes 300-D data, and it maps it in a very non-linear way to a 2D space. And so the mapping that t-SNE learns, this is a very complicated and very non-linear mapping. So after the t-SNE mapping, you should not expect these types of parallelogram relationships, like the one we saw on the left, to hold true. And it’s really in this original 300 dimensional space that you can more reliably count on these types of parallelogram relationships in analogy pairs to hold true. And it may hold true after a mapping through t-SNE, but in most cases, because of t-SNE’s non-linear mapping, you should not count on that. And many of the parallelogram analogy relationships will be broken by t-SNE. Now, before moving on, let me just quickly describe the similarity function that is most commonly used. So the most commonly used similarity function is called cosine similarity. So this is the equation we had from the previous slide. So in cosine similarity, you define the similarity between two vectors u and v as u transpose v divided by the lengths by the Euclidean lengths. So ignoring the denominator for now, this is basically the inner product between u and v. And so if u and v are very similar, their inner product will tend to be large. And this is called cosine similarity because this is actually the cosine of the angle between the two vectors, u and v. So that’s the angle phi, so this formula is actually the cosine between them. And so you remember from calculus that if this phi, then the cosine of phi looks like this. So if the angle between them is 0, then the cosine similarity is equal to 1. And if their angle is 90 degrees, the cosine similarity is 0. And then if they’re 180 degrees, or pointing in completely opposite directions, it ends up being -1. So that’s where the term cosine similarity comes from, and it works quite well for these analogy reasoning tasks. If you want, you can also use square distance or Euclidian distance, u-v squared. Technically, this would be a measure of dissimilarity rather than a measure of similarity. So we need to take the negative of this, and this will work okay as well. Although I see cosine similarity being used a bit more often. And the main difference between these is how it normalizes the lengths of the vectors u and v. So one of the remarkable results about word embeddings is the generality of analogy relationships they can learn. So for example, it can learn that man is to woman as boy is to girl, because the vector difference between man and woman, similar to king and queen and boy and girl, is primarily just the gender. It can learn that Ottawa, which is the capital of Canada, that Ottawa is to Canada as Nairobi is to Kenya. So that’s the city capital is to the name of the country. It can learn that big is to bigger as tall is to taller, and it can learn things like that. Yen is to Japan, since yen is the currency of Japan, as ruble is to Russia. And all of these things can be learned just by running a word embedding learning algorithm on the large text corpus. It can spot all of these patterns by itself, just by running from very large bodies of text. So in this video, you saw how word embeddings can be used for analogy reasoning. And while you might not be trying to build an analogy reasoning system yourself as an application, this I hope conveys some intuition about the types of feature-like representations that these representations can learn. And you also saw how cosine similarity can be a way to measure the similarity between two different word embeddings. Now, we talked a lot about properties of these embeddings and how you can use them. Next, let’s talk about how you’d actually learn these word embeddings, let’s go on to the next video. 04_embedding-matrixLet’s start to formalize the problem of learning a good word embedding. When you implement an algorithm to learn a word embedding, what you end up learning is an embedding matrix. Let’s take a look at what I means. Let’s say, as usual we’re using our 10,000-word vocabulary. So, the vocabulary has A, Aaron, Orange, Zulu, maybe also unknown word as a token. What we’re going to do is learn embedding matrix E, which is going to be a 300 dimensional by 10,000 dimensional matrix, if you have 10,000 words vocabulary or maybe 10,001 is our word token, there’s one extra token. And the columns of this matrix would be the different embeddings for the 10,000 different words you have in your vocabulary. So, Orange was word number 6257 in our vocabulary of 10,000 words. So, one piece of notation we’ll use is that 06257 was the one-hot vector with zeros everywhere and a one in position 6257. And so, this will be a 10,000-dimensional vector with a one in just one position. So, this isn’t quite a drawn scale. Yes, this should be as tall as the embedding matrix on the left is wide. And if the embedding matrix is called capital E then notice that if you take E and multiply it by just one-hot vector by 0 of 6257, then this will be a 300-dimensional vector. So, E is 300 by 10,000 and 0 is 10,000 by 1. So, the product will be 300 by 1, so with 300-dimensional vector and notice that to compute the first element of this vector, of this 300-dimensional vector, what you do is you will multiply the first row of the matrix E with this. But all of these elements are zero except for element 6257 and so you end up with zero times this, zero times this, zero times this, and so on. And then, 1 times whatever this is, and zero times this, zero times this, zero times and so on. And so, you end up with the first element as whatever is that elements up there, under the Orange column. And then, for the second element of this 300-dimensional vector we’re computing, you would take the vector 0657 and multiply it by the second row with the matrix E. So again, you have zero times this, plus zero times this, plus zero times all of these are the elements and then one times this, and then zero times everything else and add that together. So you end up with this and so on as you go down the rest of this column. So, that’s why the embedding matrix E times this one-hot vector here winds up selecting out this 300-dimensional column corresponding to the word Orange. So, this is going to be equal to E 6257 which is the notation we’re going to use to represent the embedding vector that 300 by one dimensional vector for the word Orange. And more generally, E for a specific word W, this is going to be embedding for a word W. And more generally, E times O substitute J, one-hot vector with one that position J, this is going to be E_J and that’s going to be the embedding for word J in the vocabulary. So, the thing to remember from this slide is that our goal will be to learn an embedding matrix E and what you see in the next video is you initialize E randomly and you’re straight in the sense to learn all the parameters of this 300 by 10,000 dimensional matrix and E times this one-hot vector gives you the embedding vector. Now just one note, when we’re writing the equation, it’ll be convenient to write this type of notation where you take the matrix E and multiply it by the one-hot vector O. But if when you’re implementing this, it is not efficient to actually implement this as a mass matrix vector multiplication because the one-hot vectors, now this is a relatively high dimensional vector and most of these elements are zero. So, it’s actually not efficient to use a matrix vector multiplication to implement this because if we multiply a whole bunch of things by zeros and so the practice, you would actually use a specialized function to just look up a column of the Matrix E rather than do this with the matrix multiplication. But writing of the map, it is just convenient to write it out this way. So, in Keras’s for example there is a embedding layer and we use the embedding layer then it more efficiently just pulls out the column you want from the embedding matrix rather than does it with a much slower matrix vector multiplication. So, in this video you saw the notations were used to describe algorithms to learning these embeddings and the key terminology is this matrix capital E which contain all the embeddings for the words of the vocabulary. In the next video, we’ll start to talk about specific algorithms for learning this matrix E. Let’s go onto the next video. 02_learning-word-embeddings-word2vec-glove01_learning-word-embeddingsIn this video, you’ll start to learn some concrete algorithms for learning word embeddings. In the history of deep learning as applied to learning word embeddings, people actually started off with relatively complex algorithms. And then over time, researchers discovered they can use simpler and simpler and simpler algorithms and still get very good results especially for a large dataset. But what happened is, some of the algorithms that are most popular today, they are so simple that if I present them first, it might seem almost a little bit magical, how can something this simple work? So, what I’m going to do is start off with some of the slightly more complex algorithms because I think it’s actually easier to develop intuition about why they should work, and then we’ll move on to simplify these algorithms and show you some of the simple algorithms that also give very good results. So, let’s get started. Let’s say you’re building a language model and you do it with a neural network. So, during training, you might want your neural network to do something like input, I want a glass of orange, and then predict the next word in the sequence. And below each of these words, I have also written down the index in the vocabulary of the different words. So it turns out that building a neural language model is the small way to learn a set of embeddings. And the ideas I present on this slide were due to Yoshua Bengio, Rejean Ducharme, Pascals Vincent, and Christian Jauvin. So, here’s how you can build a neural network to predict the next word in the sequence. Let me take the list of words, I want a glass of orange, and let’s start with the first word I. So I’m going to construct one add vector corresponding to the word I. So there’s a one add vector with a one in position, 4343. So this is going to be 10,000 dimensional vector. And what we’re going to do is then have a matrix of parameters E, and take E times O to get an embedding vector e4343, and this step really means that e4343 is obtained by the matrix E times the one add vector 43. And then we’ll do the same for all of the other words. So the word want, is where 9665 one add vector, multiply by E to get the embedding vector. And similarly, for all the other words. A, is a first word in dictionary, alphabetic comes first, so there is O one, gets this E one. And similarly, for the other words in this phrase. So now you have a bunch of three dimensional embedding, so each of this is a 300 dimensional embedding vector. And what we can do, is fill all of them into a neural network. So here is the neural network layer. And then this neural network feeds to a softmax, which has it’s own parameters as well. And a softmax classifies among the 10,000 possible outputs in the vocab for those final word we’re trying to predict. And so, if in the training slide we saw the word juice then, the target for the softmax in training repeat that it should predict the other word juice was what came after this. So this hidden name here will have his own parameters. So have some, I’m going to call this W1 and there’s also B1. The softmax there was this own parameters W2, B2, and they’re using 300 dimensional word embeddings, then here we have six words. So, this would be six times 300. So this layer or this input will be a 1,800 dimensional vector obtained by taking your six embedding vectors and stacking them together. Well, what’s actually more commonly done is to have a fixed historical window. So for example, you might decide that you always want to predict the next word given say the previous four words, where four here is a hyperparameter of the algorithm. So this is how you adjust to either very long or very short sentences or you decide to always just look at the previous four words, so you say, I will still use those four words. And so, let’s just get rid of these. And so, if you’re always using a four word history, this means that your neural network will input a 1,200 dimensional feature vector, go into this layer, then have a softmax and try to predict the output. And again, variety of choices. And using a fixed history, just means that you can deal with even arbitrarily long sentences because the input sizes are always fixed. So, the parameters of this model will be this matrix E, and use the same matrix E for all the words. So you don’t have different matrices for different positions in the proceedings four words, is the same matrix E. And then, these weights are also parameters of the algorithm and you can use backprop to perform gradient descent to maximize the likelihood of your training set to just repeatedly predict given four words in a sequence, what is the next word in your text corpus? And it turns out that this algorithm we’ll learn pretty decent word embeddings. And the reason is, if you remember our orange juice, apple juice example, is in the algorithm’s incentive to learn pretty similar word embeddings for orange and apple because doing so allows it to fit the training set better because it’s going to see orange juice sometimes, or see apple juice sometimes, and so, if you have only a 300 dimensional feature vector to represent all of these words, the algorithm will find that it fits the training set fast. If apples, oranges, and grapes, and pears, and so on and maybe also durians which is a very rare fruit and that with similar feature vectors. So, this is one of the earlier and pretty successful algorithms for learning word embeddings, for learning this matrix E. But now let’s generalize this algorithm and see how we can derive even simpler algorithms. So, I want to illustrate the other algorithms using a more complex sentence as our example. Let’s say that in your training set, you have this longer sentence, I want a glass of orange juice to go along with my cereal. So, what we saw on the last slide was that the job of the algorithm was to predict some word juice, which we are going to call the target words, and it was given some context which was the last four words. And so, if your goal is to learn a embedding of researchers I’ve experimented with many different types of context. If it goes to build a language model then is natural for the context to be a few words right before the target word. But if your goal is into learn the language model per se, then you can choose other contexts. For example, you can pose a learning problem where the context is the four words on the left and right. So, you can take the four words on the left and right as the context, and what that means is that we’re posing a learning problem where the algorithm is given four words on the left. So, a glass of orange, and four words on the right, to go along with, and this has to predict the word in the middle. And posing a learning problem like this where you have the embeddings of the left four words and the right four words feed into a neural network, similar to what you saw in the previous slide, to try to predict the word in the middle, try to put it target word in the middle, this can also be used to learn word embeddings. Or if you want to use a simpler context, maybe you’ll just use the last one word. So given just the word orange, what comes after orange? So this will be different learning problem where you tell it one word, orange, and will say well, what do you think is the next word. And you can construct a neural network that just fits in the word, the one previous word or the embedding of the one previous word to a neural network as you try to predict the next word. Or, one thing that works surprisingly well is to take a nearby one word. Some might tell you that, well, take the word glass, is somewhere close by. Some might say, I saw the word glass and then there’s another words somewhere close to glass, what do you think that word is? So, that’ll be using nearby one word as the context. And we’ll formalize this in the next video but this is the idea of a Skip-Gram model, and just an example of a simpler algorithm where the context is now much simpler, is just one word rather than four words, but this works remarkably well. So what researchers found was that if you really want to build a language model, it’s natural to use the last few words as a context. But if your main goal is really to learn a word embedding, then you can use all of these other contexts and they will result in very meaningful work embeddings as well. I will formalize the details of this in the next video where we talk about the Word2Vec model. To summarize, in this video you saw how the language modeling problem which causes the pose of machines learning problem where you input the context like the last four words and predicts some target words, how posing that problem allows you to learn input word embedding. In the next video, you’ll see how using even simpler context and even simpler learning algorithms to mark from context to target word, can also allow you to learn a good word embedding. Let’s go on to the next video where we’ll discuss the Walter VEC. 02_word2vecIn the last video, you saw how you can learn a neural language model in order to get good word embeddings. In this video, you see the Word2Vec algorithm which is simple and comfortably more efficient way to learn this types of embeddings. Lets take a look. Most of the ideas I’ll present in this video are due to Tomas Mikolov, Kai Chen, Greg Corrado, and Jeff Dean. Let’s say you’re given this sentence in your training set. In the skip-gram model, what we’re going to do is come up with a few context to target pairs to create our supervised learning problem. So rather than having the context be always the last four words or the last end words immediately before the target word, what I’m going to do is, say, randomly pick a word to be the context word. And let’s say we chose the word orange. And what we’re going to do is randomly pick another word within some window. Say plus minus five words or plus minus ten words of the context word and we choose that to be target word. So maybe just by chance you might pick juice to be a target word, that’s just one word later. Or you might choose two words before. So you have another pair where the target could be glass or, Maybe just by chance you choose the word my as the target. And so we’ll set up a supervised learning problem where given the context word, you’re asked to predict what is a randomly chosen word within say, a plus minus ten word window, or plus minus five or ten word window of that input context word. And obviously, this is not a very easy learning problem, because within plus minus 10 words of the word orange, it could be a lot of different words. But a goal of setting up this supervised learning problem, isn’t to do well on the supervised learning problem per se, it is that we want to use this learning problem to learn good word embeddings. So, here are the details of the model. Let’s say that we’ll continue to our vocab of 10,000 words. And some have been on vocab sizes that exceeds a million words. But the basic supervised learning problem we’re going to solve is that we want to learn the mapping from some Context c, such as the word orange to some target, which we will call t, which might be the word juice or the word glass or the word my, if we use the example from the previous slide. So in our vocabulary, orange is word 6257, and the word juice is the word 4834 in our vocab of 10,000 words. And so that’s the input x that you want to learn to map to that open y. So to represent the input such as the word orange, you can start out with some one hot vector which is going to be write as $o_c$, so there’s a one hot vector for the context words. And then similar to what you saw on the last video you can take the embedding matrix E, multiply E by the vector $o_c$, and this gives you your embedding vector for the input context word, so here $e_c$ is equal to capital E times that one hot vector. Then in this new network that we formed we’re going to take this vector $e_c$ and feed it to a softmax unit. So I’ve been drawing softmax unit as a node in a neural network. That’s not an o, that’s a softmax unit. And then there’s a drop in the softmax unit to output $\hat{y}$. So to write out this model in detail. This is the model, the softmax model, probability of different tanka words given the input context word as e to the e, theta t transpose,$e_c$. Divided by some over all words, so we’re going to say, sum from J equals one to all 10,000 words of e to the theta j transposed $e_c$. So here theta T is the parameter associated with, I’ll put t, but really there’s a chance of a particular word, t, being the label. So I’ve left off the biased term to solve mass but we could include that too if we wish. And then finally the loss function for softmax will be the usual. So we use y to represent the target word. And we use a one-hot representation for y hat and y here. Then the lost would be The negative log liklihood, so sum from i equals 1 to 10,000 of $y_ilog(\hat{y}_i)$. So that’s a usual loss for softmax where we’re representing the target y as a one hot vector. So this would be a one hot vector with just 1 1 and the rest zeros. And if the target word is juice, then it’d be element 4834 from up here. That is equal to 1 and the rest will be equal to 0. And similarly Y hat will be a 10,000 dimensional vector output by the softmax unit with probabilities for all 10,000 possible targets words. So to summarize, this is the overall little model, little neural network with basically looking up the embedding and then just a soft max unit. And the matrix E will have a lot of parameters, so the matrix E has parameters corresponding to all of these embedding vectors, $e_c$. And then the softmax unit also has parameters that gives the theta T parameters but if you optimize this loss function with respect to the all of these parameters, you actually get a pretty good set of embedding vectors. So this is called the skip-gram model because is taking as input one word like orange and then tr$y_i$ng to predict some words skipping a few words from the left or the right side. To predict what comes little bit before little bit after the context words. Now, it turns out there are a couple problems with using this algorithm. And the primary problem is computational speed. In particular, for the softmax model, every time you want to evaluate this probability, you need to carry out a sum over all 10,000 words in your vocabulary. And maybe 10,000 isn’t too bad, but if you’re using a vocabulary of size 100,000 or a 1,000,000, it gets really slow to sum up over this denominator every single time. And, in fact, 10,000 is actually already that will be quite slow, but it makes even harder to scale to larger vocabularies. So there are a few solutions to this, one which you see in the literature is to use a hierarchical softmax classifier. And what that means is, instead of trying to categorize something into all 10,000 carries on one go. Imagine if you have one classifier, it tells you is the target word in the first 5,000 words in the vocabulary? Or is in the second 5,000 words in the vocabulary? And lets say this binary cost that it tells you this is in the first 5,000 words, think of second class to tell you that this in the first 2,500 words of vocab or in the second 2,500 words vocab and so on. Until eventually you get down to classify exactly what word it is, so that the leaf of this tree, and so having a tree of classifiers like this, means that each of the retriever nodes of the tree can be just a binding classifier. And so you don’t need to sum over all 10,000 words or else it will capsize in order to make a single classification. In fact, the computational classifying tree like this scales like log of the vocab size rather than linear in vocab size. So this is called a hierarchical softmax classifier. I should mention in practice, the hierarchical softmax classifier doesn’t use a perfectly balanced tree or this perfectly symmetric tree, with equal numbers of words on the left and right sides of each branch. In practice, the hierarchical softmax classifier can be developed so that the common words tend to be on top, whereas the less common words like durian can be buried much deeper in the tree. Because you see the more common words more often, and so you might need only a few traversals to get to common words like the and of. Whereas you see less frequent words like durian much less often, so it says okay that are buried deep in the tree because you don’t need to go that deep. So there are various heuristics for building the tree how you used to build the hierarchical software spire. So this is one idea you see in the literature, the speeding up the softmax classification. But I won’t spend too much more time. And you can read more details of this on the paper that I referenced by Thomas and others, on the first slide. But I won’t spend too much more time on this. Because in the next video, where she talk about a different method, called nectar sampling, which I think is even simpler. And also works really well for speeding up the softmax classifier and the problem of needing the sum over the entire cap size in the denominator. So you see more of that in the next video. But before moving on, one quick Topic I want you to understand is how to sample the context C. So once you sample the context C, the target T can be sampled within, say, a plus minus ten word window of the context C, but how do you choose the context C? One thing you could do is just sample uniformly, at random, from your training corpus. When we do that, you find that there are some words like the, of, a, and, to and so on that appear extremely frequently. And so, if you do that, you find that in your context to target mapping pairs just get these these types of words extremely frequently, whereas there are other words like orange, apple, and also durian that don’t appear that often. And maybe you don’t want your training site to be dominated by these extremely frequently or current words, because then you spend almost all the effort updating $e_c$, for those frequently occurring words. But you want to make sure that you spend some time updating the embedding, even for these less common words like e durian. So in practice the distribution of words $P(c)$ isn’t taken just entirely uniformly at random for the training set purpose, but instead there are different heuristics that you could use in order to balance out something from the common words together with the less common words. So that’s it for the Word2Vec skip-gram model. If you read the original paper by that I referenced earlier, you find that that paper actually had two versions of this Word2Vec model, the skip gram was one. And the other one is called the CBow, the continuous backwards model, which takes the surrounding contexts from middle word, and uses the surrounding words to try to predict the middle word, and that algorithm also works, it has some advantages and disadvantages. But the key problem with this algorithm with the skip-gram model as presented so far is that the softmax step is very expensive to calculate because needing to sum over your entire vocabulary size into the denominator of the soft packs. In the next video I show you an algorithm that modifies the training objective that makes it run much more efficiently therefore lets you apply this in a much bigger fitting set as well and therefore learn much better word embeddings. Lets go onto the next video. 03_negative-samplingIn the last video, you saw how the Skip-Gram model allows you to construct a supervised learning task. So we map from context to target and how that allows you to learn a useful word embedding. But the downside of that was the Softmax objective was slow to compute. In this video, you’ll see a modified learning problem called negative sampling that allows you to do something similar to the Skip-Gram model you saw just now, but with a much more efficient learning algorithm. Let’s see how you can do this. Most of the ideas presented in this video are due to Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeff Dean. So what we’re going to do in this algorithm is create a new supervised learning problem. And the problem is, given a pair of words like orange and juice, we’re going to predict, is this a context-target pair? So in this example, orange juice was a positive example. And how about orange and king? Well, that’s a negative example, so I’m going to write 0 for the target. So what we’re going to do is we’re actually going to sample a context and a target word. So in this case, we have orange and juice and we’ll associate that with a label of 1, so just put words in the middle. And then having generated a positive example, so the positive example is generated exactly how we generated it in the previous videos. Sample a context word, look around a window of say, plus-minus ten words and pick a target word. So that’s how you generate the first row of this table with orange, juice, 1. And then to generate a negative example, you’re going to take the same context word and then just pick a word at random from the dictionary. So in this case, I chose the word king at random and we will label that as 0. And then let’s take orange and let’s pick another random word from the dictionary. Under the assumption that if we pick a random word, it probably won’t be associated with the word orange, so orange, book, 0. And let’s pick a few others, orange, maybe just by chance, we’ll pick the 0 and then orange. And then orange, and maybe just by chance, we’ll pick the word of and we’ll put a 0 there. And notice that all of these are labeled as 0 even though the word of actually appears next to orange as well. So to summarize, the way we generated this data set is, we’ll pick a context word and then pick a target word and that is the first row of this table. That gives us a positive example. So context, target, and then give that a label of 1. And then what we’ll do is for some number of times say, k times, we’re going to take the same context word and then pick random words from the dictionary, king, book, the, of, whatever comes out at random from the dictionary and label all those 0, and those will be our negative examples. And it’s okay if just by chance, one of those words we picked at random from the dictionary happens to appear in the window, in a plus-minus ten word window say, next to the context word, orange. Then we’re going to create a supervised learning problem where the learning algorithm inputs x, inputs this pair of words, and it has to predict the target label to predict the output y. So the problem is really given a pair of words like orange and juice, do you think they appear together? Do you think I got these two words by sampling two words close to each other? Or do you think I got them as one word from the text and one word chosen at random from the dictionary? It’s really to try to distinguish between these two types of distributions from which you might sample a pair of words. So this is how you generate the training set. How do you choose k, Mikolov et al, recommend that maybe k is 5 to 20 for smaller data sets. And if you have a very large data set, then chose k to be smaller. So k equals 2 to 5 for larger data sets, and large values of k for smaller data sets. Okay, and in this example, I’ll just use k = 4. Next, let’s describe the supervised learning model for learning a mapping from x to y. So here was the Softmax model you saw from the previous video. And here is the training set we got from the previous slide where again, this is going to be the new input x and this is going to be the value of y you’re trying to predict. So to define the model, I’m going to use this to denote, this was c for the context word, this to denote the possible target word, t, and this, I’ll use y to denote 0, 1, this is a context target pair. So what we’re going to do is define a logistic regression model. Say, that the chance of y = 1, given the input c, t pair, we’re going to model this as basically a regression model, but the specific formula we’ll use s sigma applied to theta transpose, theta t transpose, e c. So the parameters are similar as before, you have one parameter vector theta for each possible target word. And a separate parameter vector, really the embedding vector, for each possible context word. And we’re going to use this formula to estimate the probability that y is equal to 1. So if you have k examples here, then you can think of this as having a k to 1 ratio of negative to positive examples. So for every positive examples, you have k negative examples with which to train this logistic regression-like model. And so to draw this as a neural network, if the input word is orange, Which is word 6257, then what you do is, you input the one hop vector passing through e, do the multiplication to get the embedding vector 6257. And then what you have is really 10,000 possible logistic regression classification problems. Where one of these will be the classifier corresponding to, well, is the target word juice or not? And then there will be other words, for example, there might be ones somewhere down here which is predicting, is the word king or not and so on, for these possible words in your vocabulary. So think of this as having 10,000 binary logistic regression classifiers, but instead of training all 10,000 of them on every iteration, we’re only going to train five of them. We’re going to train the one responding to the actual target word we got and then train four randomly chosen negative examples. And this is for the case where k is equal to 4. So instead of having one giant 10,000 way Softmax, which is very expensive to compute, we’ve instead turned it into 10,000 binary classification problems, each of which is quite cheap to compute. And on every iteration, we’re only going to train five of them or more generally, k + 1 of them, of k negative examples and one positive examples. And this is why the computation cost of this algorithm is much lower because you’re updating k + 1, let’s just say units, k + 1 binary classification problems. Which is relatively cheap to do on every iteration rather than updating a 10,000 way Softmax classifier. So you get to play with this algorithm in the problem exercise for this week as well. So this technique is called negative sampling because what you’re doing is, you have a positive example, the orange and then juice. And then you will go and deliberately generate a bunch of negative examples, negative samplings, hence, the name negative sampling, with which to train four more of these binary classifiers. And on every iteration, you choose four different random negative words with which to train your algorithm on. Now, before wrapping up, one more important detail with this algorithm is, how do you choose the negative examples? So after having chosen the context word orange, how do you sample these words to generate the negative examples? So one thing you could do is sample the words in the middle, the candidate target words. One thing you could do is sample it according to the empirical frequency of words in your corpus. So just sample it according to how often different words appears. But the problem with that is that you end up with a very high representation of words like the, of, and, and so on. One other extreme would be to say, you use 1 over the vocab size, sample the negative examples uniformly at random, but that’s also very non-representative of the distribution of English words. So the authors, Mikolov et al, reported that empirically, what they found to work best was to take this heuristic value, which is a little bit in between the two extremes of sampling from the empirical frequencies, meaning from whatever’s the observed distribution in English text to the uniform distribution. And what they did was they sampled proportional to their frequency of a word to the power of three-fourths. So if f of wi is the observed frequency of a particular word in the English language or in your training set corpus, then by taking it to the power of three-fourths, this is somewhere in-between the extreme of taking uniform distribution. And the other extreme of just taking whatever was the observed distribution in your training set. And so I’m not sure this is very theoretically justified, but multiple researchers are now using this heuristic, and it seems to work decently well. So to summarize, you’ve seen how you can learn word vectors in a Softmax classier, but it’s very computationally expensive. And in this video, you saw how by changing that to a bunch of binary classification problems, you can very efficiently learn words vectors. And if you run this algorithm, you will be able to learn pretty good word vectors. Now of course, as is the case in other areas of deep learning as well, there are open source implementations. And there are also pre-trained word vectors that others have trained and released online under permissive licenses. And so if you want to get going quickly on a NLP problem, it’d be reasonable to download someone else’s word vectors and use that as a starting point. So that’s it for the Skip-Gram model. In the next video, I want to share with you yet another version of a word embedding learning algorithm that is maybe even simpler than what you’ve seen so far. So in the next video, let’s learn about the Glove algorithm. 04_glove-word-vectorsYou learn about several algorithms for computing words embeddings. Another algorithm that has some momentum in the NLP community is the GloVe algorithm. This is not used as much as the Word2Vec or the skip-gram models, but it has some enthusiasts. Because I think, in part of its simplicity. Let’s take a look. The GloVe algorithm was created by Jeffrey Pennington, Richard Socher, and Chris Manning. And GloVe stands for global vectors for word representation. So, previously, we were sampling pairs of words, context and target words, by picking two words that appear in close proximity to each other in our text corpus. So, what the GloVe algorithm does is, it starts off just by making that explicit. So, let’s say $X_{ij}$ be the number of times that a word i appears in the context of j. And so, here i and j play the role of t and c, so you can think of $X_{ij}$ as being x subscript tc. But, you can go through your training corpus and just count up how many words does a word i appear in the context of a different word j. How many times does the word t appear in context of different words c. And depending on the definition of context and target words, you might have that $X_{ij}$ equals $X_{ji}$. And in fact, if you’re defining context and target in terms of whether or not they appear within plus minus 10 words of each other, then it would be a symmetric relationship. Although, if your choice of context was that, the context is always the word immediately before the target word, then $X_{ij}$ and $X_{ji}$ may not be symmetric like this. But for the purposes of the GloVe algorithm, we can define context and target as whether or not the two words appear in close proximity, say within plus or minus 10 words of each other. So, $X_{ij}$ is a count that captures how often do words i and j appear with each other, or close to each other. So what the GloVe model does is, it optimizes the following. We’re going to minimize the difference between theta i transpose e_j minus log of $X_{ij}$ squared. I’m going to fill in some of the parts of this equation. But again, think of i and j as playing the role of t and c. So this is a bit like what you saw previously with theta t transpose e_c. And what you want is, for this to tell you how related are those two words? How related are words t and c? How related are words i and j as measured by how often they occur with each other? Which is affected by this $X_{ij}$. And so, what we’re going to do is, solve for parameters theta and e using gradient descent to minimize the sum over i equals one to 10,000 sum over j from one to 10,000 of this difference. So you just want to learn vectors, so that their end product is a good predictor for how often the two words occur together. Now, just some additional details, if $X_{ij}$ is equal to zero, then log of 0 is undefined, is negative infinity. And so, what we do is, we want sum over the terms where $X_{ij}$ is equal to zero. And so, what we’re going to do is, add an extra weighting term. So this is going to be a weighting term, and this will be equal to zero if $X_{ij}$ is equal to zero. And we’re going to use a convention that zero log zero is equal to zero. So what this means is, that if $X_{ij}$ is equal to zero, just don’t bother to sum over that $X_{ij}$ pair. So then this log of zero term is not relevant. So this means the sum is sum only over the pairs of words that have co-occurred at least once in that context-target relationship. The other thing that $F(X_{ij})$ does is that, there are some words they just appear very often in the English language like, this, is, of, a, and so on. Sometimes we used to call them stop words but there’s really a continuum between frequent and infrequent words. And then there are also some infrequent words like durion, which you actually still want to take into account, but not as frequently as the more common words. And so, the weighting factor can be a function that gives a meaningful amount of computation, even to the less frequent words like durion, and gives more weight but not an unduly large amount of weight to words like, this, is, of, a, which just appear lost in language. And so, there are various heuristics for choosing this weighting function F that need or gives these words too much weight nor gives the infrequent words too little weight. You can take a look at the GloVe paper, they are referenced in the previous slide, if you want the details of how F can be chosen to be a heuristic to accomplish this. And then, finally, one funny thing about this algorithm is that the roles of theta and e are now completely symmetric. So, theta i and e_j are symmetric in that, if you look at the math, they play pretty much the same role and you could reverse them or sort them around, and they actually end up with the same optimization objective. One way to train the algorithm is to initialize theta and e both uniformly around gradient descent to minimize its objective, and then when you’re done for every word, to then take the average. For a given words w, you can have e final to be equal to the embedding that was trained through this gradient descent procedure, plus theta trained through this gradient descent procedure divided by two, because theta and e in this particular formulation play symmetric roles unlike the earlier models we saw in the previous videos, where theta and e actually play different roles and couldn’t just be averaged like that. That’s it for the GloVe algorithm. I think one confusing part of this algorithm is, if you look at this equation, it seems almost too simple. How could it be that just minimizing a square cost function like this allows you to learn meaningful word embeddings? But it turns out that this works. And the way that the inventors end up with this algorithm was, they were building on the history of much more complicated algorithms like the newer language model, and then later, there came the Word2Vec skip-gram model, and then this came later. And we really hope to simplify all of the earlier algorithms. Before concluding our discussion of algorithms concerning word embeddings, there’s one more property of them that we should discuss briefly. Which is that? We started off with this featurization view as the motivation for learning word vectors. We said, “Well, maybe the first component of the embedding vector to represent gender, the second component to represent how royal it is, then the age and then whether it’s a food, and so on.” But when you learn a word embedding using one of the algorithms that we’ve seen, such as the GloVe algorithm that we just saw on the previous slide, what happens is, you cannot guarantee that the individual components of the embeddings are interpretable. Why is that? Well, let’s say that there is some space where the first axis is gender and the second axis is royal. What you can do is guarantee that the first axis of the embedding vector is aligned with this axis of meaning, of gender, royal, age and food. And in particular, the learning algorithm might choose this to be axis of the first dimension. So, given maybe a context of words, so the first dimension might be this axis and the second dimension might be this. Or it might not even be orthogonal, maybe it’ll be a second non-orthogonal axis, could be the second component of the word embeddings you actually learn. And when we see this, if you have a subsequent understanding of linear algebra is that, if there was some invertible matrix A, then this could just as easily be replaced with A times theta i transpose A inverse transpose e_j. Because we expand this out, this is equal to theta i transpose A transpose A inverse transpose times e_j. And so, the middle term cancels out and we’re left with theta i transpose e_j, same as before. Don’t worry if you didn’t follow the linear algebra, but that’s a brief proof that shows that with an algorithm like this, you can’t guarantee that the axis used to represent the features will be well-aligned with what might be easily humanly interpretable axis. In particular, the first feature might be a combination of gender, and royal, and age, and food, and cost, and size, is it a noun or an action verb, and all the other features. It’s very difficult to look at individual components, individual rows of the embedding matrix and assign the human interpretation to that. But despite this type of linear transformation, the parallelogram map that we worked out when we were describing analogies, that still works. And so, despite this potentially arbitrary linear transformation of the features, you end up learning the parallelogram map for figure analogies still works. So, that’s it for learning word embeddings. You’ve now seen a variety of algorithms for learning these word embeddings and you get to play them more in this week’s programming exercise as well. Next, I’d like to show you how you can use these algorithms to carry out sentiment classification. Let’s go onto the next video. 03_applications-using-word-embeddings01_sentiment-classificationSentiment classification is the task of looking at a piece of text and telling if someone likes or dislikes the thing they’re talking about. It is one of the most important building blocks in NLP and is used in many applications. One of the challenges of sentiment classification is you might not have a huge label training set for it. But with word embeddings, you’re able to build good sentiment classifiers even with only modest-size label training sets. Let’s see how you can do that. So here’s an example of a sentiment classification problem. The input X is a piece of text and the output Y that you want to predict is what is the sentiment, such as the star rating of, let’s say, a restaurant review. So if someone says, “The dessert is excellent” and they give it a four-star review, “Service was quite slow” two-star review, “Good for a quick meal but nothing special” three-star review. And this is a pretty harsh review, “Completely lacking in good taste, good service, and good ambiance.” That’s a one-star review. So if you can train a system to map from X or Y based on a label data set like this, then you could use it to monitor comments that people are saying about maybe a restaurant that you run. So people might also post messages about your restaurant on social media, on Twitter, or Facebook, or Instagram, or other forms of social media. And if you have a sentiment classifier, they can look just a piece of text and figure out how positive or negative is the sentiment of the poster toward your restaurant. Then you can also be able to keep track of whether or not there are any problems or if your restaurant is getting better or worse over time. So one of the challenges of sentiment classification is you might not have a huge label data set. So for sentimental classification task, training sets with maybe anywhere from 10,000 to maybe 100,000 words would not be uncommon. Sometimes even smaller than 10,000 words and word embeddings that you can take can help you to much better understand especially when you have a small training set. So here’s what you can do. We’ll go for a couple different algorithms in this video. Here’s a simple sentiment classification model. You can take a sentence like “dessert is excellent” and look up those words in your dictionary. We use a 10,000-word dictionary as usual. And let’s build a classifier to map it to the output Y that this was four stars. So given these four words, as usual, we can take these four words and look up the one-hot vector. So there’s 0 8 9 2 8 which is a one-hot vector multiplied by the embedding matrix E, which can learn from a much larger text corpus. It can learn in embedding from, say, a billion words or a hundred billion words, and use that to extract out the embedding vector for the word “the”, and then do the same for “dessert”, do the same for “is” and do the same for “excellent”. And if this was trained on a very large data set, like a hundred billion words, then this allows you to take a lot of knowledge even from infrequent words and apply them to your problem, even words that weren’t in your labeled training set. Now here’s one way to build a classifier, which is that you can take these vectors, let’s say these are 300-dimensional vectors, and you could then just sum or average them. And I’m just going to put a bigger average operator here and you could use sum or average. And this gives you a 300-dimensional feature vector that you then pass to a soft-max classifier which then outputs Y-hat. And so the softmax can output what are the probabilities of the five possible outcomes from one-star up to five-star. So this will be assortment of the five possible outcomes to predict what is Y. So notice that by using the average operation here, this particular algorithm works for reviews that are short or long because even if a review that is 100 words long, you can just sum or average all the feature vectors for all hundred words and so that gives you a representation, a 300-dimensional feature representation, that you can then pass into your sentiment classifier. So this average will work decently well. And what it does is it really averages the meanings of all the words or sums the meaning of all the words in your example. And this will work to [inaudible]. So one of the problems with this algorithm is it ignores word order. In particular, this is a very negative review, “Completely lacking in good taste, good service, and good ambiance”. But the word good appears a lot. This is a lot. Good, good, good. So if you use an algorithm like this that ignores word order and just sums or averages all of the embeddings for the different words, then you end up having a lot of the representation of good in your final feature vector and your classifier will probably think this is a good review even though this is actually very harsh. This is a one-star review. So here’s a more sophisticated model which is that, instead of just summing all of your word embeddings, you can instead use a RNN for sentiment classification. So here’s what you can do. You can take that review, “Completely lacking in good taste, good service, and good ambiance”, and find for each of them, the one-hot vector. And so I’m going to just skip the one-hot vector representation but take the one-hot vectors, multiply it by the embedding matrix E as usual, then this gives you the embedding vectors and then you can feed these into an RNN. And the job of the RNN is to then compute the representation at the last time step that allows you to predict Y-hat. So this is an example of a many-to-one RNN architecture which we saw in the previous week. And with an algorithm like this, it will be much better at taking word sequence into account and realize that “things are lacking in good taste” is a negative review and “not good” a negative review unlike the previous algorithm, which just sums everything together into a big-word vector mush and doesn’t realize that “not good” has a very different meaning than the words “good” or “lacking in good taste” and so on. And so if you train this algorithm, you end up with a pretty decent sentiment classification algorithm and because your word embeddings can be trained from a much larger data set, this will do a better job generalizing to maybe even new words now that you’ll see in your training set, such as if someone else says, “Completely absent of good taste, good service, and good ambiance” or something, then even if the word “absent” is not in your label training set, if it was in your 1 billion or 100 billion word corpus used to train the word embeddings, it might still get this right and generalize much better even to words that were in the training set used to train the word embeddings but not necessarily in the label training set that you had for specifically the sentiment classification problem. So that’s it for sentiment classification, and I hope this gives you a sense of how once you’ve learned or downloaded from online a word embedding, this allows you to quite quickly build pretty effective NLP systems. 02_debiasing-word-embeddingsMachine learning and AI algorithms are increasingly trusted to help with, or to make, extremely important decisions. And so we like to make sure that as much as possible that they’re free of undesirable forms of bias, such as gender bias, ethnicity bias and so on. What I want to do in this video is show you some of the ideas for diminishing or eliminating these forms of bias in word embeddings. When I use the term bias in this video, I don’t mean the bias variants or sense of the bias, instead I mean gender, ethnicity, sexual orientation bias. That’s a different sense of bias then is typically used in the technical discussion on machine learning. But mostly the problem, we talked about how word embeddings can learn analogies like man is to woman as king is to queen. But what if you ask it, man is to computer programmer as woman is to what? And so the authors of this paper Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai found a somewhat horrifying result where a learned word embedding might output Man:Computer_Programmer as Woman:Homemaker. And that just seems wrong and it enforces a very unhealthy gender stereotype. It’d be much more preferable to have algorithm output man is to computer programmer as a woman is to computer programmer. And they found also, Father:Doctor as Mother is to what? And the really unfortunate result is that some learned word embeddings would output Mother:Nurse. So word embeddings can reflect the gender, ethnicity, age, sexual orientation, and other biases of the text used to train the model. One that I’m especially passionate about is bias relating to socioeconomic status. I think that every person, whether you come from a wealthy family, or a low income family, or anywhere in between, I think everyone should have great opportunities. And because machine learning algorithms are being used to make very important decisions. They’re influencing everything ranging from college admissions, to the way people find jobs, to loan applications, whether your application for a loan gets approved, to in the criminal justice system, even sentencing guidelines. Learning algorithms are making very important decisions and so I think it’s important that we try to change learning algorithms to diminish as much as is possible, or, ideally, eliminate these types of undesirable biases. Now in the case of word embeddings, they can pick up the biases of the text used to train the model and so the biases they pick up or tend to reflect the biases in text as is written by people. Over many decades, over many centuries, I think humanity has made progress in reducing these types of bias. And I think maybe fortunately for AI, I think we actually have better ideas for quickly reducing the bias in AI than for quickly reducing the bias in the human race. Although I think we’re by no means done for AI as well and there’s still a lot of research and hard work to be done to reduce these types of biases in our learning algorithms. But what I want to do in this video is share with you one example of a set of ideas due to the paper referenced at the bottom by Bolukbasi and others on reducing the bias in word embeddings. So here’s the idea. Let’s say that we’ve already learned a word embedding, so the word babysitter is here, the word doctor is here. We have grandmother here, and grandfather here. Maybe the word girl is embedded there, the word boy is embedded there. And maybe she is embedded here, and he is embedded there. So the first thing we’re going to do it is identify the direction corresponding to a particular bias we want to reduce or eliminate. And, for illustration, I’m going to focus on gender bias but these ideas are applicable to all of the other types of bias that I mention on the previous slide as well. And so how do you identify the direction corresponding to the bias? For the case of gender, what we can do is take the embedding vector for he and subtract the embedding vector for she, because that differs by gender. And take e male, subtract e female, and take a few of these and average them, right? And take a few of these differences and basically average them. And this will allow you to figure out in this case that what looks like this direction(the horizontal direction in the slide) is the gender direction, or the bias direction. Whereas this direction(the vertical direction in the slide) is unrelated to the particular bias we’re trying to address. So this is the non-bias direction. And in this case, the bias direction, think of this as a 1D subspace whereas a non-bias direction, this will be 299-dimensional subspace. Okay, and I’ve simplified the description a little bit in the original paper. The bias direction can be higher than 1-dimensional, and rather than take an average, as I’m describing it here, it’s actually found using a more complicated algorithm called a SVD, a singular value decomposition. Which is closely related to, if you’re familiar with principle component analysis, it uses ideas similar to the pca or the principle component analysis algorithm. After that, the next step is a neutralization step. So for every word that’s not definitional, project it to get rid of bias. So there are some words that intrinsically capture gender. So words like grandmother, grandfather, girl, boy, she, he, a gender is intrinsic in the definition. Whereas there are other word like doctor and babysitter that we want to be gender neutral. And really, in the more general case, you might want words like doctor or babysitter to be ethnicity neutral or sexual orientation neutral, and so on, but we’ll just use gender as the illustrating example here. But so for every word that is not definitional, this basically means not words like grandmother and grandfather, which really have a very legitimate gender component, because, by definition, grandmothers are female, and grandfathers are male. So for words like doctor and babysitter, let’s just project them onto this axis to reduce their components, or to eliminate their component, in the bias direction. So reduce their component in this horizontal direction. So that’s the second neutralize step. And then the final step is called equalization in which you might have pairs of words such as grandmother and grandfather, or girl and boy, where you want the only difference in their embedding to be the gender. And so, why do you want that? Well in this example, the distance, or the similarity, between babysitter and grandmother is actually smaller than the distance between babysitter and grandfather. And so this maybe reinforces an unhealthy, or maybe undesirable, bias that grandmothers end up babysitting more than grandfathers. So in the final equalization step, what we’d like to do is to make sure that words like grandmother and grandfather are both exactly the same similarity, or exactly the same distance, from words that should be gender neutral, such as babysitter or such as doctor. So there are a few linear algebra steps for that. But what it will basically do is move grandmother and grandfather to a pair of points that are equidistant from this axis in the middle. And so the effect of that is that now the distance between babysitter, compared to these two words, will be exactly the same. And so, in general, there are many pairs of words like this grandmother-grandfather, boy-girl, sorority-fraternity, girlhood-boyhood, sister-brother, niece-nephew, daughter-son, that you might want to carry out through this equalization step. So the final detail is, how do you decide what word to neutralize? So for example, the word doctor seems like a word you should neutralize to make it non-gender-specific or non-ethnicity-specific. Whereas the words grandmother and grandmother should not be made non-gender-specific. And there are also words like beard, right, that it’s just a statistical fact that men are much more likely to have beards than women, so maybe beards should be closer to male than female. And so what the authors did is train a classifier to try to figure out what words are definitional, what words should be gender-specific and what words should not be. And it turns out that most words in the English language are not definitional, meaning that gender is not part of the definition. And it’s such a relatively small subset of words like this, grandmother-grandfather, girl-boy, sorority-fraternity, and so on that should not be neutralized. And so a linear classifier can tell you what words to pass through the neutralization step to project out this bias direction, to project it on to this essentially 299-dimensional subspace. And then, finally, the number of pairs you want to equalize, that’s actually also relatively small, and is, at least for the gender example, it is quite feasible to hand-pick most of the pairs you want to equalize. So the full algorithm is a bit more complicated than I present it here, you can take a look at the paper for the full details. And you also get to play with a few of these ideas in the programming exercises as well. So to summarize, I think that reducing or eliminating bias of our learning algorithms is a very important problem because these algorithms are being asked to help with or to make more and more important decisions in society. In this video I shared just one set of ideas for how to go about trying to address this problem, but this is still a very much an ongoing area of active research by many researchers. So that’s it for this week’s videos. Best of luck with this week’s programming exercises and I look forward to seeing you next week.]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[recurrent neural networks]]></title>
    <url>%2F2018%2F06%2F01%2F01_recurrent-neural-networks%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal lecture note after studying the course nlp sequence models at the 1st week and the copyright belongs to deeplearning.ai. 01_why-sequence-modelsWelcome to this fifth course on deep learning. In this course, you learn about sequence models, one of the most exciting areas in deep learning. Models like recurrent neural networks or RNNs have transformed speech recognition, natural language processing and other areas. And in this course, you learn how to build these models for yourself. Let’s start by looking at a few examples of where sequence models can be useful. In speech recognition you are given an input audio clip X and asked to map it to a text transcript Y. Both the input and the output here are sequence data, because X is an audio clip and so that plays out over time and Y, the output, is a sequence of words. So sequence models such as a recurrent neural networks and other variations, you’ll learn about in a little bit have been very useful for speech recognition. Music generation is another example of a problem with sequence data. In this case, only the output Y is a sequence, the input can be the empty set, or it can be a single integer, maybe referring to the genre of music you want to generate or maybe the first few notes of the piece of music you want. But here X can be nothing or maybe just an integer and output Y is a sequence. In sentiment classification the input X is a sequence, so given the input phrase like, “There is nothing to like in this movie” how many stars do you think this review will be? Sequence models are also very useful for DNA sequence analysis. So your DNA is represented via the four alphabets A, C, G, and T. And so given a DNA sequence can you label which part of this DNA sequence say corresponds to a protein. In machine translation you are given an input sentence, voulez-vou chante avec moi? And you’re asked to output the translation in a different language. In video activity recognition you might be given a sequence of video frames and asked to recognize the activity. And in name entity recognition you might be given a sentence and asked to identify the people in that sentence. So all of these problems can be addressed as supervised learning with label data X, Y as the training set. But, as you can tell from this list of examples, there are a lot of different types of sequence problems. In some, both the input X and the output Y are sequences, and in that case (speech recognition), sometimes X and Y can have different lengths, or in this example (at DNA case) and this example(at Name entity recognition), X and Y have the same length. And in some of these examples only either X or only the opposite Y is a sequence. So in this course you learn about sequence models are applicable, so all of these different settings. So I hope this gives you a sense of the exciting set of problems that sequence models might be able to help you to address. With that let us go on to the next video where we start to define the notation we use to define these sequence-models. 02_notationIn the last video, you saw some of the wide range of applications through which you can apply sequence models. Let’s start by defining a notation that we’ll use to build up these sequence models. As a motivating example, let’s say you want to build a sequence model to input a sentence like this, Harry Potter and Hermione Granger invented a new spell. And these are characters by the way, from the Harry Potter sequence of novels by J. K. Rowling. And let say you want a sequence model to automatically tell you where are the peoples names in this sentence. So, this is a problem called Named-entity recognition and this is used by search engines for example, to index all of say the last 24 hours news of all the people mentioned in the news articles so that they can index them appropriately. And name into the recognition systems can be used to find people’s names, companies names, times, locations, countries names, currency names, and so on in different types of text. Now, given this input x let’s say that you want a model to operate y that has one outputs per input word and the target output the design y tells you for each of the input words is that part of a person’s name. And technically this maybe isn’t the best output representation, there are some more sophisticated output representations that tells you not just is a word part of a person’s name, but tells you where are the start and ends of people’s names their sentence, you want to know Harry Potter starts here, and ends here, starts here, and ends here. But for this motivating example, I’m just going to stick with this simpler output representation. Now, the input is the sequence of nine words. So, eventually we’re going to have nine sets of features to represent these nine words, and index into the positions and sequence, I’m going to use X and then superscript angle brackets 1, 2, 3 and so on up to X angle brackets nine to index into the different positions. I’m going to use $X^{}$ with the index t to index into positions, in the middle of the sequence. And t implies that these are temporal sequences although whether the sequences are temporal one or not, I’m going to use the index t to index into the positions in the sequence. And similarly for the outputs, we’re going to refer to these outputs as y and go back at 1, 2, 3 and so on up to y nine. Let’s also used T sub of x to denote the length of the input sequence, so in this case there are nine words. So $T_x$ is equal to 9 and we used $T_y$ to denote the length of the output sequence. In this example $T_x$ is equal to $T_y$ but you saw on the last video $T_x$ and $T_y$ can be different. So, you will remember that in the notation we’ve been using, we’ve been writing X round brackets i to denote the i training example. So, to refer to the TIF element or the TIF element in the sequence of training example i will use this notation and if $T_x$ is the length of a sequence then different examples in your training set can have different lengths. And so $T_x^i$ would be the input sequence length for training example i, and similarly $y^{(i)}$ means the TIF element in the output sequence of the i for an example and $T_y^i$ will be the length of the output sequence in the i training example. So into this example, $T_x^i$ is equal to 9 would be the highly different training example with a sentence of 15 words and $T_x^i$ will be close to 15 for that different training example. Now, that we’re starting to work in NLP or Natural Language Processing. Now, this is our first serious foray into NLP or Natural Language Processing. And one of the things we need to decide is, how to represent individual words in the sequence. So, how do you represent a word like Harry, and why should $x^{}$ really be? Let’s next talk about how we would represent individual words in a sentence. So, to represent a word in the sentence the first thing you do is come up with a Vocabulary. Sometimes also called a Dictionary and that means making a list of the words that you will use in your representations. So the first word in the vocabulary is a, that will be the first word in the dictionary. The second word is Aaron and then a little bit further down is the word and, and then eventually you get to the words Harry then eventually the word Potter, and then all the way down to maybe the last word in dictionary is Zulu. And so, a will be word one, Aaron is word two, and in my dictionary the word and appears in positional index 367. Harry appears in position 4075, Potter in position 6830, and Zulu is the last word to the dictionary is maybe word 10,000. So in this example, I’m going to use a dictionary with size 10,000 words. This is quite small by modern NLP applications. For commercial applications, for visual size commercial applications, dictionary sizes of 30 to 50,000 are more common and 100,000 is not uncommon. And then some of the large Internet companies will use dictionary sizes that are maybe a million words or even bigger than that. But you see a lot of commercial applications used dictionary sizes of maybe 30,000 or maybe 50,000 words. But I’m going to use 10,000 for illustration since it’s a nice round number. So, if you have chosen a dictionary of 10,000 words and one way to build this dictionary will be be to look through your training sets and find the top 10,000 occurring words, also look through some of the online dictionaries that tells you what are the most common 10,000 words in the English Language saved. What you can do is then use one hot representations to represent each of these words. For example, $x^{}$ which represents the word Harry would be a vector with all zeros except for a 1 in position 4075 because that was the position of Harry in the dictionary. And then $x^{}$ will be again similarly a vector of all zeros except for a 1 in position 6830 and then zeros everywhere else. The word and was represented as position 367 so $x^{}$ would be a vector with zeros of 1 in position 367 and then zeros everywhere else. And each of these would be a 10,000 dimensional vector if your vocabulary has 10,000 words. And this one A, I guess because A is the first whether the dictionary, then $x^{}$ which corresponds to word a, that would be the vector 1. This is the first element of the dictionary and then zero everywhere else. So in this representation, $x^{}$ for each of the values of t in a sentence will be a one-hot vector, one-hot because there’s exactly one one is on and zero everywhere else and you will have nine of them to represent the nine words in this sentence. And the goal is given this representation for X to learn a mapping using a sequence model to then target output y, I will do this as a supervised learning problem, I’m sure given the table data with both x and y. Then just one last detail, which we’ll talk more about in a later video is, what if you encounter a word that is not in your vocabulary? Well the answer is, you create a new token or a new fake word called Unknown Word which under note as follows and go back as UNK to represent words not in your vocabulary, we’ll come more to talk more about this later. So, to summarize in this video, we described a notation for describing your training set for both x and y for sequence data. In the next video let’s start to describe a Recurrent Neural Networks for learning the mapping from X to Y. 03_recurrent-neural-network-modelIn the last video, you saw the notation we used to define sequence learning problems. Now, let’s talk about how you can build a model, build a neural network to drawing the mapping from X to Y. Now, one thing you could do is try to use a standard neural network for this task. So in our previous example, we had nine input words. So you could imagine trying to take these nine input words, maybe the nine one hot vectors and feeding them into a standard neural network, maybe a few hidden layers and then eventually, have this output the nine values zero or one that tell you whether each word is part of a person’s name. But this turns out not to work well, and there are really two main problems with this. The first is that the inputs and outputs can be different lengths in different examples. So it’s not as if every single example has the same input length $T^{}$ or the same output length $T^{}$. And maybe if every sentence had a maximum length, maybe you could pad, or zero pad every input up to that maximum length, but this still doesn’t seem like a good representation. And in a second, it might be more serious problem is that a naive neural network architecture like this, it doesn’t share features learned across different positions of techs. In particular, if the neural network has learned that maybe the word heavy appearing in position one gives a sign that that is part of a person’s name, then one would be nice if it automatically figures out that heavy appearing in some other position, $X^{}$ also means that that might be a person’s name. And this is maybe similar to what you saw in convolutional neural networks where you want things learned for one part of the image to generalize quickly to other parts of the image, and we’d like similar effect for sequence data as well. And similar to what you saw with ConvNets using a better representation will also let you reduce the number of parameters in your model. So previously, we said that each of these is a 10,000 dimensional one vector. And so, this is just a very large input layer. If the total input size was maximum number of words times 10,000, and the weight matrix of this first layer would end up having an enormous number of parameters. So a recurrent neural network which will start to describe in the next slide, does not have either of these disadvantages. So what is a recurrent neural network? Let’s build one out. So if you are reading the sentence from left to right, the first word you read is the some first where say X1. What we’re going to do is take the first word and feed it into a neural network layer. I’m going to draw it like this. So that’s a hidden layer of the first neural network. And look at how the neural network maybe try to predict the output. So is this part of a person’s name or not? And what a recurrent neural network does is when it then goes on to read the second word in a sentence, say X2, instead of just predicting Y2 using only X2, it also gets to input some information from whether a computer that time-step ones. So in particular, the activation value from time-step one is passed on to time-step 2. And then, at the next time-step, a recurrent neural network inputs the third word X3, and it tries to predict, output some prediction y-hat 3, and so on, up until the last time-step where inputs $X^{&lt;T_x&gt;}$, and then it outputs Y hat TY. In this example, Tx=Ty, and the architecture will change a bit if Tx and Ty are not identical. And so, at each time-step, the recurrent neural network passes on this activation to the next time-step for it to use. And to kick off the whole thing, we’ll also have some made up activation at time zero. This is usually the vector of zeroes. Some researchers will initialize a zero randomly have other ways to initialize a zero but really having a vector zero is just a fake. Time Zero activation is the most common choice. And so that does input into the neural network. In some research papers or in some books, you see this type of neural network drawn with the following diagram in which every time-step, you input X and output Y hat, maybe sometimes there will be a T index there, and then to denote the recurrent connection, sometimes people will draw a loop like that, that the layer feeds back to itself. Sometimes they’ll draw a shaded box to denote that this is the shaded box here, denotes a time delay of one step. I personally find these recurrent diagrams much harder to interpret. And so throughout this course, I will tend to draw the on the road diagram like the one you have on the left. But if you see something like the diagram on the right in a textbook or in a research paper, what it really means, or the way I tend to think about it is the mentally unrolled into the diagram you have on the left hand side. The recurrent neural network scans through the data from left to right. And the parameters it uses for each time step are shared. So there will be a set of parameters which we’ll describe in greater detail on the next slide, but the parameters governing the connection from X1 to the hidden layer will be some set of the parameters we’re going to write as WAX, and it’s the same parameters $W_{ax}$ that it uses for every time-step I guess you could write $W_{ax}$ there as well. And the activations, the horizontal connections, will be governed by some set of parameters $W_{aa}$, and is the same parameters $W_{aa}$ use on every time-step, and similarly, the sum $W_{ya}$ that governs the output predictions. And I’ll describe in the next slide exactly how these parameters work. So in this recurrent neural network, what this means is that we’re making the prediction for Y3 against the information not only from X3, but also the information from X1 and X2, because the information of X1 can pass through this way to help the prediction with Y3. Now one weakness of this RNN is that it only uses the information that is earlier in the sequence to make a prediction, in particular, when predicting Y3, it doesn’t use information about the words X4, X5, X6 and so on. And so this is a problem because if you’re given a sentence, he said, “Teddy Roosevelt was a great president.” In order to decide whether or not the word Teddy is part of a person’s name, it be really useful to know not just information from the first two words but to know information from the later words in the sentence as well, because the sentence could also happen, he said, “Teddy bears are on sale!” And so, given just the first three words, it’s not possible to know for sure whether the word Teddy is part of a person’s name. In the first example, it is, in the second example, is not, but you can’t tell the difference if you look only at the first three words. So one limitation of this particular neural network structure is that the prediction at a certain time uses inputs or uses information from the inputs earlier in the sequence but not information later in the sequence. We will address this in a later video where we talk about a bidirectional recurrent neural networks or BRNNs. But for now, this simpler uni-directional neural network architecture will suffice for us to explain the key concepts. And we just have to make a quick modifications in these ideas later to enable say the prediction of Y-hat 3 to use both information earlier in the sequence as well as information later in the sequence, but we’ll get to that in a later video. So let’s not write to explicitly what are the calculations that this neural network does. Here’s a cleaned out version of the picture of the neural network. As I mentioned previously, typically, you started off with the input a0 equals the vector of all zeroes. Next. This is what a forward propagation looks like. To compute a1, you would compute that as an activation function g, applied to Waa times a0 plus W a x times x1 plus a bias was going to write it as ba, and then to compute y hat 1 the prediction of times that one, that will be some activation function, maybe a different activation function, than the one above. But apply to WYA times a1 plus b y. And the notation convention I’m going to use for the sub zero of these matrices like that example, W a x. The second index means that this W a x is going to be multiplied by some x like quantity, and this means that this is used to compute some a like quantity. Like like so. And similarly, you notice that here WYA is multiplied by a sum a like quantity to compute a y type quantity. The activation function used in-to compute the activations will often be a tonnage and the choice of an RNN and sometimes, values are also used although the tonnage is actually a pretty common choice. And we have other ways of preventing the vanishing gradient problem which we’ll talk about later this week. And depending on what your output y is, if it is a binary classification problem, then I guess you would use a sigmoid activation function or it could be a soft Max if you have a ky classification problem. But the choice of activation function here would depend on what type of output y you have. So, for the name entity recognition task, where Y was either zero or one. I guess the second g could be a signal and activation function. And I guess you could write g2 if you want to distinguish that this is these could be different activation functions but I usually won’t do that. And then, more generally at time t, a t will be g of W a a times a, from the previous time-step, plus W a x of x from the current time-step plus B a, and y hat t is equal to g, again, it could be different activation functions but g of WYA times a t plus B y. So, these equations define for propagation in the neural network. Where you would start off with a zeroes [inaudible] and then using a zero and X1, you will compute a1 and y hat one, and then you, take X2 and use X2 and A1 to compute A2 and Y hat two and so on, and you carry out for propagation going from the left to the right of this picture. Now, in order to help us develop the more complex neural networks, I’m actually going to take this notation and simplify it a little bit. So, let me copy these two equations in the next slide. Right. Here they are, and what I’m going to do is actually take- so to simplify the notation a bit, I’m actually going to take that and write in a slightly simpler way. And someone very does this a = g times just a matrix $W_a$ times a new quantity is going to be $a^{}$ comma $x^{}$ and then, plus B a. And so, that underlining quantity on the left and right are supposed to be equivalent. So, the way we define $W_{a}$ is we’ll take this matrix $W_{aa}$ and this matrix $W_{ax}$. And put them side by side and stack them horizontally as follows. And this will be the matrix $W_{a}$. So for example, if a was a hundred dimensional, and then another example, X was 10,000 dimensional, then $W_{aa}$ would have been a 100 by 100 dimensional matrix and $W_{ax}$ would have been a 100 by 10,000 dimensional matrix. And so stacking these two matrices together this would be 100 dimensional. This would be 100, and this would be I guess 10,000 elements. So $W_{a}$ will be a 100 by one zero one zero zero zero dimensional matrix. I guess this diagram on the left is not drawn to scale. Since $W_{ax}$ would be a very wide matrix. And what this notation means, is to just take the two vectors, and stack them together. So, let me use that notation to denote that we’re going to take the vector $a^{}$. So there’s a 100 dimensional and stack it on top of $a^{}$. So this ends up being a one zero one zero zero dimensional vector. And so hopefully, you check for yourself that this matrix times this vector, just gives you back to the original quantity. Right. Because now, this matrix $W_{aa}$ times $W_{ax}$ multiplied by this $a^{}$ $x^{}$ vector, this is just equal to $W_{aa}$ times $a^{}$ plus $W_{ax}$ times x t which is exactly what we had back over here. So, the advantages of this notation is that rather than carrying around two parameter matrices $W_{aa}$ and $W_{ax}$, we can compress them into just one parameter matrix $W_{a}$. And this will simplify a notation for when we develop more complex models. And then, for this, in a similar way I’m just going to rewrite this slightly with the ranges as $W_y$ $a^{}$ plus $b_y$. And now, we just have the substrates in the notation $W_y$ and $b_y$, it denotes what type of output quantity over computing. So $W_y$ indicates that there’s a weight matrix of computing a y like quantity and here a Wa and ba on top. In the case of those the parameters of computing that an a and activation output quantity. So, that’s it. You now know, what is a basic recurrent network. Next, let’s talk about back propagation and how you learn with these RNNs. 04_backpropagation-through-timeYou’ve already learned about the basic structure of an RNN. In this video, you’ll see how backpropagation in a recurrent neural network works. As usual, when you implement this in one of the programming frameworks, often, the programming framework will automatically take care of backpropagation. But I think it’s still useful to have a rough sense of how backprop works in RNNs. Let’s take a look. You’ve seen how, for forward prop, you would computes these activations from left to right as follows in the neural network, and so you’ve outputs all of the predictions. In backprop, as you might already have guessed, you end up carrying backpropagation calculations in basically the opposite direction of the forward prop arrows. So, let’s go through the forward propagation calculation. You’re given this input sequence $x^{}, x^{}, x^{}$, up to $x^{&lt;T_x&gt;}$. And then using $x^{}$ and say, $a^{}$, you’re going to compute the activation, times that one, and then together, $x^{}$ together with $a^{}$ are used to compute $a^{}$, and then $a^{}$, and so on, up to $a^{&lt;T_x&gt;}$. All right. And then to actually compute $a^{}$, you also need the parameters. We’ll just draw this in green, $W_a$ and $b_a$, those are the parameters that are used to compute $a^{}$. And then, these parameters are actually used for every single timestep so, these parameters are actually used to compute $a^{}$, $a^{}$, and so on, all the activations up to last timestep depend on the parameters $W_a$ and $b_a$. Let’s keep fleshing out this graph. Now, given $a^{}$, your neural network can then compute the first prediction, $\hat{y}^{}$, and then the second timestep, $\hat{y}^{}$, $\hat{y}^{}$, and so on, with $\hat{y}^{&lt;T_y&gt;}$. And let me again draw the parameters of a different color. So, to compute $\hat{y}$, you need the parameters, $W_y$ as well as $b_y$, and this goes into this node as well as all the others. So, I’ll draw this in green as well. Next, in order to compute backpropagation, you need a loss function. So let’s define an element-wise loss force, which is supposed for a certain word in the sequence. It is a person’s name, so $y^{}$ is one. And your neural network outputs some probability of maybe 0.1 of the particular word being a person’s name. So I’m going to define this as the standard logistic regression loss, also called the cross entropy loss. This may look familiar to you from where we were previously looking at binary classification problems. So this is the loss associated with a single prediction at a single position or at a single time set, t, for a single word. Let’s now define the overall loss of the entire sequence, so L will be defined as the sum overall t equals one to, i guess, $T_x$ or $T_y$. $T_x$ is equals to $T_y$ in this example of the losses for the individual timesteps, comma $y^{}$. And then, so, just have to L without this superscript T. This is the loss for the entire sequence. So, in a computation graph, to compute the loss given $\hat{y}^{}$, you can then compute the loss for the first timestep given that you compute the loss for the second timestep, the loss for the third timestep, and so on, the loss for the final timestep. And then lastly, to compute the overall loss, we will take these and sum them all up to compute the final L using that equation, which is the sum of the individual per timestep losses. So, this is the computation problem and from the earlier examples you’ve seen of backpropagation, it shouldn’t surprise you that backprop then just requires doing computations or parsing messages in the opposite directions. So, all of the four propagation steps arrows, so you end up doing that. And that then, allows you to compute all the appropriate quantities that lets you then, take the riveters, respected parameters, and update the parameters using gradient descent. Now, in this back propagation procedure, the most significant message or the most significant recursive calculation is this one, which goes from right to left, and that’s why it gives this algorithm as well, a pretty fast full name called backpropagation through time. And the motivation for this name is that for forward prop, you are scanning from left to right, increasing indices of the time, t, whereas, the backpropagation, you’re going from right to left, you’re kind of going backwards in time. So this gives this, I think a really cool name, backpropagation through time, where you’re going backwards in time, right? That phrase really makes it sound like you need a time machine to implement this output, but I just thought that backprop through time is just one of the coolest names for an algorithm. So, I hope that gives you a sense of how forward prop and backprop in RNN works. Now, so far, you’ve only seen this main motivating example in RNN, in which the length of the input sequence was equal to the length of the output sequence. In the next video, I want to show you a much wider range of RNN architecture, so I’ll let you tackle a much wider set of applications. Let’s go on to the next video. 05_different-types-of-rnnsSo far, you’ve seen an RNN architecture where the number of inputs, Tx, is equal to the number of outputs, Ty. It turns out that for other applications, Tx and Ty may not always be the same, and in this video, you’ll see a much richer family of RNN architectures. You might remember this slide from the first video of this week, where the input x and the output y can be many different types. And it’s not always the case that $T_x$ has to be equal to $T_y$. In particular, in this example, $T_x$ can be length one or even an empty set. And then, an example like movie sentiment classification, the output y could be just an integer from 1 to 5, whereas the input is a sequence. And in name entity recognition, in the example we’re using, the input length and the output length are identical, but there are also some problems were the input length and the output length can be different.They’re both our sequences but have different lengths, such as machine translation where a French sentence and English sentence can mean two different numbers of words to say the same thing. So it turns out that we could modify the basic RNN architecture to address all of these problems. And the presentation in this video was inspired by a blog post by Andrej Karpathy, titled, The Unreasonable Effectiveness of Recurrent Neural Networks. Let’s go through some examples. The example you’ve seen so far use $T_x$ equals $T_y$, where we had an input sequence x(1), x(2) up to x(Tx), and we had a recurrent neural network that works as follows when we would input x(1) to compute y hat (1), y hat (2), and so on up to y hat (Ty), as follows. And in early diagrams, I was drawing a bunch of circles here to denote neurons but I’m just going to make those little circles for most of this video, just to make the notation simpler. So, this is what you might call a many-to-many architecture because the input sequence has many inputs as a sequence and the outputs sequence is also has many outputs. Now, let’s look at a different example. Let’s say, you want to address sentiments classification. Here, x might be a piece of text, such as it might be a movie review that says, “There is nothing to like in this movie.” So x is going to be sequenced, and y might be a number from 1 to 5, or maybe 0 or 1. This is a positive review or a negative review, or it could be a number from 1 to 5. Do you think this is a one-star, two-star, three, four, or five-star review? So in this case, we can simplify the neural network architecture as follows. I will input x(1), x(2). So, input the words one at a time. So if the input text was, “There is nothing to like in this movie.” So “There is nothing to like in this movie,” would be the input. And then rather than having to use an output at every single time-step, we can then just have the RNN read into entire sentence and have it output y at the last time-step when it has already input the entire sentence. So, this neural network would be a many-to-one architecture. Because as many inputs, it inputs many words and then it just outputs one number. For the sake of completeness, there is also a one-to-one architecture. So this one is maybe less interesting. The smaller the standard neural network, we have some input x and we just had some output y. And so, this would be the type of neural network that we covered in the first two courses in this sequence. Now, in addition to many-to-one, you can also have a one-to-many architecture. So an example of a one-to-many neural network architecture will be music generation. And in fact, you get to implement this yourself in one of the primary exercises for this course where you go is have a neural network, output a set of notes corresponding to a piece of music. And the input x could be maybe just an integer, telling it what genre of music you want or what is the first note of the music you want, and if you don’t want to input anything, x could be a null input, could always be the vector zeroes as well. For that, the neural network architecture would be your input x. And then, have your RNN output. The first value, and then, have that, with no further inputs, output. The second value and then go on to output. The third value, and so on, until you synthesize the last notes of the musical piece. If you want, you can have this input a(0) as well. One technical now what you see in the later video is that, when you’re actually generating sequences, often you take these first synthesized output and feed it to the next layer as well. So the network architecture actually ends up looking like that. So, we’ve talked about many-to- many, many-to-one, one-to-many, as well as one-to-one. It turns out there’s one more interesting example of many-to-many which is worth describing. Which is when the input and the output length are different. So, in the many-to-many example, you saw just now, the input length and the output length have to be exactly the same. For an application like machine translation, the number of words in the input sentence, say a French sentence, and the number of words in the output sentence, say the translation into English, those sentences could be different lengths. So here’s an alternative new network architecture where you might have a neural network, first, reading the sentence. So first, reading the input, say French sentence that you want to translate to English. And having done that, you then, have the neural network output the translation. As all those y hat of (Ty). And so, with this architecture, Tx and Ty can be different lengths. And again, you could draw on the a(0) that you want. And so, this that neural network architecture has two distinct parts. There’s the encoder which takes as input, say a French sentence, and then, there’s is a decoder, which having read in the sentence, outputs the translation into a different language. So this would be an example of a many-to-many architecture. So by the end of this week, you have a good understanding of all the components needed to build these types of architectures. And then, technically, there’s one other architecture which we’ll talk about only in week four, which is attention based architectures. Which maybe isn’t cleanly captured by one of the diagrams we’ve drawn so far. So, to summarize the wide range of RNN architectures, there is one-to-one, although if it’s one-to-one, we could just give it this, and this is just a standard generic neural network. Well, you don’t need an RNN for this. But there is one-to-many. So, this was a music generation or sequenced generation as example. And then, there’s many-to-one, that would be an example of sentiment classification. Where you might want to read as input all the text with a movie review. And then, try to figure out that they liked the movie or not. There is many-to-many, so the name entity recognition, the example we’ve been using, was this where $T_x$ is equal to $T_y$. And then, finally, there’s this other version of many-to-many, where for applications like machine translation, $T_x$ and $T_y$ no longer have to be the same. So, now you know most of the building blocks, the building are pretty much all of these neural networks except that there are some subtleties with sequence generation, which is what we’ll discuss in the next video. So, I hope you saw from this video that using the basic building blocks of an RNN, there’s already a wide range of models that you might be able put together. But as I mentioned, there are some subtleties to sequence generation, which you’ll get to implement yourself as well in this week’s primary exercise where you implement a language model and hopefully, generate some fun sequences or some fun pieces of text. So, what I want to do in the next video, is go deeper into sequence generation. Let’s see the details in the next video. 06_language-model-and-sequence-generationLanguage modeling is one of the most basic and important tasks in natural language processing. There’s also one that RNNs do very well. In this video, you learn about how to build a language model using an RNN, and this will lead up to a fun programming exercise at the end of this week. Where you build a language model and use it to generate Shakespeare-like texting, other types of text. Let’s get started. So what is a language model? Let’s say you’re building this speech recognition system and you hear the sentence, the apple and pear salad was delicious. So what did you just hear me say? Did I say the apple and pair salad, or did I say the apple and pear salad? You probably think the second sentence is much more likely, and in fact, that’s what a good speech recognition system would help with even though these two sentences sound exactly the same. And the way a speech recognition system picks the second sentence is by using a language model which tells it what the probability is of either of these two sentences. For example, a language model might say that the chance for the first sentence is 3.2 by 10 to the -13. And the chance of the second sentence is say 5.7 by 10 to the -10. And so, with these probabilities, the second sentence is much more likely by over a factor of 10 to the 3 compared to the first sentence. And that’s why speech recognition system will pick the second choice. So what a language model does is given any sentence is job is to tell you what is the probability of a sentence, of that particular sentence. And by probability of sentence I mean, if you want to pick up a random newspaper, open a random email or pick a random webpage or listen to the next thing someone says, the friend of you says. What is the chance that the next sentence you use somewhere out there in the world will be a particular sentence like the apple and pear salad? [COUGH] And this is a fundamental component for both speech recognition systems as you’ve just seen, as well as for machine translation systems where translation systems wants output only sentences that are likely. And so the basic job of a language model is to input a sentence, which I’m going to write as a sequence $y^{}$, $y^{}$ up to $y^{&lt;T_y&gt;}$. And for language model will be useful to represent a sentences as outputs y rather than inputs x. But what the language model does is it estimates the probability of that particular sequence of words. So how do you build a language model? To build such a model using an RNN you would first need a training set comprising a large corpus of english text. Or text from whatever language you want to build a language model of. And the word corpus is an NLP terminology that just means a large body or a very large set of english text of english sentences. So let’s say you get a sentence in your training set as follows. Cats average 15 hours of sleep a day. The first thing you would do is tokenize this sentence. And that means you would form a vocabulary as we saw in an earlier video. And then map each of these words to, say, one hot vectors, alter indices in your vocabulary. One thing you might also want to do is model when sentences end. So another common thing to do is to add an extra token called a EOS. That stands for End Of Sentence that can help you figure out when a sentence ends. We’ll talk more about this later, but the EOS token can be appended to the end of every sentence in your training sets if you want your models explicitly capture when sentences end. We won’t use the end of sentence token for the programming exercise at the end of this week where for some applications, you might want to use this. And we’ll see later where this comes in handy. So in this example, we have y1, y2, y3, 4, 5, 6, 7, 8, 9. Nine inputs in this example if you append the end of sentence token to the end. And doing the tokenization step, you can decide whether or not the period should be a token as well. In this example, I’m just ignoring punctuation. So I’m just using day as another token. And omitting the period, if you want to treat the period or other punctuation as explicit token, then you can add the period to you vocabulary as well. Now, one other detail would be what if some of the words in your training set, are not in your vocabulary. So if your vocabulary uses 10,000 words, maybe the 10,000 most common words in English, then the term Mau as in the Egyptian Mau is a breed of cat, that might not be in one of your top 10,000 tokens. So in that case you could take the word Mau and replace it with a unique token called UNK or stands for unknown words and would just model, the chance of the unknown word instead of the specific word now. Having carried out the tokenization step which basically means taking the input sentence and mapping out to the individual tokens or the individual words in your vocabulary. Next let’s build an RNN to model the chance of these different sequences. And one of the things we’ll see on the next slide is that you end up setting the inputs x = y or you see that in a little bit. So let’s go on to built the RNN model and I’m going to continue to use this sentence as the running example. This will be an RNN architecture. At time 0 you’re going to end up computing some activation $a^{}$ as a function of some inputs $x^{}$, and $x^{}$ will just be set it to the set of all zeroes, to 0 vector. And the previous $a^{}$, by convention, also set that to vector zeroes. But what $a^{}$ does is it will make a soft max prediction to try to figure out what is the probability of the first words y. And so that’s going to be $y^{}$. So what this step does is really, it has a soft max it’s trying to predict. What is the probability of any word in the dictionary? That the first one is a, what’s the chance that the first word is Aaron? And then what’s the chance that the first word is cats? All the way to what’s the chance the first word is Zulu? Or what’s the first chance that the first word is an unknown word? Or what’s the first chance that the first word is the in the sentence they’ll have, shouldn’t have to read? Right, so $\hat{y}^{}$ is output to a softmax, it just predicts what’s the chance of the first word being, whatever it ends up being. And in our example, it wind up being the word cats, so this would be a 10,000 way soft max output, if you have a 10,000-word vocabulary. Or 10,002, I guess you could call unknown word and the sentence is two additional tokens. Then, the RNN steps forward to the next step and has some activation, $a^{}$ to the next step. And at this step, his job is try figure out, what is the second word? But now we will also give it the correct first word. So we’ll tell it that, gee, in reality, the first word was actually Cats so that’s $y^{}$. So tell it cats, and this is why $y^{} = x^{}$, and so at the second step the output is again predicted by a soft max. The RNN’s jobs to predict was the chance of a being whatever the word it is. Is it a or Aaron, or Cats or Zulu or unknown whether EOS or whatever given what had come previously. So in this case, I guess the right answer was average since the sentence starts with cats average. And then you go on to the next step of the RNN. Where you now compute $a^{}$. But to predict what is the third word, which is 15, we can now give it the first two words. So we’re going to tell it cats average are the first two words. So this next input here, $x^{} = y^{}$, so the word average is input, and this job is to figure out what is the next word in the sequence. So in other words trying to figure out what is the probability of anywhere than dictionary given that what just came before was cats. Average, right? And in this case, the right answer is 15 and so on. Until at the end, you end up at, I guess, time step 9, you end up feeding it $x^{}$, which is equal to $y^{}$, which is the word, day. And then this has $a^{}$, and its jpob iws to output $\hat{y}^{}$, and this happens to be the EOS token. So what’s the chance of whatever this given, everything that comes before, and hopefully it will predict that there’s a high chance of it, EOS and the sentence token. So each step in the RNN will look at some set of preceding words such as, given the first three words, what is the distribution over the next word? And so this RNN learns to predict one word at a time going from left to right. Next to train us to a network, we’re going to define the cos function. So, at a certain time, t, if the true word was yt and the new networks soft max predicted some $\hat{y}^{}$, then this is the soft max loss function that you should already be familiar with. And then the overall loss is just the sum overall time steps of the loss associated with the individual predictions. And if you train this RNN on the last training set, what you’ll be able to do is given any initial set of words, such as cats average 15 hours of, it can predict what is the chance of the next word. And given a new sentence say, $y^{}$, $y^{}$, $y^{}$with just a three words, for simplicity, the way you can figure out what is the chance of this entire sentence would be. Well, the first soft max tells you what’s the chance of $y^{}$. That would be this first output. And then the second one can tell you what’s the chance of p of $y^{}$ given $y^{}$. And then the third one tells you what’s the chance of $y^{}$ given $y^{}$ and $y^{}$. And so by multiplying out these three probabilities. And you’ll see much more details of this in the previous exercise. By multiply these three, you end up with the probability of the three sentence, of the three-word sentence. So that’s the basic structure of how you can train a language model using an RNN. If some of these ideas still seem a little bit abstract, don’t worry about it, you get to practice all of these ideas in their program exercise. But next it turns out one of the most fun things you could do with a language model is to sample sequences from the model. Let’s take a look at that in the next video. 07_sampling-novel-sequencesAfter you train a sequence model, one of the ways you can informally get a sense of what is learned is to have a sample novel sequences. Let’s take a look at how you could do that. So remember that a sequence model, models the chance of any particular sequence of words as follows, and so what we like to do is sample from this distribution to generate novel sequences of words. So the network was trained using this structure shown at the top. But to sample, you do something slightly different, so what you want to do is first sample what is the first word you want your model to generate. And so for that you input the usual $x^{}$ equals 0, $a^{}$ equals 0. And now your first time stamp will have some max probability over possible outputs. So what you do is you then randomly sample according to this softmax distribution. So what the soft max distribution gives you is it tells you what is the chance that it refers to this a, what is the chance that it refers to this Aaron? What’s the chance it refers to Zulu, what is the chance that the first word is the Unknown word token. Maybe it was a chance it was a end of sentence token. And then you take this vector and use, for example, the numpy command np.random.choice to sample according to distribution defined by this vector probabilities, and that lets you sample the first words. Next you then go on to the second time step, and now remember that the second time step is expecting this $\hat{y}^{}$ as input. But what you do is you then take the $\hat{y}^{}$ that you just sampled and pass that in here as the input to the next timestep. So whatever works, you just chose the first time step passes this input in the second position, and then this softmax will make a prediction for what is $\hat{y}^{}$. Example, let’s say that after you sample the first word, the first word happened to be “The”, which is very common choice of first word. Then you pass in “The” as $x^{}$, which is now equal to $\hat{y}^{}$. And now you’re trying to figure out what is the chance of what the second word is given that the first word is d. And this is going to be $\hat{y}^{}$. Then you again use this type of sampling function to sample $\hat{y}^{}$. And then at the next time stamp, you take whatever choice you had represented say as a one hard encoding. And pass that to next timestep and then you sample the third word to that whatever you chose, and you keep going until you get to the last time step. And so how do you know when the sequence ends? Well, one thing you could do is if the end of sentence token is part of your vocabulary, you could keep sampling until you generate an EOS token. And that tells you you’ve hit the end of a sentence and you can stop. Or alternatively, if you do not include this in your vocabulary then you can also just decide to sample 20 words or 100 words or something, and then keep going until you’ve reached that number of time steps. And this particular procedure will sometimes generate an unknown word token. If you want to make sure that your algorithm never generates this token, one thing you could do is just reject any sample that came out as unknown word token and just keep resampling from the rest of the vocabulary until you get a word that’s not an unknown word. Or you can just leave it in the output as well if you don’t mind having an unknown word output. So this is how you would generate a randomly chosen sentence from your RNN language model. Now, so far we’ve been building a words level RNN, by which I mean the vocabulary are words from English. Depending on your application, one thing you can do is also build a character level RNN. So in this case your vocabulary will just be the alphabets. Up to z, and as well as maybe space, punctuation if you wish, the digits 0 to 9. And if you want to distinguish the uppercase and lowercase, you can include the uppercase alphabets as well, and one thing you can do as you just look at your training set and look at the characters that appears there and use that to define the vocabulary. And if you build a character level language model rather than a word level language model, then your sequence $y^{}, y^{}, y^{}$, would be the individual characters in your training data, rather than the individual words in your training data. So for our previous example, the sentence cats average 15 hours of sleep a day. In this example, c would be $y^{}$, a would be $y^{}$, t will be $y^{}$, the space will be $y^{}$ and so on. Using a character level language model has some pros and cons. One is that you don’t ever have to worry about unknown word tokens. In particular, a character level language model is able to assign a sequence like mau, a non-zero probability. Whereas if mau was not in your vocabulary for the word level language model, you just have to assign it the unknown word token. But the main disadvantage of the character level language model is that you end up with much longer sequences. So many english sentences will have 10 to 20 words but may have many, many dozens of characters. And so character language models are not as good as word level language models at capturing long range dependencies between how the the earlier parts of the sentence also affect the later part of the sentence. And character level models are also just more computationally expensive to train. So the trend I’ve been seeing in natural language processing is that for the most part, word level language model are still used, but as computers gets faster there are more and more applications where people are, at least in some special cases, starting to look at more character level models. But they tend to be much hardware, much more computationally expensive to train, so they are not in widespread use today. Except for maybe specialized applications where you might need to deal with unknown words or other vocabulary words a lot. Or they are also used in more specialized applications where you have a more specialized vocabulary. So under these methods, what you can now do is build an RNN to look at the purpose of English text, build a word level, build a character language model, sample from the language model that you’ve trained. So here are some examples of text thatwere examples from a language model, actually from a culture level language model. And you get to implement something like this yourself in the programming exercise. If the model was trained on news articles, then it generates texts like that shown on the left. And this looks vaguely like news text, not quite grammatical, but maybe sounds a little bit like things that could be appearing news, concussion epidemic to be examined. And it was trained on Shakespearean text and then it generates stuff that sounds like Shakespeare could have written it. The mortal moon hath her eclipse in love. And subject of this thou art another this fold. When besser be my love to me see sabl’s. For whose are ruse of mine eyes heaves. So that’s it for the basic RNN, and how you can build a language model using it, as well as sample from the language model that you’ve trained. In the next few videos, I want to discuss further some of the challenges of training RNNs, as well as how to adjust some of these challenges, specifically vanishing gradients by building even more powerful models of the RNN. So in the next video let’s talk about the problem of vanishing the gradient and we will go on to talk about the GRU, Gate Recurring Unit as well as the LSTM models. 08_vanishing-gradients-with-rnnsYou’ve learned about how RNNs work and how they can be applied to problems like name entity recognition, as well as to language modeling, and you saw how backpropagation can be used to train in RNN. It turns out that one of the problems with a basic RNN algorithm is that it runs into vanishing gradient problems. Let’s discuss that, and then in the next few videos, we’ll talk about some solutions that will help to address this problem. So, you’ve seen pictures of RNNS that look like this. And let’s take a language modeling example. Let’s say you see this sentence, “The cat which already ate and maybe already ate a bunch of food that was delicious dot, dot, dot, dot, was full.” And so, to be consistent, just because cat is singular, it should be the cat was, were then was, “The cats which already ate a bunch of food was delicious, and apples, and pears, and so on, were full.” So to be consistent, it should be cat was or cats were. And this is one example of when language can have very long-term dependencies, where it worked at this much earlier can affect what needs to come much later in the sentence. But it turns out the basics RNN we’ve seen so far it’s not very good at capturing very long-term dependencies. To explain why, you might remember from our early discussions of training very deep neural networks, that we talked about the vanishing gradients problem. So this is a very, very deep neural network say, 100 layers or even much deeper than you would carry out forward prop, from left to right and then back prop. And we said that, if this is a very deep neural network, then the gradient from just output y, would have a very hard time propagating back to affect the weights of these earlier layers, to affect the computations in the earlier layers. And for an RNN with a similar problem, you have forward prop came from left to right, and then back prop, going from right to left. And it can be quite difficult, because of the same vanishing gradients problem, for the outputs of the errors associated with the later time steps to affect the computations that are earlier. And so in practice, what this means is, it might be difficult to get a neural network to realize that it needs to memorize the just see a singular noun or a plural noun, so that later on in the sequence that can generate either was or were, depending on whether it was singular or plural. And notice that in English, this stuff in the middle could be arbitrarily long, right? So you might need to memorize the singular/plural for a very long time before you get to use that bit of information. So because of this problem, the basic RNN model has many local influences, meaning that the output $y^{}$ is mainly influenced by values close to $y^{}$. And a value here is mainly influenced by inputs that are somewhere close. And it’s difficult for the output here to be strongly influenced by an input that was very early in the sequence. And this is because whatever the output is, whether this got it right, this got it wrong, it’s just very difficult for the area to backpropagate all the way to the beginning of the sequence, and therefore to modify how the neural network is doing computations earlier in the sequence. So this is a weakness of the basic RNN algorithm. One, which was not addressed in the next few videos. But if we don’t address it, then RNNs tend not to be very good at capturing long-range dependencies. And even though this discussion has focused on vanishing gradients, you will remember when we talked about very deep neural networks, that we also talked about exploding gradients. We’re doing back prop, the gradients should not just decrease exponentially, they may also increase exponentially with the number of layers you go through. It turns out that vanishing gradients tends to be the bigger problem with training RNNs, although when exploding gradients happens, it can be catastrophic because the exponentially large gradients can cause your parameters to become so large that your neural network parameters get really messed up. So it turns out that exploding gradients are easier to spot because the parameters just blow up and you might often see NaNs, or not a numbers, meaning results of a numerical overflow in your neural network computation. And if you do see exploding gradients, one solution to that is apply gradient clipping. And what that really means, all that means is look at your gradient vectors, and if it is bigger than some threshold, re-scale some of your gradient vector so that is not too big. So there are clips according to some maximum value. So if you see exploding gradients, if your derivatives do explode or you see NaNs, just apply gradient clipping, and that’s a relatively robust solution that will take care of exploding gradients. But vanishing gradients is much harder to solve and it will be the subject of the next few videos. So to summarize, in an earlier course, you saw how the training of very deep neural network, you can run into a vanishing gradient or exploding gradient problems with the derivative, either decreases exponentially or grows exponentially as a function of the number of layers. And in RNN, say in RNN processing data over a thousand times sets, over 10,000 times sets, that’s basically a 1,000 layer or they go 10,000 layer neural network, and so, it too runs into these types of problems. Exploding gradients, you could sort of address by just using gradient clipping, but vanishing gradients will take more work to address. So what we do in the next video is talk about GRU, the greater recurrent units, which is a very effective solution for addressing the vanishing gradient problem and will allow your neural network to capture much longer range dependencies. So, lets go on to the next video. 09_gated-recurrent-unit-gruYou’ve seen how a basic RNN works. In this video, you learn about the Gated Recurrent Unit which is a modification to the RNN hidden layer that makes it much better capturing long range connections and helps a lot with the vanishing gradient problems. Let’s take a look. You’ve already seen the formula for computing the activations at time t of RNN. It’s the activation function applied to the parameter Wa times the activations in the previous time set, the current input and then plus ba. So I’m going to draw this as a picture. So the RNN unit, I’m going to draw as a picture, drawn as a box which inputs a of t-1, the activation for the last time-step. And also inputs x and these two go together. And after some weights and after this type of linear calculation, if g is a tanh activation function, then after the tanh, it computes the output activation a. And the output activation a(t) might also be passed to say a softener unit or something that could then be used to output y. So this is maybe a visualization of the RNN unit of the hidden layer of the RNN in terms of a picture. And I want to show you this picture because we’re going to use a similar picture to explain the GRU or the Gated Recurrent Unit. Lots of the idea of GRU were due to these two papers respectively by Yu Young Chang, Kagawa, Gaza Hera, Chang Hung Chu and Jose Banjo. And I’m sometimes going to refer to this sentence which we’d seen in the last video to motivate that. Given a sentence like this, you might need to remember the cat was singular, to make sure you understand why that was rather than were. So the cat was for or the cats were for. So as we read in this sentence from left to right, the GRU unit is going to have a new variable called c, which stands for cell, for memory cell. And what the memory cell do is it will provide a bit of memory to remember, for example, whether cat was singular or plural, so that when it gets much further into the sentence it can still work under consideration whether the subject of the sentence was singular or plural. And so at time t the memory cell will have some value c of t. And what we see is that the GRU unit will actually output an activation value a of t that’s equal to c of t. And for now I wanted to use different symbol c and a to denote the memory cell value and the output activation value, even though they are the same. I’m using this notation because when we talk about LSTMs, a little bit later, these will be two different values. But for now, for the GRU, c of t is equal to the output activation a of t. So these are the equations that govern the computations of a GRU unit. And every time-step, we’re going to consider overwriting the memory cell with a value c tilde of t. So this is going to be a candidate for replacing c of t. And we’re going to compute this using an activation function tanh of Wc. And so that’s the parameter to make sure it’s Wc and we’ll plus this parameter matrix, the previous value of the memory cell, the activation value as well as the current input value $x^{}$, and then plus the bias. So c tilde of t is going to be a candidate for replacing $c^{}$. And then the key, really the important idea of the GRU it will be that we have a gate. So the gate, I’m going to call gamma u. This is the capital Greek alphabet gamma subscript u, and u stands for update gate, and this will be a value between zero and one. And to develop your intuition about how GRUs work, think of gamma u, this gate value, as being always zero or one. Although in practice, your compute it with a sigmoid function applied to this. So remember that the sigmoid function looks like this. And so it’s value is always between zero and one. And for most of the possible ranges of the input, the sigmoid function is either very, very close to zero or very, very close to one. So for intuition, think of gamma as being either zero or one most of the time. And this alphabet u stands for- I chose the alphabet gamma for this because if you look at a gate fence, looks a bit like this I guess, then there are a lot of gammas in this fence. So that’s why gamma u, we’re going to use to denote the gate. Also Greek alphabet G, right. G for gate. So G for gamma and G for gate. And then next, the key part of the GRU is this equation which is that we have come up with a candidate where we’re thinking of updating c using c tilde, and then the gate will decide whether or not we actually update it. And so the way to think about it is maybe this memory cell c is going to be set to either zero or one depending on whether the word you are considering, really the subject of the sentence is singular or plural. So because it’s singular, let’s say that we set this to one. And if it was plural, maybe we would set this to zero, and then the GRU unit would memorize the value of the $c^{}$ all the way until here, where this is still equal to one and so that tells it, oh, it’s singular so use the choice was. And the job of the gate, of gamma u, is to decide when do you update these values. In particular, when you see the phrase, the cat, you know they you’re talking about a new concept the especially subject of the sentence cat. So that would be a good time to update this bit and then maybe when you’re done using it, the cat blah blah blah was full, then you know, okay, I don’t need to memorize anymore, I can just forget that. So the specific equation we’ll use for the GRU is the following. Which is that the actual value of $c^{}$ will be equal to this gate times the candidate value plus one minus the gate times the old value, $c^{}$. So you notice that if the gate, if this update value, this equal to one, then it’s saying set the new value of $c^{}$ equal to this candidate value. So that’s like over here, set gate equal to one so go ahead and update that bit. And then for all of these values in the middle, you should have the gate equals zero. So this is saying don’t update it, don’t update it, don’t update it, just hang onto the old value. Because if gamma u is equal to zero, then this would be zero, and this would be one. And so it’s just setting $c^{}$ equal to the old value, even as you scan the sentence from left to right. So when the gate is equal to zero, we’re saying don’t update it, don’t update it, just hang on to the value and don’t forget what this value was. And so that way even when you get all the way down here, hopefully you’ve just been setting $c^{}$ equals $c^{}$ all along. And it still memorizes, the cat was singular. So let me also draw a picture to denote the GRU unit. And by the way, when you look in online blog posts and textbooks and tutorials these types of pictures are quite popular for explaining GRUs as well as we’ll see later, LSTM units. I personally find the equations easier to understand in a pictures. So if the picture doesn’t make sense. Don’t worry about it, but I’ll just draw in case helps some of you. So a GRU unit inputs $c^{}$, for the previous time-step and just happens to be equal to 80 minus one. So take that as input and then it also takes as input $x^{}$, then these two things get combined together. And with some appropriate weighting and some tanh, this gives you c tilde t which is a candidate for placing $c^{}$, and then with a different set of parameters and through a sigmoid activation function, this gives you gamma u, which is the update gate. And then finally, all of these things combine together through another operation. And I won’t write out the formula, but this box here which wish I shaded in purple represents this equation which we had down there. So that’s what this purple operation represents. And it takes as input the gate value, the candidate new value, or there is this gate value again and the old value for $c^{}$, right. So it takes as input this, this and this and together they generate the new value for the memory cell. And so that’s $c^{}$ equals a. And if you wish you could also use this process to soft max or something to make some prediction for $y^{}$. So that is the GRU unit or at least a slightly simplified version of it. And what is remarkably good at is through the gates deciding that when you’re scanning the sentence from left to right say, that’s a good time to update one particular memory cell and then to not change, not change it until you get to the point where you really need it to use this memory cell that is set even earlier in the sentence. And because the sigmoid value, now, because the gate is quite easy to set to zero right. So long as this quantity is a large negative value, then up to numerical around off the uptake gate will be essentially zero. Very, very, very close to zero. So when that’s the case, then this updated equation and subsetting $c^{}$ equals $c^{}$. And so this is very good at maintaining the value for the cell. And because gamma can be so close to zero, can be 0.000001 or even smaller than that, it doesn’t suffer from much of a vanishing gradient problem. Because when you say gamma so close to zero this becomes essentially $c^{}$ equals $c^{}$ and the value of $c^{}$ is maintained pretty much exactly even across many many many many time-steps. So this can help significantly with the vanishing gradient problem and therefore allow a neural network to go on even very long range dependencies, such as a cat and was related even if they’re separated by a lot of words in the middle. Now I just want to talk over some more details of how you implement this. In the equations I’ve written, $c^{}$ can be a vector. So if you have 100 dimensional or hidden activation value then $c^{}$ can be a 100 dimensional say. And so $\tilde{c}^{}$ would also be the same dimension, and gamma would also be the same dimension as the other things on drawing boxes. And in that case, these asterisks are actually element wise multiplication. So here if gamma u, if the gate is 100 dimensional vector, what it is really a 100 dimensional vector of bits, the value is mostly zero and one. That tells you of this 100 dimensional memory cell which are the bits you want to update. And, of course, in practice gamma won’t be exactly zero or one. Sometimes it takes values in the middle as well but it is convenient for intuition to think of it as mostly taking on values that are exactly zero, pretty much exactly zero or pretty much exactly one. And what these element wise multiplications do is it just element wise tells the GRU unit which other bits in your- It just tells your GRU which are the dimensions of your memory cell vector to update at every time-step. So you can choose to keep some bits constant while updating other bits. So, for example, maybe you use one bit to remember the singular or plural cat and maybe use some other bits to realize that you’re talking about food. And so because you’re talk about eating and talk about food, then you’d expect to talk about whether the cat is four letter, right. You can use different bits and change only a subset of the bits every point in time. You now understand the most important ideas of the GRU. What I’m presenting in this slide is actually a slightly simplified GRU unit. Let me describe the full GRU unit. So to do that, let me copy the three main equations. This one, this one and this one to the next slide. So here they are. And for the full GRU unit, I’m sure to make one change to this which is, for the first equation which was calculating the candidate new value for the memory cell, I’m going just to add one term. Let me pushed that a little bit to the right, and I’m going to add one more gate. So this is another gate $\Gamma_r$. You can think of r as standing for relevance. So this gate $\Gamma_r$ tells you how relevant is $c^{}$ to computing the next candidate for $c^{}$. And this gate $\Gamma_r$ is computed pretty much as you’d expect with a new parameter matrix $W_r$, and then the same things as input $x^{}$ plus $b_r$. So as you can imagine there are multiple ways to design these types of neural networks. And why do we have $\Gamma_r$ ? Why not use a simpler version from the previous slides? So it turns out that over many years researchers have experimented with many, many different possible versions of how to design these units, to try to have longer range connections, to try to have more the longer range effects and also address vanishing gradient problems. And the GRU is one of the most commonly used versions that researchers have converged to and found as robust and useful for many different problems. If you wish you could try to invent new versions of these units if you want, but the GRU is a standard one, that’s just common used. Although you can imagine that researchers have tried other versions that are similar but not exactly the same as what I’m writing down here as well. And the other common version is called an LSTM which stands for Long Short Term Memory which we’ll talk about in the next video. But GRUs and LSTMs are two specific instantiations of this set of ideas that are most commonly used. Just one note on notation. I tried to define a consistent notation to make these ideas easier to understand. If you look at the academic literature, you sometimes see people- If you look at the academic literature sometimes you see people using alternative notation to be $\tilde{x}$, u, r and h to refer to these quantities as well. But I try to use a more consistent notation between GRUs and LSTMs as well as using a more consistent notation gamma to refer to the gates, so hopefully make these ideas easier to understand. So that’s it for the GRU, for the Gate Recurrent Unit. This is one of the ideas in RNN that has enabled them to become much better at capturing very long range dependencies has made RNN much more effective. Next, as I briefly mentioned, the other most commonly used variation of this class of idea is something called the LSTM unit, Long Short Term Memory unit. Let’s take a look at that in the next video. 10_long-short-term-memory-lstmIn the last video, you learned about the GRU, the gated recurrent units, and how that can allow you to learn very long range connections in a sequence. The other type of unit that allows you to do this very well is the LSTM or the long short term memory units, and this is even more powerful than the GRU. Let’s take a look. Here are the equations from the previous video for the GRU. And for the GRU, we had $a^{}$ equals $c^{}$, and two gates, the optic gate and the relevance gate, $\tilde{c}^{}$, which is a candidate for replacing the memory cell, and then we use the update gate, $\Gamma_u$, to decide whether or not to update $c^{}$ using $\tilde{c}^{}$. The LSTM is an even slightly more powerful and more general version of the GRU, and is due to Sepp Hochreiter and Jurgen Schmidhuber. And this was a really seminal paper, a huge impact on sequence modelling. I think this paper is one of the more difficult to read. It goes quite along into theory of vanishing gradients. And so, I think more people have learned about the details of LSTM through maybe other places than from this particular paper even though I think this paper has had a wonderful impact on the Deep Learning community. But these are the equations that govern the LSTM. So, the book continued to the memory cell, c, and the candidate value for updating it, $\tilde{c}^{}$, will be this, and so on. Notice that for the LSTM, we will no longer have the case that $a^{}$ is equal to $c^{}$. So, this is what we use. And so, this is just like the equation on the left except that with now, more specially use $a^{}$ there or $a^{}$ instead of $c^{}$. And we’re not using this gamma or this relevance gate. Although you could have a variation of the LSTM where you put that back in, but with the more common version of the LSTM, doesn’t bother with that. And then we will have an update gate, same as before. So, W updates and we’re going to use $a^{}$ here, $x^{}$ plus $b_u$. And one new property of the LSTM is, instead of having one update gate control, both of these terms, we’re going to have two separate terms. So instead of $\Gamma_u$ and one minus $\Gamma_u$, we’re going have $\Gamma_u$ here. And forget gate, which we’re going to call $\Gamma_f$. So, this gate, $\Gamma_f$, is going to be sigmoid of pretty much what you’d expect, $x^{}$ plus $b_f$. And then, we’re going to have a new output gate which is sigma of $W_o$. And then again, pretty much what you’d expect, plus $b_o$. And then, the update value to the memory so will be $c^{}$ equals $\Gamma_u$. And this asterisk denotes element-wise multiplication. This is a vector-vector element-wise multiplication, plus, and instead of one minus $\Gamma_u$, we’re going to have a separate forget gate, $\Gamma_f$, times c of t minus one. So this gives the memory cell the option of keeping the old value $c^{}$ minus one and then just adding to it, this new value, $\tilde{c}^{}$. So, use a separate update and forget gates. So, this stands for update, forget, and output gate. And then finally, instead of $a^{}$ equals $c^{}$ $a^{}$ is $a^{}$ equal to the output gate element-wise multiplied by $c^{}$. So, these are the equations that govern the LSTM and you can tell it has three gates instead of two. So, it’s a bit more complicated and it places the gates into slightly different places. So, here again are the equations governing the behavior of the LSTM. Once again, it’s traditional to explain these things using pictures. So let me draw one here. And if these pictures are too complicated, don’t worry about it. I personally find the equations easier to understand than the picture. But I’ll just show the picture here for the intuitions it conveys. The bigger picture here was very much inspired by a blog post due to Chris Ola, title, Understanding LSTM Network, and the diagram drawing here is quite similar to one that he drew in his blog post. But the key thing is to take away from this picture are maybe that you use $a^{}$ and $x^{}$ to compute all the gate values. In this picture, you have $a^{}$, $x^{}$ coming together to compute the forget gate, to compute the update gates, and to compute output gate. And they also go through a tanh to compute $\tilde{c}^{}$. And then these values are combined in these complicated ways with element-wise multiplies and so on, to get $c^{}$ from the previous $c^{}$. Now, one element of this is interesting as you have a bunch of these in parallel. So, that’s one of them and you connect them. You then connect these temporally. So it does the input $x^{}$ then $x^{}$, $x^{}$. So, you can take these units and just hold them up as follows, where the output a at the previous timestep is the input a at the next timestep, the c. I’ve simplified to diagrams a little bit in the bottom. And one cool thing about this you’ll notice is that there’s this line at the top that shows how, so long as you set the forget and the update gate appropriately, it is relatively easy for the LSTM to have some value $c^{}$ and have that be passed all the way to the right to have your, maybe, $c^{}$ equals $c^{}$. And this is why the LSTM, as well as the GRU, is very good at memorizing certain values even for a long time, for certain real values stored in the memory cell even for many, many timesteps. So, that’s it for the LSTM. As you can imagine, there are also a few variations on this that people use. Perhaps, the most common one is that instead of just having the gate values be dependent only on $a^{}$, $x^{}$, sometimes, people also sneak in there the values $c^{}$ as well. This is called a peephole connection. Not a great name maybe but you’ll see, peephole connection. What that means is that the gate values may depend not just on $a^{}$ and on $x^{}$, but also on the previous memory cell value, and the peephole connection can go into all three of these gates’ computations. So that’s one common variation you see of LSTMs. One technical detail is that these are, say, 100-dimensional vectors. So if you have a 100-dimensional hidden memory cell unit, and so is this. And the, say, fifth element of $c^{}$ affects only the fifth element of the corresponding gates, so that relationship is one-to-one, where not every element of the 100-dimensional $c^{}$ can affect all elements of the case. But instead, the first element of $c^{}$ affects the first element of the case, second element affects the second element, and so on. But if you ever read the paper and see someone talk about the peephole connection, that’s when they mean that $c^{}$ is used to affect the gate value as well. So, that’s it for the LSTM. When should you use a GRU? And when should you use an LSTM? There isn’t widespread consensus in this. And even though I presented GRUs first, in the history of deep learning, LSTMs actually came much earlier, and then GRUs were relatively recent invention that were maybe derived as Pavia’s simplification of the more complicated LSTM model. Researchers have tried both of these models on many different problems, and on different problems, different algorithms will win out. So, there isn’t a universally-superior algorithm which is why I want to show you both of them. But I feel like when I am using these, the advantage of the GRU is that it’s a simpler model and so it is actually easier to build a much bigger network, it only has two gates, so computationally, it runs a bit faster. So, it scales the building somewhat bigger models but the LSTM is more powerful and more effective since it has three gates instead of two. If you want to pick one to use, I think LSTM has been the historically more proven choice. So, if you had to pick one, I think most people today will still use the LSTM as the default first thing to try. Although, I think in the last few years, GRUs had been gaining a lot of momentum and I feel like more and more teams are also using GRUs because they’re a bit simpler but often work just as well. It might be easier to scale them to even bigger problems. So, that’s it for LSTMs. Well, either GRUs or LSTMs, you’ll be able to build neural network that can capture a much longer range dependancy. 11_bidirectional-rnnBy now, you’ve seen most of the cheap building blocks of RNNs. But, there are just two more ideas that let you build much more powerful models. One is bidirectional RNNs, which lets you at a point in time to take information from both earlier and later in the sequence, so we’ll talk about that in this video. And second, is deep RNNs, which you’ll see in the next video. So let’s start with Bidirectional RNNs. So, to motivate bidirectional RNNs, let’s look at this network which you’ve seen a few times before in the context of named entity recognition. And one of the problems of this network is that, to figure out whether the third word Teddy is a part of the person’s name, it’s not enough to just look at the first part of the sentence. So to tell, if Y three should be zero or one, you need more information than just the first three words because the first three words doesn’t tell you if they’ll talking about Teddy bears or talk about the former US president, Teddy Roosevelt. So this is a unidirectional or forward directional only RNN. And, this comment I just made is true, whether these cells are standard RNN blocks or whether they’re GRU units or whether they’re LSTM blocks. But all of these blocks are in a forward only direction. So what a bidirectional RNN does or BRNN, is fix this issue. So, a bidirectional RNN works as follows. I’m going to use a simplified four inputs or maybe a four word sentence. So we have four inputs. X one through X four. So this networks heading there will have a forward recurrent components. So I’m going to call this, A one, A two, A three and A four, and I’m going to draw a right arrow over that to denote this is the forward recurrent component, and so they’ll be connected as follows. And so, each of these four recurrent units inputs the current X, and then feeds in to help predict Y-hat one, Y-hat two, Y-hat three, and Y-hat four. So, so far I haven’t done anything. Basically, we’ve drawn the RNN from the previous slide, but with the arrows placed in slightly funny positions. But I drew the arrows in this slightly funny positions because what we’re going to do is add a backward recurrent layer. So we’d have A one, left arrow to denote this is a backward connection, and then A two, backwards, A three, backwards and A four, backwards, so the left arrow denotes that it is a backward connection. And so, we’re then going to connect to network up as follows. And this A backward connections will be connected to each other going backward in time. So, notice that this network defines a Acyclic graph. And so, given an input sequence, X one through X four, the fourth sequence will first compute A forward one, then use that to compute A forward two, then A forward three, then A forward four. Whereas, the backward sequence would start by computing A backward four, and then go back and compute A backward three, and then as you are computing network activation, this is not backward this is forward prop. But the forward prop has part of the computation going from left to right and part of computation going from right to left in this diagram. But having computed A backward three, you can then use those activations to compute A backward two, and then A backward one, and then finally having computed all you had in the activations, you can then make your predictions. And so, for example, to make the predictions, your network will have something like Y-hat at time t is an activation function applied to WY with both the forward activation at time t, and the backward activation at time t being fed in to make that prediction at time t. So, if you look at the prediction at time set three for example, then information from X one can flow through here, forward one to forward two, they’re are all stated in the function here, to forward three to Y-hat three. So information from X one, X two, X three are all taken into account with information from X four can flow through a backward four to a backward three to Y three. So this allows the prediction at time three to take as input both information from the past, as well as information from the present which goes into both the forward and the backward things at this step, as well as information from the future. So, in particular, given a phrase like, “He said, Teddy Roosevelt…” To predict whether Teddy is a part of the person’s name, you take into account information from the past and from the future. So this is the bidirectional recurrent neural network and these blocks here can be not just the standard RNN block but they can also be GRU blocks or LSTM blocks. In fact, for a lots of NLP problems, for a lot of text with natural language processing problems, a bidirectional RNN with a LSTM appears to be commonly used. So, we have NLP problem and you have the complete sentence, you try to label things in the sentence, a bidirectional RNN with LSTM blocks both forward and backward would be a pretty views of first thing to try. So, that’s it for the bidirectional RNN and this is a modification they can make to the basic RNN architecture or the GRU or the LSTM, and by making this change you can have a model that uses RNN and or GRU or LSTM and is able to make predictions anywhere even in the middle of a sequence by taking into account information potentially from the entire sequence. The disadvantage of the bidirectional RNN is that you do need the entire sequence of data before you can make predictions anywhere. So, for example, if you’re building a speech recognition system, then the BRNN will let you take into account the entire speech utterance but if you use this straightforward implementation, you need to wait for the person to stop talking to get the entire utterance before you can actually process it and make a speech recognition prediction. So for a real type speech recognition applications, they’re somewhat more complex modules as well rather than just using the standard bidirectional RNN as you’ve seen here. But for a lot of natural language processing applications where you can get the entire sentence all the same time, the standard BRNN algorithm is actually very effective. So, that’s it for BRNNs and next and final video for this week, let’s talk about how to take all of these ideas RNNs, LSTMs and GRUs and the bidirectional versions and construct deep versions of them. 12_deep-rnnsThe different versions of RNNs you’ve seen so far will already work quite well by themselves. But for learning very complex functions sometimes is useful to stack multiple layers of RNNs together to build even deeper versions of these models. In this video, you’ll see how to build these deeper RNNs. Let’s take a look. So you remember for a standard neural network, you will have an input X. And then that’s stacked to some hidden layer and so that might have activations of say, $a^{}$ for the first hidden layer, and then that’s stacked to the next layer with activations $a^{}$, then maybe another layer, activations $a^{}$ and then you make a prediction $ŷ$. So a deep RNN is a bit like this, by taking this network that I just drew by hand and unrolling that in time. So let’s take a look. So here’s the standard RNN that you’ve seen so far. But I’ve changed the notation a little bit which is that, instead of writing this as $a^{}$ for the activation time zero, I’ve added this square bracket 1 to denote that this is for layer one. So the notation we’re going to use is $a^{[l]}$ to denote that it’s an activation associated with layer l and then to denote that that’s associated over time t. So this will have an activation on $a^{[1]}$, this would be $a^{[1]}$, $a^{[1]}$, $a^{[1]}$. And then we can just stack these things on top and so this will be a new network with three hidden layers. So let’s look at an example of how this value is computed. So $a^{[2]}$ has two inputs. It has the input coming from the bottom, and there’s the input coming from the left. So the computer has an activation function g applied to a weight matrix. This is going to be $W_a$ because computing an a quantity, an activation quantity. And for the second layer, and so I’m going to give this $a^{[2]}$, there’s that thing, comma $a^{[1]}$, there’s that thing, plus $b_a$ associated to the second layer. And that’s how you get that activation value. And so the same parameters $W_a^{[2]}$ and $b_a^{[2]}$ are used for every one of these computations at this layer. Whereas, in contrast, the first layer would have its own parameters $W_a^{[1]}$ and $b_a^{[1]}$. So whereas for standard RNNs like the one on the left, you know we’ve seen neural networks that are very, very deep, maybe over 100 layers. For RNNs, having three layers is already quite a lot. Because of the temporal dimension, these networks can already get quite big even if you have just a small handful of layers. And you don’t usually see these stacked up to be like 100 layers. One thing you do see sometimes is that you have recurrent layers that are stacked on top of each other. But then you might take the output here, let’s get rid of this, and then just have a bunch of deep layers that are not connected horizontally but have a deep network here that then finally predicts $y^{}$. And you can have the same deep network here that predicts $y^{}$. So this is a type of network architecture that we’re seeing a little bit more where you have three recurrent units that connected in time, followed by a network, followed by a network after that, as we seen for $y^{}$ and $y^{}$, of course. There’s a deep network, but that does not have the horizontal connections. So that’s one type of architecture we seem to be seeing more of. And quite often, these blocks don’t just have to be standard RNN, the simple RNN model. They can also be GRU blocks LSTM blocks. And finally, you can also build deep versions of the bidirectional RNN. Because deep RNNs are quite computationally expensive to train, there’s often a large temporal extent already, though you just don’t see as many deep recurrent layers. This has, I guess, three deep recurrent layers that are connected in time. You don’t see as many deep recurrent layers as you would see in a number of layers in a deep conventional neural network. So that’s it for deep RNNs. With what you’ve seen this week, ranging from the basic RNN, the basic recurrent unit, to the GRU, to the LSTM, to the bidirectional RNN, to the deep versions of this that you just saw, you now have a very rich toolbox for constructing very powerful models for learning sequence models. I hope you enjoyed this week’s videos. Best of luck with the problem exercises and I look forward to seeing you next week.]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>nlp-sequence-models</tag>
      </tags>
  </entry>
</search>
